\chapter{Generic Chaining}

The goal is to tightly bound $\mathbf{E} \sup_{t \in T} X_t$, where $X_t$ is a centered Gaussian process, where we assume no structure on the set $T$.

\begin{theorem}
    Let $X_t$ be a subgaussian process with a metric $d$ on $T$ such that
    %
    \[ \| X_t - X_s \|_{\psi_2} \lesssim d(t,s) \]
    %
    Then $\mathbf{E} \sup_{t \in T} X_t \lesssim \gamma_2(T,d)$. On the other hand, if $Y_t$ is a Gaussian process with tight induced metric $\| Y_t - Y_s \|_{\psi_2} = d(t,s)$. Then $\mathbf{E} \sup Y_t \gtrsim \gamma_2(T,d)$.
\end{theorem}

If the increments of subgaussian processes induce a metric bounded by the metric of a Gaussian process, the latter has higher expected supremum.

For simplicity, assume $T$ is finite. Then for $t_0 \in T$,
%
\[ \mathbf{E} \max_{t \in T} X_t = \mathbf{E} \sup_{t \in T} X_t - X_{t_0} = \int_0^\infty \mathbf{P}(\max X_t - X_{t_0} > u)\; du \]
%
Applying 1 step chaining, we find
%
\[ \mathbf{P}(\max X_t - X_{t_0} > u) \leq \mathbf{P}(X_{t_1} - X_{t_0} > u/2) + \sum_{t \in S_1} \mathbf{P}(X_t - X_{t_0} > u/2) \]











Interpretation and uses of theorem of conditioning on A|_K. If A is subgaussian, then

E sup_{x in K} | |Ax| - \sqrt{m} |x| | \lesssim w(k) + rad(k).

We also proved that

P ( sup_{x in K} | |Ax| - \sqrt{m} |x| | \geq C(w(k) + urad(k) ) <= e^{-u^2}.

Usually rad(k) << w(k), so if m >= w(k)^2.

INTERPRETATION OF w(k).

The spherical mean width is w_s(K) = E sup (x - y) cdot u, where x,y \in K and U is uniformly distributed on a sphere. One variant of this is the Gaussian mean width W_s(K), which is the same result where u is Gaussian. But these are essentially comparable up to a multiple of sqrt(n). The mean width of a set is equal to the mean width of it's convex hull. The mean width of $K$ is lower bounded by a constant multiple of it's diameter. But this is normally not tight at all.

If K is a cone, then w(K) = \infty. But we can still apply the theorem by restriction attention to K cap S^{n-1} and rescaling. Thus we know

\sup_{x \in K \cap B_2^n} | |Ax|/\sqrt{m} - |x|| \leq c \omega(k \cap B_2^n)/\sqrt{m})

Thus $|Ax|/\sqrt{m}$ is almost an isometry with $\varepsilon = c \sqrt{d(K)/m}$. If we set d(K) = w(K \cap B_2^n)^2 to be the effective dimension of a cone, then we can a near isometry if m >= d(k). 

As another example, if K is the cone of n times n rank r matrices, then d(K) is equal to Esup_M (M,G) <= E sup ||M||_* |G|, where ||cdot||_* is the Nuclear norm.




Lemma: Let k_1, ..., K_l \subset B_2^n. Then w(bigcup K_i) \leq max w(K_i) + c sqrt(log l).

proof: w(bigcup k-i) = E max sup(x cdot g), which is w(K_i) + N(0,1).

As an example, if K is the cone generated by x_1,..., x_l, then w(K \cap B_2^n) \leq C \sqrt{log l}.

If K is the set of all x with |x|_0 <= s, then w(k) <= C \sqrt{s \log(n/s)}. This gives the restricted isometry property with m >= c s log(n/s).


Applications to signal processing:

Consider the model y = Ax, where y is known, and x in K is unknown. This generalizes many problems, including compressed sensing. If we assume K is bounded, then a natural question is to cut K with a random subspace, and look at the diameter of K.

If x^ satisfies Ax^ = y, then x^ - x is in K - K intersected with the nullspace of A. And then $|x| \leq w(K-K)/\sqrt{m}$

LOOK UP LOW M^* estimate to get knowledge of this.

Theorem (Escape through the mesh): If m >= c d(K-K), then w.h.p N(A) cap (K - K) = {0}.