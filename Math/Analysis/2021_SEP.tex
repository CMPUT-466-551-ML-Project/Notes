\documentclass{exam}
\usepackage[utf8]{inputenc}

\usepackage{comment}
\usepackage{amsmath, mathtools}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage[shortlabels]{enumitem}
\usepackage{esint}

\DeclareMathOperator{\CC}{\mathbb{C}}
\DeclareMathOperator{\RR}{\mathbb{R}}
\DeclareMathOperator{\ZZ}{\mathbb{Z}}

% Define a custom "problem" environment
\newtheoremstyle{problemstyle}  % <name>
        {3pt}                                               % <space above>
        {3pt}                                               % <space below>
        {\normalfont}                               % <body font>
        {}                                                  % <indent amount}
        {\bfseries\itshape}                 % <theorem head font>
        {\normalfont\bfseries:}         % <punctuation after theorem head>
        {.5em}                                          % <space after theorem head>
        {}                                                  % <theorem head spec (can be left empty, meaning `normal')>
\theoremstyle{problemstyle}
\newtheorem{problem}{Problem}%[section] % Comment out [section] to remove section number dependence


%Custom Math Commands
\newcommand{\vt}{\vskip 5mm} % vertical space
\newcommand{\fl}{\noindent\textbf} % first line
\newcommand{\Fl}{\vt\noindent\textbf} % first line with space above
\newcommand{\norm}[1]{\left\lVert#1\right\rVert} % norm
\newcommand{\pnorm}[1]{\left\lVert#1\right\rVert_p} % p-norm
\newcommand{\qnorm}[1]{\left\lVert#1\right\rVert_q} % q-norm
\newcommand{\1}[1]{\textbf{1}_{\left[#1\right]}} % indicator function 
\newcommand{\EE}[1]{\mathbb{E}\left[#1\right]} % expectation with bracket
\def\limn{\lim_{n\to\infty}} % shortcut for lim as n-> infinity
\def\sumn{\sum_{n=1}^{\infty}} % shortcut for sum from n=1 to infinity
\def\sumkn{\sum_{k=1}^{n}} % shortcut for sum from k=1 to n
\def\sumin{\sum_{i=1}^{n}} % shortcut for sum from i=1 to n
\def\SAs{\sigma\text{-algebras}} % shortcut for $\sigma$-algebras
\def\SA{\sigma\text{-algebra}} % shortcut for $\sigma$-algebra
\def\Ft{\mathcal{F}_t} % time-indexed sigma-algebra (t)
\def\Fs{\mathcal{F}_s} % time-indexed sigma-algebra (s)
\def\F{\mathcal{F}} % sigma-algebra
\def\G{\mathcal{G}} % sigma-algebra
\def\R{\mathbb{R}} % Real numbers
\def\Z{\mathbb{Z}} % Integers
\def\E{\mathbb{E}} % Expectation
\def\P{\mathbb{P}} % Probability
\def\Q{\mathbb{Q}} % Q probability
\def\dist{\text{dist}} %Text 'dist' for things like 'dist(x,y)'

% Brackets and Parentheses
%\def\[{\left [}
%\def\]{\right ]}
\def\({\left (}
\def\){\right )}

\usepackage{titling}



% Colorful Notes
\usepackage{color}
\definecolor{Red}{rgb}{1,0,0}
\definecolor{Blue}{rgb}{0,0,1}
\def\red{\color{Red}}
\def\blue{\color{Blue}}
\newcommand{\rnote}[1]{\red#1} % \rnote{foo} then 'foo' is red
\newcommand{\bnote}[1]{{\blue#1}} % \bnote{foo} then 'foo' is blue



\pagestyle{headandfoot}
\runningheadrule
\runningheader{Analysis SEP}{}{}
\firstpagefooter{}{}{}
\runningfooter{}{Page \thepage}{}




\title{Analysis SEP Problems \& Solutions}
\date{July-August 2022}
\author{Max Bacharach and Jacob Denson}

\begin{document}

\maketitle

\tableofcontents

\begin{questions}

\newpage

\section*{Day 1: Warm Up Question}

\question (Fall 2016)
  For $n \geq 2$ an integer, define
  %
  \[ F(n)= \max \left\{ k\in \Z: 2^{k}/k\leq n \right\}. \]
  %
  Does the infinite series
  %
  \[ \sum_{n=2}^{\infty}2^{-F(n)} \]
  %
  converge or diverge?
\begin{solution}
	The series diverges.

	One solution follows by applying the intuitive identity
	%
	\[ \sum_{n = 2}^\infty 2^{-F(n)} = \sum_{k = 1}^\infty 2^{-k} \cdot \# \left\{ n \geq 1 : F(n) = k \right\}, \]
	%
	which can also be interpreted as viewing the left hand side as a kind of discrete Lebesgue integral of a non-negative step function. But $F(n) = k$ holds if and only if
	%
	\[ 2^k/k \leq n < 2^{k+1} / (k+1), \]
	%
	and there are at most $2^{k+1} / (k+1) - 2^k / k \sim 2^k / k$ integers between $2^k / k$ and $2^{k+1} / (k+1)$, which implies that
	%
	\begin{align*}
		\sum_{k = 1}^\infty \# \left\{ n \geq 1: F(n) = k \right\} &\geq \sum_{k=1}^{\infty} 2^{-k} \cdot \left( \frac{2^{k+1}}{k+1}-\frac{2^{k}}{k} \right)\\
    &\sim \sum_{k=1}^{\infty} 2^{-k} \cdot (2^k / k)\\
    &=\sum_{k=1}^{\infty} 1/k = \infty.
  \end{align*}
  %
  Thus we have proved the series diverges.

  Another method to prove this result is to use the \emph{Cauchy condensation theorem}, which states that for any non-increasing sequence of non-negative real numbers $\{ a(n) \}$, the series
	%
	\[ \sum_n a(n) \]
	%
	converges if and only if the sum
	%
	\[ \sum_n 2^n a(2^n) \]
	%
	converges. The advantage of the theorem is that in many series, especially those involving \emph{logarithms}, the values $\{ a(2^n) \}$ will be easier to study than the sequence $\{ a(n) \}$. Though $F(n)$ is not explicitly a logarithm, it behaves very similarly to a logarithm, i.e. we notice that there exists $C > 0$ such that
	%
	\[ \log_2(n) \leq F(n) \leq \log_2(n) + \log_2 \log_2(n) + C. \]
	%
	This means that
	%
	\[ \sum_n 2^{-F(n)} = \sum_n 2^n 2^{-F(2^n)} \geq \sum_n 2^n 2^{- n - \log_2 \log_2(n) - C } \gtrsim \sum_n \frac{1}{\log_2(n)}, \]
	%
	and another application of the Cauchy condensation theorem shows that this series diverges.
\end{solution}

\newpage

\section{Day 1: Basic Analysis}

\question (Fall 2017) Let $\{ a_n \}$ be a sequence of complex numbers and let
%
\[ c_n = n^{-5} \sum_{k = 1}^n k^4 a_k. \]
\begin{parts}
	\part Prove or Disprove: If $\lim_{n \to \infty} a_n = a$ exists, then $\lim_{n \to \infty} c_n = c$ exists.
	\begin{solution}
		The result is \emph{true}, so let us prove it. Roughly speaking, the value of $c_n$ is the weighted average of the sequence $\{ a_1, \dots, a_n \}$, heavily weighted in the favor of the later elements of the sequence. Thus it is intuitive that our proof should decompose the sum defining $c_n$ into earlier terms, which must be shown to be neglible, and later terms, which are more well behaved because they are close to the value $a$. Indeed, fix $\varepsilon > 0$. Then there is $k_0$ such that $|a_k - a| < \varepsilon$ for $k > k_0$. But this means that
		%
		\begin{align*}
			c_n &= n^{-5} \sum_{k = 1}^n k^4 a_k\\
			&= n^{-5} \sum_{k = 1}^{k_0} k^4 a_k + \sum_{k = k_0 = 1}^n k^4 a_k\\
			&= O_{k_0}(n^{-5}) + n^{-5} \sum_{k = k_0 + 1}^n k^4 a_k\\
			&= O_{k_0}(n^{-5}) + n^{-5} \sum_{k = k_0 + 1}^n k^4 a + n^{-5} \sum_{k = k_0 + 1}^n k^4 (a_k - a)\\
			&= O_{k_0}(n^{-5}) + a \cdot \left( \frac{1}{n^5} \sum_{k = k_0+1}^n k^4 \right) + O(\varepsilon n^{-5} \sum_{k = k_0+1}^n k^4)\\
			&= O_{k_0}(n^{-5}) + a(1/5 + O_{k_0}(1/n)) + O(\varepsilon).
		\end{align*}
		%
		But this calculation implies that there exists a positive integer $n_0$ and a constant $C > 0$ such that for $n \geq n_0$,
		%
		\[ |c_n - a/5| \leq C \varepsilon. \]
		%
		Since $\varepsilon$ was arbitrary, we have shown that $c_n \to a/5$.

		A key step in the calculation of the limit above was that
		%
		\[ \frac{1}{n^5} \sum_{k = k_0 + 1}^n k^4 = 1/5 + O_{k_0}(1/n). \]
		%
		One can prove this using the technique of \emph{replacing sums with integrals}, which we are often more used to manipulating. Observe that
		%
		\begin{align*}
			\frac{1}{n^5} \sum_{k = k_0}^n k^4 = \frac{1}{n^5} \sum_{k = k_0}^n \int_k^{k+1} k^4\; dx \leq \frac{1}{n^5} \int_{k_0}^{n+1} x^4\; dx = \frac{(n+1)^5 - k_0^5}{5 n^5} = \frac{1}{5} + O_{k_0}(1/n).
		\end{align*}
		%
		A similar argument shows the opposite inequality, namely, that
		%
		\[ \frac{1}{n^5} \sum_{k = k_0}^n k^4 \geq \frac{1}{5} + O_{k_0}(1/n), \]
		%
		which gives this result.

		One can perform a similar calculation using \emph{limsup} and \emph{liminf} arguments, which yields a slightly different method and avoids some of the more quantitative calculations as above. It is intuitive that for large $n$ we should have
		%
		\[ c_n \approx \frac{a}{n^5} \sum_{k = 1}^n k^4. \]
		%
		Indeed, we will show that
		%
		\[ \limsup_{n \to \infty} \left| c_n - \frac{a}{n^5} \sum_{k = 1}^n k^4 \right| = 0. \]
		%
		To do this, we utilize a \emph{$\varepsilon$ of room argument}, i.e. showing that for any $\varepsilon > 0$,
		%
		\[ \limsup_{n \to \infty} \left| c_n - \frac{a}{n^5} \sum_{k = 1}^n k^4 \right| \leq \varepsilon. \]
		%
		One can pick $k_0$ such that for $k \geq k_0$, $|a_k - a| \leq \varepsilon$. Thus
		%
		\begin{align*}
			\limsup_{n \to \infty}  \left| c_n - \frac{a}{n^5} \sum_{k = 1}^n k^4 \right| &= \limsup_{n \to \infty} \left| \frac{1}{n^5} \sum_{k = 1}^n k^4 (a_k - a) \right|\\
			&= \limsup_{n \to \infty} \left| \frac{1}{n^5} \sum_{k = k_0}^n k^4 (a_k - a) \right|\\
			&= \limsup_{n \to \infty} \frac{\varepsilon}{n^5} \sum_{k = k_0}^n k^4\\
			&= \varepsilon \cdot (1/5) \leq \varepsilon.
		\end{align*}
		%
		But now we find that
		%
		\[ \lim_{n \to \infty} c_n = \lim_{n \to \infty} \frac{a}{n^5} \sum_{k = 1}^n k^4 = a \lim_{n \to \infty} (1/5 + O(1/n)) = a/5. \]
  \end{solution}


	\part Prove or Disprove: If $\lim_{n \to \infty} c_n = c$ exists, then $\lim_{n \to \infty} a_n = a$ exists.
	\begin{solution}
		This is false, as we might expect since averages like those defining the sequence $\{ c_n \}$ are often much more well behaved than the original sequence $\{ a_n \}$ upon which they are defined. In particular, if the sequence $\{ a_n \}$ has a lot of \emph{oscillation}, then we might expect cancellation in the average to yield a much smaller sequence $\{ c_n \}$. Thus we are lead to consider the sequence $a_n = (-1)^n$ to obtain a counterexample. The sequence $\{ a_n \}$ clearly does not converge. But we claim that the sequence $\{ c_n \}$ \emph{does} converge. We have
		%
		\[ c_n = \frac{1}{n^5} \sum_{k = 1}^n (-1)^k k^4 = \frac{S_n}{n^5}, \]
		%
		where $S_n = \sum (-1)^k k^4$. We will prove by induction that for any $n \geq 1$, $|S_n| \leq n^4$, and $\text{Sign}(S_n) = (-1)^n$. The case $n = 1$ is obvious. Assuming the result for $n$, we have
		%
		\[ S_{n+1} = S_n + (-1)^{n+1} (n+1)^4. \]
		%
		Now $S_n$ and $(-1)^{n+1}(n+1)^4$ have opposite signs, $|S_n| \leq n^4$, and $(n+1)^4 > n^4$, so we conclude that $S_n + (-1)^{n+1} (n+1)^4$ has sign $(-1)^{n+1}$, and
		%
		\[ |S_{n+1}| \leq (n+1)^4. \]
		%
		But this implies that $c_n = S_n/n^5 = O(1/n)$, so $c_n \to 0$.

		Another counterexample follows if we let $\{ a_n \}$ be a \emph{sparse sequence}, i.e. most elements are zero, in which case the averaged sequence $\{ c_n \}$ will be small. Let
		%
		\[ a_k = \begin{cases} 1 &: k = 2^l\ \text{for some $l = 0,1,2,\dots$,} \\ 0 &: \text{otherwise.} \end{cases} \]
		%
		Then we find that for any $n \geq 1$, and any $m < 2^n$, $c_{2^n + m} < c_{2^n}$, and
		%
		\[ c_{2^n} = \frac{1}{2^{5n}} \sum_{k = 0}^n 2^{4k} = \frac{1}{2^{5n}} \frac{2^{4n} - 1}{2^4 - 1} \leq 2^{-n} \to 0. \]
		%
		Thus $\{ c_n \} \to 0$.
	\end{solution}
\end{parts}

\question (Fall 2018)
  For $c_k\in \R$, say that $\Pi c_k$ converges if $\lim_{K\to\infty} \prod_{k=1}^{K} c_k = C$ exists with $C\neq 0, \infty$.
\begin{parts}
  \part Prove that if $0<a_{k}<1$ for all $k$, or if $-1<a_{k}<0$, for all $k$, then $\prod (1+a_{k})$ converges if and only if $\sum_{k}a_{k}$ converges.
  
  \begin{solution}
  	Let
  	%
  	\[ P_n = \prod_{k = 1}^n (1 + a_k) \quad\text{and}\quad S_n = \sum_{k = 1}^n a_k. \]
  	%
  	Then we have
  	%
  	\[ P_n = \exp \left( \sum_{k = 1}^n \log(1 + a_k) \right) = \exp \left( Q_n \right). \]
  	%
  	Thus $\{ P_n \}$ converges to a nonzero value if and only if $\{ Q_n \}$ converges to a finite value, and it suffices to show that the convergence of $\{ Q_n \}$ is equivalent to the convergence of $\{ S_n \}$. To do this, we apply the \emph{limit comparison test} for series, which states that for any two non-negative sequences $\{ a_n \}$ and $\{ b_n \}$, if $\lim_{n \to \infty} (a_n/b_n) = c$, where $0 < c < \infty$, then either both of the series $\sum a_n$ and $\sum b_n$ converge, or they both diverge. We employ the inequality
  	%
  	\[ \frac{x}{1 + x} \leq \log(1 + x) \leq x, \]
  	%
  	which holds for any $x \geq 1$. Thus
  	%
  	\[ \frac{\log(1 + a_k)}{a_k} \leq \frac{a_k}{a_k} = 1 \]
  	%
  	and
  	%
  	\[ \frac{\log(1 + a_k)}{a_k} \geq \frac{1}{1 + a_k} \geq 1 - a_k. \]
  	%
  	Thus if $\{ a_k \}$ converges to zero, then the convergence of $\{ Q_n \}$ and $\{ S_n \}$ are equivalent. But if $\{ S_n \}$ converges, it is obvious that $\{ a_k \}$ converges to zero, and if $\{ Q_n \}$ converges, then $\log(1 + a_k) \to 0$, which implies $a_k \to 0$. Thus we see that the convergence of $\{ Q_n \}$ and $\{ S_n \}$ are always equivalent.

  	Here is another approach, avoiding the limit comparison test, but at the cost of requiring some more quantitative calculations:

  \textbf{Case 1:} Assume $a_k \in (0,1)$ for all $k$.

  First assume $\lim_{n\to\infty} S_n = L <\infty$. We wish to show that $\lim_{n\to\infty} P_n$ converges. Since $\{ P_n \}$ is an increasing sequence, it suffices to show that $\{ P_n \}$ is bounded above. Since $e^x\geq 1+x$ for all $x \in \RR$, we have
  \begin{equation*}
  P_n \leq \prod_{k=1}^n e^{a_k} = e^{S_n} \leq e^L
  \end{equation*}
  %
  where we have used the fact that $S_n$ is increasing and convergent, so that $S_n \leq L$.
  
  Next, assume that $\lim_{n\to\infty} P_n = M$ where $M \neq 0,\infty$. We wish to show that $\lim_{n\to\infty} S_n$ exists. Since $\{ S_n \}$ is increasing, it again suffices to show that $\{ S_n \}$ is bounded above. This will follow from an argument showing $S_n \leq P_n - 1$, since we then have $S_n \leq M - 1$ for all $n$. We prove this using induction. The case $n=1$ is trivial since $S_1 = P_1 - 1$. For $n>1$, observe that
  \begin{align*}
  P_n 
  &= (1+a_n) P_{n-1}\\
  &\geq (1+a_n)(1+S_{n-1}) &&\text{by induction hypothesis}\\
  &= 1+ S_{n} + a_nS_{n-1}\\
  &\geq 1+S_n
  \end{align*}
  This proves the claim. Alternatively, using the inequality
  %
  \[ \log(1+x)\geq \frac{x}{1+x}, \]
  %
  which holds for all $x>-1$, we conclude that
  \begin{align*}
  P_n = \exp \(\sum_{k=1}^n \log(1+a_k)\) \geq \exp\(\sum_{k=1}^{n} \frac{a_k}{1+a_k} \)\geq \exp\(\frac{1}{2}\sum_{k=1}^n a_k\) = e^{S_n/2},
  \end{align*}
  %
  which implies $\{ S_n \}$ is bounded from above if $\{ P_n \}$ is bounded from above.
  
  \textbf{Case 2:} Assume that $a_k\in (-1,0)$ for all $k$.

  Suppose $\lim_{n\to\infty} P_n = L$, with $L \neq 0$. We wish to show that $S_n$ converges as well. Since $\{ S_n \}$ is now decreasing, it suffices to show that $\{ S_n \}$ is bounded from below. But this follows because $S_n \geq \log P_n$, so that $S_n \geq \log L$. Indeed, using the inequality $\log(1+x)\leq x$, which holds for $x>-1$, we have
  \begin{align*}
  	P_n = \exp \left( \sum_{k=1}^n \log(1+a_k) \right) \leq e^{S_n}.
  \end{align*}
  %
  Taking logarithms on both sides verifies the inequality.
  
  Next, assume that $\{ S_n \}$ converges. We will show that $\{ P_n \}$ converges. It suffices to show that $\{ P_n \}$ is bounded from below, since it is decreasing. But this follows from the inequality
  %
  \[ \log(1 + x) \geq \frac{x}{1 + x}, \]
  %
  which holds for $x > -1$, so that
  %
  \[ P_n = \exp \left( \sum_{k = 1}^n \log(1 + a_k) \right) \geq \exp \left( \sum_{k = 1}^n \frac{a_k}{1 + a_k} \right) \geq \exp \left( \sum_{k = 1}^n a_k (1 - a_k) \right), \]
  %
  The fact that $\{ S_k \}$ tends to zero implies that $\{ a_k \}$ tends to zero, so that there is $c > 0$ such that $1 - a_k \geq c$, and thus
  %
  \[ P_n \geq \exp \left( c \cdot S_n \right). \]
  %
  Thus $\{ P_n \}$ is lower bounded.
  \end{solution}
  
  \part However, prove that $\prod_{k>1} \left( 1+ \frac{(-1)^{k}}{\sqrt{k}} \right)$ diverges.
  
  \begin{solution}
  \noindent\textbf{Proof of Part (b):} Let $P_n = \prod_{k=2}^n \(1+ \frac{(-1)^k}{\sqrt{k}}\)$. Our goal is to show the sequence $\{ P_n \}$ diverges. Though one cannot apply the result above directly to this product, since the terms are not all positive or negative, we can apply the result to a subsequence of the products $\{ P_n \}$ above by noting that \emph{consecutive products} are negative. Thus
  \begin{align*}
  P_{2n-1} 
  &= \prod_{k=2}^{2n+1} \(1+\frac{(-1)^k}{\sqrt{k}}\) \\
  &= \prod_{k=1}^n \(1-\frac{1}{\sqrt{2k+1}}\)\(1+\frac{1}{\sqrt{2k}}\) \\
  &= \prod_{k=1}^n \left( 1 - \frac{1 - (\sqrt{2k+1} - \sqrt{2k})}{\sqrt{2k} \sqrt{2k+1}} \right). \\
  \end{align*}
  %
  Using the fact that
  %
  \[ \sqrt{x} - \sqrt{y} = (\sqrt{x} - \sqrt{y}) \frac{\sqrt{x} + \sqrt{y}}{\sqrt{x} + \sqrt{y}} = \frac{x - y}{\sqrt{x} + \sqrt{y}}, \]
  %
  we find that
  %
  \[ 0 < \frac{1 - (\sqrt{2k + 1} - \sqrt{2k})}{\sqrt{2k} \sqrt{2k+1}} \leq \frac{1 - 1/(2k+1)}{2k} = \frac{1}{2k+1} < 1. \]
  %
  Thus the previous result implies $\{ P_{2n-1} \}$ converges to zero. Since
  %
  \[ \sum_{k = 1}^n \frac{1 - (\sqrt{2k+1} - \sqrt{2k})}{\sqrt{2k} \sqrt{2k+1}} \sim \sum_{k = 1}^n \frac{1}{k} \]
  %
  diverges, it follows that the product diverges.
 \end{solution}
\end{parts}
  
\question (Fall 2019) Let $f$ be a continuous function on $\RR$ satisfying
%
\[ |f(x)| \leq \frac{1}{1 + x^2}. \]
%
Define a function $F$ on $\RR$ by
%
\[ F(x) = \sum_{n = -\infty}^\infty f(x + n). \]
\begin{parts}
	\part Prove that $F$ is continuous and periodic with period 1.
	\begin{solution}
		For each $x \in \RR$, the sum
		%
		\[ \sum_{n = -\infty}^\infty f(x + n) \]
		%
		converges \emph{absolutely}. This justifies that the limit of the infinite series remains the same if we rearrange the series. Thus
		%
		\[ F(x + m) = \sum_{n = -\infty}^\infty f(x +n + m) = \sum_{n = -\infty}^\infty f(x+n) = F(x). \]
		%
		Thus $F$ is periodic. Since the finite sum of continuous functions is continuous, each of the partial sums
		%
		\[ F_K(x) = \sum_{n = -K}^K f(x+n) = \sum_{n = -K}^K f_n(x) \]
		%
		is continuous. Fix $N > 0$ and define $I_N = [-N,N]$. That $F$ is continuous will follow if we can show that $\{ F_N \}$ converges to $F$ uniformly on $I_N$ for each $N$, since the uniform limit of continuous functions is continuous. Thus we find $F$ is continuous on $I_N$ for each $N$, and taking $N \to \infty$ gives that $F$ is continuous everywhere.

		So let us show that $\{ F_K \}$ converges to $F$ uniformly in $L^\infty(I_N)$ for each $N$. If $|n| \geq 2N$, if $x \in I_N$, then
		%
		\[ |x+n| \geq |n| - |x| \geq n/2. \]
		%
		Thus
		%
		\[ |f_n(x)| \leq \frac{1}{1 + (n/2)^2} \leq \frac{1}{1 + n^2/4} \lesssim 1/n^2. \]
		%
		Thus for $K_2 \geq K_1 \geq T \geq 2N$, and $x \in I_N$,
		%
		\[ \| F_{K_2} - F_{K_1} \|_{L^\infty(I_N)} = \| \sum_{K_1 < |n| \leq K_2} f_n \|_{L^\infty(I_N)} \lesssim \sum_{K_1 < n \leq K_2} 1/n^2 \lesssim 1/K_1 \leq 1/T. \]
		%
		Taking $T \to \infty$ shows that the sequence $\{ F_K \}$ is Cauchy in $L^\infty(I_N)$, and thus converges uniformly to $F$.
	\end{solution}

	\part Prove that if $G$ is continuous and periodic with period one, then
	%
	\[ \int_0^1 F(x) G(x) = \int_{-\infty}^\infty f(x) G(x)\; dx. \]
	\begin{solution}
		Since $F$ is the locally uniform limit of the functions $\{ F_N \}$ defined above, we can interchange integrals and limits, writing
		%
		\begin{align*}
			\int_0^1 F(x) G(x)\; dx &= \lim_{N \to \infty} \int_0^1 F_N(x) G(x)\; dx\\
			&= \lim_{N \to \infty} \sum_{n = -N}^N \int_0^1 f_n(x) G(x)\; dx\\
			&= \lim_{N \to \infty} \int_{-N}^{N+1} f(x) G(x)\; dx\\
			&= \int_{-\infty}^\infty f(x) G(x)\; dx,
		\end{align*}
		%
		where the last result follows because $G$ is continous and periodic, and thus bounded, and so
		%
		\[ |f(x) G(x)| \lesssim |f(x)| \lesssim \frac{1}{1 + x^2}. \]
	\end{solution}
\end{parts}

\question (Fall 2015) Let $a_1, a_2, \dots$ be a sequence of positive real numbers and assume that
%
\[ \lim_{n \to \infty} \frac{a_1 + \dots + a_n}{n} = 1. \]
\begin{parts}
	\part Show that $\lim_{n \to \infty} a_n n^{-1} = 0$.
	\begin{solution}
		We can extract the value $a_n/n$ from the sequence $\{ S_n/n \}$, since
		%
		\[ \frac{a_n}{n} = \frac{S_n}{n} - \frac{n-1}{n} \frac{S_{n-1}}{n-1}. \]
		%
		Thus
		%
		\[ \lim_{n \to \infty} \frac{a_n}{n} = \lim_{n \to \infty} \frac{S_n}{n} - \lim_{n \to \infty} \frac{n-1}{n} \frac{S_{n-1}}{n-1} = 1 - 1 = 0. \]
	\end{solution}

	\part If $b_n = \max(a_1,\dots,a_n)$, show that $\lim_{n \to \infty} b_n n^{-1} = 0$.
	\begin{solution}
		We have
		%
		\[ b_n/n = \max(1/n \cdot a_1/1, 2/n \cdot a_2/2, \dots, (n/n) \cdot (a_n/n)). \]
		%
		Fix $\varepsilon > 0$. There exists $N > 0$ such that for $n \geq N$, $|a_n/n| \leq \varepsilon$. Thus if $A = \max(a_1/1, \dots, a_N/N)$, we conclude that for $M \geq N$,
		%
		\[ b_M/M \leq \max(A (N/M), \varepsilon). \]
		%
		Thus if $M \geq AN/\varepsilon$, $b_M/M \leq \varepsilon$. Taking $\varepsilon \to 0$ completes the proof.
	\end{solution}

	\part Show that
	%
	\[ \lim_{n \to \infty} \frac{a_1^\beta + \dots + a_n^\beta}{n^\beta} = \begin{cases} 0 &: \beta > 1 \\ \infty &: \beta < 1. \end{cases}. \]
	\begin{solution}
		For $\beta > 1$, we have
		%
		\begin{align*}
			\frac{a_1^\beta + \dots + a_n^\beta}{n^\beta} &= \frac{a_1 a_1^{\beta - 1} + \dots + a_n a_n^{\beta - 1}}{n^\beta}\\
			&\leq \frac{a_1 + \dots + a_n}{n} \max(a_1/n, \dots, a_n/n)^{\beta - 1}\\
			&\leq (S_n/n) \cdot (b_n/n)^{\beta-1}.
		\end{align*}
		%
		Thus
		%
		\[ \lim_{n \to \infty} \frac{a_1^\beta + \dots + a_n^\beta}{n^\beta} = \lim_{n \to \infty} (S_n / n) \cdot ( \lim_{n \to \infty} b_n/n )^{\beta-1} = 1 \cdot 0 = 0. \]

		To prove the result for $\beta < 1$, let us begin with an intuitive argument. For $\beta < 1$, if $x \ll 1$, then $x^\beta \gg x$. Because $b_n / n \to 0$, for suitably large $n$ we have $a_k/n \ll 1$ for $1 \leq k \leq n$. But this means that $(a_k/n)^\beta \gg a_k/n$, which implies
		%
		\[ \frac{a_1^\beta + \dots + a_n^\beta}{n^\beta} \gg \frac{a_1 + \dots + a_n}{n} \approx 1. \]
		%
		More precisely, if $\delta > 0$, then there exists $n_0$ such that for $n \geq n_0$, and $1 \leq k \leq n$,
		%
		\[ a_k/n \leq \delta \quad \text{and} \quad S_n/n \geq 1/2. \]
		%
		But this implies that
		%
		\[ (a_k/n)^\beta = \frac{(a_k/n)}{(a_k/n)^{1 - \beta}} \geq \frac{(a_k/n)}{\delta^{1-\beta}}. \]
		%
		Thus
		%
		\[ \frac{a_1^\beta + \dots + a_n^\beta}{n^\beta} = (a_1/n)^\beta + \dots + (a_n/n)^\beta \geq \frac{1}{\delta^{1 - \beta}} \frac{a_1 + \dots + a_n}{n} \geq (1/2) \frac{1}{\delta^{1-\beta}}. \]
		%
		Taking $\delta \to 0$ completes the proof.

		Another solution is to use an \emph{interpolation argument}. We have
		%
		\[ \left( \frac{a_1^\beta + \dots + a_n^\beta}{n^\beta} \right) = \left\| \frac{a_1e_1 + \dots + a_ne_n}{n} \right\|_{l^\beta}^\beta. \]
		%
		It thus suffices to show that for $\beta > 1$,
		%
		\[ \limsup_{n \to \infty} \left\| \frac{a_1e_1 + \dots + a_ne_n}{n} \right\|_{l^\beta} = 0, \]
		%
		and for $\beta < 1$,
		%
		\[ \liminf_{n \to \infty} \left\| \frac{a_1e_1 + \dots + a_ne_n}{n} \right\|_{l^\beta} = \infty, \]
		%
		For each $1 \leq \beta \leq \infty$, let
		%
		\[ A_n(\beta) = \left\| \frac{a_1e_1 + \dots + a_ne_n}{n} \right\|_{l^\beta}. \]
		%
		Then
		%
		\[ \limsup_{n \to \infty} A_n(1) = \limsup_{n \to \infty} \left\| \frac{a_1e_1 + \dots + a_ne_n}{n} \right\|_{l^1} = 1 \]
		%
		and
		%
		\[ \limsup_{n \to \infty} A_n(\infty) = \limsup_{n \to \infty} \left\| \frac{a_1e_1 + \dots + a_ne_n}{n} \right\|_{l^\infty} = \max_{1 \leq i \leq n}(a_i/n) = 0. \]
		%
		H\"{o}lder's inequality implies that
		%
		\[ A_n(\beta) \leq A_n(1)^{1/\beta} A_n(\infty)^{1-1/\beta}. \]
		%
		Thus
		%
		\[ \limsup_{n \to \infty} A_n(\beta) \leq (\limsup_n A_n(1))^{1/\beta} (\limsup_n A_n(\infty))^{1 - 1/\beta} = 1^{1/\beta} \cdot 0^{1 - 1/\beta} = 0. \]
		%
		This is the interpolation argument (upper bounding a quantity for two parameters $\beta_0$ and $\beta_1$, and then using those results to upper bound a quantity for a parameter $\beta$ lying in between $\beta_0$ and $\beta_1$). For $0 < \beta < 1$, another application of H\"{o}lder's inequality shows that if
		%
		\[ \lambda = \frac{1/\beta - 1}{1/\beta - 1/2} \]
		%
		then $0 < \lambda < 1$, and
		%
		\[ A_n(1) \leq A_n(\beta)^{1-\lambda} A_n(2)^\lambda. \]
		%
		Thus using the fact that $A_n(2) \to 0$ as $n \to \infty$ (we proved this above), we conclude that
		%
		\[ \liminf_{n \to \infty} A_n(\beta) \geq \frac{\left( \lim_{n \to \infty} A_n(1) \right)^{\frac{1}{1 - \lambda}}}{\left( \lim_{n \to \infty} A_n(2) \right)^{\frac{\lambda}{1 - \lambda}}} = \frac{1}{0} = \infty. \]
	\end{solution}
\end{parts}


\question (Fall 2021) Let $f \in C^1[0,1]$. Show that for every $\varepsilon > 0$, there exists a polynomial $p$ such that
%
\[ \| f - p \|_{L^\infty[0,1]} + \| f' - p' \|_{L^\infty[0,1]} \leq \varepsilon. \]
\begin{solution}
	Since $f' \in C[0,1]$, applying the \emph{Stone-Weirstrass theorem}, for any $\varepsilon > 0$, we can find a polynomial $q$ such that $\| f' - q \|_{L^\infty[0,1]} \leq \varepsilon / 2$. But then if we consider the polynomial $p$ such that $p(0) = f(0)$, and $p' = q$, then the fundamental theorem of calculus shows that for $x \in [0,1]$,
	%
	\[ |f(x) - p(x)| = \left| \int_0^x f'(x) - p'(x) \right| \leq \int_0^x |f'(x) - p'(x)| \leq (\varepsilon / 2) \cdot x \leq (\varepsilon/2). \]
	%
	Thus we have proved that
	%
	\[ \| f - p \|_{L^\infty[0,1]} + \| f' - p' \|_{L^\infty[0,1]} \leq \varepsilon. \]
\end{solution}
   
\newpage
\section*{Day 2: Warm Up Question}

\newpage
\section{Day 2: Basic Analysis}

\question (Spring 2017, Spring 2011, and Spring 2007)
Show that the sequence of functions
  \begin{equation*}
    S_n(x)= \sum_{k=1}^{n}\frac{\sin(kx)}{k}, \quad \quad n=1,2,3,\ldots,
  \end{equation*}
  is uniformly bounded in $\R$.

\bnote{Hint: Summation by parts. Break the sum into two parts for $kx\leq 1$ and $kx>1$ respectively.}
\begin{solution}
The idea is to use \emph{Summation by Parts}, also known as \emph{Abel's lemma}, which says the following: Let $\{ a_k \}$ and $\{ b_k \}$ be two sequences of real numbers, and let
%
\[ T_n = \sum_{k = 1}^n a_k \]
%
be the partial sum of the terms $\{ a_k \}$. Then
%
\[ \sum_{k = m}^n a_k b_k = T_n b_n - T_{m-1} b_m + \sum_{k = m}^{n-1} T_k (b_k - b_{k+1}). \]
%
%we have:
%\begin{equation*}
%  \sum_{k=m}^{n}a_{k}b_{k} = \sum_{k=m}^{n}\left( T_{k}-T_{k-1} \right)b_{k} = \sum_{k=m}^{n}T_{k}b_{k}-\sum_{k=m}^{n}T_{k-1}b_k = T_{n}b_{n} - T_{m-1}b_{m} + \sum_{k=m}^{n-1}T_{k}\left( b_{k}-b_{k+1} \right).
%\end{equation*}
Summation by parts can be useful in many different scenarios, but the most useful scenario occurs when the sequence $\{ a_k \}$ is oscillating, and the sequence $\{ b_k \}$ is `smooth' (i.e. changing gradually). The oscillation of $\{ a_k \}$ causes the sequence $\{ T_k \}$ to be small overall because of cancellation, and the smoothness of $\{ b_k \}$ causes the terms $b_k - b_{k+1}$ to be small.

Let us use summation by parts to prove this result. It suffices to prove a uniform bound for $-\pi/2 < x < \pi/2$, because the absolute value of each of the functions $S_n$ is periodic with degree $\pi$. First, we break the sum into terms will little oscillation, but whose terms are overall relatively small, and terms with large oscillation to which we can apply summation by parts.. Thus for each $x \in (-\pi/2,\pi/2)$, find a non-negative integer such that $N < 1/|x| < N+1$. Write
%
\[ S_n(x) = L_n(x) + H_n(x), \]
%
where
%
\[ L_n(x) = \sum_{k = 1}^N \frac{\sin(kx)}{k} \quad\text{and}\quad H_n(x) = \sum_{k = N+1}^n \frac{\sin(kx)}{k}. \]
%
Utilizing the inequality $|\sin(a)| \leq |a|$ and the triangle inequality, we find that
%
\begin{equation*}
  |L_n(x)| = \left| \sum_{k=1}^{N}\frac{\sin(kx)}{k} \right|\leq \sum_{k=1}^{N}\frac{|kx|}{k}\leq N|x|<1.
\end{equation*}
%
To estimate $H_n$, we apply summation by parts. Let
%
\[ T_k(x) = \sum_{j = 1}^k \sin(jx). \]
%
The triangle inequality gives the trivial estimate $|T_k(x)| \leq k$ for each $k > 0$ and all $x \in \RR$. A more robust estimate follows via the calculation that for $x \in (-\pi/2,\pi/2)$,
%
\begin{equation*}
  |T_k(x)| = \left| \sum_{j=1}^{k}\sin(jx) \right| = \left| \text{Im} \left( \sum_{j=1}^{k}e^{ijx} \right) \right|\leq \left| e^{ix} \frac{ e^{kix} - 1}{e^{ix} - 1}\right| \leq \frac{2}{|\sin x|} \lesssim \frac{1}{|x|}.
\end{equation*}
%
Thus
%
\begin{align*}
  |H_n(x)| &= \left| \sum_{k=N+1}^{n}\frac{\sin(kx)}{k} \right|\\
          &= \left| \frac{T_n(x)}{n}-\frac{T_N(x)}{N+1} + \sum_{k=N+1}^{n-1}T_k(x) \left(\frac{1}{k}-\frac{1}{k+1}  \right) \right|\\
         &\leq 2 + \left| \sum_{k=N+1}^{n-1}\frac{T_k(x)}{k(k+1)} \right|\\
         &\lesssim 1 + \frac{1}{|x|} \sum_{k = N+1}^{n-1} \frac{1}{k^2}\\
         &\lesssim 1 + \frac{1}{N |x|} \lesssim 1.
\end{align*}
%
This shows that both $L_n$ and $H_n$ are uniformly bounded in $n$, and thus the same is true of $S_n$.

An alternate approach is to employ \emph{Fourier analysis} together with a \emph{Tauberian theorem}. Tauberian theorems use tools that  are beyond the scope of the prerequisites of the qualifying exam, though the proof may be of interest for those who know some basic Fourier analysis. The sums here are the partial Fourier series corresponding to the Fourier coefficients of a bounded, $2 \pi$-periodic function $f \in L^\infty[-\pi,\pi]$, i.e. the function $f$ such that
%
\[ f \sim \sum_{k = 1}^\infty \frac{\sin(kx)}{k}. \]
%
It is not necessary to know the function $f$, but it is a constant multiple of the function
%
\[ x \mapsto \begin{cases} -\pi/2 - x/2 &: -\pi < x < 0 \\ +\pi/2 - x/2 &: 0 < x < \pi \end{cases} \]
%
It follows that $S_n = D_n * f$, where $D_n$ is the \emph{Dirichlet kernel}. If $\| D_n \|_{L^1([0,2\pi])}$ was uniformly bounded in $n$, we could apply Young's inequality, implying
%
\[ \| S_n \|_{L^\infty([0,2\pi])} \leq \| D_n \|_{L^1([0,2\pi])} \| f \|_{L^\infty[0,2\pi])} \lesssim 1. \]
%
Unfortunately, we have $\| D_n \|_{L^1([0,2\pi])} \sim \log n$, so this approach doesn't work completely. But there is a general result for functions $f \in L^1([0,2\pi])$ such that
%
\[ |\widehat{f}(n)| \lesssim 1/n, \]
%
known as a \emph{Tauberian theorem}, which shows that $D_n * f$ is uniformly bounded in $n$ if and only if $P_r * f$ is bounded as $r \to 1$, where $\{ P_r \}$ is the \emph{Poisson kernel}. Since $\| P_r \|_{L^1([0,2\pi])} \lesssim 1$ for $0 < r < 1$, we can use Young's inequality to conclude that $\| P_r * f \|_{L^\infty((0,2\pi))} \lesssim 1$. Thus $P_r * f$ is uniformly bounded in $r$, and thus $D_n * f$ is uniformly bounded in $n$, completing the proof.

\end{solution}

\question (Spring 2021)
%
\[ \sum_{k = 1}^\infty \frac{\cos(\sqrt{k})}{k} \]
%
converges.
\begin{solution}
    Since the sum of $1/k$ diverges, this convergence could only possibly happen from the oscillation present in the $\cos(\sqrt{k})$ causes enough cancellation. Thus trying to control the oscillation is key to understanding this series. In particular, let us try and understand how often $\cos(\sqrt{k})$ changes sign. We can use conjugation, an often useful trick to understand square roots, to write
    %
    \begin{align*}
        \sqrt{k+n} - \sqrt{k} &= (\sqrt{k+n} - \sqrt{k}) \left( \frac{\sqrt{k+n} + \sqrt{k}}{\sqrt{k+n} + \sqrt{k}} \right)\\
        &= \frac{n}{\sqrt{k+n} + \sqrt{k}}\\
        &= \frac{n}{2\sqrt{k} + O(n/\sqrt{k})}\\
        &= \frac{n}{2\sqrt{k}} + O \left( \frac{n^2}{k^{3/2}} \right).
    \end{align*}
    %
    In particular,
    %
    \[ \sqrt{k + 2 \pi \sqrt{k}} + \sqrt{k} = \pi + O(1/\sqrt{k}). \]
    %
    Thus for each $n$, the sign of $\cos(\sqrt{k})$ stays roughly the same for $k = n \pm \sqrt{n}$. More precisely, let $\{ m_1, m_2, \dots \}$ be an increasing family of integers with $m_1 = 1$, and such that for each $i$, and $k \in \{ m_i, m_i + 1, \dots, m_{i+1} - 1 \}$ , $\cos(\sqrt{k})$ has the same sign. Then $m_{i+1} - m_i \sim \sqrt{m_i}$. For more than half of the values in $\{ m_i, \dots, m_{i+1} - 1 \}$, $|\cos(\sqrt{k})| \geq 1/2$, and it follows that
    %
    \begin{align*}
        \left| \sum_{k = m_i}^{m_{i+1} - 1} \frac{\cos(\sqrt{k})}{k} \right| \sim \frac{1}{\sqrt{m_i}}.
    \end{align*}
    %
    Roughly speaking, an application of Leibnitz's alternating series test then implies that this series converges.
\end{solution}


\question (Fall 2015) Consider the series
%
$$ \sum_{n = 1}^\infty \frac{1}{\sqrt{n}} \sin(x/n). $$
%
\begin{parts}
	\part Show that the series converges pointwise to some function $f$ on $\RR$.
	\begin{solution}
		The main idea to understanding the sum is to use the bound $|\sin(y)| \leq |y|$, which is tight when $|y| \leq 1$. Thus little is lost in our estimates if we apply this bound in the sum above for $|n| \geq |x|$, since then $|x/n| \leq 1$. We will actually find this sum converges absolutely, locally uniformly in $x$. Indeed, plugging in the bound $|\sin(x)| \leq |x|$, we find that
		%
		\[ \sum_{n = 1}^\infty \left| \frac{1}{\sqrt{n}} \sin(x/n) \right| \leq \sum_{n = 1}^\infty \frac{1}{\sqrt{n}} \frac{|x|}{n} = |x| \sum_{n = 1}^\infty \frac{1}{n^{3/2}} \lesssim |x|. \]
		%
		Since the sum converges pointwise absolutely, it must converge pointwise.
	\end{solution}

	\part Is $f$ continuous on $\RR$? Does $f'(x)$ exist for all $x \in \RR$?
	\begin{solution}
		To see that $f$ is continuous, it suffices to show that the partial sums
		%
		\[ f_N(x) = \sum_{n = 1}^N \frac{1}{\sqrt{n}} \sin(x/n) \]
		%
		converge \emph{locally uniformly} in $x$ to $f$, since the limit of a sequence of continuous functions converging locally uniformly is continuous. But employing the bound $|\sin(x)| \leq |x|$ as above, we find that for $M > N$,
		%
		\[ |f_M(x) - f_N(x)| \leq \sum_{n = N+1}^M \left| \frac{1}{\sqrt{n}} \sin(x/n) \right| \leq |x| \sum_{n = N+1}^M 1/n^{3/2} \lesssim |x| / N^{1/2}. \]
		%
		But this equation implies locally uniform convergence, i.e. for any interval $I = [-K,K]$, we have proved that
		%
		\[ \| f_M - f_N \|_{L^\infty(I)} \lesssim K/N^{1/2}. \]
		%
		Thus $f$ is a continuous function.

		Now to understand the differentiability of $f$, it makes sense to look at the derivatives
		%
		\[ f_N'(x) = - \sum_{n = 1}^N (1/n^{3/2}) \cos(x/n). \]
		%
		Since the cosine term is uniformly bounded in $x$ and $n$, it is fairly negligible to studying the limit of this quantity as $N \to \infty$. Indeed, we find that for $M > N$, we can plug in the bound $|\cos(y)| \leq 1$ to conclude that
		%
		\[ |f_M'(x) - f_N'(x)| \leq \sum_{n = N+1}^M 1/n^{3/2} \lesssim N^{-1/2}. \]
		%
		Thus $\| f_M - f_N \|_{L^\infty(\RR)} \lesssim N^{-1/2}$, which implies the sequence is uniformly Cauchy, and thus converges to some function $g$. But it is a general rule that if $\{ f_N \}$ converges locally uniformly to some function $f$, and $\{ f_N' \}$ converges locally uniformly to some function $g$, then $f$ is differentiable, and $f' = g$. Thus $f$ is differentiable.

		An alternative, but less clean way to show $f$ is differentiable is using the definition of the derivative, i.e. by proving that for each $x \in \RR$, the limit
		%
		\[ \lim_{h \to 0} \frac{f(x + h) - f(x)}{h} = \lim_{h \to 0} \sum_{n = 1}^\infty \frac{1}{\sqrt{n}} \frac{\sin((x + h)/n) - \sin(x/n)}{h}. \]
		%
		exists. One way to do this is to write
		%
		\[ \lim_{h \to 0} \frac{f(x + h) - f(x)}{h} = \lim_{h \to 0} \frac{f_N(x + h) - f_N(x)}{h} + \lim_{h \to 0} \sum_{n = N+1}^\infty \frac{1}{\sqrt{n}} \frac{\sin((x + h)/n) - \sin(x/n)}{h}. \]
		%
		Since $f_N$ is differentiable, the first limit is easily shown to exist. One might understand the second sum by using the mean value theorem to conclude that $|\sin((x + h)/n) - \sin(x/n)| \lesssim |h| / n$, thus obtaining that
		%
		\[ \limsup_{h \to 0} \left| \sum_{n = N+1}^\infty \frac{1}{\sqrt{n}} \frac{\sin((x + h)/n) - \sin(x/n)}{h} \right| \lesssim 1/N^{1/2}. \]
		%
		This means that
		%
		\[ \limsup_{h \to 0} \frac{f(x + h) - f(x)}{h} - \liminf_{h \to 0} \frac{f(x + h) - f(x)}{h} \lesssim 1/N^{1/2}. \]
		%
		Taking $N \to \infty$ shows that $\liminf = \limsup$, so that the limit exists, and thus $f$ is differentiable.
	\end{solution}

	\part Does the series converge uniformly on $\RR$?
	\begin{solution}
		The bounds in part (a) of the problem, which depend on $x$, should hint to you that the series does \emph{not} converge uniformly in $x$, in particular, as we take $x$ to be very large. To show that the sum does not converge uniformly, we need lower bounds, and to do this we again employ the heuristic that $|\sin(y)| \leq |y|$ is a good estimate for $|y| \leq 1$, so we should get a similar estimate which lower bounds $\sin(y)$ in this range. In fact, for $0 \leq y \leq 1$ we actually have $\sin(y) \geq y/2$. Concentrating on the case where $x$ is a positive integer for simplicity, we find that
		%
		\[ \sum_{n \geq x} \frac{1}{\sqrt{n}} \sin(x/n) \gtrsim \sum_{n \geq x} \frac{x}{n^{3/2}} \gtrsim \sqrt{x}. \]
		%
		Thus if $N = x$, we can reword this inequality as saying that
		%
		\[ \lim_{M \to \infty} f_M(x) - f_N(x) \gtrsim \sqrt{x}. \]
		%
		Thus
		%
		\[ \limsup_{M \to \infty} \| f_M - f_N \|_{L^\infty(\RR)} \gtrsim \sqrt{N}. \]
		%
		In particular, this implies $\{ f_N \}$ is not a Cauchy sequence in $L^\infty(\RR)$, and thus does not converge uniformly.

		Alternatively, we can use the fact that if $\pi/3 \leq y \leq 2\pi/3$, then $\sin(y) \geq 1/2$. Thus, for any $N$, if we pick $x = (\pi/3)N$, then for $1 \leq n \leq N$, $\sin(x/n)$ is non-negative, and for $N/2 \leq n \leq N$, $\sin(x/n) \geq 1/2$, so
		%
		\[ |f_N(x)| \gtrsim \sum_{N/2 \leq n \leq N} 1/\sqrt{n} \gtrsim \sqrt{N}. \]
		%
		Thus $\| f_N \|_{L^\infty(\RR)} \gtrsim \sqrt{N}$, which implies $\{ f_N \}$ is not bounded in $L^\infty(\RR)$, and so $\{ f_N \}$ is not a Cauchy suquence in $L^\infty(\RR)$.
	\end{solution}
\end{parts}

\question (Fall 2019) Show that if $K \subset \RR^n$, and every continuous function on $K$ is bounded, then $K$ is compact.
\begin{solution}
	The \emph{Heine-Borel theorem} implies that it suffices to show that $K$ is bounded and closed.

	If $K$ was not closed, then we could find $y \in \overline{K} - K$. The function $f(x) = 1/|x-y|$ would then be continuous, but not bounded, on $K$. Thus $\overline{K} - K$ is empty, so $\overline{K} = K$.

	If $K$ was not bounded, then $f(x) = |x|$ would be a continuous, unbounded function. Thus $K$ is bounded.

	Thus $K$ is compact.
\end{solution}

\question (Spring 2015) Prove that the integral
%
\[ f(a) = \int_0^\infty \frac{\sin(x^2 + ax)}{x}\; dx \]
%
converges for $a \geq 0$, and $f$ is continuous on $[0,\infty)$.
\begin{solution}
	Let
	%
	\[ f_0(a) = \int_0^1 \frac{\sin(x^2 + ax)}{x}\; dx. \]
	%
	We perform a \emph{dyadic decomposition}, writing, for $n \in \ZZ$,
	%
	\[ f_n(a) = \int_{2^n}^{2^{n+1}} \frac{\sin(x^2 + ax)}{x}\; dx. \]
	%
	Then $f = \sum_{n = 0}^\infty f_n$. Since the integrand is continuous, and each integral is on a finite interval, all of the functions $\{ f_n \}$ are continuous.

	We claim this sum converges locally uniformly. To do this we \emph{must} exploit the oscillation in the integrand, for $1/x$ is not summable. To do this, we perform an \emph{integration by parts}. Indeed, for $a \geq 0$,
	%
	\begin{align*}
		\int_{2^n}^{2^{n+1}} \frac{\sin(x^2 + ax)}{x} &= \int_{2^n}^{2^{n+1}} \frac{d}{dx} \left( \cos(x^2 + ax) \right) \cdot \frac{-1}{x(2x + a)}\\
		&= \left( \int_{2^n}^{2^{n+1}} \frac{\cos(x^2 + ax)}{x(2x + a)} \right) + O(1/2^n)\\
		&\leq \int_{2^n}^{2^{n+1}} \frac{1}{x(2x + a)} + O(1/2^n)\\
		&\lesssim 1/2^n.
	\end{align*}
	%
	Thus $\| f_n \|_{L^\infty[0,\infty)} \lesssim 1/2^n$. Thus $\{ f_n \}$ is uniformly summable, and so $f = \sum f_n$ exists, and is continuous for $a \geq 0$.
\end{solution}

\question (Fall 2017) Consider the sequence of functions $f_n: \RR \to \RR$ defined by
%
\[ f_n(x) = \int_0^n \frac{\sin(sx)}{\sqrt{s}}\; ds. \]
%
\begin{parts}
	\part Show that $\{ f_n \}$ converges locally uniformly on $(0,\infty)$.
	\begin{solution}
		This integral can only converge because of the oscillation of $\sin(sx)$ for large $x$. Thus we \emph{integrate by parts}, writing
		%
		\begin{align*}
			\int_{n_0}^{n_1} \frac{\sin(sx)}{\sqrt{s}}\; ds &= \int_{n_0}^{n_1} \frac{- d/ds (\cos(sx))}{x \sqrt{s}}\; ds\\
			&= \left. \frac{- \cos(sx)}{x s^{1/2}} \right|^{n_1}_{n_0} + \int_{n_0}^{n_1} \frac{\cos(sx)}{x s^{3/2}} (-1/2)\; ds\\
			&= O(1/xn_0^{1/2}) + O(1/x \int_{n_0}^{n_1} 1/s^{3/2}\; ds)\\
			&= O(1/xn_0^{1/2}).
		\end{align*}
		%
		Thus for any fixed $\alpha > 0$, we get uniform convergence for $\alpha < x < \infty$.
	\end{solution}

    
	\part Show that $\{ f_n \}$ does \emph{not} converge uniformly on $(0,1]$.
	\begin{solution}
		If the convergence was uniform, there would be $n_0$ such that for any $n_1 \geq n_0$, and $x \in [0,1)$,
		%
		\[ \left| \int_{n_0}^{n_1} \frac{\sin(sx)}{\sqrt{s}}\; ds \right| \leq 1. \]
		%
		Now pick $x \in [0,1)$ with $x \sim 1/n_1$, such that for $s \in [n_0,n_1]$, $\sin(sx) \geq sx/2$. Then
		%
		\[ 1 \geq \int_{n_0}^{n_1} \frac{\sin(sx)}{\sqrt{s}}\; ds \geq \frac{x}{2} \int_{n_0}^{n_1} \sqrt{s}\; ds \gtrsim n_1^{3/2} x \gtrsim n_1^{1/2}. \]
		%
		This gives a contradiction for sufficiently large $n_1$.
	\end{solution}

	\part Does the sequence $\{ f_n \}$ converge uniformly on $[1,\infty)$ as $n \to \infty$?
	\begin{solution}
		Yes, as implied by the calculations in part (a).
	\end{solution}
\end{parts}

\question (Fall 2021) Does the improper integral
%
\[ \int_2^\infty \frac{x \sin(e^x)}{x + \sin(e^x)}\; dx \]
%
converge?
\begin{solution}
	Here's one solution. Our first intuition is to break the integrand into an oscillatory part, and a fairly well behaved part. A good option seems to be breaking the sum into the product
	%
	\[ \frac{x}{x + \sin(e^x)} \cdot \sin(e^x) = A(x) \cdot B(x). \]
	%
	For large $x$, the well behaved part of the integrand is close to one. So let's do a Taylor expansion. A first order expansion yields $A(x) = 1 + O(1/x)$, but the error term here is not good enough (it's not integrable in $x$). Thus we perform a second order expansion, i.e. writing $A(x) = 1 - \sin(e^x) / x + O(1/x^2)$. Now the error term is integrable, so we conclude that the convergence of the improper integral above is equivalent to the convergence of the integral
	%
	\[ \int_2^\infty \left( 1 - \frac{\sin(e^x)}{x} \right) \sin(e^x)\; dx. \]
	%
	A change of variables might help us out here. We write $y = e^x$. Then $dy = y dx$, and so this integral is equal to
	%
	\[ \int_c^\infty \frac{\sin(y)}{y} \left( 1 - \frac{\sin(y)}{\ln y} \right). \]
	%
	A simple integration by parts (antidifferentiate the oscillating term $\sin(y)$, differentiate the term going to zero $1/y$) yields
	%
	\[ \int_c^\infty \frac{\sin(y)}{y} = C + \int_{O(1)}^\infty \frac{\cos(y)}{y^2}. \]
	%
	The integral on the right hand side is summable, and thus the integral converges. Thus the convergence of the overall integral is equivalent to
	%
	\[ \int_c^\infty \frac{\sin(y)^2}{y \ln(y)}. \]
	%
	This integral is positive, and \emph{not integrable}, i.e. a dyadic decomposition (a tool for breaking a sum into parts on powers of two, which helps us out in an analogous to the Cauchy condensation theorem) shows that
	%
	\[ \int_c^\infty \frac{\sin(y)^2}{y \ln(y)} \gtrsim \sum_{k = 1}^\infty \int_{2^k}^{2^{k+1}} \frac{\sin(y)^2}{y \ln(y)} \gtrsim \sum_{k = 1}^\infty 2^k \frac{1}{k 2^k} = \sum_{k = 1}^\infty \frac{1}{k} = \infty. \]
	%
	Thus we conclude that the integral diverges, and thus the same must be true for the overall integral.

	Here is an alternate solution. If $y = e^x$, then $dy = e^x dx = y dx$, and so
	%
	\[ \int_2^\infty \frac{x \sin(e^x)}{x + \sin(e^x)}\; dx = \int_{\log 2}^\infty \frac{\sin y}{y} \frac{\log(y)}{\log(y) + \sin(y)} \]
	%
	The integrand oscillates between positive and negative values, so it suffices to decompose the integral onto the intervals where the integrand is positive or negative. For suitably large $n$, write
	%
	\[ C_n = \left| \int_{\pi n}^{\pi (n + 1)} \frac{\sin y}{y} \frac{\log y}{\log y + (-1)^n \sin y} \right| \]
	%
	The integral will then converge if and only if the alternating series
	%
	\[ \sum_{n = 1}^\infty (-1)^n C_n \]
	%
	converges. For $\pi n \leq y \leq \pi(n+1)$, a Taylor series expansion shows that
	%
	\begin{align*}
		\frac{1}{y} \frac{\log y}{\log y + (-1)^n \sin y} &= \frac{1}{y} \frac{1}{1 + (-1)^n \sin y / \log y}\\
		&= \left( \frac{1}{\pi n} + O(1/n^2) \right) \left(1 - (-1)^n \frac{\sin y}{\log y} + O \left( \frac{1}{(\log n)^2} \right) \right)\\
		&= \left( \frac{1}{\pi n} + O(1/n^2) \right) \left(1 - (-1)^n \frac{\sin y}{\log(\pi n)} + O \left( \frac{1}{(\log n)^2} \right) \right)\\
		&= \left( \frac{1}{\pi n} - (-1)^n \frac{\sin y}{\pi n \log \pi n} + O \left( \frac{1}{n (\log n)^2} \right) \right).
	\end{align*}
	%
	Thus
	%
	\begin{align*}
		C_n &= \left| \int_0^\pi \frac{\sin y}{\pi n} - (-1)^n \frac{(\sin y)^2}{\pi n \log \pi n}\; dy \right| + O \left( \frac{1}{n (\log n)^2} \right)\\
		&= \left| \frac{2}{\pi n} - (-1)^n \frac{1}{2 n \log(\pi n)} \right| + O \left( \frac{1}{n (\log n)^2} \right)\\
		&= C_n' + O \left( \frac{1}{n (\log n)^2} \right).
	\end{align*}
	%
	For large $n$, we have
	%
	\[ C_n' = \frac{2}{\pi n} - (-1)^n \frac{1}{2 n \log(\pi n)}. \]
	%
	Thus
	%
	\[ \sum (-1)^n C_n' = \sum (-1)^n (2 / \pi n) - \frac{1}{2 n \log(\pi n)}. \]
	%
	The sum $\sum (-1)^n (2 / \pi n)$ converges by the \emph{Leibnitz test}. But the other part of the series diverges, i.e.
	%
	\[ \sum_n \frac{1}{2 n \log(\pi n)} = \infty. \]
	%
	One can prove this using the Cauchy condensation theorem. Thus the alternating series \emph{diverges}, and so the entire integral diverges.
\end{solution}

\newpage
\section*{Day 3: Warm Up Question}

\question (Fall 2018) 
Prove that for $1\leq p \leq 2$ and $0<b<a$, $$(a+b)^p + (a-b)^p \geq 2a^p + p(p-1)a^{p-2}b^2.$$

\begin{solution}
	\bnote{Hint: Apply Taylors Theorem.}

  By subtracting the right-hand side, factoring out $a^p$, and noting that $0<b/a<1$, it suffices to show that the function
  \begin{equation*}
    f(x) = \left( 1+x \right)^{p} + \left( 1-x \right)^{p} - 2 - p(p-1)x ^{2}
  \end{equation*}
  is nonnegative for all $x\in (0,1)$. Let $g(x) = (1+x)^p$, so that
  \begin{equation*}
    f(x) = g(x)+ g(-x) -2 - p(p-1)x^{2}.
  \end{equation*}
  We next Taylor expand $g$ about the point $0$.
  For any $x>-1$, we have
  \begin{equation*}
    g(x) = 1 + px + \frac{1}{2}p(p-1)x^{2} + R_{2}(x)
  \end{equation*}
  where
  \begin{equation*}
    R_{2}(x)= \frac{p(p-1)(p-2)(1+\xi)^{p-3}x^{3}}{6}
  \end{equation*}
  for some number $\xi$ between $0$ and $x$. Substituting the Taylor expansions for $g(x)$ and $g(-x)$ into the formula for $f$ gives
  \begin{equation*}
    f(x) = R_{2}(x) + R_{2}(-x)
  \end{equation*}
  or equivalently
  \begin{equation*}
    f(x) = \frac{p(p-1)(p-2)}{6} \left[(1+\xi_{+})^{p-3}-(1+\xi_{-})^{p-3}\right]x^{3}
  \end{equation*}
  for some $\xi_{+}\in(0,x)$ and $\xi_{-}\in(-x,0)$. Since $\xi_{-}<\xi_{+}$ and $p<3$, the part in brackets is negative, and since $0\leq p \leq 2$, it follows that $f(x)$ is nonnegative, as required.
\end{solution}

\question (Spring 2018 and Spring 2021)
  Determine if
  \begin{equation*}
    \sum_{k=1}^{\infty}\frac{\cos(\sqrt{k})}{k}
  \end{equation*}
  converges.

\begin{solution}
\bnote{Hint: Integration by parts, Mean value theorem}

Yes, the series converges. We first claim that the integral
%
\[ \int_{1}^{\infty}\frac{\cos(x^{1/2})}{x}dx \]
%
converges. To see this, we use integration by parts:
  \begin{align*}
    \int_{1}^{\infty}\frac{\cos(x^{1/2})}{x}dx
    &= \int_{1}^{\infty}2x^{-1/2}\frac{d}{dx}\left[ \sin(x^{1/2}) \right]dx\\
    &= -2\sin(1)+ \int_{1}^{\infty}\frac{\sin(x^{1/2})}{x^{3/2}}dx<\infty.
  \end{align*}
  This proves the claim. Next we claim that
  %
  \[ \sum_{k=1}^{n} \frac{\cos(\sqrt{k})}{k} \]
  %
  is a Cauchy sequence (and therefore converges as $n\to\infty$). To prove this, it suffices by the previous claim to show that for every $\epsilon>0$ there exists $N$ such that $m,n>N$ implies 
  \begin{equation}\label{partial-sum-difference}
    \left|\sum_{k=m}^{n}\frac{\cos(\sqrt{k})}{k} - \int_{m}^{n+1}\frac{\cos(\sqrt{x})}{x}dx\right|<\epsilon.
  \end{equation}
  Let $1\leq n \leq m$. Then
  \begin{equation}\label{eq:series-integral}
    \sum_{k=m}^{n}\frac{\cos(\sqrt{k})}{k} - \int_{m}^{n+1}\frac{\cos(\sqrt{x})}{x}dx
    = \sum_{k=m}^{n}\int_{k}^{k+1}\(\frac{\cos(\sqrt{k})}{k} - \frac{\cos(\sqrt{x})}{x}\)dx.
  \end{equation}
  By the Mean Value Theorem, since
  %
  \[ \frac{d}{dx}\left[ \frac{\cos(x^{1/2})}{x} \right]= -\frac{\cos(x^{1/2})}{x^{2}} - \frac{\sin(x^{1/2})}{2x^{3/2}}, \]
  %
  it follows that for every $x\in [k,k+1]$, there exists some $\xi\in (x,k+1)$ such that
  \begin{equation*}
    \frac{\cos(\sqrt{k})}{k} - \frac{\cos(\sqrt{x})}{x} \leq \frac{1}{\xi^{2}}+ \frac{1}{2\xi^{3/2}}
  \end{equation*}
  and since $\xi \geq k$,
  \begin{equation*}
     \frac{\cos(\sqrt{k})}{k} - \frac{\cos(\sqrt{x})}{x} \leq\frac{1}{k^{2}} + \frac{1}{2k^{3/2}}.
   \end{equation*}
   Therefore
  \begin{equation*}
    \left|\int_{k}^{k+1}\(\frac{\cos(\sqrt{k})}{k} - \frac{\cos(\sqrt{x})}{x}\)dx\right| \leq  \frac{1}{k^{2}} + \frac{1}{2k^{3/2}}
  \end{equation*}
  Therefore using equation \eqref{eq:series-integral},
  \begin{equation*}
    \left| \sum_{k=m}^{n}\frac{\cos(\sqrt{k})}{k} - \int_{m}^{n+1}\frac{\cos(\sqrt{x})}{x}dx \right| \leq \sum_{k=m}^{n}\frac{1}{k^{2}} + \frac{1}{2k^{3/2}}
  \end{equation*}
  Since the series on the right hand side is summable, it tends to zero as $m,n\to\infty$, which establishes \eqref{partial-sum-difference}.
\end{solution}




\newpage
\section{Day 3: Basic Analysis}

\question (Spring 2015) Let $g$ be a non-constant differentiable real function on a finite interval $[a,b]$, with $g(a) = g(b) = 0$. Show that there exists $c \in (a,b)$ such that
%
\[ |g'(c)| > \frac{4}{(b - a)^2} \int_a^b |g(t)|\; dt. \]
\begin{solution}
	My main intuition is that the fundamental theorem of calculus enables us to relate $g'$ and $g$. If we are clever about using the fundamental theorem, we obtain the bound, namely
	%
	\begin{align*}
		\int_a^b |g(t)|\; dt &= \int_a^b \min \left( \left| \int_a^t g'(s)\; ds \right|, \left| \int_t^b g'(s) \right| \right)\; dt\\
		&\leq \max_{a < c < b} |g'(c)| \cdot \int_a^b \min(|t - a|, |t - b|)\; dt\\
		&= \max_{a < c < b} |g'(c)| \cdot \left( \int_a^{(a + b)/2} (t - a)\; dt + \int_{(a + b)/2}^b (b - t)\; dt \right)\\
		&= \max_{a < c < b} |g'(c)| \cdot \left( (b - a)^2/8 + (b - a)^2/8 \right)\\
		&= \max_{a < c < b} |g'(c)| \cdot (b-a)^2/4.
	\end{align*}
	%
	Rearranging this inequality completes the proof, except in the case that all the inequalities in this calculation are equalities (so we cannot get the strict inequality required). But this only holds if $g'$ is a constant function, hence $g$ is linear, which is impossible since $g$ is non constant, and $g(0) = g(1) = 0$.

	(Thanks to Karthik for the geometric intuition behind this proof) It suffices to prove the result when $g \in C_c^\infty((0,1))$, since the general result 

	Here is another, geometric proof, using the calculus of variations. Let $X$ denote the space of all continuous functions $g$ which are differentiable except at finitely many points, have $g(a) = g(b) = 0$, and such that whenever the function is differentiable, the magnitude of the derivative is less than one. It suffices to prove that for any $g \in X$,
	%
	\[ \frac{4}{(b-a)^2} \int_a^b |g(t)|\; dt < 1, \]
	%
	except if $g$ is the `triangle function', with $g(t) = t$ for $a \leq t \leq (a + b)/2$ and $g(t) = (a + b)/2 - t$ for $(a + b)/2 \leq t \leq b$. This will prove the theorem since the triangle function does not satisfy the constraints of the actual problem we are trying to solve (it is not differentiable everywhere).

	Clearly, we can assume that $g \geq 0$, since $X$ is invariant under taking absolute values. We claim that we have $|g'(x)| = 1$ whever $x$ is differentiable. By splitting $[a,b]$ into finite intervals, we may assume that $g$ is differentiable on the entirety of some particular interval $I = [x_0,x_1]$. Then the fundamental theorem of calculus kicks into play, and we have that for $t \in I$,
	%
	\[ g(t) = \int_{x_0}^t g'(t)\; dt. \]

	Indeed, if $|g'(x_0)| < 1$, then $g$ is differentiable on an interval containing $x_0$, then consider the function $j$, which is a non-negative function vanishing everywhere on $I$, with $j'(x) = 1 - |g'(x)|$ on $I$. Then $j$ is differentiable except at finitely many points, and $j \geq 0$. We have $g + j \in X$, and 

	on a dense set of $x$. Indeed, if $|g'(x)| < 1$ in a small interval $I$, then the function $g + j$ lies in $X$, where $j$ is a non-negative function vanishing everywhere on $I$, where 
\end{solution}

\question (Fall 2019) If $f: \RR^n \to \RR$ is differentiable on $\RR^n - \{ 0 \}$, continuous at 0, and
%
\[ \lim_{x \to 0} \frac{\partial f}{\partial x^i}(x) = 0, \]
%
for $1 \leq i \leq n$, then $f$ is differentiable at 0.
\begin{solution}
	We claim that $\nabla f(0) = 0$, i.e. that $|f(x) - f(0)| = o(|x|)$ as $x \to 0$. To see this, we apply the mean value theorem. Assume without loss of generality that $f(0) = 0$. Fix $\varepsilon > 0$. For any $x \in \RR^n - \{ 0 \}$, since $f$ is continuous at $0$, there is $\lambda_0 \leq 1/2$ depending only on $|x|$ such that
	%
	\[ |f(\lambda_0 x)| \leq |x|^2. \]
	%
	Now we apply the mean value theorem, writing $g(\lambda) = f(\lambda x)$. The mean value theorem implies that there is $\lambda \in (\lambda_0, 1)$ such that
	%
	\[ g'(\lambda) = \frac{f(x) - f(\lambda_0 x)}{1 - \lambda_0}. \]
	%
	But
	%
	\[ g'(\lambda) = \sum_{i = 1}^n \frac{\partial f}{\partial x^i}(\lambda x) \cdot x_i. \]
	%
	If $|x|$ is suitably small, then $|\frac{\partial f}{\partial x^i}(\lambda x)| \leq \varepsilon$, which implies that
	%
	\[ |f(x) - f(\lambda_0 x)| \leq 2 \left| \frac{f(x) - f(\lambda_0 x)}{1 - \lambda_0} \right| \lesssim \varepsilon |x|. \]
	%
	Thus
	%
	\[ |f(x)| \leq |f(\lambda_0 x)| + 2 \varepsilon |x| \leq |x|^2 + \varepsilon |x|. \]
	%
	Thus if $|x|$ is suitably small, we conclude that $|f(x)| \lesssim \varepsilon |x|$. Since $\varepsilon$ was arbitrary, this shows that $f$ is differentiable at zero, and $\nabla f(0) = 0$.
\end{solution}




\question (Spring 2017) For any pair of sequences $\{ a_k \}$ and $\{ b_n \}$, show that
%
\[ \sum_{n = 1}^\infty \sum_{m = 1}^\infty \frac{a_n b_m}{n + m} \lesssim \left( \sum_{n = 1}^\infty a_n^2 \right)^{1/2} \left( \sum_{m = 1}^\infty b_m^2 \right)^{1/2}. \]
\begin{solution}
	This is a hard problem, with the simplest proof relying on some clever applications of Cauchy-Schwartz. Fix $\lambda$, and write
	%
	\[ a_{nm} = \frac{a_n}{\sqrt{m + n}} (n/m)^\lambda \quad\text{and}\quad b_{nm} = \frac{b_m}{\sqrt{m + n}} (m/n)^\lambda. \]
	%
	Then Cauchy-Schwartz implies that
	%
	\[ \sum_{n,m} \frac{a_n b_m}{n + m} = \sum_{n,m} a_{nm} b_{nm} \leq \left( \sum_{n,m} a_{nm}^2 \right)^{1/2} \left( \sum_{n,m} b_{nm}^2 \right)^{1/2}. \]
	%
	Now
	%
	\begin{align*}
		\sum_{n,m} a_{nm}^2 &= \sum_{n = 1}^\infty a_n^2 \sum_{m = 1}^\infty \frac{(n/m)^{2\lambda}}{n + m}\\
		&= \sum_{n = 1}^\infty a_n^2 n^{2\lambda} \sum_{m = 1}^\infty \frac{1}{m^{2\lambda} (n + m)}. 
	\end{align*}
	%
	Now
	%
	\begin{align*}
		\sum_{m = 1}^\infty \frac{1}{m^{2\lambda} (n + m)} &\lesssim \frac{1}{n} \sum_{m = 1}^n \frac{1}{m^{2\lambda}} + \sum_{m = n+1}^\infty \frac{1}{m^{1 + 2\lambda}}\\
		&\lesssim \frac{1}{n^{2\lambda}}
	\end{align*}
	%
	Thus
	%
	\[ \left( \sum_{n,m} a_{nm}^2 \right) \lesssim \sum_{n = 1}^\infty a_n^2. \]
	%
	Similarily, one can show
	%
	\[ \left( \sum_{n,m} b_{nm}^2 \right) \lesssim \sum_{n = 1}^\infty b_n^2. \]
	%
	Combining these estimates completes the proof.

	If one is less sneaky, but knows more analysis, there is an alternate, much more technical proof. If we consider the operator $T$, mapping a sequence $a(n)$ to a sequence $(Ta)(m)$ such that
	%
	\[ (Ta)(m) = \sum_{n = 1}^\infty \frac{a_n}{n+m}, \]
	%
	then the bound holds if and only if $T$ is bounded from $l^2(\mathbf{N})$ to $l^2(\mathbf{N})$. For $1 < p < \infty$, one can show that if $a$ is the \emph{indicator function} of some finite set $E \subset \{ 1, \dots, \infty \}$, then we can show that
	%
	\[ \left( \sum_{m = 1}^\infty |Ta(m)|^p \right)^{1/p} \lesssim_p \#(E)^{1/p}. \]
	%
	Indeed, if $N = \#(E)$, then it is simple to see that $Ta \leq Ta^*$, where $a^*$ is the indicator function of $\{ 1, \dots, N \}$, and so one needs only estimate $Ta^*$, and it is not too difficult to show that $\| Ta^* \|_{L^p} \lesssim_p \#(E)^{1/p}$ for $1 < p < \infty$. This is known as a \emph{restricted strong type} estimate. Standard techniques in the theory of real interpolation thus imply that
	%
	\[ \left( \sum_{m = 1}^\infty |Ta(m)|^2 \right)^{1/2} \lesssim \left( \sum_{n = 1}^\infty |a(n)|^2 \right)^{1/2}, \]
	%
	which completes the argument.
\end{solution}
    

\newpage
\section*{Day 4: Warm Up Problems}

\newpage
\section{Day 4: Measure Theory}

\question  (Spring 2021 and Spring 2016)
  Let $E\subset \R$ be a Lebesgue measurable set with $|E|<\infty$. Prove that the function $f:\R\to \R$ defined by $f(r)= |E\cap (E+r)|$ is continuous
\begin{solution}
    It suffices to prove that for all $s \in \RR$,
    %
    \[ \lim_{r \to s} f(r) = f(s). \]
    %
    We write
    %
    \[ f(r) = \int \mathbf{I}_E(x) \mathbf{I}_E(x + r). \]
    %
    Now
    %
    \[ f(r) - f(s) = \int \mathbf{I}_E(x) [\mathbf{I}_E(x + r) - \mathbf{I}_E(x + s)]. \]
    %
    The result then follows from the fact that
    %
    \[ \lim_{r \to s} \int | \mathbf{I}_E(x + r) - \mathbf{I}_E(x + s)|\; dx \to 0, \]
    %
    because for any $f \in L^1(\RR)$,
    %
    \[ \lim_{r \to 0} \int |f(x + r) - f(x)|\; dx = 0. \]
    %
    To see that this equation is true, we note it is certainly true for any $f \in C_c^\infty(\RR)$, which is a dense subfamily of $L^1(\RR)$. But the operators $A_r f(x) = f(x + r) - f(x)$ are uniformly bounded in $L^1(\RR)$, i.e. $\| A_r f \|_{L^1(\RR)} \leq 2 \| f \|_{L^1(\RR)}$. And thus the result follows by the uniform boundedness theorem.

    Alternatively, write $g = \mathbf{I}_E$. Then $g$ lies in $L^1$ and $L^\infty$, and $f(r) = (g * g)(-r)$. But Young's convolution inequality implies that $g * g$ is continuous, so that $f$ is continuous.

    \begin{comment}
    As $r \to s$, the integrand tends to zero pointwise. And moreover, the integrands are uniformly dominated by $2\mathbf{I}_E$, which is an integrable function. Thus the dominated convergence theorem implies that
    %
    \[ \lim_{r \to s} f(r) - f(s) = 0. \]
    %
    Thus $f$ is a continuous function.

  Let $\epsilon>0$. Since $E$ is measurable and $|E|<\infty$, $\chi_{E}$ is integrable. Since $C_{c}(\R)$ is dense in $L^{1}(\R)$, there exists $g\in C_{c}(\R)$ such that $\norm{g-\chi_{E}}_{L^{1}(\R)}<\epsilon/3$. Since $g$ is continuous and compactly supported, it is unformly continuous. Therefore (again using the fact that $|E|<\infty$) we may choose $\delta>0$ such
  \begin{equation}
    |g(x)-g(y)|< \frac{\epsilon}{3|E|}\label{eq:uniform-continuity}
  \end{equation}
  whenever $|x-y|<\delta$.
Let $r,s\in \R$ such that $|s-r|<\delta$. Writing $f(r)= \int_{\R}\chi_{E}(x)\chi_{E}(x-r)dx$, we have
\begin{align*}
  f(r)-f(s) & = \int_{\R}\chi_{E}(x)\left[\chi_{E}(x-r)-\chi_{E}(x-s)\right]dx\\
            & = \int_{\R}\chi_{E}(x)\left[\chi_{E}(x-r)-g(x-r)+g(x-r)-g(x-s)+g(x-s)-\chi_{E}(x-s)\right]dx.
\end{align*}
Therefore by the triangle inequality,
\begin{align*}
  |f(r)-f(s)| &\leq \int_{\R}\chi_{E}(x)|\chi_{E}(x-r)-g(x-r)|dx + \int_{\R}\chi_{E}(x)|g(x-r)-g(x-s)|dx \\ 
  &\quad +\int_{\R}\chi_{E}|g(x-s)-\chi_{E}(x-s)|dx.\\
              &\leq \int_{\R}|\chi_{E}(x-r)-g(x-r)|dx + \int_{E}|g(x-r)-g(x-s)|dx \\\quad &+\int_{\R}|g(x-s)-\chi_{E}(x-s)|dx.
\end{align*}
By translation invariance (i.e. do a u-substitution), the first and third integrals both equal  $\int_{\R}|\chi_{E}(x)-g(x)|dx < \epsilon/3$. By \eqref{eq:uniform-continuity}, the second integral is less than $\epsilon/3$. Therefore $|f(r)-f(s)|<\epsilon$. We have shown that $\delta$ responds to the $\epsilon$-challenge in the definition of continuity. Therefore $f$ is continuous.
\end{comment}
\end{solution}



\question (Spring 2015) Does there exists a Borel measurable function $f: \RR \to [0,\infty)$ such that
%
\[ \int_a^b f(x)\; dx = \infty \]
%
for all real numbers $a < b$. Find an example or show that no such function exists.
\begin{solution}
    Here I construct such a function. There are many ways of doing this. We note that it suffices to construct such a function $f: [0,1] \to [0,\infty)$, since one can piece together a Borel measurable function on $\RR$ by patching together translates of this function.
    
    For each rational number $x \in [0,1]$ expressed in the simplest form as $p/q$, let $E_x = \{ y \in \RR: |y - p/q| \leq 1/q^3 \}$. Then
    %
    \[ \sum_{x \in \mathbf{Q} \cap [0,1]} |E_x| \leq \sum_{q = 1}^\infty \sum_{p = 1}^q 2/q^3 = \sum_{q = 1}^\infty 2/q^2 < \infty. \]
    %
    The Borel-Cantelli lemma implies that $E = \limsup_x E_x$ is a set of measure zero. We set
    %
    \[ f(x) = \mathbf{I}(x \not \in E) \sum_{p/q \in \mathbf{Q} \cap [0,1]} q^4 \cdot \mathbf{I}(x \in (p/q - 1/q^3, p/q + 1/q^3)). \]
    %
    If $x \not \in E$, then $x$ lies in at most finitely many of the intervals $(p/q - 1/q^3, p/q + 1/q^3)$, so $f(x)$ is finite. Moreover, $f$ is a limit of simple functions, and is therefore measurable. And if $a < b$, there exists arbitrarily large denominators $q$ for which there is a simple fraction $p/q$ with
    %
    \[ a < p/q - 1/q^3 < p/q + 1/q^3 < b. \]
    %
    On $(p/q - 1/q^3, p/q + 1/q^3) \cap E^c$, we have $f(x) \geq q^4$, and thus
    %
    \[ \int_a^b f(x)\; dx \geq \int_{p/q - 1/q^3}^{p/q + 1/q^3} q^4 = 2q. \]
    %
    Taking $q \to \infty$ shows that
    %
    \[ \int_a^b f(x)\; dx = \infty. \]
\end{solution}

\question (Fall 2018) Two parts:

    \begin{parts}
    \part Give an example, with explanation, of each of the following:
        \begin{itemize}
        \item A sequence of functions on $\R$ that converges to zero in $L^1(\R)$, but it does not converge almost anywhere on $\R$ to any function. 
        \item A sequence of functions in $L^1(\R)$ that converges almost everywhere to zero, but it does not converge in measure to any function. 
        \end{itemize}
  \begin{solution} 
  	For the first part of (a), It suffices to construct such a function on $L^1([0,1))$, since one can patch together functions onto all of $\RR$. Given $a \in \RR$, let $[a] \in [0,1)$ be the unique value such that $a - [a] \in \ZZ$. Define
  	%
  	\[ H_n = \sum_{k = 1}^n \frac{1}{k}. \]
  	%
  	Define $I_n = \{ [x] \in [0,1): x \in [H_n,H_{n+1}] \}$ and then let $f_n = \chi_{I_n}$. These functions are known as the \emph{typewriter sequence}, for some reason. Then
  	%
  	\[ \| f_n \|_{L^1(\RR)} = H_{n+1} - H_n \lesssim 1/n. \]
  	%
  	Thus the sequence $\{ f_n \}$ converges to zero in $L^1(\RR)$. However, since $H_n \to \infty$, $\limsup I_n = [0,1)$, which means that for any $x \in [0,1)$,
  	%
  	\[ \liminf_{n \to \infty} f_n(x) = 1. \]
  	%
  	Thus $\{ f_n \}$ does not converge pointwise to zero at any point in $[0,1)$.

  	For the second part, let $I_n = [n,n+1]$, and let $f_n = \chi_{I_n}$. The sequence $\{ f_n \}$ is then the required example. This is the classic ``moving bump''.
  \end{solution}    
        
    \part Prove that a sequence of functions on $\R$ that converges to zero in measure must have a subsequence that converges to zero almost everywhere. Do not quote any theorems that trivialize the problem. 
    
    \begin{solution}
    By definition of convergence in measure, there exists a subsequence $(n_k)$ such that 
    $$|\{ x: f_{n_k}(x)>2^{-k} \}|< 2^{-k}$$
    for every $k\geq1$. Therefore by the Borel-Cantelli lemma, it follows that 
    $$|\{x: f_{n_k}(x) > 2^{-k} \mathrm{\ i.o.}\}|=0.$$ 
    In other words, for a.e. $x\in\R$, there exists an integer $k_0$ (possibly depending on $x$) such that $f_{n_k}(x)<2^{-k}$ whenever $k\geq k_0$. Therefore $\lim_{k\to\infty} f_{n_k}(x)= 0$ a.e.
    \end{solution}
    \end{parts}
    
    
\question (Spring 2017)
    Let $f: [0,\infty) \to \RR$ be a continuously differentiable function for which $\| f' \|_\infty < \infty$. Define, for $x > 0$,
    %
    \[ F(x) = \int_0^\infty f(x + yx) \psi(y)\; dy, \]
    %
    where $\psi$ satisfies
    %
    \[ \int_0^\infty |\psi(y)|\; dy \quad\text{and}\quad \int_0^\infty y \cdot |\psi(y)|\; dy < \infty. \]
    %
    Show that $F(x)$ is well defined for all $x \geq 0$, and that $F$ is continuously differentiable.
\begin{solution}
    The fundamental theorem of calculus implies that
    %
    \[ |f(x + yx)| = \left| f(x) + \int_x^{x+yx} f'(t)\; dt \right| = f(x) + O(yx). \]
    %
    Thus combined with the fact that $\int_0^\infty |\psi(y)|\; dy < \infty$ and $\int_0^\infty y \cdot |\psi(y)|\; dy < \infty$, this implies that
    %
    \[ \int_0^\infty |f(x + yx)| |\psi(y)|\; dy \lesssim |f(x)| + |x| < \infty, \]
    %
    so $F$ is well defined.
    
    To show that $F$ is continuously differentiable, we note that for a fixed $x \in [0,\infty)$, we calculate that, for $h$ with $0 \leq x + h$,
    %
    \begin{align*}
        \frac{F(x+h) - F(x)}{h} &= \int_0^\infty \frac{|f(x+yx + h(1 + y)) - f(x + yx)|}{h} \psi(y)\; dy.
    \end{align*}
    %
    Now
    %
    \[ \left| \frac{|f(x+yx + h(1 + y)) - f(x + yx)|}{h} \psi(y) \right| \lesssim (1 + y) |\psi(y)|. \]
    %
    Since $(1 + y) \psi(y)$ is integrable, the dominated convergence theorem implies that $F$ is differentiable at $x$, and
    %
    \[ F'(x) = \int_0^\infty f'(x + yx) (1 + y) \psi(y)\; dy. \]
    %
    Finally, we show $F'$ is continuous. We note that for any $\varepsilon > 0$, there exists $R > 0$ such that
    %
    \[ \int_R^\infty (1 + y) |\psi(y)|\; dy \leq \varepsilon. \]
    %
    Since $f'$ is continuous, it is uniformly continuous on $[0,2R(1 + x)]$. Thus there exists $\delta > 0$ such that for $|h| \leq \delta$, and $0 \leq y \leq R$, $|f'((x + yx) + h(1 + y)) - f(x + yx)| \leq \varepsilon$, and so
    %
    \begin{align*}
        F'(x+h) - F(x) &\lesssim \varepsilon + \int_0^R [f'((x + yx) + h(1 + y)) - f'(x + yx)] (1 + y) \psi(y)\; dy\\
        &\lesssim \varepsilon + \int_0^R \varepsilon (1 + y) \psi(y)\; dy \lesssim \varepsilon.
    \end{align*}
\end{solution}

\question (Fall 2016) Let $f:[0,1]\to \R$ be continuous with $\min_{0\leq x\leq 1} f(x) = 0$. Assume that for any $0\leq a\leq b\leq 1$ we have
%
\[ \int_{a}^{b}[f(x)-\min_{a\leq y\leq b}f(y)]dx \leq \frac{|b-a|}{2}. \]
%
Prove that for any $\lambda\geq 0$, we have
\begin{equation*}
\left| \left\{ x:f(x)>\lambda+1 \right\} \right|\leq \frac{1}{2} \left| \left\{ f(x)>\lambda) \right\} \right|.
\end{equation*}

\begin{solution}
  Since $f$ is continuous, $\left\{x: f(x)>\lambda \right\}$ is open. Therefore we may write $\left\{x: f(x)>\lambda \right\} = \bigcup_{i=1}^{\infty}I_{i}$ where $(I_{i})_{i=1}^{\infty}$ is a countable pairwise disjoint sequence of intervals $I_{i}=(a_{i},b_{i})$ with $a_{i}\leq b_{i}$.

  We claim that for each $i$,
  %
  \[ \inf_{t \in \overline{I}_i} f(t) \leq \lambda. \]
  %
  If $I_i = \emptyset$, there is nothing to show. If $a_i = 0$ and $b_i = 1$, then by hypothesis, $\min_{[a_{i},b_{i}]}f =\min_{[0,1]}f = 0\leq \lambda$. So assume that is not the case; i.e., that at least one of $a_{i},b_{i}$ is in $(0,1)$. Without loss of generality, assume that $a_i \in (0,1)$. Then $a_i \in \partial \left\{ x: f(x)\leq \lambda \right\}$. Hence by continuity of $f$, we have $f(a_{i})\leq \lambda$.

  Now that this has been proved, we have:
  \begin{align*}
    \frac{1}{2}|I_{i}| &\geq \int_{I_{i}} \left\{ f(x)-\min_{t\in {\bar{I}_{i}}}f(t) \right\} dx
    &&\text{by assumption}\\
                       &\geq\int_{I_{i}\cap \left\{ f > \lambda+1 \right\}} \left\{ f(x)-\min_{t\in{\bar{I}_{i}}}f(t) \right\} dx
    &&\text{by nonnegative of the integrand}\\
                       &\geq \int_{I_{i}\cap \left\{ f > \lambda+1 \right\}} (f(x)-\lambda) dx
    &&\text{by the claim}\\
                       &\geq \int_{I_{i}\cap \left\{ f > \lambda+1 \right\}} ((\lambda+1)-\lambda) dx
    &&\text{by domain of integration}\\
                       &= \left|I_{i} \cap \left\{  f> \lambda+1\right\} \right|.
  \end{align*}
  By countable additivity for disjoint sets, summing over $i$ and using $\left\{f>\lambda+1 \right\}\subseteq
 \left\{f>\lambda \right\}$ gives
  \begin{equation*}
    \frac{1}{2}\left| \left\{f >\lambda \right\} \right|  \geq \left| \left\{ f>\lambda \right\}\cap \left\{ f>\lambda+1 \right\} \right| = \left| \left\{ f>\lambda+1 \right\} \right|.
  \end{equation*}
\end{solution}





\newpage
\section*{Day 5: Warm Up Problems}

\question (Spring 2015) Let $f \in L^2[0,1]$ satisfy $\int_0^1 t^n f(t)\; dt = (n+2)^{-1}$ for $n = 0, 1,\dots$. Must then $f(t) = t$ for almost every $t \in [0,1]$?
\begin{solution}
    The identity $\int_0^1 t^n f(t)\; dt = (n+2)^{-1}$ shows that for any polymonial $p(t)$,
    %
    \[ \int_0^1 p(t) [f(t) - t]\; dt = 0. \]
    %
    Applying some type of density theorem (the Stone-Weirstrass theorem, and its variants), we find that the family of all polynomials is dense in $L^2[0,1]$. Applying the continuity of the map $g \mapsto \int_0^1 g(t) [f(t) - t]$, we conclude that for all $g \in L^2[0,1]$,
    %
    \[ \int_0^1 g(t) [f(t) - t]\; dt = 0. \]
    %
    But setting $g(t) = \overline{f(t) - t}$, we conclude that
    %
    \[ \int_0^1 |f(t) - t|^2\; dt = 0. \]
    %
    This implies $f(t) = t$ for almost every $t$.
\end{solution}

\question (Fall 2021) Let $\{ f_n \}$ be a sequence of monotonic functions on $[0,1]$ converging to a function $f$ in measure. Show that $f$ coincides almost everywhere with a monotonic function $f_0$, and that $f_n(x) \to f_0(x)$ at every point of continuity of $f_0$.
\begin{solution}
	If the sequence $\{ f_n \}$ converges uniformly, then for each $0 \leq x < y \leq 1$, we would have
	%
	\[ f(y) - f(x) = \lim_n f_n(y) - f_n(x) \geq 0. \]
	%
	Thus $f$ is monotonic. This sequence does \emph{not} converge uniformly. But we utilize one of \emph{Littlewood's three principles}, namely, that every convergent sequence of functions is nearly uniformly convergent.

	We do this by first giving us a $\varepsilon$ of room. It suffices to show that for each $\varepsilon$, there exists a set $E(\varepsilon)$ such that if $x,y \in E(\varepsilon)$ and $x < y$, then
	%
	\[ f(y) - f(x) \geq - 2 \varepsilon. \]
	%
	If we set
	%
	\[ E_n(\varepsilon) = \{ x \in [0,1] : |f_n(x) - f(x)| \geq \varepsilon \} \]
	%
	then $|E_n(\varepsilon)| \to 1$. If $x < y$, with $x,y \in E_n$ for some $n$, then
	%
	\[ f(y) - f(x) \geq [f_n(y) - f_n(x)] - 2 \varepsilon \geq -2\varepsilon. \]
	%
	Thus if $E(\varepsilon) = \bigcup_n E_n(\varepsilon)$, then $|E(\varepsilon)^c| = 0$, and for $x,y \in E(\varepsilon)$ with $x < y$,
	%
	\[ f(y) - f(x) \geq - 2 \varepsilon. \]
	%
	The set $E = \bigcap E(1/2^k)$ is also a set with $|E^c| = 0$, and for any $x,y \in E$ with $x < y$,
	%
	\[ f(y) - f(x) \geq - 2 / 2^k \]
	%
	for all $k > 0$, so taking $k \to \infty$ gives
	%
	\[ f(y) - f(x) \geq 0. \]
	%
	Thus $f$ is monotonic when restricted to $E$, and thus agrees with a monotonic function $f_0: [0,1] \to \RR$ on $E$.

	It now suffices to show that $f_n(x_0) \to f_0(x_0)$, whenever $x_0$ is a continuity point of $f_0$. Here we use the same Littlewood's principle more explicitly, i.e. using Egorov's theorem: for any $\delta > 0$, there is $E_\delta \subset [0,1]$ with $|E_\delta|^c < \delta/2$, such that $f_n \to f_0$ uniformly on $E_\delta$. If $x_0$ is a continuity point of $f_0$, then for any $\varepsilon > 0$, there exists $\delta > 0$ such that if $|x - x_0| \leq \delta$, $|f(x) - f(x_0)| \leq \varepsilon$. Since $|E_\delta|^c < \delta/2$, there must exist
	%
	\[ x_0 - \delta < x_1 < x_0 < x_2 < x_0 + \delta \]
	%
	with $x_1,x_2 \in E_\delta$. But this implies that
	%
	\[ \limsup_n f_n(x_0) \leq \limsup_n f_n(x_2) = f_0(x_2) \leq f_0(x_0) + \varepsilon \]
	%
	and
	%
	\[ \liminf_n f_n(x_0) \geq \liminf_n f_n(x_1) = f_0(x_1) \geq f_0(x_0) - \varepsilon. \]
	%
	Since $\varepsilon$ was arbitrary, it is clear from these inequalities that $f_n(x_0)$ converges to $f_0(x_0)$.
\end{solution}

\newpage
\section{Day 5: Functional Analysis}

\question (Fall 2015) Find all $f \in L^2[0,\pi]$ such that
%
\[ \int_0^\pi |f(x) - \sin x|^2\; dx \leq \frac{4\pi}{9} \]
%
and
%
\[ \int_0^\pi |f(x) - \cos x|^2\; dx \leq \frac{\pi}{9} \]
\begin{solution}
    We note that $\sin x$ and $\cos x$ are orthogonal in $L^2[0,1]$. Thus we may consider a family of orthogonal vectors $\{ e_n \}$ such that $\{ \sin x, \cos x \} \cup \{ e_n \}$ is an orthogonal basis for $L^2[0,1]$. Thus there exists constants $a$, $b$, and $\{ c_n \}$ such that
    %
    \[ f(x) = a \sin x + b \cos x + \sum c_n e_n, \]
    %
    where the convergence of this sum is in the $L^2$ norm. Applying Parseval's theorem, we find that
    %
    \begin{align*}
        \int_0^\pi |f(x) - \sin x|^2\; dx &= |a - 1|^2 \int_0^\pi |\sin x|^2\; dx + |b|^2 \int_0^\pi |\cos x|^2\; dx + \sum |c_n|^2\\
        &= |a - 1|^2 (\pi/2) + |b|^2 (\pi/2) + \sum |c_n|^2.
    \end{align*}
    %
    Similarily,
    %
    \begin{align*}
        \int_0^\pi |f(x) - \cos x|^2\; dx &= |a|^2 (\pi/2) + |b - 1|^2 (\pi/2) + \sum_n |c_n|^2.
    \end{align*}
    %
    Next, we note that
    %
    \[ \int_0^\pi |\sin x - \cos x|^2 = \pi. \]
    %
    The triangle inequality thus implies that for any $f$ satisfying the requirements above, we actually have \emph{equality}, i.e.
    %
    \[ \int_0^\pi |f(x) - \sin x|^2\; dx = \frac{4\pi}{9} \]
    %
    and
    %
    \[ |f(x) - \cos x|^2\; dx = \frac{\pi}{9} \]
    %
    Thus we must find all families of coefficients $a$, $b$, and $\{ c_n \}$ such that
    %
    \[ |a - 1|^2 (\pi/2) + |b|^2 (\pi / 2) + \sum |c_n|^2 = 4 \pi / 9 \]
    %
    and
    %
    \[ |a|^2 (\pi/2) + |b - 1|^2 (\pi / 2) + \sum |c_n|^2 = \pi / 9. \]
    %
    Subtracting the second identity from the first, then multiplying by $2/\pi$, we conclude that
    %
    \[ (|a - 1|^2 - |a|^2) + (|b|^2 - |b-1|^2) = 2/3. \]
    %
    This equation can be simplified to
    %
    \[ \text{Re}(b) = \text{Re}(a) + 1/3. \]
    %
    %
    Substituting this into the first equation, we find that if $a = x + iy$, $b = (x + 1/3) + iz$, and $C = 2/\pi \sum c_n^2$, then
    %
    \[ 2x^2 - (4/3)x + (y^2 + z^2 + 2/9 + C) = 0. \]
    %
    The quadratic formula implies that there exists $x$ satisfying this equation for a fixed $y$ and $z$ if
    %
    \[ 16/9 - 8(y^2 + z^2 + 2/9 + C) \geq 0, \]
    %
    which can be rearranged to read
    %
    \[ 8(y^2 + z^2 + C) \leq 0. \]
    %
    This can only occur if $y = z = C = 0$, and it then follows from the quadratic formula that $x = 1/6$. Thus the only function $f$ satisfying this result is the function
    %
    \[ f(x) = (1/6) \sin x + (1/2) \cos x. \]
    %
    This completes the argument.

    Alternatively, let $u(x)= \sin(x)-f(x)$ and $v(x)=f(x)-\cos(x)$. Then

\begin{equation*}
  \norm{u+v}_{L^{2}}^{2} = \int_{0}^{\pi}\left( \sin(x)+\cos(x) \right)^{2}dx = \int_{0}^{\pi} \sin^{2}(x)+ \cos^{2}(x)dx =\pi.
\end{equation*}
By the triangle inequality and the hypotheses of the problem,

\begin{equation*}
  \sqrt{\pi}=\norm{u+v}_{L^{2}} \leq \norm{u}_{L^{2}} + \norm{v}_{L^{2}} \leq \frac{2}{3}\sqrt{\pi}+ \frac{1}{3}\sqrt{\pi}=\sqrt{\pi}.
\end{equation*}
Therefore
$\norm{u+v}_{L^{2}} = \norm{u}_{L^{2}} + \norm{v}_{L^{2}}$. Therefore
\begin{equation*}
  \left( \norm{u}_{L^{2}}+ \norm{v}_{L^{2}} \right)^{2} = \norm{u+v}_{L^{2}}^{2} = \langle u+v, u+v\rangle = \norm{u}^{2}_{L^{2}}+\norm{v}^{2}_{L^{2}} + 2\langle u,v\rangle.
\end{equation*}
Therefore since $  \left( \norm{u}_{L^{2}}+ \norm{v}_{L^{2}} \right)^{2}= \norm{u}_{L^{2}}^{2}+\norm{v}_{L^{2}}^{2}+2 \norm{u}_{L^{2}}\norm{v}_{L^{2}}$, it follows that $\norm{u}_{L^{2}}\norm{v}_{L^{2}}= \langle u, v \rangle$. Therefore by the Cauchy Schwarz inequality, $u$ and $v$ are linearly dependent (viz. the condition for equality in the statement of Cauchy Schwarz). That is, there exists $\alpha\in \R$ such that $\sin(x)-f(x)= \alpha \left( f(x)-\cos(x) \right)$ almost everywhere. Solving for $f$ gives
\begin{equation}\label{eq:form-of-f}
  f(x) = \frac{1}{1+\alpha}\sin(x)+ \frac{\alpha}{1+\alpha}\cos(x).
\end{equation}
It remains to determine $\alpha$. Since $\norm{u}_{L^{2}}+ \norm{v}_{L^{2}}=\sqrt{\pi}$, $\norm{u}_{L^{2}}\leq 2 \sqrt{\pi}/3$, and $\norm{v}_{L^{2}}\leq \sqrt{\pi}/3$, it follows that $\norm{u}^2_{L^{2}} = 4 \pi/9$. Therefore
\begin{align*}
  \frac{4}{9}\pi &= \int_{0}^{\pi}\left( \sin(x)- \frac{1}{1+\alpha}\sin(x) -\frac{\alpha}{1+\alpha}\cos(x)\right)^{2}dx\\
                 &= \left( \frac{\alpha}{1+\alpha} \right)^{2}\int_{0}^{\pi}\left( \sin(x)-\cos(x) \right)^{2}dx\\
                 &=  \left( \frac{\alpha}{1+\alpha} \right)^{2}\pi.
\end{align*}
This implies $\alpha = 2$ or $\alpha= -2/5$. A similar calculation using $\norm{v}^{2}_{L^{2}}= \pi /9$ implies that $\alpha = 2$ or $\alpha = -4$. The only value for $\alpha$ consistent with both of these conditions is $\alpha =2$. Therefore by \eqref{eq:form-of-f},
\begin{equation*}
  f(x) = \frac{1}{3}\sin(x)+ \frac{2}{3}\cos(x).
\end{equation*}
\end{solution}

\question (Spring 2017)
    Let $l^1(\mathbf{N})$ be the space of summable sequences, i.e.
    %
    \[ l^1(\mathbf{N}) = \{ x : \sum_{n = 1}^\infty |x_n| < \infty \}. \]
    %
    Let $\{ a_n \}$ be a sequence with $a_n \geq 0$ for all $n \in \mathbf{N}$ and consider the subset $K \subset l^1(\mathbf{N})$ defined by
    %
    \[ K = \{ x \in l^1(\mathbf{N}) : 0 \leq x_n \leq a_n\ \text{for all $n$} \}. \]
    %
    Show that $K$ is compact if and only if the sequence $\{ a_n \}$ itself belongs to $l^1(\mathbf{N})$.
\begin{solution}
    First suppose $K$ is compact. Then consider the family of sequences
    %
    \[ \{ x(k) = (a_1,\dots,a_k,0,\dots) \}. \]
    %
    Each sequence $x(k)$ belongs to $K$. Thus by compactness, this sequence has a convergent subsequence $x(k_i)$ converging to some sequence in $l^1(\mathbf{N})$. But $x(k_i)$ converges pointwise to $a$, and so $x(k_i)$ can only converge to $a$, which implies $a \in l^1(\mathbf{N})$.
    
    On the other hand, suppose $a \in l^1(\mathbf{N})$. Consider an arbitrary sequence $\{ x(k) \}$ in $K$. For any $N > 0$, the projection of $K$ onto the first $N$ elements of the sequence is compact, and thus we may inductively find subsequences $x(k_{N,i})$ for each $N$ such that $\{ k_{N,i} \}$ is a subsequence of $\{ k_{M,i} \}$ for each $M \leq N$, and the first $N$ elements of the sequences $x(k_{N,i})$ converge pointwise. Consider the subsequence of sequences $y(n) = x(k_{n,n})$. For each $N > 0$, this sequence is a subsequence of the sequence $\{ x(k_{N,i} \}$ and so we can conclude from this that $\{ y(n) \}$ converges pointwise to some sequence $y$. Since $K$ is defined pointwise, it is simple to see that $y \in K$. For each $\varepsilon > 0$, we pick $N > 0$ such that
    %
    \[ \sum_{k > N} a_k < \varepsilon. \]
    %
    Then exists $M$ such that for $n > M$, and $1 \leq k \leq N$, $|y(n)_k - y_k| \leq \varepsilon / N$. And thus
    %
    \[ \| y(n) - y \|_1 \leq \sum_{k = 1}^N |y(n)_k - y_k| + \sum_{n > N} |y(n)_k| + |y_k| \leq \sum_{k = 1}^N (\varepsilon / N) + \sum_{n > N} 2a_n \leq 3\varepsilon. \]
    %
    Taking $\varepsilon \to 0$ completes the proof.
\end{solution}

\question (Spring 2018, Spring 2021)
  Let $K$ be a continuous function on $[0,1]\times [0,1]$. Suppose that $g$ is a continuous function on $[0,1]$. Show that there exists a unique continuous function $f$ on $[0,1]$ such that
  \begin{equation*}
    f(x)=g(x)+\int_{0}^{x}f(y)K(x,y)dy
  \end{equation*}

  \begin{solution}
    Define an integral operator $T$ by setting
    %
    \[ Tf(x) = g(x)+\int_{0}^{x}f(y)K(x,y)dy. \]
    %
    Since $g$ is continuous, $T:X\to X$ where $X=C[0,1]$, a Banach space with respect to the norm $\norm{f}_{L^\infty[0,1]} = \max_{x\in[0,1]} |f(x)|$. It suffices to show there is a unique $f$ such that $Tf = f$. Since $K$ is continuous and $[0,1]\times[0,1]$ is compact, there exists $M$ such that $|K(x,y)|\leq M$ for all $x,y\in [0,1]\times[0,1]$.
    By induction we can show that for any $f_{1},f_{2}\in C[0,1]$ and any $x\in [0,1]$,
    \begin{align*}
      |T^{n}f_{1}(x)-T^{n}f_{2}(x)|
      &\leq \norm{f_{1}-f_{2}}_{L^\infty[0,1]} M^{n}\int_{0}^{x}\int_{0}^{x_{1}}\cdots \int_{0}^{x_{n-1}}x_{n}dx_{n}\cdots dx_{1}\\
      &=\norm{f_{1}-f_{2}}_{L^\infty[0,1]} \frac{M^n x^{n}}{n! }\\
      &\leq \norm{f_{1}-f_{2}}_{L^\infty[0,1]} \frac{M^{n}}{n!}.
    \end{align*}
    %
    Thus for sufficiently large $n$, we conclude $T^n$ is a contraction map. Thus by the Banach fixed point theorem, we conclude that there exists a unique $f \in C[0,1]$ such that $T^n f = f$. Clearly, this shows that there exists at most one $f$ such that $Tf = f$, since if $Tf = f$, then $T^n f = f$. TODO: Does it show existence?

    In the previous solution, we wished to apply the contraction mapping theorem, but could not do so directly with $T$ because $T$ is not a contraction on $C[0,1]$. We got around this difficulty by considering the operator $T^n$ for some positive integer $n$, which fortuitously turned out to be a contraction when $n$ is large enough. Another approach is to consider $T$ as an operator on ``smaller'' Banach spaces, e.g. $C[0,x_1], C[x_1,x_2], \ldots$ etc, where $x_i-x_{i-1}$ are small enough that $T$ is a contraction on $C[x_{i-1},x_{i}]$, and then patch the solutions together. We detail this second proof here. If $|K|\leq M$ for all $x,y$, define
    %
    \[ T_{x_0 x_1}(f)= g(x)+\int_{0}^{x}f(y)K(x,y)dy \]
    %
    on $C([x_0,x_1])$. Then
    %
    \[ \| T_{x_0 x_1} f_1 - T_{x_0 x_1} f_2 \|_{L^\infty[x_0,x_1]} \leq \int_{x_0}^{x_1} |K(x,y)|\; dy \cdot \| f_1 - f_2 \|_{L^\infty[x_0,x_1]} \leq M(x_1 - x_0) \| f_1 - f_2 \|_{L^\infty[0,x_0]}. \]
    %
    Thus if $x_1 - x_0 < 1/2M$, then we conclude that $T_{x_0 x_1}$ is a contraction map on $C([x_0,x_1])$. This means that for any such interval, there exists a unique $f_{x_0x_1} \in C[x_0,x_1]$ such that $T_{x_0 x_1} f = f$. But clearly if $f$ satisfies this property, then it's restriction to any subinterval also satisfies this property. This implies that any two solutions $f_{x_0x_1}$ and $f_{x_2x_3}$ agree on $[x_0,x_1] \cap [x_2,x_3]$. But the locality of continuity means we can patch these functions together to find $f \in C[0,1]$ such that $T_{x_0x_1} f = f$ for any $x_0$ and $x_1$ with $x_1 - x_0 < 1/2M$. But this implies that $Tf = f$.
\end{solution}

\question Let $\phi: \RR \to \RR$ be a continuous function with compact support.
\begin{parts}
    \part Prove that there exists a constant $A$ such that
    %
    \[ \| f * \phi \|_q \leq A \| f \|_p \]
    %
    for $1 \leq p \leq q \leq \infty$.
\begin{solution}
    Let us begin by showing that
    %
    \[ \| f * \phi \|_\infty \leq A \| f \|_p. \]
    %
    Indeed, H\"{o}lder's inequality implies that if $p^*$ is the conjugate to $p$, then
    %
    \begin{align*}
        \| f * \phi \|_{L^\infty_x} &= \left\| \int f(x-y) \phi(y)\; dy \right\|_{L^\infty_x}\\
        &\leq \left\| \left( \int |\phi(y)|^{p^*}\; dy \right)^{1/p^*} \cdot \left( \int |f(x-y)|^p\; dy \right)^{1/p} \right\|_{L^\infty_x}\\
        &\leq \| \phi \|_{L^{p^*}} \| f \|_{L^p}.
    \end{align*}
    %
    Next, we calculate using Minkowski's integral inequality that
    %
    \[ \| f * \phi \|_{L^p_x} = \left\| \int f(x-y) \phi(y)\; dy \right\|_{L^p_x} \leq \int |\phi(y)| \| f \|_{L^p}\; dy = \| \phi \|_{L^1} \| f \|_{L^p}. \]
    %
    Thus we have found a constant $A = \max_{1 \leq r \leq \infty} (\| \phi \|_{L^r})$ such that the result holds for $(p,q)$ of the form $(p,\infty)$, and of the form $(p,p)$. The remaining estimates lie `in-between' this range. One approach here is to apply the Riesz-Thorin interpolation theorem, which gives all the intermediary bounds $(p,q)$ for $p \leq q \leq \infty$. Alternatively, a sneaky application of H\"{o}lder's inequality shows that for $q \geq p$, if we fix $\alpha \in (0,1)$ such that $\alpha q = p$, then
    %
    \begin{align*}
        \| f * \phi \|_{L^q_x}^q &= \int |(f * \phi)(x)|^{\alpha q} |(f * \phi)(x)|^{(1 - \alpha)q}\\
        &\leq \left( \int |(f * \phi)(x)|^p |(f * \phi)(x)|^{(1 - \alpha)q}\; dx \right)\\
        &\leq \int |(f * \phi)(x)|^p\; dx \cdot \sup_x |(f * \phi)(x)|^{(1 - \alpha)q}\\
        &\leq \| f * \phi \|_{L^p_x}^p \| f * \phi \|_{L^\infty_x}^{(1 - \alpha)q}\\
        &\leq \left( A \| f \|_{L^p} \right)^p \left( A \| f \|_{L^p} \right)^{(1 - \alpha) q}\\
        &= A \| f \|_{L^p}.
    \end{align*}
\end{solution}
    
    \part Show by example that such general inequality cannot hold for $p > q$.
\begin{solution}
  It suffices, for any $R > 0$, to construct a function $f \in L^p$ such that
  %
  \[ \| f * \phi \|_{L^q} \geq R \| f \|_{L^p}. \]
  %
  One should think of bounds on $L^r$ for large $r$ as controlling how `peaked' a function can be, whereas bounds on $L^r$ for small $r$ control how `long' a function can be. Thus, intuitively, we must construct a function $f$ which is not peaked, but can be long (small in $L^p$ norm), into a function $f * \phi$ which is long (has large $L^q$ norm).
  
  Here is one approach that `almost' works for all $\phi$. A good choice for a non-peaked, long function is just a constant function supported on a large set. For instance, if $f(x) = \mathbf{I}(|x| \leq R)$, then $\| f \|_p \sim R^{1/p}$. If we assume that $\phi$ is supported on $|x| \leq S$, then for $|x| \leq R - S$,
  %
  \[ (\phi * f)(x) = \int \phi(x)|\; dx. \]
  %
  Provided that $\int \phi(x)\; dx \neq 0$, this implies that $|(\phi * f)(x)| \gtrsim_\phi 1$ for $|x| \leq R - S$, which implies that for $R \geq 2S$,
  %
  \[ \| \phi * f \|_{L^q} \gtrsim_\phi \left( \int_{|x| \leq R - S} 1 \right)^{1/q}] \gtrsim (R - S)^{1/q} \gtrsim R^{1/q}. \]
  %
  Thus
  %
  \[ \| \phi * f \|_{L^q} \gtrsim_\phi R^{1/q - 1/p} \| f \|_{L^p}. \]
  %
  If $q < p$, then $1/q - 1/p > 0$, and so we may take $R \to \infty$ to show that the operator $Tf = \phi * f$ cannot be bounded. The only problem is that this idea cannot approach functions such that
  %
  \[ \int \phi(x)\; dx = 0. \]
  %
  If this is the case, in our examples above, $f * \phi = 0$ on $|x| \leq R - S$, which means $\| f * \phi \|_q \sim 1$, so this approach doesn't work. In order to approach this case, we must choose $f$ more carefully so that $f * \phi$ cannot average out to zero on a large set.
  
  Fix a large integer $N > 0$, and set
  %
  \[ f(x) = \sum_{n = -N}^N \overline{\phi(2R \cdot n-x)}, \]
  %
  where we assume $\phi(x) = 0$ for $|x| \geq R$.  Then $f$ is not `peaked' (because $\phi$ is continuous, and not peaked), but supported on a very wide set. Indeed, we find that
  %
  \[ \| f \|_p \sim_\phi N^{1/p}. \]
  %
  For $|n| \leq N$, we compute that
  %
  \[ (\phi * f)(2Rn) = \int \phi(x) f(2Rn - x) = \int \phi(x) \overline{\phi(x)} = \| \phi \|_{L^1}. \]
  %
  By continuity of the quantities involved, there exists $\delta$ such that for $|x| \leq \delta$, $|(\phi * f)(x)| \geq \| \phi \|_{L^1} / 2$. Thus $\phi * f$ is non-zero on  a very wide set. More precisely, $|(\phi * f)| \gtrsim \| \phi \|_1$ on $N$ intervals of radius $\delta$, so
  %
  \[ \| \phi * f \|_q \gtrsim (N \delta)^{1/q} \| \phi \|_{L^1} \gtrsim_\delta N^{1/q}. \]
  %
  Thus
  %
  \[ \| \phi * f \|_q \gtrsim_{\phi,\delta} N^{1/q - 1/p} \| f \|_p. \]
  %
  Taking $N \to \infty$ thus shows that the operator is unbounded.
\end{solution}
\end{parts}


\item (Fall 2016)
  Give an example of a non-empty closed subset of $L^{2}([0,1])$ that does not contain a vector of smallest norm. Prove your assertion.

\begin{solution}
	Let $\{ e_n \}$ be an orthonormal basis for $L^2[0,1]$, and define
	%
	\[ x_n = (1 + 1/n) e_n. \]
	%
	Then $\{ x_n \}$ is a closed subset of $L^2[0,1]$ (it is actually \emph{discrete}), and has no element of smallest norm.

\begin{comment}
  Let $e_{n} = \sqrt{2}\sin(n\pi x)$ for $n=1,2,\ldots$, and define $x_{n}= \left(\frac{n+1}{n}\right)e_{n}$. Then $\{x_{n}\}_{n=1}$ is a nonempty closed subset with no element of smallest norm. To prove this, we first prove two claims:

  

  \vt
  \noindent \textit{Claim 1:} $\{e_{n}\}_{n\geq 1}$ is an orthonormal set in $L^{2}([0,1])$.
  \begin{proof}[Proof of Claim 1:]
    Need to show two things: (1) that $\norm{e_{n}}_{L^{2}([0,1])}=1$ for all $n$, and (2) that $\langle e_{n},e_{m}\rangle$ whenever $m\neq n$. These can be shown by direct computation using trig identities:

    Suppose $n\geq 1$. Then using $\sin^{2}(\theta) = \frac{1}{2}\left( 1-\cos(2\theta) \right)$,

    \begin{equation*}
      \norm{e_{n}}_{L^{2}([0,1])}^{2} = \int_{0}^{1}2\sin^{2}(n\pi x)dx = \int_{0}^{1}1-\cos(n\pi x)dx=1.
    \end{equation*}
    This proves (1).
    Next, suppose $m\neq n$. Using $\sin(A)\sin(B) = \frac{1}{2}\left(\cos(A-B)-\cos(A+B)  \right)$,
    \begin{align*}
      \langle e_{n},e_{m}\rangle
      &= \int_{0}^{1}2\sin(n\pi x)\sin(m\pi x)dx\\
      &= \int_{0}^{1} \cos((n-m)\pi x) - \cos((n+m)\pi x)dx\\
      &=0.
    \end{align*}
    This proves (2), which completes the proof of the claim.
  \end{proof}

  \vt
  \noindent  \textit{Claim 2:} If $m\neq n$ then $\norm{x_{n}-x_{m}}_{L^{2}([0,1])}\geq \sqrt{2}$.
  \begin{proof}[Proof of Claim 2:]
    Suppose $m\neq n$. Denote $c_{n}= \frac{n+1}{n}$. Then 
    \begin{align*}
      \norm{x_{n}-x_{m}}^{2}_{L^{2}([0,1])}
      &= \langle x_{n}-x_{m}, x_{n}-x_{m}\rangle\\
      &= \langle c_{n} e_{n}-c_{m} e_{m},  c_{n}e_{n}- c_{m}e_{m}\rangle\\
      &=c_{n}^{2}\langle e_{n},e_{n}\rangle -2c_{m}c_{n}\langle e_{n},e_{m}\rangle + c_{m}^{2}\langle e_{m},e_{m}\rangle\\
      &= c_{n}^{2}+ c_{m}^{2}
    \end{align*}
    where the final equality is justified by Claim 1. It follows that
    \begin{equation*}
       \norm{x_{n}-x_{m}}_{L^{2}([0,1])} = \sqrt{c_{n}^{2}+c_{m}^{2}} = \sqrt{\left(1+ \frac{1}{n} \right)^{2}  + \left( 1+\frac{1}{m} \right)^{2}}\geq \sqrt{2}.
     \end{equation*}
     This proves claim 2.
  \end{proof}



Using Claim 1, we see that $\norm{x_{n}}_{L^2([0,1])} = \left( \frac{n+1}{n} \right)\norm{e_{n}}_{L^{2}([0,1])} = 1+\frac{1}{n}$, and from this it is clear that $(x_{n})$ has no element of smallest norm. The fact that $x_{n}$ is closed follows from Claim 2, which implies that any convergent sequence consisting of elements in $\left\{ x_{n} \right\}_{n}$ is eventually constant, and hence its limit point must again be an element in $\left\{ x_{n} \right\}_{n}$.
\end{comment}

  
\end{solution}






\newpage
\section*{Day 6: Warm Up Problems}


\newpage
\section{Day 6: Functional Analysis}

\question (Fall 2019) Show that there is no sequence $\{ a_n \}$ of positive numbers such that $\sum a_n |c_n| < \infty$ if and only if $\{ c_n \}$ is a bounded sequence. Hint: Suppose that there exists a sequence and consider the map $T: l^\infty \to l^1$ given by $Tf(n) = a_n f(n)$. The set of $f$ such that $f(n) = 0$ for all but finitely many $n$ is dense in $l^1$ but not in $l^\infty$.
\begin{solution}
    Consider the operator $T$ defined in the hint. Then $T$ is injective, since all elements of the sequence $\{ a_n \}$ are positive. We also claim that $T$ is surjective, given the property assumed of the sequence. Indeed, if $\sum b_n < \infty$, and we define $c_n = b_n/a_n$, then $\sum a_n |c_n| < \infty$, and thus $\{ c_n \}$ is a bounded sequence, and $Tc = b$.
    
    Thus we conclude that $T$ is an isomorphism, so that there exists a constants $C_1,C_2 > 0$ such that $C_1 \| c \|_{l^\infty} \leq \| Tc \|_{l^1} \leq C_2 \| c \|_{l^\infty}$. Plugging in the bounded sequence $(1,1,1,\dots)$ into the operator $T$ gives the sequence $\{ a_n \}$, and so this sequence lies in $l^1$. Clearly any sequence $\{ a_n \}$ in $l^1$ will give rise to a bounded operator from $l^\infty$ to $l^1$, so we wish to disprove that the lower bound exists, i.e there does not exist a sequence $\{ a_n \}$ in $l^1$ such that $\| Tc \|_{l^1} \gtrsim \| c \|_{l^\infty}$.
    
    To do this, we must construct a non-negative sequence $c$ where $\| c \|_{l^\infty}$ is large, but such that $\| Tc \|_{l^1}$ is small. The best way to do this is to concetrate the sequence $c$ at a single point, i.e. we consider the sequence $c = (0,\dots,0,1,\dots)$, where the $1$ is in the $N$th term in the sequence. Then $Tc = (0,\dots,0,a_n,\dots)$, and so we have
    %
    \[ \| c \|_{l^\infty} = 1 \quad\text{and}\quad \| Tc \|_{l^1} = |a_N|. \]
    %
    As $N \to \infty$, $|a_N| \to 0$ since the sequence is integrable. But this is impossible since the bound $\| Tc \|_{l^1} \gtrsim \| c \|_{l^\infty}$ implies that $|a_N| \gtrsim 1$ for all $N$.
\end{solution}

\question (Fall 2020)

Suppose that $X,Y$ and $Z$ are Banach spaces, and $T:X\times Y\to Z$ is a mapping such that:
\begin{enumerate}[(a)]
\item For each $x\in X$, the map $y\mapsto T(x,y)$ is a bounded linear map $Y\to Z$.
\item For each $y\in Y$, the map $x\mapsto T(x,y)$ is a bounded linear map $X\to Z$.
\end{enumerate}
Prove there exists a constant $C$ such that
\begin{equation*}
\norm{T(x,y)}_{Z}\leq C \norm{x}_{X}\norm{y}_{Y}
\end{equation*}

\begin{solution}
  Without loss of generality we may assume that $x,y$ are nonzero since if either $x=0$ or $y=0$, the inequality holds for any $C$. Further, we claim that it suffices to show the inequality holds for all $x,y$ satisfying $\norm{x}_{X}=1$ and $\norm{y}_{Y}=1.$ To see this, suppose $\norm{T(\tilde{x},\tilde{y})}_{Z}\leq C$ for all $\tilde{x}\in X$ and $\tilde{y}\in Y$ with $\norm{\tilde{x}}_{X}=1$, $\norm{\tilde{y}}_{Y}=1$. Then for any nonzero $x\in X$, $y\in Y$,
  \begin{equation*}
    \norm{T(x,y)}_{Z}= \norm{x}_{X}\norm{y}_{Y}\norm{T \left( \frac{x}{\norm{x}_{X}},\frac{y}{\norm{y}_{Y}} \right)}_{Z} \leq C \norm{x}_{X}\norm{y}_{Y}.
  \end{equation*}
  This proves the claim.

  Next, let for each $x\in X$, let $\phi_{x}:Y\to Z$ denote the map $y\mapsto T(x,y)$, and for each $y\in Y$, let $\psi_{y}:X\to Z$ denote the map $x\mapsto T(x,y)$. Let $B= \left\{ x\in X: \norm{x}_{X}=1 \right\}$. Then since $\phi_{x}(y)=T(x,y)=\psi_{y}(x)$ and since $\psi_{y}$ is assumed to be a bounded operator, it holds for all $y\in Y$ that
  \begin{equation*}
    \sup_{x\in B}\norm{\phi_{x}(y)}_{Z} = \sup_{x\in B}\norm{\psi_{y}(x)}_{Z} \leq \sup_{x\in B}\norm{\psi_{y}} \norm{x}_{X} = \norm{\psi_{y}} <\infty.
  \end{equation*}
  Therefore by the Uniform Boundedness Principle,
  \begin{equation*}
    \sup_{x\in B}\norm{\phi_{x}}= C <\infty.
  \end{equation*}
  Therefore for any $x,y$ with $\norm{x}_{X}=1$ and $\norm{y}_{Y}=1$,
  \begin{equation*}
    \norm{T(x,y)}_{Z} = \norm{\phi_{x}(y)}_{Z}\leq \norm{\phi_{x}}\norm{y}_{Y} = \norm{\phi_{x}} \leq C.
  \end{equation*}
\end{solution}

\question (Fall 2015) For $p \in (1,\infty)$, and for $f \in L^p(\RR)$ define
%
\[ Tf(x) = \int_0^1 f(x + y)\; dy. \]
%
\begin{parts}
    \part Show that $\| Tf \|_p \leq \| f \|_p$ and equality holds if and only if $f = 0$ almost everywhere.
    \begin{solution}
        We use Minkowski's inequality. For those unfamiliar, recall the triangle inequality, which for $1 \leq p \leq \infty$ says that
        %
        \[ \| \sum_n f_n \|_p \leq \sum_n \| f_n \|_p. \]
        %
        Minkowski's inequality occurs when we replace the sum with an integral, i.e. for $1 \leq p \leq \infty$,
        %
        \[ \left\| \int_y f_y\; dy \right\|_p \leq \int_y \| f_y \|_p\; dy. \]
        %
        In this case, we set $f_y(x) = f(x + y)$. Then
        %
        \[ Tf(x) = \int_0^1 f_y(x)\; dy. \]
        %
        Thus, using the fact that $\| f_y \|_p = \| f \|_p$, we conclude
        %
        \[ \| Tf \|_p = \left\| \int_0^1 f_y\; dy \right\|_p \leq \int_0^1 \| f_y \|_p\; dy = \int_0^1 \| f \|_p\; dy = \| f \|_p. \]
        %
        To address the case of equality, we recall the case of the triangle inequality, i.e. that
        %
        \[ \| \sum_n f_n \|_p = \sum_n \| f_n \|_p \]
        %
        if and only if $\{ f_n \}$ are all scalar multiples of one another. Similarily, \emph{Minkowski's inequality} is an equality if and only if $\{ f_y \}$ are all scalar multiples of one another \emph{almost everywhere}. Thus if $\| Tf \|_p = \| f \|_p$, then the application of Minkowski's inequality is an equality, which implies that there exists $c(y)$ such that $f_y(x) = c(y) f(x)$, i.e. $f(x + y) = c(y) f(x)$. But if
        %
        \[ \int_0^y |f(x)|^p\; dx \neq 0, \]
        %
        the dominated convergence theorem implies that
        %
        \begin{align*}
            \| f \|_p^p &= \lim_{n \to \infty} \sum_{k = -n}^n \int_0^y f(x + ky)\; dy\\
            &= \lim_{n \to \infty} \int_0^y f(x) \sum_{k = -n}^n c(y)^k.
        \end{align*}
        %
        But this sum cannot possibly converge. Thus
        %
        \[ \int_0^y |f(x)|^p = 0, \]
        %
        which implies $f = 0$ almost everywhere.
    \end{solution}
    
    \part (Fall 2015) Prove that the map $f \mapsto Tf - f$ does not map $L^p(\RR)$ onto $L^p(\RR)$.
    \begin{solution}
        First, we claim that $Tf - f$ is injective. Indeed, if $Tf = f$, then $\| Tf \|_p = \| f \|_p$, and by the last part of the problem, this implies that $f = 0$. If $Tf - f$ was surjective, then the open mapping theorem would therefore imply that there would exist a constant $C > 0$ such that for all $f \in L^p(\RR)$, $\| Tf - f \|_p \geq C \| f \|_p$. Our goal is to show this is not true.
        
        Intuitively, we wish to find a function $f$ such that $Tf - f$ is small. This would be possible if $Tf \approx f$, which would hold if $f$ was approximately constant. So we consider the `almost' constant function $f(x) = \mathbf{I}(|x| \leq R)$. Then $Tf(x)$ is supported on $|x| \leq R + 1$, $Tf(x) = f(x)$ for $|x| \leq R - 1$, and $\| Tf \|_\infty \leq 1$. Thus $Tf - f$ is supported on a set of total mass $O(1)$, and $\| Tf - f \|_\infty = O(1)$, so
        %
        \[ \| Tf - f \|_p \lesssim 1,  \]
        %
        independently of $R$. On the other hand, $\| f \|_p \sim R^{1/p}$, so it is impossible for a bound of the form $\| Tf - f \|_p \geq C \| f \|_p$ to hold.
    \end{solution}
\end{parts}



\question (Fall 2014)
\begin{parts}
\part For any $n \geq 1$ an integer, there exists two positive measures $\mu_{1}^{n},\mu_{2}^{n}$ supported on $[0,1]$ such that for any polynomial $P(x)$ with $\deg P(x)\leq n$ it holds:
\begin{equation*}
  P'(0) = \int_{0}^{1}P(x)d\mu^{n}_{1}(x)-\int_{0}^{1}P(x)d\mu_{2}^{n}(x).
\end{equation*}

\begin{solution}
	Let $X_n$ be the space of all real-valued polynomials on $[0,1]$ with degree at most $n$. Then $X_n$ is a finite dimensional linear subspace of $C[0,1]$. Let $T: X_n \to \RR$ be the operator $TP = P'(0)$. Then $T$ is bounded on $X_n$ since $X_n$ is finite dimensional. Thus by the Hahn-Banach theorem, we can find an extension of $T$ to a bounded functional on $C[0,1]$. The \emph{Reisz Representation Theorem} says that $C[0,1]^*$ is the space of all finite signed Borel measures on $[0,1]$. Thus there exists a finite signed Borel measure $\mu$ on $[0,1]$ such that for any $f \in C[0,1]$,
	%
	\[ Tf = \int f(x) d\mu(x). \]
	%
	In particular, if $P \in X_n$,
	%
	\[ P'(0) = \int P(x) d\mu(x). \]
	%
	Then we perform a \emph{Jordan decomposition} to write $\mu = \mu_1 - \mu_2$, where $\mu_1$ and $\mu_2$ are finite non-negative Borel measures, and this completes the construction.

	One can also define the measures explicitly, though the argument becomes more technical. This is Chun's solution from the 2016 SEP, problem 39. We will show that there exists constants $c_{j}\in \R, x_{j}\in (0,1)$ with $0\leq j\leq n$ such that the measure $\mu^{n}$ defined by
  \begin{equation}\label{eq:mu-form}
    \mu^{n}:= \sum_{j=0}^{n}c_{j}\delta_{x_{j}}
  \end{equation}
  satisfies
  %
  \[ \int_0^1 P(x) d\mu^{(n)}(x) = P'(0) \]
  %
  for any polynomial $P \in X_n$. The result will then follow by taking
  \begin{equation*}
    \mu_{1}^{n} = \sum_{j=0}^{n}c_{j}^{+}\delta_{a_{j}} \quad \text{and}\quad    \mu_{2}^{n} = \sum_{j=0}^{n}c_{j}^{-}\delta_{a_{j}}
  \end{equation*}
  where $c_{j}^{+}=\max\{c_{j},0\}$ and $c^{-}= \max\{-c_{j},0\}$.

  Let us determine what conditions the measure $\mu^n$ must satisfy. If $P(x)=\sum_{k=0}^{n}a_{k}x^{k}$, then $P'(0)=a_{1}$. By \eqref{eq:mu-form}, $\int_{0}^{1}x^{k}d\mu^{n}(x) = \sum_{j=0}^{n}c_{j}x_{j}^{k}$ whenever $0\leq k\leq n$, and hence
  \begin{equation*}
    \int_{0}^{1}P(x)d\mu^{n}(x)
    = \int_{0}^{1}\sum_{k=0}^{n}a_{k}x^{k}d\mu^{n}(x)
    = \sum_{k=0}^{n}a_{k}\int_{0}^{1}x^{k}d\mu^{n}(x)
    = \sum_{k=0}^{n}\sum_{j=0}^{n}a_{k}c_{j}x_{j}^{k}.
  \end{equation*}
  Therefore $\int_{0}^{1}f(x)d\mu^{n}(x)=P'(0)$ holds if and only if
  \begin{equation*}
    \sum_{k=0}^{n}a_{k}\sum_{j=0}^{n}x_{j}^{k}=a_{1}
  \end{equation*}
  or equivalently,
  \begin{equation*}
    \sum_{j=0}^{n}c_{j}x_{j}^{k}=\left\{ \begin{array}{l@{\quad:\quad}l}
                                           1  &k=1 \\
                                           0  &k\neq 1
                                         \end{array}\right.
  \end{equation*}
  or equivalently
  \begin{equation}\label{eq:matrix-equation}
    \begin{pmatrix}
      1 & 1 & \cdots & 1 \\
      x_{0} & x_{1} & \cdots & x_{n} \\
      x_{0}^{2} & x_{1}^{2} & \cdots & x_{n}^{2}\\ 
      \vdots  & \vdots  & \ddots & \vdots  \\
      x_{0}^{n} & x_{1}^{n} & \cdots & x_{n}^{n} 
    \end{pmatrix}
    \begin{pmatrix}
      c_{0}\\
      c_{1}\\
      c_{2}\\
      \vdots\\
      c_{n}\\
    \end{pmatrix}
    =
    \begin{pmatrix}
      0\\
      1\\
      0\\
      \vdots\\
      0\\
    \end{pmatrix}.
  \end{equation}
  Define 
  \begin{equation*}
    Q(x_{0},\ldots,x_{n}):= \det
    \begin{pmatrix}
      1 & 1 & \cdots & 1 \\
      x_{0} & x_{1} & \cdots & x_{n} \\
      x_{0}^{2} & x_{1}^{2} & \cdots & x_{n}^{2}\\ 
      \vdots  & \vdots  & \ddots & \vdots  \\
      x_{0}^{n} & x_{1}^{n} & \cdots & x_{n}^{n} 
    \end{pmatrix}.
  \end{equation*}
  %
  The matrix equation necessarily has a solution $\vec{c}=(c_{0},\ldots,c_{n})\in \R^{n+1}$ if and only if we have $Q(x_{0},\ldots,x_{n})\neq 0$. Therefore to prove the existence of the desired $\mu^{n}$, it will suffice to show there exists $(x_{0},\ldots,x_{n})\in (0,1)^{n+1}$ such that $Q(x_{0},\ldots,x_{n})\neq 0$. But this follows because $Q$ is a polynomial, hence an entire function on $\CC^{n+1}$, and because it's derivatives do not vanish at the origin, i.e.
  %
  \[ \partial_{x_1} \partial_{x_2}^2 \dots \partial_{x_n}^n Q(0) \neq 0. \]
  %
  Thus $Q \neq 0$, so $Q$ cannot vanish on any open set. In particular, it cannot vanish on $(0,1)^{n+1}$.
\end{solution}

\part Does there exist two finite positive measures $\mu_{1},\mu_{2}$ supported on $[0,1]$ such that for any polynomial $P(x)$, it holds
\begin{equation*}
  P'(0) = \int_{0}^{1}P(x)d\mu_{1}(x) - \int_{0}^{1}P(x)d\mu_{2}(x)?
\end{equation*}

\begin{solution}
    There does not exist such a pari of measures. If the measures existed, and if $\mu = \mu_1 - \mu_2$, then $\mu$ corresponds to a continuous linear function $T$ on $C[0,1]$, such that $Tf = f'(0)$ for any polynomial $f$. But then $T$ cannot be continuous, for if $f_n = (1 - x)^n$, then $\| f_n \|_{L^\infty[0,1]} = 1$ for all $n$, whereas $Tf_n = n$.
\end{solution}
\end{parts}







\newpage
\section*{Day 7: Warm Up Problems}


\newpage
\section{Day 7: Baire Category}

\item (Spring 2020)
  A \textbf{Hamel basis} for a vector space $X$ is a collection $\mathcal{H}\subset X$ of vectors such that $x\in X$ can be written uniquely as a finite linear combination of elements in $\mathcal{H}$. Prove that an infinite dimensional Banach space cannot have a countable Hamel basis. (Hint: otherwise the Banach space would be first category in itself.)

\begin{solution}
  Denote the norm of $X$ by $\norm{\cdot}$. Suppose $\mathcal{H}$ is countable. Then there exists an enumeration $f_{1},f_{2},\ldots$ of $\mathcal{H}$. Let $E_{n}= \text{span}\left\{ f_{1},\ldots,f_{n} \right\}$. By definition of Hamel basis, $X=\cup_{n=1}^{\infty} E_{n}$. It will suffice to show that $E_{n}$ is nowhere dense, as this will imply that $X$---which is a complete metric space with respect to the norm topology---is a countable union of nowhere dense sets, contradicting the \emph{Baire Category theorem}. But this follows because \emph{any} finite dimensional subspace of a Banach space is closed, and if $E_n$ itself had nonempty interior, then it would contain a neighborhood of the origin, which is absorbing, and thus we would conclude that $E_n = X$.

  Let us argue in slightly more detail some steps of this argument:

  \noindent \textit{Lemma 1:} Any finite dimensional subspace $Y$ of a Banach space $X$ is closed.
  \begin{proof}
  	Since $Y$ is finite dimensional, it has some basis $\{ e_1, \dots, e_n \}$. If we define a norm
  	%
  	\[ \| \sum a_i e_i \|_Y = \left( \sum |a_i|^2 \right)^{1/2}, \]
  	%
  	then $Y$ is a Banach space with this norm, because it is easily seen to be isometric to $\RR^n$ or $\CC^n$ (depending on the underlying field). But all norms on a finite dimensional vector space are equivalent, so that $\| \cdot \|_Y$ is comparable to $\| \cdot \|_X$, and this shows $Y$ is a Banach space under the norm induced by $X$. But this implies that $Y$ is closed in $X$, since any sequence $\{ y_n \}$ converging to an element $x \in X$ is Cauchy, and thus converges in $Y$.
  \end{proof}

  \noindent \textit{Lemma 2:} If $Y$ is a proper subspace of a Banach space $X$, then $Y$ has nonempty interior in $X$.
  \begin{proof}
  	For each $x_0 \in X$ and $\varepsilon > 0$, let
  	%
  	\[ B_\varepsilon(x_0) = \{ x \in X: \| x - x_0 \| < \varepsilon \}. \]
  	%
  	Suppose that there exists $y_0 \in Y$ and $\varepsilon > 0$ such that $B_\varepsilon(y_0) \subset Y$. Then
  	%
  	\[ B_\varepsilon(0) = B_\varepsilon(y_0) - y_0 \subset Y - y_0 = Y. \]
  	%
  	If $r > 0$, then
  	%
  	\[ B_r(0) = (r / \varepsilon) \cdot B_\varepsilon(0) \subset (r / \varepsilon) \cdot Y = Y. \]
  	%
  	Thus $Y$ contains every ball centered at the origin. But this clearly means $X = Y$.
  \end{proof}
\end{solution}

\question (Fall 2017) Let $f_n$ be a sequence of real functions on $\RR$ such that each $f_n'$ is continuous on $\RR$. Suppose that as $n \to \infty$, $f_n$ converges to a function $f: \RR \to \RR$ pointwise, and $f_n'$ converges to a function $g$ pointwise.

Prove that there exists a non-empty interval $(a,b)$ and a constant $L < \infty$ such that
%
\[ |f(x) - f(y)| \leq L |x-y|. \]
%
Hint: Consider the sets $K_c = \{ x: \sup_n |f_n'(x)| \leq c \}$.
\begin{solution}
    For each $x$, $f_n'(x) \to f(x)$, so that the sequence of quantities $\{ f_n(x) \}$ is bounded. It follows that if
    %
    \[ K_c = \{ x : \sup_n |f_n'(x)| \leq c \}, \]
    %
    then $\bigcup_{c = 1}^\infty K_c = \RR$. Since the supremum of continuous functions is lower semi-continuous, each $K_c$ is a closed set. And thus the \emph{Baire category theorem} implies that there exists $L$ such that $K_L$ contains an interval. Thus there is an interval $(a,b)$ such that for all $x \in (a,b)$, $\sup |f_n'(x)| \leq L$. Thus by the mean value theorem
    %
    \[ |f(x) - f(y)| = \limsup_{n \to \infty} |f_n(x) - f_n(y)| \leq \limsup_{n \to \infty} |x - y| \| f_n \|_{L^\infty(a,b)} \leq L |x - y|. \]
\end{solution}

\item (Fall 2016) Show that there is a continuous real valued function on $[0,1]$ that is not monotone on any open interval $(a,b)\subset [0,1]$.

% \begin{solution}
%   Suppose not. Let $E_{q,n}= \left\{ f\in C([0,1]): f \text{ monotone on }(q-1/n,q+1/n) \right\}.$ Then
%   \begin{equation}\label{eq:big-union}
%     C([0,1]) = \bigcup_{q\in \Q}\bigcup_{n\geq 1}E_{q,n}.
%   \end{equation}
%   We claim that each $E_{q,n}$ is closed, indeed suppose $(f_{k})$ is a sequence of monotone function on $(q-1/n,q+1/n)$ such that $f_{k}\to f$ uniformly. Then there exists a subsequence $(k_{j})$ with $k_{j}\to \infty$ and such that $f_{k_{j}}$ is an increasing function for all $j$ or $f_{k_{j}}$ is a decreasing function for all $j$. Without loss suppose that they are increasing functions. If $x\leq y$ then $f_{k_{j}}(x)-f_{k_{j}}(y)\leq $ for all $j$, and hence sending $j\to \infty$, we have $f(x)\leq f(y)$. Thus $f$ is increasing. This proves that $E_{q,n}$ is closed.
  
%   Since $C([0,1])$ is a complete metric space and  \eqref{eq:big-union} is a countable union of close sets, it follows by Baire's theorem that there exists some $q\in Q$ and $n\geq 1$ such that $E_{q,n}$ has nonempty interior. That is, there exists $f^{*}\in E_{q,n}$ and $\epsilon>0$ such that $\left\{ f\in C([0,1]): \norm{f^{*}-f}_{\infty} < \epsilon\right\}\subset E_{q,n}.$ Therefore every $f$ with $\norm{f^{*}-f}_{\infty}<\epsilon$ is monotone on the interval $(q-1/n,q+1/n)$. But this is absurd, since any function can by approximated uniformly by zig-zag functions (draw a picture). 

% \end{solution}

\begin{solution}
  Suppose not. Then we can write $C([0,1])$ as the following countable union:
  \begin{equation}
    C([0,1]) = \bigcup_{a,b\in \Q, a<b}E_{a,b}.
  \end{equation}
  where $E_{a,b}= \left\{ f\in C([0,1]): f \text{ monotone on }(a,b) \right\}.$
  
  We claim that each $E_{a,b}$ is closed. Indeed suppose $(f_{k})$ is a sequence of monotone function on $(a,b)$ such that $f_{k}\to f$ uniformly. Then there exists a subsequence $(k_{j})$ with $k_{j}\to \infty$ and such that $f_{k_{j}}$ is an increasing function for all $j$ or $f_{k_{j}}$ is a decreasing function for all $j$. Without loss suppose that they are increasing functions. If $x\leq y$ then $f_{k_{j}}(x)-f_{k_{j}}(y)\leq $ for all $j$, and hence sending $j\to \infty$, we have $f(x)\leq f(y)$. Thus $f$ is increasing. Thus $E_{a,b}$ is closed. This proves the claim.
  
  The $C[0,1]$ is a complete metric space which is the countable union of closed sets. It follows by Baire's theorem that there exists some $a,b\in Q$ with $a<b$ such that $E_{a,b}$ has nonempty interior. That is, there exists $f^{*}\in E_{a,b}$ and $\epsilon>0$ such that $\left\{ f\in C([0,1]): \norm{f^{*}-f}_{\infty} < \epsilon\right\}\subset E_{a,b}.$ Therefore every $f$ with $\norm{f^{*}-f}_{\infty}<\epsilon$ is monotone on the interval $(a,b)$. But this is absurd, since any function can by approximated uniformly by ``zig-zag functions''.

\end{solution}

\item (Spring 2014)
  Does there exist a sequence of continuous functions $f_{n}:[0,1]\to \R$ such that $f_{n}\to \chi_{\Q}$ pointwise?

\begin{solution}
  No. Suppose such a sequence $(f_{n})$ exists. Then we can write
  \begin{equation*}
    \left[ 0,1 \right] = \bigcup_{k\geq 1} \left\{ x\in [0,1]: \left| f_{n}(x)-f_{m}(x) \right| \leq \frac{1}{4} \text{ for all }m,n\geq k\right\}.
  \end{equation*}

  Since the right hand side is a countable union of closed sets, it follow by the Baire Category theorem taht there exists some $k_{0}$ and some $(a,b)$ such that for all $x\in (a,b)$,
  \begin{equation*}
    |f_{n}(x)-f_{m}(x)| \leq \frac{1}{4}\quad\text{ for all }n,m\geq k_{0}.
  \end{equation*}
  Sending $m\to \infty$,
  \begin{equation*}
    |f_{n}(x)-f(x)| \leq \frac{1}{4}\quad\text{ for all }n\geq k_{0}.
  \end{equation*}
  Let $n\geq k_{0}$. If $x\in (a,b)\backslash \Q$ then $|f_{n}(x)|\leq \frac{1}{4}$. But if $x\in (a,b)\cap \Q$ then $|f_{n}(x)-1|\geq \frac{1}{4}$, so that $|f_{n}(x)|\geq \frac{3}{4}$. But this contradicts the continuity of $f_{n}$.
\end{solution}


\item (Fall 2014)
  Let $X,Y$ be Banach spaces and $\left\{ T_{j,k}: j,k\in \mathbb{N} \right\}$ be a set of bounded linear transformations $X\to Y$. Suppose for each $k$, there exists $x\in X$ such that $\sup \left\{ \norm{T_{j,k}x}:  j\in \mathbb{N} \right\} = \infty.$ Then there is an $x\in X$ such that $\sup \left\{ \norm{T_{j,k} x} : j\in \mathbb{N}\right\} = \infty$ for all $k.$
  \begin{solution}
    Proof by contrapositive. Suppose that the conclusion does not hold: that is, suppose that for all $x\in X$ there exists some $k$ such that $\sup \{\norm{T_{j,k_{0}}x}:j\in\mathbb{N}\}<\infty$. Then we can write
\begin{equation*}
  X=\bigcup_{k= 1}^{\infty} \bigcup_{N= 1}^{\infty} \bigcap_{j= 1}^{\infty} \left\{ x\in X: \norm{T_{j,k}x}\leq N \right\}.
\end{equation*}
By the Baire Category theorem, there exists $k_{0},N_{0}\geq 1$ and some ball $\bar{B}_{r}(x_{0})= \{x\in X : \norm{x-x_0}\leq r\}$ with $r>1$ such that $$\sup_{j\geq 1}\norm{T_{j,k_{0}}x}\leq N_{0}$$ for all $x\in \bar{B}_r(x_0)$.

It will suffice to show that for this choice of $k_{0}$, we have $\sup \{\norm{T_{j,k_{0}}x}: j\in \mathbb{N}\}<\infty$ for all $x\in X$.

Let $x\in X$. There are two cases:

\textit{Case 1.} Suppose $\norm{x}\leq 1$. Then $rx+x_{0}\in \bar{B}_{r}(x_{0})$, and hence
\begin{align*}
  \norm{T_{j,k_{0}}x}
  &= \frac{1}{r}\norm{T_{j,k_{0}}(rx)}\\ 
  &=  \frac{1}{r} \norm{T_{j,k_{0}}(rx + x_{0})-T_{j,k_{0}}x_{0}}\\ 
  &\leq \frac{1}{r}\norm{T_{j,k_{0}}(rx + x_{0})} +\frac{1}{r}\norm{T_{j,k_{0}}x_{0}}\\ 
  &\leq 2N_{0}/r
\end{align*}
for all $j$.

\textit{Case 2.} Suppose $\norm{x}>1$. Let $x_1 = x/\norm{x}$. By Case 1,
\begin{equation*}
\norm{T_{j,k_0} x} = \norm{x} \norm{T_{j,k_0}x_1} \leq 2\norm{x}N_0/r
\end{equation*}
for all $j$. 

Combining the two cases, the result follows.
\end{solution}








\newpage
\section*{Day 8: Warm Up Problems}

\newpage
\section{Day 8: Intro to Distribution}

\item (Fall 2020)
Consider the function $f:\R^{2}\to \R$ given by $f(x,y)=|x|$. Find $\left( \frac{\partial ^{2}}{\partial x^{2}}+ \frac{\partial ^{2}}{\partial y^{2}} \right)f$, where the derivative is taken in the sense of distributions.

\begin{solution}
  Let $\phi\in C_{c}^{\infty}(\R^{2})$. By definition of distributional derivative, 
  \begin{align*}
    \Big\langle  \left( \frac{\partial ^{2}}{\partial x^{2}}+ \frac{\partial ^{2}}{\partial y^{2}} \right)f, \phi\Big\rangle
    &= \Big\langle f, \left(\frac{\partial ^{2}}{\partial x^{2}}+ \frac{\partial ^{2}}{\partial y^{2}} \right) \phi \Big\rangle\\ 
    &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|x| \phi_{xx}(x,y)+ |x|\phi_{yy}(x,y)dxdy\\
    &= \underbrace{\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|x| \phi_{xx}(x,y)dxdy}_{A}+ \underbrace{\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|x|\phi_{yy}(x,y)dxdy}_{B}.
  \end{align*}
  Integrating by parts,
  \begin{align*}
    A&= \int_{-\infty}^{\infty}\left[\int_{0}^{\infty}x \phi_{xx}(x,y)dx - \int_{-\infty}^{0}x\phi_{xx}(x,y)\right]dy\\
     &= \int_{-\infty}^{\infty}\left[-\int_{0}^{\infty}\phi_{x}(x,y)dx + \int_{-\infty}^{0}\phi_{x}(x,y)\right]dy\\
     &= \int_{-\infty}^{\infty} -\left[ \phi(x,y) \right]^{x=\infty}_{x=0}+ \left[ \phi(x,y) \right]^{x=0}_{x=-\infty}dy\\
     &=2\int_{-\infty}^{\infty}\phi(0,y)dy
  \end{align*}
  Next, by Fubini's theorem,
  \begin{align*}
    B= \int_{-\infty}^{\infty}|x|\int_{-\infty}^{\infty}\phi_{yy}(x,y)dydx = 0
  \end{align*}
  because $\int_{-\infty}^{\infty}\phi_{yy}(x,y)dy = \left[ \phi_{y}(x,y) \right]^{y=\infty}_{y=-\infty}= 0$ for every $x$. It follows from our calcuations for $A$ and $B$ that
  \begin{equation*}
    \Big\langle  \left( \frac{\partial ^{2}}{\partial x^{2}}+ \frac{\partial ^{2}}{\partial y^{2}} \right)f, \phi\Big\rangle = 2\int_{-\infty}^{\infty}\phi(0,y)dy
  \end{equation*}
  for every $\phi$. Therefore $\left( \frac{\partial ^{2}}{\partial x^{2}}+ \frac{\partial ^{2}}{\partial y^{2}} \right)f= 2\delta(x)$ in the sense of distributions, where $\delta$ is the point mass at $x=0$.
\end{solution} 


\question (Spring 2017)
\begin{parts}
    \part Let $f \in L^1(\RR)$ and consider the sequence of distributions $T_n(x) = \sin(nx^2) f(x)$. Show that $\lim_{n \to \infty} T_n = 0$ in the sense of distributions.
    \begin{solution}
        We perform an integration by parts, which is always handy when dealing with oscillatory things. If $\Lambda_n(x) = \sin(nx^2)$, then
        %
        \begin{align*}
            \int \Lambda_n(x) \phi(x)\; dx &= \int \Lambda_n'(x) \frac{\phi(x)}{2n x}\\
            &= - \int \Lambda_n(x) \frac{d}{dx} \left( \frac{\phi(x)}{2nx} \right)\\
            &= - \int \Lambda_n(x) \left( \frac{\phi'(x)}{2nx} - \frac{\phi(x)}{2nx^2} \right).
        \end{align*}
        %
        Now performing a Taylor expansion, we can write
        %
        \[ \frac{\phi'(x)}{2nx} - \frac{\phi(x)}{2nx^2} = \frac{\phi'(0)}{2nx} - \frac{\phi(0)}{2nx^2} - \frac{\phi'(0)}{2nx} + O(1/n) = \frac{\phi'(0)}{2nx} + O(1/n). \]
        %
        But since $1/x$ is odd, and $\sin(nx^2)$ is even, we have
        %
        \[ \int \Lambda_n(x) \frac{\phi'(0)}{2nx}\; dx = 0. \]
        %
        Thus
        %
        \[ \int \Lambda_n(x) \phi(x) \lesssim_\phi O(1/n), \]
        %
        Taking $n \to \infty$ shows that $\Lambda_n$ converges to 0 distributionally.
    \end{solution}
    
    \part Find a distribution $T \in \mathcal{D}'(\RR)$ such that $T_n = \sin(nx^2)T$ does not converge to $0$ in the sense of distributions as $n \to \infty$.
    \begin{solution}
        One can find a distribution $T$ such that for $\phi$ supported away from the origin,
        %
        \[ \int \phi(x) (1/x^2)\; dx. \]
        %
        One choice is provided in a previous question. Consider a non-negative test function $\phi$ supported away from the origin (for simplicity, supported on $x > 0$), we have
        %
        \[ \int \sin(nx^2) T(x) \phi(x)\; dx = \int \frac{\sin(nx^2) \phi(x)}{x^2}. \]
        %
        For large $n$, we have $\sin(nx^2) \geq nx^2/2$ on the support of $\phi$, which implies that
        %
        \[ \int \frac{\sin(nx^2) \phi(x)}{x^2} \geq n/2 \int \phi(x)\; dx.  \]
        %
        This clearly does not converge to zero as $n \to \infty$.
    \end{solution}
\end{parts}

\question (Spring 2015)
\begin{parts}
    \part If $f \in C[0,1]$, and the distributional derivative $f'$ of $f$ on $(0,1)$ is in $L^1((0,1))$, prove that
    %
    \[ f(1) - f(0) = \int_0^1 f'(x)\; dx. \]
    \begin{solution}
        In order to relate $f'$ and $f$, since $f'$ is a distributional derivative, we need to integrate it against a test function, which reduces the question to a basic analysis problem. Fix $\delta > 0$ and consider $\phi_\delta \in C_c^\infty[0,1]$ with $\| \phi_\delta \|_\infty \leq 1$, $\phi_\delta(x) = 1$ for $\delta \leq x \leq 1-\delta$, and supported on $\delta/2 \leq x \leq 1 - \delta/2$, and with $\| \phi_\delta' \|_\infty \leq 10/\delta$. Then $\phi_\delta'$ is supported on $\delta/2 \leq x \leq \delta$ and $1-\delta/2 \leq x \leq 1-\delta$. Since $f'$ is integrable, we have
        %
        \[ \int_0^1 f'(x)\; dx = \lim_{\delta \to 0} \int_0^1 f'(x) \phi_\delta(x)\; dx. \]
        %
        Since $f'$ is a distributional derivative, we can apply an integration by parts, from which we conclude that
        %
        \[ \int_0^1 f'(x) \phi_\delta(x)\; dx = - \int_0^1 f(x) \phi_\delta'(x)\; dx. \]
        %
        We can write
        %
        \[ \int_0^1 f(x) \phi_\delta'(x)\; dx = \int_0^{\delta} f(x) \phi_\delta'(x)\; dx + \int_{1-\delta} f(x) \phi_\delta'(x)|; dx. \]
        %
        Since $f \in C[0,1]$, for each $\varepsilon > 0$, there exists $\delta_0 > 0$ such that for $x,y \in [0,1]$ with $|x-y| \leq \delta_0$, $|f(x) - f(y)| \leq \varepsilon$. If $\delta \leq \delta_0$, this implies that
        %
        \[ \left| \int_0^\delta [f(x) - f(0)] \phi_\delta'(x)\; dx \right| \leq \int_0^\delta \varepsilon |\phi_\delta'(x)| \leq \varepsilon. \]
        %
        Similarily,
        %
        \[ \left| \int_{1 - \delta}^1 [f(x) - f(1)] \phi_\delta'(x)\; dx \right| \leq \int_{1-\delta}^1 \varepsilon |\phi_\delta'(x)| \leq \varepsilon. \]
        %
        The fundamental theorem of calculus implies that
        %
        \[ \int_0^\delta f(0) \phi_\delta'(x) = f(0) [\phi_\delta(\delta) - \phi_\delta(0)] = f(0), \]
        %
        and
        %
        \[ \int_{1-\delta}^1 f(1) \phi_\delta'(x) = f(1) [\phi_\delta(1) - \phi_\delta(1-\delta)] = - f(1). \]
        %
        Putting these calculations together shows that
        %
        \[ \lim_{\delta \to 0} \int_0^1 f(x) \phi_\delta'(x)\; dx = f(0) - f(1), \]
        %
        and thus
        %
        \[ \int_0^1 f'(x)\; dx = - \lim_{\delta \to 0} \int_0^1 f(x) \phi_\delta'(x) = f(1) - f(0). \]
    \end{solution}
    
    \part Let $p \in [1,\infty)$ and let $F \subset C[0,1]$ be such that for each $f \in F$ we have $\| f \|_{L^1[0,1]} \leq 1$ and $\| f' \|_{L^p[0,1]} \leq 1$, where $f'$ is the distributional derivative of $f$. Prove that $F$ is precompact in $C[0,1]$, or find a counter-example.
    \begin{solution}
        Problems about pre-compactness in $C[0,1]$ normally want us to apply the Arzela-Ascoli theorem, so lets try that here. We wish to show there is a constant $C > 0$ such that for each $f \in C[0,1]$,
        %
        \[ \| f \|_\infty \leq C, \]
        %
        (uniform boundedness), and that for each $\varepsilon > 0$, there is $\delta > 0$ such that for $|x - y| \leq \delta$ and $f \in F$, $|f(x) - f(y)| \leq \varepsilon$ (equicontinuity).
        
        Using the last part of the problem, together with H\"{o}lder's inequality, if $p$ and $p^*$ are conjugates, then for $x < y$,
        %
        \[ |f(x) - f(y)| = \left| \int_x^y f'(x)\; dx \right| \leq \| f' \|_p \left( \int_x^y 1 \right)^{1/p^*} = |y - x|^{1/p^*} \| f' \|_p \leq |y - x|^{1/p^*}. \]
        %
        Thus $f$ satisfies a H\"{o}lder condition, uniformly over elements of $f$. This gives equicontinuity.
        
        To show $F$ is uniformly bounded, we must exploit the fact that $\| f \|_1 \leq 1$. Suppose that $\| f \|_\infty \geq M$. Then there exists $x_0 \in [0,1]$ such that $|f(x_0)| \geq M$. Using the H\"{o}lder condition, we find that $|f(x_0)| \geq M/2$ for $|y - x_0| \leq (M/2)^{p^*}$. And so we find that $\| f \|_1 \geq (M/2) \cdot 2 (M/2)^{p^*} \gtrsim_p M^{1 + p^*}$. Since $\| f \|_1 \leq 1$, this means that $M^{1 + p^*} \lesssim_p 1$, and so $M \lesssim_p 1$. This means we have shown that there exists a constant $C_p$ such that for all $f \in F$, $\| f \|_\infty \leq C_p$.
        
        Thus we have shown that the hypothesis of the Arzela-Ascoli theorem applies, and so $F$ is precompact.
    \end{solution}
\end{parts}


\item (Fall 2021, Spring 2016) Prove or disprove:
  \begin{parts}
    \part There exists a distribution $u\in \mathcal{D}(\R)$ so that the restriction to $(0,\infty)$ is given by
    \begin{equation*}
      \langle u, f \rangle = \int_{0}^{\infty}e^{1/x^{2}}f(x)dx
    \end{equation*}
    for all $f\in C^{\infty}(\R)$ which are compactly supported in $(0,\infty)$.
    \begin{solution}
      Part (a): Does not exist. Idea: if such a distribution exists, then there exists $N,C$ such that $$|\langle u,\phi\rangle|\leq C \norm{\phi}_{N}.$$ But we will construct a sequence $\phi_{n}$ so that the left hand side blows up faster than the right hand side. We will want the functions $\phi_{n}$ to get more an more concentrated near 0.
      
      Let $\phi\in C_{c}^{\infty}(0,2)$ such that $\phi(x)=1$ for all $x\in [1/2,1]$. Define $\phi_{n}(x) = \phi(2^{n}x)$. Then $\phi_{n}(x)=1$ for all $x\in [2^{-(n+1)},2^{-n}]=:I_{n}$. We have
      \begin{equation*}
        \langle u, \phi_{n}\rangle \geq \int_{I_{n}} e^{x^{-2}}dx \geq e^{4^{n}}2^{-(n+1)}
      \end{equation*}
      On the other hand,
      \begin{equation*}
        \norm{\phi_{n}}_{N} = \sum_{k\leq N}\norm{\phi_{n}^{(k)}}_{\infty}= \sum_{k\leq N}\norm{2^{kn}\phi^{(k)}(2^{n}x)}_{\infty}\leq 2^{Nn}\norm{\phi}_{N}
      \end{equation*}
      Therefore,
      \begin{equation*}
        \frac{\langle u, \phi_{n}\rangle }{\norm{\phi_{n}}_{N}}\geq \frac{e^{4^{n}}2^{-(n+1)}}{2^{Nn}}\to \infty \text{ as }n\to\infty.
      \end{equation*}
    \end{solution}

    \part There exists a distribution $u\in \mathcal{D}'(\R)$ so that its restriction to $(0,\infty)$ is given by
    \begin{equation*}
      \langle u,f \rangle = \int_{0}^{\infty}x^{-2}e^{i/x^{2}}f(x)dx
    \end{equation*}
    for all $f\in C^{\infty}$ which are compactly supported in $(0,\infty)$.
    \begin{solution}
      Part (b): Yes, such a distribution exists. Let $f\in \mathcal{D}(\R)$ with $\text{supp}(f)\subset (0,\infty)$. Write $x^{-2}e^{ix^{-2}} = \frac{ix}{2}\frac{d}{dx}\left[ e^{ix^{-2}} \right]$, so that
      \begin{align*}
        \langle u , f \rangle &= \int_{0}^{\infty} \frac{d}{dx}\left[ e^{ix^{-2}} \right]\frac{ix}{2}f(x) dx\\
                              &= \int_{0}^{\infty} e^{ix^{-2}}\left(\frac{i}{2}f(x) + \frac{ix}{2}f'(x)\right) dx\\
                              &= \frac{i}{2}\int_{0}^{\infty} e^{ix^{-2}}\left(f(x) + xf'(x)\right) dx.
      \end{align*}
      and it is easy to verify that this formula defines a distribution for all $f\in \mathcal{D}(\R)$.
    \end{solution}
  \end{parts}





\question (Fall 2017) A distribution $T \in \mathcal{S}'(\RR^n)$ is said to be \emph{nonnegative} if $\langle T, \phi \rangle \geq 0$ for every test function $\phi \in \mathcal{S}(\RR^n)$ with $\phi(x) \geq 0$ for all $x \in \RR^n$.
\begin{parts}
    \part Suppose $f \in L^1_{\text{loc}}(\RR^n)$, and let $T_f$ be the distribution defined by $f$. Show that $T_f \geq 0$ if and only if $f \geq 0$ for almost all $x \in \RR^n$.
    \begin{solution}
        Suppose $f < 0$ on a set of positive measure. Then we may find $R > 0$ and $c > 0$ such that the set $E = \{ |x| \leq R: f(x) < -c \}$ has positive measure. Fix $\delta > 0$ and find an open set $U$ contained $E$ with $|U - E| \leq \delta$. Since $U$ is pre-compact, we can find a smooth function $\phi$ compactly supported on a set $V$ such that $f(x) = 1$ for $x \in U$, and $|V - U| \leq \delta$. Then
        %
        \[ \left| \int f(x) \phi(x)\; dx - \int_E f(x)\; dx \right| \leq \int_{V - U} |f(x)|\;dx. \]
        %
        Since $f$ is integrable, and $|V - E| \leq 2\delta$, for any fixed $\varepsilon > 0$, if $\delta$ is suitably small, then
        %
        \[ \int_{V - U} |f(x)|\; dx \leq \varepsilon. \]
        %
        On the other hand,
        %
        \[ \int_E f(x)\; dx \leq -c|E|, \]
        %
        and so
        %
        \[ 0 \leq \int f(x) \phi(x) \leq \varepsilon - c |E|. \]
        %
        Taking $\varepsilon \to 0$ gives a contradiction.
    \end{solution}
    
    \part Show that if $T_n \to T$ in the sense of distributions, and if $T_n \geq 0$ for all $n$, then $T \geq 0$.
    \begin{solution}
        If $\phi \geq 0$, then $\langle T_n, \phi \rangle \geq 0$ for all $n$ since $T_n \geq 0$. But $\langle T, \phi \rangle = \lim_n \langle T_n, \phi \rangle$, which implies that $\langle T,\phi \rangle \geq 0$.
    \end{solution}
    
    \part Suppose $\Phi: \RR \to \RR$ is a $C^2$ function with $\Phi'' \geq 0$ in $\RR$, and let $f \in C^2(\RR^n)$ have compact support. Show that $\Delta(\Phi(f(x)) \geq \Phi'(f(x)) \Delta f(x)$.
    \begin{solution}
        We calculate directly that
        %
        \[ \Delta(\Phi(f(x))) = \Phi'(f(x)) \Delta f + \sum_{i = 1}^n \Phi''(f(x)) \left( \frac{\partial f}{\partial x^i} \right)^2. \]
        %
        Since $\Phi''(f(x)) (\partial f / \partial x^i)^2 \geq 0$, this easily implies the inequality.
    \end{solution}
    
    \part Suppose $f \in C^2(\RR^n)$ has compact support. Show that $\Delta |f| \geq \text{sign}(f(x)) \Delta f(x)$ holds in the sense of distributions. (Hint use (c) with $\Phi(t) = \sqrt{\varepsilon + t^2})$.
    \begin{solution}
        For each $\varepsilon > 0$, if $\Phi_\varepsilon(t) = \sqrt{\varepsilon + t^2}$, then $\Phi_\varepsilon \in C^\infty(\RR)$, and the part (c) implies that
        %
        \[ \Delta(\sqrt{\varepsilon + f^2}) \geq \frac{f \cdot \Delta f}{\sqrt{\varepsilon + f^2}} \]
        %
        Our goal is to take $\varepsilon \to 0$ to show that $\Delta|f| \geq \text{sign}(f) \Delta f$.
        
        As $\varepsilon \to 0$, $\sqrt{\varepsilon + f^2}$ converges distributionally to $|f|$ by the dominated convergence theorem. Given $\phi \in C_c^\infty(\RR^d)$, $\phi(x) \sqrt{\varepsilon + f(x)^2} \to \phi(x) |f(x)|$ pointwise for every $x$, and is upper bounded by $2|\phi(x)| (1 + |f(x)|)$, which is integrable since $\phi$ is compactly supported. Thus
        %
        \[ \int \sqrt{\varepsilon + f(x)^2} \phi(x)\; dx \to \int |f(x)| \phi(x)|; dx. \]
        %
        Similarily, one can argue using the dominated convergence theorem that $f(x) \Delta f(x) / \sqrt{\varepsilon + f(x)^2}$ converges distributionally to $\text{sign}(f(x)) \Delta f(x)$, since it converges pointwise to this function, and is uniformly upper bounded in absolute value by $\Delta f(x)$. Applying (b) shows that $\Delta |f(x)| \geq \text{sign}(f(x)) \cdot \Delta f(x)$.
    \end{solution}
\end{parts}






\item (Spring 2020)
\begin{parts}
  \part Suppose $\Lambda$ is a distribution on $\R^{n}$ such that $\text{supp}(\Lambda)=\left\{ 0 \right\}.$ If $f\in C^{\infty}(\R^{n})$ satisfies $f(0)=0$, does it follow that $f\Lambda = 0$ as a distribution?
  \begin{solution}
    No. Let $n=1$, $f(x)=x$, and $\Lambda=D\delta$. It is clear that $\Lambda$ has support $\left\{ 0 \right\}$. But $\langle f\Lambda, \phi \rangle = \langle \Lambda , x\phi\rangle = \langle \delta, -(x\phi)'\rangle = \langle \delta, -\phi' - x\phi\rangle = -\phi'(0)$. Therefore $f\Lambda = -\delta$ in the sense of distributions, and hence is not the zero distribution.
  \end{solution}

  \part Suppose $\Lambda$ is a distribution on $\R^{n}$ such that $\text{supp}(\Lambda)\subseteq K$, where $K=\left\{ x\in \R^{n}: |x|\leq 1 \right\}$. If $f\in C^{\infty}(\R^{n})$ vanishes on $K$, does it follow that $f\Lambda =0$ as a distribution?
  \begin{solution}
    This part is true. Since $\text{supp}(\Lambda)$ is compact, it follows that $\Lambda$ is of finite order. Then $\Lambda = \sum_{\alpha\in \mathcal{A}}D^{\alpha}g_{\alpha}$ where $g_{\alpha}$ is a continuous function for each $\alpha\in \mathcal{A}$ and $\mathcal{A}$ is a finite set of multi-indices. Let $N = \max\left\{|\alpha|: \alpha\in \mathcal{A}\right\}$.

    Let $\phi\in C^{\infty}_{c}(\R^{n})$ and let $M>0$ such that $\text{supp}(\phi)\subseteq \left\{ x: |x|\leq M \right\}$. Then
    \begin{align*}
      \langle f\Lambda, \phi \rangle
      &=\langle \Lambda, f\phi \rangle\\
      &=\int_{\R^{n}} \sum_{\alpha\in \mathcal{A}}(-1)^{|\alpha|}g_{\alpha}(x)D^{\alpha}(f\phi)(x)dx\\
      &=\sum_{\alpha \geq \beta }(-1)^{|\alpha|}c_{\alpha\beta}\int_{\R^{n}} g_{\alpha}(x)(D^{\alpha-\beta}f)(D^{\beta}\phi)dx
      &&\text{by Leibniz Formula}\\
      &=\sum_{\alpha \geq \beta }(-1)^{|\alpha|}c_{\alpha\beta}\int_{|x|>1} g_{\alpha}(x)(D^{\alpha-\beta}f)(D^{\beta}\phi)dx
      &&\text{since $D^{\alpha-\beta}f(x)=0$ for all $x\in K$}\\
      &=\lim_{\epsilon\to 0^{+}}\sum_{\alpha \geq \beta }(-1)^{|\alpha|}c_{\alpha\beta}\int_{|x|>1+\epsilon} g_{\alpha}(x)(D^{\alpha-\beta}f)(D^{\beta}\phi)dx
    \end{align*}
    We claim this limit is zero. To see this, we first define a cutoff function $\eta_1(x)\in C_c^{\infty}(\R^n)$ with $0\leq \eta_1\leq 1$ such that $\eta_1(x)= 0$ if $|x|<1$, and $\eta_1(x)= 1$ if $2\leq |x|\leq M$. Next, for each $\epsilon\in (0,1)$, define $\eta_{\epsilon}\in C_c^{\infty}$ satisfying
    \begin{equation*}
      \eta_{\epsilon}(x) = \left\{ \begin{array}{l@{\quad:\quad}l}
                                   0  & |x|\leq 1+\epsilon/2 \\
                                   \eta_{1}\left(\frac{2x}{\epsilon} - \frac{2}{\epsilon}\right) & 1+\epsilon/2 < |x| \leq 1+\epsilon \\
                                   1  & 1+\epsilon <|x|\leq M \\
                                   \text{smooth} & |x|>M
                                   \end{array}\right.
    \end{equation*}
    (Draw a picture for the $n=1$ case. This isn't as complicated as it looks.) 
    
    
    By the chain rule, $|D^{\alpha} \eta_{\epsilon}| \leq C\cdot 2^{|\alpha|}\epsilon^{-|\alpha|}$ where $C<\infty$ is an upper bound for the first order derivatives of $\eta_1$. Then taking $\phi_{\epsilon} = \eta_{\epsilon} \phi$,
    
    
    \begin{align*}
      \langle f\Lambda, \phi_{\epsilon}\rangle
      &= \sum_{\alpha \geq \beta }(-1)^{|\alpha|}c_{\alpha\beta}\int_{1+ \epsilon\geq |x|>1+\frac{\epsilon}{2}} g_{\alpha}(x)(D^{\alpha-\beta}f)(D^{\beta}\phi_{\epsilon})dx\\
      &\quad +  \sum_{\alpha \geq \beta }(-1)^{|\alpha|}c_{\alpha\beta}\int_{|x|>1+\epsilon} g_{\alpha}(x)(D^{\alpha-\beta}f)(D^{\beta}\phi)dx
    \end{align*}
    On the other hand, $\text{supp}(f\phi_{\epsilon}) \subset \R^n\backslash B(0,1+\frac{\epsilon}{2})$. Therefore $f\phi_{\epsilon}\in \mathcal{D}(K^{c})$ and hence by definition of distribution support, $\langle f\Lambda, \phi_{\epsilon}\rangle = \langle \Lambda, f\phi_{\epsilon}\rangle =0$. Therefore
    \begin{align*}
      \langle f\Lambda, \phi \rangle
      &= \lim_{\epsilon\to 0^{+}}-\sum_{\alpha \geq \beta }(-1)^{|\alpha|}c_{\alpha\beta}\int_{1+ \epsilon\geq |x|>1+\frac{\epsilon}{2}} g_{\alpha}(x)(D^{\alpha-\beta}f)(D^{\beta}\phi_{\epsilon})dx
    \end{align*}
    Therefore
    \begin{align} \label{dist-up-bound}
      |\langle f\Lambda, \phi \rangle|
      &\leq \limsup_{\epsilon\to 0^{+}}\sum_{\alpha \geq \beta } |c_{\alpha\beta}|\int_{1+ \epsilon\geq |x|>1+\frac{\epsilon}{2}} \left| g_{\alpha}(x)(D^{\alpha-\beta}f)(D^{\beta}\phi_{\epsilon})\right|dx.
    \end{align}
    In particular, for each $\epsilon>0$, using the Leibniz formula again
    \begin{align*}
    &\sum_{\alpha \geq \beta } |c_{\alpha\beta}|\int_{1+ \epsilon\geq |x|>1+\frac{\epsilon}{2}} \left| g_{\alpha}(D^{\alpha-\beta}f)(D^{\beta}\phi_{\epsilon})\right|dx \\
    &\leq \sum_{\alpha \geq \beta } |c_{\alpha\beta}|\int_{1+ \epsilon\geq |x|>1+\frac{\epsilon}{2}} \left| g_{\alpha}(D^{\alpha-\beta}f)\right|\left|\sum_{\beta\geq \gamma} c_{\gamma} D^{\gamma}\phi  D^{\gamma-\beta}\eta_{\epsilon} \right|dx \\
    &\leq \sum_{\alpha \geq \beta }  \sum_{\beta\geq \gamma} |c_{\alpha\beta}| |c_{\gamma}|\frac{C}{(\epsilon/2)^N}\int_{1+ \epsilon\geq |x|>1+\frac{\epsilon}{2}} \left| g_{\alpha}(D^{\alpha-\beta}f)\right| \left| D^{\gamma}\phi \right|dx.
    \end{align*}
    Therefore as $\epsilon\to 0^+$, we have $\frac{1}{(\epsilon/2)^N}\int_{1+ \epsilon\geq |x|>1+\frac{\epsilon}{2}} \left| g_{\alpha}(D^{\alpha-\beta}f)\right| \left| D^{\gamma}\phi \right|dx \to 0$ since $(D^{\alpha-\beta}f)(x)=0$ for all $x$ with $|x|=1$. Then it follows by \eqref{dist-up-bound} that $\langle f\Lambda, \phi\rangle = 0$. Since $\phi$ was arbitrary, $f\Lambda=0$ in the sense of distributions.
  \end{solution}
\end{parts}





\newpage
\section*{Day 9: Warm Up Problems}

\newpage 
\section{Day 9: Fourier Analysis + Distribution Theory}

\question (Spring 2017) Let $f \in L^1(\RR^n)$ be a function all of whose distributional derivatives $D^\alpha f$ of order $|\alpha| = m$ also belong to $L^1(\RR^n)$. Show that if $m > n$, then $f \in C(\RR^n)$.
\begin{solution}
    The duality between differentiation and multiplication by polynomials in the Fourier transform, and the fact that the Fourier transform of an $L^1$ function is in $L^\infty$, implies that for each $\alpha$ with $|\alpha| = m$,
    %
    \[ |\xi^\alpha \widehat{f}| \in L^\infty(\RR^n). \]
    %
    We have $|\sum_{|\alpha| = m} |\xi^\alpha| \sim |\xi|^m$, so we conclude that $|\xi^m \widehat{f}| \in L^\infty$. Since $f \in L^1$, $\widehat{f} \in L^\infty$, so that we have a bound
    %
    \[ |\widehat{f}(\xi)| \lesssim \frac{1}{1 + |\xi|^m}. \]
    %
    Since $m > n$, this means $\widehat{f}$ is also integrable. The Fourier inversion formula thus implies that $f$ agrees almost everywhere with a bounded, continuous function.
\end{solution}

\question (Fall 2019)
    Let $s \in \RR$, and let $H^s(\RR)$ be the Sobolev space on $\RR$ with the norm
    %
    \[ \| u \|_{(s)} = \left( \int_{\RR} (1 + |\xi|^2)^s |\widehat{u}(\xi)|^2\; d\xi \right)^{1/2} \]
    %
    where $\widehat{u}$ is the Fourier transform of $u$. Let $r < s < t$ be real numbers. Prove that for every $\varepsilon > 0$ there is $C > 0$ such that
    %
    \[ \| u \|_{(s)} \leq \varepsilon \| u \|_{(t)} + C \| u \|_{(r)} \]
    %
    for every $u \in H^t(\RR)$.
\begin{solution}
    Intuitively, we only need the $H^t$ norm to control $H^s$ for large frequencies, since the $H^r$ norm gives enough control over low frequencies. We can let the low frequencies we apply the $H^r$ norm to be arbitrarily large, which means we depend arbitrarily in an arbitrarily small way on the $H^t$ norm. Thus we fix $R > 0$, and write
    %
    \[ \int (1 + |\xi|^2)^s |\widehat{u}(\xi)|^2\; d\xi = \int_{|\xi| \leq R} (1 + |\xi|^2)^s |\widehat{u}(\xi)|^2 + \int_{|\xi| > R} (1 + |\xi|^2)^s |\widehat{u}(\xi)|^2. \]
    %
    Now
    %
    \[ \int_{|\xi| \leq R} (1 + |\xi|^2)^s |\widehat{u}(\xi)|^2\; d\xi \leq R^{s-r} \int_{|\xi| \leq R} (1 + |\xi|^2)^r |\widehat{u}(\xi)|^2\; d\xi \leq R^{s-r} \| u \|_{(r)}^2, \]
    %
    and
    %
    \[ \int_{|\xi| \geq R} (1 + |\xi|^2)^s |\widehat{u}(\xi)|^2\; d\xi \leq R^{s-t} \int_{|\xi| \geq R} (1 + |\xi|^2)^t |\widehat{u}(\xi)|^2\; d\xi \leq R^{s-t} \| u \|_{(t)}^2. \]
    %
    Thus we find that
    %
    \[ \| u \|_{(s)} \leq \sqrt{R^{s-r} \| u \|_{(r)}^2 + R^{s-t} \| u \|_{(t)}^2} \leq (2R^{s-r})^{1/2} \| u \|_{(r)} + (2R^{s-t})^{1/2} \| u \|_{(t)}.  \]
    %
    Given $\varepsilon > 0$, the proof is completed by picking $R \geq (2/\varepsilon^2)^{1/(t-s)}$.
\end{solution}


\begin{solution}
(Alternate solution using Young's inequality). Let $A = 1+|\xi|^{2}$. 


\textit{Claim 1:} Suppose $r<s<t$. Then for each $\epsilon>0$, there exists $C>0$ such that $A^{s} \leq \epsilon A^{t}+ C A^{r}$.

To prove this claim, we will use Young's inequality for products with $\epsilon$. Recall Young's inequality states that if $a,b\geq 0$ and $p,q>1$ with $\frac{1}{p}+\frac{1}{q}=1$ then
\begin{equation*}
  ab\leq \frac{a^{p}}{p}+\frac{b^{q}}{q}.
\end{equation*}
For any $\epsilon>0$ we may write $ab= ((p\epsilon)^{1/p}a)\cdot \frac{b}{(p\epsilon)^{1/p}}$, which gives
\begin{equation}\label{eq:youngs-ineq-with-epsilon}
  ab\leq \epsilon a^{p} + (p\epsilon)^{-q/p}\frac{b^{q}}{q}.
\end{equation}
Let $\beta=\frac{t(s-r)}{t-r}$, $p=t/\beta$ and $q=\frac{p}{p-1}= \frac{t}{t-\beta} =\frac{t-r}{t-s}$. Then $p,q>1$ and $\frac{1}{p}+\frac{1}{q}=1$. Applying inequality \eqref{eq:youngs-ineq-with-epsilon} with $a=A^{\beta}$ and $b=A^{s-\beta}$, we have:
\begin{align*}
  A^{s} &= A^{\beta}A^{s-\beta}\\ 
        &\leq \epsilon A^{\beta t/\beta} + \frac{1}{q(p\epsilon)^{q/p}} A^{(s-\beta) q}\\ 
        &=\epsilon A^{t} + \frac{1}{q(p\epsilon)^{q/p}} A^{(s-\beta) q}\\
        &=\epsilon A^{t} +  \frac{1}{q(p\epsilon)^{q/p}} A^{r}.
\end{align*}
This proves Claim 1.

Next we apply Claim 1 with $r/2<s/2<t/2$, which gives
\begin{equation*}
  A^{s/2}\leq \epsilon A^{t/2}+ C A^{r/2}.
\end{equation*}
Multiplying both sides by $|\hat{u}|$ and making the substitution $A=1+|\xi|^{2}$, we have
\begin{equation*}
  (1+|\xi|^{2})^{s/2}|\hat{u}(\xi)|\leq \epsilon (1+|\xi|^{2})^{t/2}|\hat{u}(\xi)|+ C (1+|\xi|^{2})^{r/2}|\hat{u}(\xi)|.
\end{equation*}
Therefore using Minkowski's inequality,
\begin{align*}
    \norm{(1+|\xi|^{2})^{s/2}|\hat{u}(\xi)|}_{L^{2}}
  &\leq \norm{\epsilon (1+|\xi|^{2})^{t/2}|\hat{u}(\xi)|+ C (1+|\xi|^{2})^{r/2}|\hat{u}(\xi)|}_{L^{2}}\\ 
  &\leq  \epsilon\norm{(1+|\xi|^{2})^{t/2}|\hat{u}(\xi)|}_{L^{2}}+ C\norm{ (1+|\xi|^{2})^{r/2}|\hat{u}(\xi)|}_{L^{2}}.
\end{align*}
This is exactly the inequality we needed to show, so we are done.

\end{solution}
\question (Fall 2019) Let $f \in L^2(\RR)$. Define
%
\[ g(x) = \int_{-\infty}^\infty f(x-y) f(y)\; dy \]
%
Show that there exists a function $h \in L^1(\RR)$ such that
%
\[ g(\xi) = \int_{-\infty}^\infty e^{- i \xi x} h(x)\; dx, \]
%
i.e. $g$ is a Fourier transform of a function in $L^1(\RR)$. Hint: The following formal argument may be helpful:
%
\[ \widehat{g}(x) = \widehat{f * f}(x) = \widehat{f}(x)^2, \]
%
where $*$ denotes convolution, and $\widehat{\cdot}$ denotes the Fourier transform.
\begin{solution}
    If one only knows Fourier analysis in $L^1(\RR)$, the argument in the hint may be justified using the fact that $L^1(\RR) \cap L^2(\RR)$ is dense in $L^2(\RR)$, together with some continuity arguments.
    
    Young's convolution inequality implies that $g$ is a continuous function in $L^\infty(\RR)$. One cannot take the Lebesgue integral defining the Fourier transform for $g$, but one may take the distributional Fourier transform. If we consider a family of functions $\{ f_n \}$ lying in $L^2(\RR) \cap L^1(\RR)$, which converge to $f$ in $L^2(\RR)$ as $n \to \infty$, then Young's convolution inequality implies that $f_n * f_n$ converges to $f * f$ in $L^\infty(\RR)$. In particular, $f_n * f_n$ converges to $f * f$ distributionally. By continuity of the Fourier transform, $\widehat{f_n * f_n}$ converges to $\widehat{f * f}$, and $\widehat{f_n * f_n} = \widehat{f_n}^2$, which converges distributionally to $\widehat{f}^2$ (by Cauchy Schwartz it actually converges in $L^1(\RR)$, which is a stronger statement). Thus we conclude that $\widehat{g} = \widehat{f * f} = \widehat{f}^2$. But since the Fourier transform of $g$ lies in $L^1$, we know that $g$ acts as a distribution in exactly the same way as the distribution
    %
    \[ \xi \mapsto \int \widehat{f}(x)^2 e^{2 \pi i \xi \cdot x}. \]
    %
    Since $g$ is a continuous function, and the integral above defines a continuous function, it follows that for all $\xi$,
    %
    \[ g(\xi) = \int \widehat{f}(x)^2 e^{2 \pi i \xi \cdot x}. \]
\end{solution}


\question (Fall 2015) Let $f$ be a tempered distribution on $\RR$ with Fourier transform
%
\[ \widehat{f}(\xi) = 1 + \xi^{12} + \sin \xi + \text{sign}(\xi). \]
%
Find $f$ and $f'$ (specify the definition of the Fourier transform you are using).
\begin{solution}
    I will be using the Fourier transform defined by
    %
    \[ \widehat{f}(\xi) = \int f(x) e^{-2 \pi i \xi \cdot x}\; d\xi. \]
    %
    The problem is really asking us to compute the inverse Fourier transform of $1$, $\xi^{12}$, $\sin \xi$, and $\text{sign}(\xi)$ individually.
    
    If $\delta$ is the Dirac delta function at the origin, then
    %
    \[ \widehat{\delta}(\xi) = \int e^{-2 \pi i \xi \cdot x} \delta(x)\; dx = 1. \]
    %
    Thus $\widehat{\delta} = 1$.
    
    To find a function whose Fourier transform is $\xi^{12}$, we recall the applying linear differential operators with constant coefficients on one side of the Fourier transform is the same as multiplying by a polynomial on the other side. If we compute the right constants, we find that the tempered distribution $\Lambda$ with $\widehat{\Lambda}(\xi) = \xi^{12}$ is a multiple of the tempered distribution mapping a test function $\phi$ to a constant multiplier $D^{12} \phi(0)$. If we compute the relationship carefully, we find that it is actually $\phi \mapsto (1/2\pi i)^{12} D^{12} \phi(0)$.
    
    Next, we find a function whose Fourier transform is $\sin \xi$. A simple way to compute this is to write
    %
    \[ \sin \xi = [e^{i \xi} - e^{-i \xi}]/2i. \]
    %
    Since $e^{i \xi}$ is the Fourier transform of the Dirac delta $\delta_{-1/2\pi}$ at $-1/2\pi$, and $e^{-i \xi}$ is the Fourier transform of the Dirac delta $\delta_{1/2\pi}$ at $1/2\pi$. Thus if we write $\Gamma(x) = \delta_{-1/2\pi}/2i - \delta_{1/2\pi}/2i$, then $\widehat{\Gamma}(\xi) = \sin \xi$.
    
    Finally, we find a function whose Fourier transform is $g(\xi) = \text{sign}(\xi)$. The distributional derivative of $g(\xi)$ is the Dirac delta at the origin. Since the Dirac delta at the origin is equal to a constant function $1$, this implies that if $h(x)$ is the distribution $(-1/2\pi i) \text{p.v}(1/x)$, where we interpret $\text{p.v}(1/x)$ as the distribution
    %
    \[ \phi \mapsto \lim_{\delta \to 0} \int_{|x| \geq \delta} \phi(x) / x\; dx. \]
    %
    Then $-2 \pi i x h(x) = 1$, and so the symmetry of polynomial multiplication and differentiation implies that the distributional derivative of $g(\xi) - \widehat{h}(\xi)$ is equal to zero. This implies that $g(\xi) - \widehat{h}(\xi) = C$ for some constant $C$. If $\phi(x) = e^{- \pi |x|^2}$ is a Gaussian, then $\widehat{\phi}(\xi) = \phi(\xi)$, then
    %
    \[ \int [g(\xi) - \widehat{h}(\xi)] \phi(\xi) = C, \]
    %
    and we calculate directly that, first using the symmetry of the Gaussian,
    %
    \[ \int g(\xi) \phi(\xi)\; d\xi = \int \text{sign}(\xi) \phi(\xi)\; d\xi = 0,  \]
    %
    and the multiplication formula for the Fourier transform, together with the symmetry of the Gaussian, shows that
    %
    \[ \int \widehat{h}(\xi) \phi(\xi) = \int h(x) \phi(x)\; dx = \lim_{\delta \to 0} \int_{|x| \geq \delta} \phi(x) / (-2\pi i x) = 0. \]
    %
    Thus $C = 0$, and so $\widehat{h}(\xi) = \widehat{g}(\xi)$.
    
    In conclusion, we have found that $f = \delta_0 + \Lambda + \Gamma + h$, where each of these terms are calculated explicitly above.
    
    We finish by giving the rough idea of how to calculate the distributional derivative of $f$. One approach is to multiply by a power of $\xi$ on the Fourier transform side, and then take a Fourier transform, but we choose to work directly from the definition of the distributional derivative.
    
    First, we calculate directly using the definition of the distributional derivative that for any test function $\phi$,
    %
    \[ \int \delta_0'(x) \phi(x)\; dx = - \int \delta_0(x) \phi'(x)\; dx = - \phi'(0). \]
    %
    Thus $\delta_0'$ maps a test function $\phi$ to $-\phi'(0)$. Similarily, the derivative of $\delta_{-1/2\pi}$ maps $\phi$ to $-\phi'(-1/2\pi)$, and the derivative of $\delta_{1/2\pi}$ maps $\phi$ to $-\phi'(1/2\pi)$. This allows us to calculate the distributional derivative of $\Gamma$ simply. For the distribution $u: \phi \mapsto D^{12} \phi(0)$ we calculate
    %
    \[ \int u'(x) \phi(x)\; dx = -\int u(x) \phi'(x)\; dx = - D^{12} \phi'(0) = - D^{13}(0). \]
    %
    This enables us to calculate the distributional derivative of $\Lambda$. Finally,
    %
    \[ \int \text{p.v}(1/x)' \cdot \phi(x)\; dx = -\int \text{p.v}(1/x) \phi'(x)\; dx = - \lim_{\delta \to 0} \int_{|x| \geq \delta} (1/x) \phi'(x); dx. \]
    %
    Integration by parts shows that
    %
    \[ \int_\delta^\infty (1/x) \phi'(x)\; dx = - \phi(\delta) / \delta + \int_\delta^\infty (1/x^2) \phi(x)\; dx \]
    %
    and
    %
    \[ \int_{-\infty}^{-\delta} (1/x) \phi'(x)\; dx = -\phi(-\delta)/\delta + \int_{-\infty}^\delta (1/x^2) \phi(x)\; dx. \]
    %
    Thus
    %
    \begin{align*}
        \lim_{\delta \to 0} - \int_{|x| \geq \delta} (1/x) \phi'(x)\; dx &= \lim_{\delta \to 0} \frac{\phi(\delta) + \phi(-\delta)}{\delta} - \int_{|x| \geq \delta} (1/x^2) \phi(x)\; dx\\
        &= \lim_{\delta \to 0} \frac{2 \phi(0)}{\delta} - \int_{|x| \geq \delta} (1/x^2) \phi(x)\; dx.
    \end{align*}
    %
    Thus if we define the distribution $\text{f.p}(1/x^2)$ (the finite part distribution) as mapping a test function $\phi$ to
    %
    \[ \lim_{\delta \to 0} \int_{|x| \geq \delta} (1/x^2) \phi(x) - 2 \phi(0) / \delta, \]
    %
    then the derivative of $\text{p.v}(1/x)$ is $- \text{f.p}(1/x^2)$. This allows us to calculate the distributional derivative of $h$.
\end{solution}

\question (Fall 2015) Recall that $H^s(\RR^n)$ is the Sobolev space consisting of all tempered distributions $g$ on $\RR^n$ for which the Fourier transform $\widehat{g}$ of $g$ is locally integrable and satisfies
%
\[ \int_{\RR^n} (1 + |\xi|^2)^s |\widehat{g}(\xi)|^2\; d\xi < \infty. \]
%
Let $u$ be a Schwartz function on $\RR^n$ and for $a \in \CC$, let
%
\[ f_a(x) = |x|^a u(x). \]
%
Show that if $\text{Re}(a) > - n/2$ and $s \in [0,\text{Re}(a) + n/2)$, then $f_a \in H^s(\RR^n)$.
\begin{solution}
    We wish to compute the Fourier transform of $f_a$, in order to apply the $H^s$ norm. $f_a$ is the product of a Schwartz function with $|x|^a$, so a useful avenue is to study the distributional derivative of $\Lambda(x) = |x|^a$, since $\widehat{f_a} = \widehat{\Lambda} * \widehat{u}$. The important properties of $\Lambda$ we will use is that $\Lambda$ acts as a $C^\infty$ functions on $\RR^n - \{ 0 \}$, is a radial function, and is \emph{homogeneous} of degree $a$. It follows from general Fourier analysis that $\widehat{\Lambda}$ acts as a $C^\infty$ function on $\RR^n - \{ 0 \}$, is a radial function, and is homogeneous of degree $-a-n$. Thus we can write $\widehat{\Lambda}(\lambda \theta) = \lambda^{-a - d} g(\theta)$ where $g$ is a $C^\infty$ function defined for $|\theta| = 1$, and $\lambda > 0$. Since $\text{Re}(a) > -n/2$, $\text{Re}(-a-n) < -n/2$. This implies that $\widehat{\Lambda}$ decays away from the origin. In particular, $\widehat{\Lambda}$ is $L^2$ away from the origin.

    Now we write
    %
    \[ |x|^a u(x) = T_1 u(x) + T_2 u(x), \]
    %
    where
    %
    \[ \widehat{T_1 u}(\xi) = \int_{|\eta| \leq 1} |\eta|^{-a-n} g(\eta/|\eta|) \widehat{u}(\xi - \eta)\; d\eta, \]
    %
    and
    %
    \[ \widehat{T_2 u}(\xi) = \int_{|\eta| \geq 1} |\eta|^{-a-n} g(\eta/|\eta|) \widehat{u}(\xi - \eta)\; d\eta. \]
    %
    Since $u$ is Schwartz, $\widehat{u}$ is also Schwartz. In particular, for $|\eta| \geq 2|\xi|$, $|\widehat{u}(\xi - \eta)| \lesssim_N |\eta|^{-N}$ for all $N > 0$, which implies that
    %
    \[ \left| \int_{|\eta| \geq 2|\xi|} |\eta|^{-a-n} g(\eta/|\eta|) \widehat{u}(\xi - \eta)\; d\eta \right| \lesssim_M |\xi|^{-M}. \]
    %
    For $1 \leq |\eta| \leq |\xi|/2$, $|\widehat{u}(\xi - \eta)| \lesssim_N |\xi|^{-N}$, which implies that
    %
    \[ \left| \int_{1 \leq |\eta| \leq |\xi|/2} |\eta|^{-a-n} g(\eta/|\eta|) \widehat{u}(\xi - \eta)\; d\eta \right| \lesssim_N |\xi|^{-N}. \]
    %
    For $|\xi|/2 \leq |\eta| \leq 2|\xi|$, we can really only use the bound $|\widehat{u}(\xi - \eta)| \lesssim 1$, which implies that
    %
    \[ \left| \int_{|\xi|/2 \leq |\eta| \leq |\xi|} |\eta|^{-a-n} g(\eta/|\eta|) \widehat{u}(\xi - \eta)\; d\eta \right| \lesssim |\xi|^{-\text{Re}(a)}. \]
    %
    Putting these bounds together shows that
    %
    \[ |\widehat{T_2 u}(\xi)| \lesssim |\xi|^{-\text{Re}(a)}, \]
    %
    which is sufficent to show that $T_2 u \in H^s$ for $0 \leq s \leq \text{Re}(a) + n/2$.
    
    To show $T_1 u \in H^s$, we `cheat' a little, since this is the average of $u$ over `low frequencies' (and a good intuition to have is that low frequencies are very smooth). More precisely, $\widehat{T_1 u} = \widehat{\Psi} * \widehat{u}$, where $\widehat{\Psi}$ is a compactly supported distribution. The Paley-Wiener theorem implies that $\Psi$ is a $C^\infty$ function, and there exists some $N > 0$ such that $|\Psi(x)| \lesssim_N 1 + |x|^N$. And $T_1 u = \Psi \cdot u$, so $T_1 u$ is therefore actually a \emph{Schwartz function}, and thus certainly in $H^s(\RR^n)$.
\end{solution}





\newpage
\section*{Day 10: Warm Up Problems}

\newpage
\section{Day 10: Bonus Questions}

\question (Fall 2017) Let $a_1, \dots, a_n > 0$. Let $f: \RR^n \to \RR$ be defined by
%
\[ f(x) = \frac{1}{1 + \sum |x_i|^{\alpha_i}}. \]
%
Determine for each $p > 0$ whether
%
\[ \int |f(x)|^p\; dx < \infty. \]
\begin{solution}
	Let $E_i = \{ x: x_i^{\alpha_i} \geq x_1^{\alpha_1}, \dots, x_n^{\alpha_n} \}$. On $E$, we have
	%
	\[ |f(x)|^p \sim \frac{1}{1 + |x_i|^{p \alpha_1}}. \]
	%
	Let us determine what values of $p > 0$ we have
	%
	\[ \int_{E_i} \frac{1}{1 + |x_i|^{p \alpha_1}} < \infty.  \]
	%
	Since $\RR^n = \bigcup E_i$, this will suffices to determine the range of $p$ required in the problem. Set $E_i(n) = E_i \cap \{ x : 2^{n-1} \leq x_1 \leq 2^n \}$. For $n > 0$, on $E_i(n)$ we have $|f(x)|^p \sim_p 1/2^{p \alpha_i n}$, and for $n < 0$ we have $|f(x)|^p \sim 1$. It is simple to calculate that $|E_i(n)| \sim 2^n 2^{n(\alpha_i/\alpha_2 + \dots + \alpha_i / \alpha_n)} = 2^{n \alpha_i (1/\alpha_1 + \dots + 1/\alpha_n)}$. For any value of $p$, we thus have
	%
	\[ \sum_{n \leq 0} \int_{E_i(n)} |f(x)|^p \sim \sum_{n < 0} 2^{n \alpha_i (1/\alpha_1 + \dots + 1/\alpha_n)} < \infty. \]
	%
	On the other hand, we have
	%
	\[ \sum_{n > 0} \int_{E_i(n)} |f(x)|^p \sim \sum_{n = 1}^\infty 2^{-p\alpha_i n} 2^{n \alpha_i(1/\alpha_1 + \dots + 1/\alpha_n)} = \sum_{n = 1}^\infty 2^{n \alpha_i(1/\alpha_1 + \dots + 1/\alpha_n -p)}. \]
	%
	This sum is finite if and only if $1/\alpha_1 + \dots + 1/\alpha_n < p$. Thus this is the range for which
	%
	\[ \int_{E_i} |f(x)|^p < \infty. \]
	%
	But this condition is invariant of $i$, so this is precisely the range for which
	%
	\[ \int |f(x)|^p < \infty. \]
\end{solution}

\begin{solution}

Let $\gamma=\frac{1}{\alpha_{1}}+\ldots+\frac{1}{\alpha_{n}}$. We claim the integral converges if and only if $p>\gamma$.

\vt
\textit{Case 1:} Suppose $p>\gamma$. Then
\begin{equation*}
  \int_{\R^{n}} |f(x)|^{p}dx = \int_{0}^{\infty}p t^{p-1}|E_{t}|dt
\end{equation*}
where $E_{t}= \left\{ x: f(x)>1 \right\}$. Since $E_{t}=\emptyset$ if $t\geq 1$, it follows that
\begin{equation}\label{eq:10000}
  \int_{\R^{n}} |f(x)|^{p}dx = \int_{0}^{1}p t^{p-1}|E_{t}|dt.
\end{equation}
Observe that
\begin{align*}
  E_{t}
  &= \left\{ x\in \R^{n} : \sum_{i=1}^{n}|x_{i}|^{\alpha_{i}}< \frac{1-t}{t} \right\} \\
  &\subseteq \bigcap_{i=1}^{n}\left\{ x\in \R^{n}: |x_{i}| < \left(\frac{1-t}{t}\right)^{\frac{1}{\alpha_{i}}} \right\}\\
  &= \left\{ x\in \R^{n}:  -\left(\frac{1-t}{t}\right)^{\frac{1}{\alpha_{i}}} < x_{i}< \left(\frac{1-t}{t}\right)^{\frac{1}{\alpha_{i}}} \text{ for all }i\right\}
\end{align*}

Since the set on the right hand side is a rectangle with Lebesgue measure $2^{n}\left(\frac{1-t}{t}\right)^{\gamma}$, it follows that
\begin{equation*}
  |E_{t}| \leq 2^{n}\left(\frac{1-t}{t}\right)^{\gamma}.
\end{equation*}
Combining this with equation \eqref{eq:10000} gives
\begin{equation*}
  \int_{\R^{n}} |f(x)|^{p}dx \leq 2^{n}p \int_{0}^{1}t^{p-1}(1-t)^{\gamma}t^{-\gamma}dt \leq 2^{n}p \int_{0}^{1}t^{p-1-\gamma}dt
\end{equation*}
which is integrable since $p>\gamma$.


\vt
\textit{Case 2:} Suppose $p\leq\gamma$. We will prove that $\int_{\R^{n}} |f|^{p}dx = +\infty$ by induction on $n$. If $n=1$,
\begin{equation*}
  \int_{\R} \(\frac{1}{1+ |x|^{\alpha_{1}}}\)^{p}dx \geq  \frac{1}{2}\int_{|x|>1} |x|^{-\alpha_{1}p}dx \geq \frac{1}{2}\int_{|x|>1}\frac{1}{|x|}dx= +\infty.
\end{equation*}

Next assume $n>1$. For convenience of notation we will write $R=1+ \sum_{i=2}|x_{i}|^{\alpha_{i}}$. We have
\begin{align*}
  \int_{\R^{n}}|f(x)|^{p}dx
  &= \int_{\R^{n-1}} \left[ \int_{\R} \left( \frac{1}{|x_{1}|^{\alpha_{1}}+ 1+ \sum_{i=2}|x_{i}|^{\alpha_{i}}} \right)^{p}dx_{1} \right]dx_{2}\cdots dx_{n}\\
  &=\int_{\R^{n-1}} \left[ \int_{\R} \left( \frac{1}{|x_{1}|^{\alpha_{1}}+ R} \right)^{p}dx_{1} \right]dx_{2}\cdots dx_{n}\\
  &\geq \int_{\R^{n-1}} \left[ \int_{|x_{1}|>R^{1/\alpha_{1}}} \left( \frac{1}{|x_{1}|^{\alpha_{1}}+ R} \right)^{p}dx_{1} \right]dx_{2}\cdots dx_{n}\\
  &\geq \frac{1}{2^{p}}\int_{\R^{n-1}} \left[ \int_{|x_{1}|>R^{1/\alpha_{1}}} \left( \frac{1}{|x_{1}|^{\alpha_{1}}} \right)^{p}dx_{1} \right]dx_{2}\cdots dx_{n}\\
  &= \frac{1}{2^{p-1}}\int_{\R^{n-1}} \int_{R^{1/\alpha_{1}}}^{\infty}x_{1}^{-p\alpha_{1}}dx_{1}\cdots dx_{n}.
\end{align*}
If $p\alpha_{1}\leq 1$ then $\int_{\R^{n-1}} \int_{R}^{\infty}x_{1}^{-p\alpha_{1}}dx_{1}= +\infty$ and we are done. On the other hand, if $p\alpha_{1}<1$, then by the above, we have
\begin{align*}
  \int_{\R^{n}}|f(x)|^{p}dx
  &= \frac{1}{2^{p-1}(1-p\alpha_{1})}\int_{\R^{n-1}} R^{\frac{1}{\alpha_{1}}-p}dx_{2}\cdots dx_{n}\\
  &= \frac{1}{2^{p-1}(1-p\alpha_{1})}\underbrace{\int_{\R^{n-1}} \left( \frac{1}{1+ \sum_{i=2}|x_{i}|^{\alpha_{i}}}
    \right)^{p-\frac{1}{\alpha_{1}}} dx_{2}\cdots dx_{n}}_{\text{Call this }A}
\end{align*}
However, since $p\geq \gamma$, it follows that $p-\frac{1}{\alpha_{1}}\geq \frac{1}{\alpha_{2}}+ \ldots + \frac{1}{\alpha_{n}}.$ Therefore by the induction hypothesis, the integral $A=+\infty$. Therefore $\int_{\R^{n}}|f(x)|^{p}dx = +\infty$.
\end{solution}


\question Let $f \in L^1(\RR)$. Let
%
\[ G(\lambda) = \int_{-\infty}^\infty e^{i \lambda t^2} f(t)\; dt. \]
%
Prove that $G$ is a continuous function and that $\lim_{\lambda \to \infty} G(\lambda) = 0$.
\begin{solution}
    It is simple to see that the operator $Tf = G$ is continuous from $L^1(\RR)$ to $L^\infty(\RR)$, with $\| Tf \|_\infty \leq \| f \|_1$. We claim that it thus suffices to show the result for $\phi \in C_c^\infty(\RR)$ supported away from the origin. Indeed, if $f \in L^1(\RR)$, we can find $\phi_n \in C_c^\infty(\RR)$ supported away from the origin, converging in $L^1(\RR)$ to $f$. Then $T\phi_n$ converges to $Tf$ uniformly, and $T\phi_n$ is continuous for each $n$, so $Tf$ is continuous. Moreover, if $\varepsilon > 0$, there is $n$ large enough that $\| f - \phi_n \|_1 \leq \varepsilon$. This means that $\| Tf - T\phi_n \|_\infty \leq \varepsilon$, and so
    %
    \[ \limsup_{\lambda \to \infty} |Tf(\lambda)| \leq \limsup_{\lambda \to \infty} |Tf(\lambda) - T\phi_n(\lambda)| + \limsup_{\lambda \to \infty} |T\phi_n(\lambda)| \leq \varepsilon + 0 \leq \varepsilon. \]
    %
    Taking $\varepsilon \to 0$ completes the argument. So now we show the result for $\phi \in C_c^\infty(\RR)$. Continuity here is easy because $\phi$ is compactly supported (in this case, $G$ is actually a smooth function). And to show that $T\phi(\lambda)$ converges to zero as $\lambda \to \infty$, we integrate by parts, writing
    %
    \[ T\phi(\lambda) = \int_{-\infty}^\infty e^{i \lambda t^2} \phi(t)\; dt = \int_{-\infty}^\infty \frac{\phi(t)}{2i \lambda t} \frac{d}{dt} \left( e^{i \lambda t^2} \right) = - \frac{1}{2i \lambda} \int_{-\infty}^\infty \frac{d}{dt} \left( \frac{\phi(t)}{t} \right) e^{i \lambda t^2}. \]
    %
    Since $\phi$ lies in $C^\infty_c$ and is supported away from the origin, so too is $\phi(t)/t$, and thus we find
    %
    \[ |T\phi(\lambda)| \leq \frac{1}{2\lambda} \int_{-\infty}^\infty \left| \frac{d}{dt} \left( \frac{\phi(t)}{t} \right) \right|\; dt \lesssim 1/\lambda. \]
    %
    Thus $T\phi(\lambda) \to 0$ as $\lambda \to \infty$.
\end{solution}

\question Let $(X,\mu)$ be a $\sigma$-finite measure space. Let $\{ f_n \}$ be a sequence of measurable functions and assume that $f_n \to f$ almost everywhere. Prove that there exists measurable $A_1,A_2,\dots \subset X$ such that $\mu(X - \bigcup_i A_i) = 0$, and such that $f_n|_{A_i} \to f|_{A_i}$ uniformly for each $i$.
\begin{solution}
    We may assume without loss of generality that $X$ is actually a \emph{finite} measure space, since if $\bigcup X_n = X$, with $X_n$ a finite measure space, then we can find $A_{n1}, A_{n2}, \dots$ such that $\mu(X_n - \bigcup_i A_{ni}) = 0$, such that $f_k$ converges uniformly on each $A_{ni}$, and then $\mu(X - \bigcup_{n,i} A_{ni}) = 0$. Normalizing, we assume $|X| = 1$.
    
    Now let
     %
    \[ E_n(k) = \{ x \in X : \sup_{m \geq n} |f_m(x) - f(x)| \leq 1/2^k \}. \]
    %
    Since $f_n \to f$ almost everywhere, as $n \to \infty$ and for a fixed $k > 0$, $\{ E_n(k) \}$ is an increasing sequence, and $X - \bigcup_n E_n(k)$ is a set of measure zero. Thus for each $r > 0$, we can pick $n_1$ such that $|X - E_{n_1}(1)| \leq 1/2^r$. Similarily, we can for each $k$ pick $n_k$ such that $|E_{n_{k-1}}(k-1) - E_{n_k}(k)| \leq 1 / 2^{k+r}$. If we define $A_r = \bigcap_k E_{n_k}(k)$, then $|X - A_r| \leq 1/2^{r-1}$, and by definition, $f_k$ converges uniformly on $A_r$ for each $r$ since for $n \geq n_k$, $|f_n(x) - f(x)| \leq 1/2^k$. Thus we have found the required sequence $\{ A_r \}$.
\end{solution}

\question (Spring 2017) Given for each function $f \in C^0(\RR^2)$ we define for each $y \in \RR$ a function $f_y \in C[0,1]$ by $f_y(x) = f(x,y)$. Assume that for each fixed $y$, the distributional derivative of $f_y \in \mathcal{D}'(\RR)$ defines a function $a_y \in L^p(\RR)$. Assume further that
%
\[ \| a_y \|_p \leq C < \infty \]
%
for some constant $C$ independent of $y$. Show that the distributional derivative $\partial_x f \in \mathcal{D}'(\RR^2)$ is in $L^p_{\text{loc}}(\RR^2)$, provided $1 < p \leq \infty$.
\begin{solution}
    We apply duality, which says that $\partial_x f$ is in $L^p_{\text{loc}}$ if and only if for each $R > 0$, and $\phi \in C_c^\infty(\RR^d)$ supported on $|z| \leq R$,
    %
    \[ \left| \int_{\RR^2} (\partial_x f)(z) \phi(z)\; dz \right| \lesssim_R \| \phi \|_{p^*}, \]
    %
    where $p^*$ is the conjugate to $p$. Now
    %
    \[  \int_{\RR^2} (\partial_x f)(z) \phi(z) = - \int f(z) (\partial_x \phi)(z)\; dz. \]
    %
    Since $f$ is continuous, and $\partial_x \phi$ is compactly supported on $|z| \leq R$, we can apply Fubini's theorem, writing
    %
    \[ \int f(z) (\partial_x \phi)(z)\; dz = \int_{-\infty}^\infty \int_{-\infty}^\infty f(x,y) (\partial_x \phi)(x,y)\; dx\; dy. \]
    %
    But applying integration by parts in just the $x$-variable, we have
    %
    \[ \int_{-\infty}^\infty f(x,y) (\partial_x \phi)(x,y)\; dx = - \int a_y(x) \phi(x,y)\; dx. \]
    %
    Thus we conclude that
    %
    \[ \int (\partial_x f)(z) \phi(z)\; dz = \int_{-\infty}^\infty \int_{-\infty}^\infty a_y(x) \phi(x,y)\; dx\; dy. \]
    %
    But by H\"{o}lder's inequality, if $p^*$ is the dual to $p$,
    %
    \begin{align*}
        \left| \int_{-\infty}^\infty \int_{-\infty}^\infty a_y(x) \phi(x,y)\; dx\; dy \right| &= \left| \int_{-R}^R \int_{-\infty}^\infty a_y(x) \phi(x,y)\; dx\; dy \right|\\
        &\leq \left( \int_{-R}^R \int_{-\infty}^\infty |a_y(x)|^p\; dx\; dy \right)^{1/p} \| \phi \|_{p^*}\\
        &= \left( \int_{-R}^R \| a_y \|_p^p\; dy \right)^{1/p} \| \phi \|_{p^*}\\
        &\leq C \cdot (2R)^{1/p} \| \phi \|_{p^*}\\
        &\lesssim_R \| \phi \|_{p^*}.
    \end{align*}
    %
    Thus we conclude that $\partial_x f \in L^p_{\text{loc}}$.
\end{solution}

\question (Spring 2020)
  Let $E\subset[0,1]$ be a measurable set with positive Lebesgue measure. Moreover, it satisfies the following property: As long as $x$ and $x$ belong to $E$, we know $\frac{x+y}{2}$ belongs to $E$. Prove that $E$ is an interval.
  
  
\begin{solution}
  Let $m=\inf E$ and $M = \sup E$. Since $|E|>0$, $E$ contains more than one element and hence $m<M$. We first prove two claims:

  \vt
  \noindent
  \textit{Claim 1:} $E$ is dense in $(m,M)$.
  \begin{proof}[Proof of Claim 1:]
    Let $z\in (m,M)$.
    By definition of infimum and supremum, there exists $u,v\in E$ such that $m \leq u<z<v\leq M$. Let $\delta>0$. It will suffice to show there exists an element $w\in E$ such that $|w-z|<\delta$. Define a sequence of intervals inductively by $I_{0}= [u,v]$ and if $I_{n-1}=[x,y]$, then
    \begin{equation*}
      I_{n} = \left\{
        \begin{array}{l@{\quad}l}
          \left[ \frac{x+y}{2}, y \right]  & \text{if } w\geq \frac{x+y}{2}\\
          \left[ x, \frac{x+y}{2} \right]  & \text{if } w< \frac{x+y}{2}
        \end{array}\right.
    \end{equation*}
     By construction, $w\in I_{n}$, and using the midpoint convexity of $E$, the endpoints of $I_{n}$ are elements of $E$. Choosing $N$ sufficiently large that $|I_{N}|=2^{-N}(v-u)<\delta$, it follows that $|w-\max I_{N}| < |\max I_{N}-\min I_{N}|= 2^{-N}(v-u)<\delta$. This proves the claim.
   \end{proof}

   \vt
   \noindent
   \textit{Claim 2:} $E$ contains a nonempty interval.
   \begin{proof}[Proof of Claim 2:]
     Denote by $E/2$ the set $\left\{ x/2 : x\in E   \right\}$. First observe that $|E/2|>0$. (Indeed, if $|E/2|=0$, then by absolute continuity of the function $x\mapsto 2x$, $|E|=0$, a contradiction.) We will show that $\frac{E}{2}+\frac{E}{2}$ contains a nonempty interval. The proof of the claim will then follow since $ E\supset \frac{E+E}{2}=\frac{E}{2}+\frac{E}{2}.$
     
     Let $h(x)= \int_{\R} \chi_A(x) \chi_A(x-y)dy$ where $A=E/2$. If $x\notin A+A$, then $h(x) = 0$. Therefore by contrapositive, if $\phi(x)>0$ then $x\in A+A$. Moreover, $h$ is continuous, so the set $\{x: h(x)>0\}$ is open and therefore contains an interval provided it is nonempty. Therefore it remains to show that $\{x: h(x)>0\}$ is nonempty. Indeed, by the Fubini-Tonelli theorem, $\int_{\R} h(x) dx = \int_A \int_{\R} \chi_A(x-y)dxdy = \int_A \int_{\R} \chi_A(x)dxdy = |A|^2>0$. Since $h$ is nonnegative function whose integral is positive, it follows that $h(x)>0$ for some $x$, as desired.
   \end{proof}

   We now proceed with the proof. By Claim 2, there exists a nonempty interval $(a,b)\subset E$. Let $b_{0} = \sup \left\{ r>0: (a,r)\subset E \right\}$ and $a_{0} = \inf \left\{ s>0 : (s,b) \subset E \right\}$. It will suffice to show that $a_{0}=m$ and  $b_{0}=M$, since this will imply that $(m,M)\subset E$, which is enough to conclude that $E$ is an interval. We shall show that $b_{0} = M$ as the proof that $a_{0}=m$ is similar.

   Suppose $b_{0}<M$. By Claim 1, there exists $b_{1}\in E$ such that $b_{0}<b_{1}< 2b_{0}-a$. Since $(a,b_{0})\subset E$ and $b_{1}\in E$, it follows that
   \begin{equation*}
     E\supset \left\{ \frac{b_{1}+s}{2}: a<s<b_{0} \right\} = \left\{ t: \frac{a+b_{1}}{2}<t<\frac{b_{0}+b_{1}}{2} \right\}= \left( \frac{a+b_{1}}{2},\frac{b_{0}+b_{1}}{2} \right).
   \end{equation*}
   Since $b_{1}<2b_{0}-a$, we have $\frac{a+b_{1}}{2}<b_{0}$. Therefore $\left( a, \frac{b_{0}+b_{1}}{2} \right)= (a,b_{0})\cup \left( \frac{a+b_{1}}{2},\frac{b_{0}+b_{1}}{2} \right)\subset E$. Moreover, since $b_{0}< b_{1}$, we have $\frac{b_{0}+b_{1}}{2}>b_{0}$. Therefore $b_{0}$ is not an upper bound for the set $\left\{ r>0: (a,r)\subset E \right\}$. But this is a contradiction since we defined $b_{0}$ to be the supremum of that set. Therefore $b_{0}=M$.
 \end{solution}

\question
\begin{parts}
    \part Does $p_N = \prod_{n = 2}^N (1 + (-1)^n/n)$ tend to a nonzero limit as $N \to \infty$.
    \begin{solution}
        We have
        %
        \[ \sum_{n = 2}^N \log(1 + (-1)^n/n) = \sum_{n = 2}^N (-1)^n/n + O(1/n^2). \]
        %
        The error terms converge absolutely, since $1/n^2$ is summable, and Leibniz's test implies since $(-1)^n/n$ is an alternating sequence decreasing in absolute value, that
        %
        \[ \sum_{n = 2}^N (-1)^n/n \]
        %
        converges as $N \to \infty$, and that the limiting value exceeds
        %
        \[ 1/2 - 1/3 > 0. \]
        %
        Thus taking exponentials shows that $p_N$ converges as $N \to \infty$.
    \end{solution}
    
    \part Does $q_N = \prod_{n = 2}^N (1 + (-1)^n/\sqrt{n})$ tend to a nonzero limit as $N \to \infty$.
    \begin{solution}
        We apply Taylor series, writing
        %
        \[ \log(1 + (-1)^n/\sqrt{n}) = (-1)^n/\sqrt{n} - 1/2n + O(1/n^{3/2}). \]
        %
        The error term is absolutely summable, and the sum of $(-1)^n/\sqrt{n}$ converges by Leibniz test as above. Thus the convergence of the sum
        %
        \[ \sum_{n = 2}^N \log(1 + (-1)^n/\sqrt{n}) \]
        %
        is equivalent to the convergence of the sum
        %
        \[ \sum_{n = 2}^N -1/2n. \]
        %
        But this sum converges to $-\infty$, so we find that, taking exponentials, $q_N \to 0$ as $N \to \infty$.
    \end{solution}
\end{parts}

\newpage
\section{Day 11: Bonus Questions}


\question (Fall 2015) Let $\chi \in C^\infty(\RR)$ have a compact support and define
%
\[ f_n(x) = n^2 \chi'(nx). \]
%
\begin{parts}
    \part Does $f_n$ converge in the sense of distributions as $n \to \infty$? If so, what is the limit?
    \begin{solution}
        Applying an integration by parts, we calculate that
        %
        \[ \int f_n(x) \phi(x)\; dx = \int n^2 \chi'(nx) \phi(x)|; dx = - \int n \chi(nx) \phi'(x)\; dx. \]
        %
        As $n \to \infty$, each function $x \mapsto n \chi(nx)$ has total mass one, but is concentrated in small and smaller neighborhoods of the origin. In particular, these functions operate as an approximation to the identity, so that
        %
        \[ - \int n \chi(nx) \phi'(x)\; dx = - \phi'(0). \]
        %
        Thus as $n \to \infty$, $f_n$ converges distributionally to the distribution $\phi \mapsto - \phi'(0)$.
    \end{solution}
    
    \part Let $p \in [1,\infty)$ and $g \in L^p(\RR)$ be such that the distributional derivative of $g$ also lies in $L^p(\RR)$. Does $f_n * g$ converge in $L^p(\RR)$ as $n \to \infty$? If so, what is the limit?
    \begin{solution}
        As mentioned before, integration by parts shows that
        %
        \[ (f_n * g)(x) = \int_{-\infty}^\infty f_n(y) g(x-y)\; dy = \int n \chi(ny) g'(x-y)\; dy = (n \chi(ny) * g')(x). \]
        %
        The function $n \chi(ny)$ is an approximation to the identity, and so as $n \to \infty$, $(f_n * g)$ converges in $L^p$ to $g'$.
    \end{solution}
\end{parts}

\question Let
%
\[ s_N(x) = \sum_{n = 1}^N (-1)^n \frac{x^{3n}}{n^{2/3}}. \]
%
Prove that $s_N(x)$ converges to a limit $s(x)$ on $[0,1]$, and that there is a constant $C > 0$ so that for all $N \geq 1$ the inequality
%
\[ \sup_{x \in [0,1]} |s_N(x) - s(x)|\leq C N^{-2/3} \]
%
holds.
\begin{solution}
    The convergence here becomes highly singular near $x = 1$, because the power series diverges for $|x| > 1$. Thus we cannot use normal power series techniques here. It seems we must exploit the oscillation of the series here to get explicit bounds since removing the $(-1)^n$ in the definition of the series causes the sum to diverge at $x = 1$. If we define
    %
    \[ S_N(x) = \sum_{n = 1}^N (-x^3)^n \]
    %
    for $N > 0$, and $S_0 = 0$, then the geometric series formula shows that there is $C > 0$ such that $|S_N(x)| \leq C$ for all $N$ and $x \in [0,1]$, and a summation by parts shows that
    %
    \[ \sum_{n = 1}^N (-1)^n \frac{x^{3n}}{n^{2/3}} = \sum_{n = 1}^N (S_n - S_{n-1}) \frac{1}{n^{2/3}} = \sum_{n = 1}^{N-1} S_n \left( \frac{1}{n^{2/3}} - \frac{1}{(n+3)^{2/3}} \right) + S_N / n^{2/3}. \]
    %
    Now a mean value theorem / Taylor series expansion implies that
    %
    \[ \left| \frac{1}{n^{2/3}} - \frac{1}{(n+3)^{2/3}} \right| \lesssim \frac{1}{n^{5/3}}, \]
    %
    which is summable, and so for $M \geq N$,
    %
    \[ |s_M(x) - s_N(x)| \lesssim \sum_{n = N}^{M-1} S_n / n^{5/3} + S_M / M^{2/3} \lesssim C/N^{2/3}. \]
    %
    This is sufficient to show the existence of the point wise limit (since it shows $s_N$ is a Cauchy sequence), and also shows the required inequality.
\end{solution}


\question (Fall 2018)
  Prove that in an infinite dimensional Banach space,
  \begin{enumerate}[(a)]
  \item every norm bounded set is weakly bounded,
  \item every norm closed set is weakly closed
  \item a norm bounded set has empty interior in the weak topology
  \end{enumerate}
  
\begin{solution}
    We first prove part (a). Let $X$ be a Banach space and supppose $E\subset X$ is norm bounded; i.e., that $\sup_{x\in E}\norm{x}=C<\infty$. Recall that to show $E$ is weakly bounded, we need to show that $\sup_{x\in E}|\phi(x)|<\infty$ for all $\phi\in X^{*}$. This follows immediately by definition of norm:
    \begin{equation*}
      \sup_{x\in E}|\phi(x)| \leq\sup_{x\in E} \norm{\phi}\norm{x} \leq C \norm{\phi}<\infty.
    \end{equation*}

    Next we prove (c). Let $X$ be an infinite-dimensional Banach space, and let $A\subset X$ be a norm-bounded set. Then there exists $\rho>0$ such that $A\subset B:=\left\{ x\in X: \norm{x}<\rho \right\}$. It suffices to show that $B$ has empty interior in the weak topology.
    
      By definition, the weak topology has a base consisting of sets of the form
    \begin{equation}\label{eq:1}
      N(x_{0},F,r)=\left\{ x\in X : |\phi(x-x_{0})|<r \mathrm{\ for\ all\ }\phi\in F\right\}
    \end{equation}
    where $x_{0}\in X$, $r>0$, and $F$ is a finite subset of $X^{*}$. Letting $K:= \bigcap_{\phi \in F} \ker(\phi)$, observe that
    \begin{equation*}
      x_{0}+ K \subset N(x_{0},F,r).
    \end{equation*}
    Since $\ker(\phi)$ is of co-dimension 1 in $X$ for every $\phi\in X^{*}$ and since $F$ is a finite set, it follows that $K$ is of finite codimension in $X$. Therefore since $X$ is infinite-dimensional, it follows that $\dim K >0$, and therefore $K$ contains a nonzero element $y$. Since $K$ is a subspace, it follows that $\alpha y\in K$ for any $\alpha\in \R$. This implies that $K$---and therefore $N(x_{0},F,r)$---contains elements of arbitrarily large norm. Therefore $N(x_{0},F,r)\not\subset B$. Therefore $B$ has empty interior in the weak topology. This proves statement (c).

    Incidentally, we have also shown that statement (b) is false: while $B^c$ is clearly norm closed, it is not weakly closed because $B$ is not weakly open.

    Finally we claim that the converse of statement (b)---i.e., that every weakly closed set is norm closed---is true. To see this, let $F$ be weakly closed. Then $F^c$ is weakly open, and hence can be written as a union of sets of the form given in equation \eqref{eq:1}. Moreover, since $x\mapsto |\phi(x-x_{0})|$ is continuous with respect to the norm topology, it follows that $N(x_{0},F,r)$ is open in the norm topology. Therefore $F^c$ is a union of norm open sets. Therefore $F^c$ is norm open, and hence $F$ is norm closed.

    Another way to prove the converse of statement (b) is the following. Suppose $F\subset X$ is weakly closed, and suppose that $x_{n}\in F$ converges in norm to $x\in X$. To show that $F$ is strongly closed, we need to show that $x\in F$. Using the linearity of $\phi$ and the definition of operator norm,
    \begin{equation*}
      |\phi(x_{n})-\phi(x)| = |\phi(x_{n}-x)|\leq \norm{\phi} \norm{x_{n}-x}
    \end{equation*}
    for every $\phi\in X^{*}$. Since the right-hand side converges to zero as $n\to\infty$, it follows that $\phi(x_{n})\to \phi(x)$ as well, i.e. that $x_{n}$ converges weakly to $x$. Therefore since $F$ is weakly closed, $x\in F$, as required.
\end{solution}


\item (Spring 2016) Let $1<p<\infty$, and let $\chi_{[1-\frac{1}{n},1]}$ denote the characteristic function of $[1-\frac{1}{n},1]$. For which $\alpha\in \R$ does the sequences $n^{\alpha}\chi_{[1-\frac{1}{n},1]}$ converge weakly to $0$ in $L^{p}(\R)$?

\begin{solution}
  Case 1: Suppose $\alpha> 1/p$. Then $\norm{f_{n}}_{p}^{p} = \int_{1-\frac{1}{n}}^{1}n^{\alpha p}dx = n^{\alpha p - 1}\to \infty$ as $n\to\infty$. Since weakly convergent sequences are bounded in norm, it follows that $f_{n}$ does not weakly converge if $\alpha>1/p$.

  Case 2: Suppose $\alpha<1/p$. Let $g\in L^{q}(\R)$ where $\frac{1}{p}+\frac{1}{q}=1$. Using Holder's inequality,
  \begin{align*}
    \int f_{n}g &= \int_{1-\frac{1}{n}}^{1}n^{\alpha} g \leq \left( \int_{\R}n^{\alpha p}\chi_{[1-\frac{1}{n},1]} \right)^{\frac{1}{p}} \norm{g}_{q} = n^{\alpha-\frac{1}{p}}\norm{g}_{q}\to 0 \text{ as }n\to\infty.
  \end{align*}
  Since $g\in L^{q}$ was arbitrary, it follows that $f_{n}$ converges weakly to $0$ in $L^{p}$.

  Case 3: Suppose $\alpha=1/p$. Let $g\in L^{q}(\R)$ where $\frac{1}{p}+\frac{1}{q}=1$. By density of $C_{c}^{\infty}(\R)$ in $L^{q}(\R)$, there exists $\phi\in C_{c}^{\infty}(\R)$ with $\norm{\phi-g}_{q}<\epsilon$. Then
  \begin{align*}
    \left|\int_{\R }f_{n}g\right|
    &= \int_{\R} |f_{n}(g-\phi)| + \int_{\R} |f_{n}\phi|\\
    &\leq \norm{f_{n}}_{p}\underbrace{\norm{g-\phi}_{q}}_{<\epsilon} + \int_{1-\frac{1}{n}}^{1}n^{\frac{1}{p}}|\phi(x)|dx
    &&\text{ by Holder's inequality}\\
    &\leq \epsilon + \int_{1-\frac{1}{n}}^{1}n^{\frac{1}{p}}|\phi(x)|dx
    &&\text{ since }\norm{f_{n}}_{p}= 1\text{ for all }n\\
    &\leq \epsilon + \norm{\phi}_{\infty}\int_{1-\frac{1}{n}}^{1}n^{\frac{1}{p}}dx
    &&\text{ since }\phi\in C_{c}^{\infty}(\R)\\
    &= \epsilon + \norm{\phi}_{\infty}n^{\frac{1}{p}-1}.
  \end{align*}
  Since $\frac{1}{p}<1$, it follows that $\limsup_{n\to\infty}\left| \int_{\R}f_{n}g\right| \leq \epsilon$. Since $\epsilon>0$ and $g\in L^{q}(\R)$ were arbitrary, it follows that $\lim_{n\to\infty} \int f_{n}g = 0$ for all $g\in L^{q}$. So $f_{n}$ converges weakly to $0$ in $L^{p}$.
\end{solution}


\item (Spring 2018) Let $x_{n}$ be a sequence in a Hilbert space $H$. Suppose that $x_{n}$ converges to $x$ weakly. Prove that there is a subsequence $x_{n_{k}}$ such that
\begin{equation*}
  \frac{1}{N}\sum_{k=1}^{N}x_{n_{k}}
\end{equation*}
converges to $x$ (in norm) as $N\to\infty$. 

\begin{solution}
  (Note: this result is called the Banach-Saks theorem). Without loss of generality, assume that $x=0$. Since $x_{n}$ converges weakly, it follows that $x_{n}$ is bounded in norm (this is a consequence of the uniform boundedness principle). Let $C = \max_{n\geq 1}\norm{x_{n}}$. We construct a subsequence in the following manner. Let $x_{n_{1}}=x_{1}$. Having chosen $x_{n_{N}}$, choose $x_{n_{N+1}}$ so that
  \begin{equation*}
    \Big\langle \sum_{i=1}^{N}x_{n_{k}},x_{n_{N+1}}\Big\rangle \leq 2^{-(N+1)}.
  \end{equation*}
  Such an $x_{n_{N=1}}$ exists since $\langle y,x_{n}\rangle \to 0$ as $n\to \infty$ for all $y\in H$. We need to show that
  \begin{equation*}
    \norm{N^{-1}\sum_{k=1}^{N}x_{n_{k}}}\to 0 \text{ as }n\to\infty.
  \end{equation*}
  To see this, we have:
  \begin{align*}
    N^{-2}\Big\langle \sum_{k=1}^{N}x_{n_{k}}, \sum_{j=1}^{N}x_{n_{j}} \Big\rangle
    &= N^{-2}\sum_{j,k=1}^{N}\Big\langle x_{n_{k}}, x_{n_{j}}\Big\rangle\\
    &= N^{-2}\left( \sum_{j=1}^{N}\norm{x_{n_{j}}}^{2}+ \sum_{k=2}^{N}\sum_{j=1}^{k-1}\langle x_{n_{j}},x_{n_{k}}\rangle \right)\\
    &=N^{-2} \left(  \sum_{j=1}^{N}\norm{x_{n_{j}}}^{2}+ \sum_{k=2}^{N}\underbrace{\Big\langle \sum_{j=1}^{k-1} x_{n_{j}},x_{n_{k}}\Big\rangle}_{\leq 2^{-k}} \right)\\
    &\leq N^{-2} \left( C N + 1  \right) \to 0 \text{ as } N\to\infty.
  \end{align*}
\end{solution}

  
  
\question (Fall 2015) Let $E \subset \RR$ be a measurable set, such that $E + r = E$ for all $r \in \mathbf{Q}$. Show that $|E| = 0$ or $|E^c| = 0$.
\begin{solution}
    We apply the Lebesgue density theorem. Suppose $|E| \neq 0$. Fix $\delta > 0$. Then there exists $\varepsilon_0 > 0$ such that for $\varepsilon < \varepsilon_0$, we can find $a,b \in \mathbf{Q}$ with $b - a < \varepsilon$ with
    %
    \[ |E \cap (a,b)| \geq (1 - \delta) (b - a) \]
    %
    Set $\varepsilon_1 = b - a$. Because $E$ is invariant under translations in $\mathbf{Q}$,
    %
    \[ |E \cap (0,\varepsilon_1)| \geq (1 - \delta) \varepsilon_1. \]
    %
    Then $|E \cap (a,a + \varepsilon_1)| \geq (1 - \delta) \varepsilon_1$ for all $a \in \mathbf{Q}$. This implies that for any $x \in \RR$,
    %
    \[ \limsup_{\substack{x \in I\\|I| \to 0}} \frac{|E \cap I|}{|I|} \geq 1. \]
    %
    The Lebesgue density theorem thus implies that $|E^c| = 0$.
\end{solution}

\question (Spring 2015)
    Let $\{ r_n \} \in [0,1]$ be an arbitrary sequence, and define the function
    %
    \[ f(x) = \sum_{r_n < x} \frac{1}{2^n} \]
    %
    Show that $f$ is Borel measurable, find all it's points of discontinuity, and find $\int_0^1 f(x)\; dx$.
\begin{solution}
    Write
    %
    \[ f_n(x) = \sum_{k = 1}^n \mathbf{I}_{(r_n, \infty)} \cdot 2^{-n}. \]
    %
    Then for each $n$, $f_n$ is a simple function, and is thus measurable. Moreover, $f = \lim_{n \to \infty} f_n$, and this limit is monotone. The pointwise limit of measurable functions is measurable, so $f$ is measurable.
    
    Next, we look at the discontinuity points. We claim that the set of discontinuity points is \emph{precisely} the set of values $\{ r_n \}$. To see this, fix $\varepsilon > 0$, and note that
    %
    \[ f(x + \varepsilon) - f(x) = \sum_{x \leq r_n < x + \varepsilon} 2^{-n} \]
    %
    Suppose there exists some $n_0$ such that $x = r_{n_0}$. Then
    %
    \[ f(x + \varepsilon) - f(x) \geq 1/2^{n_0}, \]
    %
    and thus
    %
    \[ \limsup_{\varepsilon \to 0^+} f(x + \varepsilon) - f(x) \geq 1/2^{n_0}, \]
    %
    implying $f$ is not continuous at $x$. On the other hand, if $x \neq r_n$ for any $n$, then for any $N > 0$, there exists $\varepsilon_N$ such that for $\varepsilon < \varepsilon_N$, $\{ r_1, \dots, r_N \}$ is disjoint from $[x,x+\varepsilon)$. Therefore, for $\varepsilon < \varepsilon_0$,
    %
    \[ f(x + \varepsilon) - f(x) \leq \sum_{k = N+1}^\infty 2^{-n} = 2^{-N}. \]
    %
    Thus we conclude that
    %
    \[ \lim_{\varepsilon \to 0^+} f(x + \varepsilon) - f(x) = 0. \]
    %
    On the other hand, I claim that the left hand limit does not impact continuity, since for all $x \in [0,1]$,
    %
    \[ \lim_{\varepsilon \to 0^-} f(x) - f(x-\varepsilon) = 0. \]
    %
    Indeed, we have
    %
    \[ f(x) - f(x-\varepsilon) = \sum_{x - \varepsilon \leq r_n < x}. \]
    %
    If $N > 0$, there is $\varepsilon_0$ such that for $\varepsilon < \varepsilon_0$, $[x-\varepsilon,x)$, $\{ r_1, \dots, r_N \}$ is disjoint from $[x-\varepsilon,x)$, and thus for such $\varepsilon$,
    %
    \[ f(x) - f(x-\varepsilon) \leq 2^{-N}. \]
    %
    Taking $N \to \infty$ gives the limit, and completes the classification of discontinuity points.
    
    Finally, we note the monotone convergence theorem implies that
    %
    \[ \int_0^1 f(x)\; dx = \lim_{n \to \infty} \int_0^1 f_n(x)\; dx. \]
    %
    Since $f_n$ is a simple function, we easily evaluate
    %
    \[ \int_0^1 f_n(x)\; dx = \sum_{k = 1}^n (1 - r_k) \cdot 2^{-k}. \]
    %
    Thus
    %
    \[ \int_0^1 f(x)\; dx = \sum_{k = 1}^\infty (1 - r_k) \cdot 2^{-k}. \]
\end{solution}


\newpage
\section{Day 12: Bonus Questions}

\question (Fall 2013) Let $E= \left\{ (x_{1},x_{2}) : x_{1},x_{2}\in \R, x_{1}-x_{2}\in \Q\right\}$. Is it possible to find to Lebesgue measurable sets $A_{1},A_{2}\subset\R$ such that $|A_{1}|,|A_{2}|>0$, and $A_{1}\times A_{2}\subset E^{c}$?
\begin{solution}
  No, it is not possible. We will show that $|A_{1}|\cdot|A_{2}| = 0$ whenever $A_{1}\times A_{2}\subset E^{c}$. 

 \textbf{ Case 1:} Suppose $A_{1},A_{2}\subset [-n,n]$ for some positive integer $n$.
  Let $h(y)= \int_{\R}\chi_{A_{1}}(x)\chi_{A_{2}}(x+y)dx$. By Tonelli's Theorem,
  \begin{align*}
    \int_{\R}h(y)dy
    &=\int_{\R} \chi_{A_{1}}(x) \int_{\R} \chi_{A_{2}}(y+x)dy dx\\
    &=\int_{\R} \chi_{A_{1}}(x) \int_{\R} \chi_{A_{2}}(y)dy dx\\
    &=\left(\int_{\R} \chi_{A_{1}}(x)dx\right)\left( \int_{\R} \chi_{A_{2}}(y)dy\right)\\
    &=|A_{1}|\cdot |A_{2}|.
  \end{align*}

  Next we claim that $h$ is also continuous. This is by average continuity of the $L^{1}$ norm. For a detailed proof, let $\epsilon>0$. Since $\chi_{A_{2}}\in L^{1}(\R)$, there exists $\phi\in C_{c}(\R)$ with compact support $K\subset \R$ such that $\norm{\chi_{A_{2}}-\phi}_{L^{1}}<\epsilon/3$. Since $\phi$ is continuous on a compact set, it is uniformly continuous, so we may choose $\delta>0$ such that $|\phi(s)-\phi(t)|\leq\epsilon/3|K|$ whenever $|s-t|\leq\delta$. Then
  \begin{align*}
    |h(y+\delta)-h(y)|
    &= \int_{A_{1}}\chi_{A_{2}}(x+y+\delta)dx - \chi_{A_{2}}(x+y)dx\\
    &\leq \int_{A_{1}}|\chi_{A_{2}}(x+y+\delta) -\phi(x+y+\delta)|dx +\int_{A_{1}}|\phi(x+y+\delta)-\phi(x+y)|dx\\
    &\quad+\int_{A_{1}}|\phi(x+y) - \chi_{A_{2}}(x+y)|dx\\
    &\leq 2\int_{\R}|\chi_{A_{2}} -\phi|dx +\int_{K}|\phi(x+\delta)-\phi(x)|dx\\
    &\leq 2\epsilon/3 + \epsilon/3\\
    &= \epsilon.
  \end{align*}
  Since $\epsilon>0$ was arbitrary, $h$ is continuous.

  Finally we claim that $h(y)=0$ for all $y\in \Q$. Indeed, if $y\in \Q$ then for any $x$, we have $x-(y+x)\in \Q$ and hence $(x,y+x)\in E$. Therefore since $A_{1}\times A_{2}\subset E^{c}$, $(x,y+x)\notin A_{1}\times A_{2}$. Therefore 
  \begin{equation*}
    \chi_{A_{1}}(x)\chi_{A_{2}}(y+x)=0.
  \end{equation*}
  Since this holds for all $x\in \R$, it follows that $h(y)= 0$.
  We have shown that
  \begin{enumerate}
  \item $\int_{\R} h = |A_{1}|\cdot |A_{2}|$
  \item $h$ is continuous
  \item $h(y) = 0$ for all $y\in \Q$.
  \end{enumerate}
  By density of $\Q$ in $\R$, (2) and (3) imply that $h\equiv 0$, so that by (1), $|A_{1}|\cdot |A_{2}| = 0$.

  \textbf{Case 2:} Let $A_{1},A_{2}\subset \R$ be arbitrary measurable sets. By case 1,  $|A_{1}\cap [-n,n]|\cdot |A_{2}\cap [-n,n]| =0$ for all $n$. Therefore by continuity of Lebesgue measure,
  \begin{equation*}
    |A_{1}|\cdot|A_{2}| = \lim_{n\to\infty}|A_{1}\cap [-n,n]|\cdot |A_{2}\cap [-n,n]| =\lim_{n\to\infty}0=0.
  \end{equation*}
\end{solution}

\question (Fall 2015) Let $(X, \mu)$ be a measure space, and let $f: X \to \RR$ be measurable. Then if $1 \leq p < r < q < \infty$ and there is $C < \infty$ such that
%
\[ \mu(\{ x : |f(x)| > \lambda \}) \leq \frac{C}{\lambda^p + \lambda^q} \]
%
for every $\lambda > 0$. Then $f \in L^r(\mu)$.
\begin{solution}
    We perform a dyadic decomposition, writing
    %
    \begin{align*}
        \int |f(x)|^r\; dx &= \sum_{k = -\infty}^\infty \int_{2^k < |f(x)| \leq 2^{k+1}} |f(x)|^r\; dx\\
        &\leq \sum_{k = -\infty}^\infty \mu(\{ 2^k < |f(x)|) \cdot 2^{kr}.
    \end{align*}
    %
    For $\lambda < 1$, $\lambda^p \leq \lambda^q$, so
    %
    \[ \mu(\{ 2^k < |f(x)| \}) \leq C/2^{kp}. \]
    %
    For $\lambda \geq 1$, $\lambda^q \leq \lambda^p$, so
    %
    \[ \mu(\{ 2^k < |f(x)| \}) \leq C/2^{kq}. \]
    %
    Thus we conclude
    %
    \[ \int |f(x)|^r\; dx \leq C \left( \sum_{k = -\infty}^0 2^{k(r - p)} + \sum_{k = 1}^\infty 2^{k(r - q)} \right). \]
    %
    Both of these sums converge, so
    %
    \[  \int |f(x)|^r\; dx \lesssim_{p,q} C < \infty. \]
\end{solution}

\begin{solution}
  \textit{Case 1:} Suppose that $(X,\mathcal{M},\mu)$ is $\sigma$-finite. This assumption of $\sigma$-finiteness allows us to apply the Fubini-Tonelli Theorem in the following calcuation:
  \begin{align*}
    \int_{X}|f(x)|^{r}dx
    &= \int_{X} \int_{0}^{|f(x)|}r t^{r-1}dt dx\\
    &=\int_{X} \int_{0}^{\infty}  rt^{r-1}\chi_{[ |f(x)|>t]}dtdx \\
    &=\int_{0}^{\infty}r t^{r-1}\int_{X} \chi_{[ |f(x)|>t]}dxdt &&\text{by Tonelli's Theorem}\\
    &=\int_{0}^{\infty}r t^{r-1}\mu\{x: |f(x)|>t\}dt\\ 
    &\leq Cr\int_{0}^{1}\frac{t^{r-1}}{t^{p}+t^{q}}dt + Cr\int_{1}^{\infty} \frac{t^{r-1}}{t^{p}+t^{q}}dt\\ 
    &\leq Cr\int_{0}^{1}t^{r-1-p}dt + Cr\int_{1}^{\infty} t^{r-1-q}dt
  \end{align*}
  and the right-hand side is finite since $1<p<r<q<\infty$.
  
  \vt
  \textit{Case 2:} Suppose $(X,\mathcal{M},\mu)$ is not $\sigma$-finite. Let $X'= \left\{ x\in X: |f(x)|>0 \right\}$. Also define $\mathcal{M}'=\left\{ M\cap X' : M\in \mathcal{M} \right\}$ (it is easy to see this is a $\sigma$-algebra) and define $\mu'$ as the restriction of $\mu$ to $\mathcal{M}'$. Then $(X',\mathcal{M}',\mu')$ is $\sigma$-finite because
  \begin{equation*}
    X'= \bigcup_{n=1}^{\infty}\left\{ x\in X': |f(x)|>\frac{1}{n} \right\}
  \end{equation*}
  and 
  \begin{equation*}
    \mu\left\{ x\in X': |f(x)|>\frac{1}{n} \right\} \leq \frac{C}{n^{-p}+n^{-q}}<\infty
  \end{equation*}
  for all $n$.

  Therefore by Case 1, $\int_{X'}|f(x)|^{r}dx<\infty$. And since $\int_{X}|f(x)|^{r}dx = \int_{X'}|f(x)|^{r}dx$, we are done. 
  \end{solution}

\question (Spring 2017)
    Let $E \subset \RR^n$ be a set of finite, positive measure, and let $\{ t_k \}$ be a sequence with $\{ t_k \} > 0$ and $\lim_k t_k = 0$. Define, for $f \in L^p(\RR^n)$,
    %
    \[ Mf(x) = \sup_k \fint_{t_k E} |f(x-y)|\; dy. \]
    %
    Suppose furthermore that there is $C > 0$ such that
    %
    \[ | \{ x: Mf(x) > \lambda \}| \leq C \lambda^{-p} \| f \|_p^p. \]
    %
    Show that for every $f \in L^p(\RR^n)$,
    %
    \[ \lim_k \fint_{t_k E} f(x-y)\; dy = f(x). \]
    %
    for almost every $x \in \RR^d$.
\begin{solution}
    We note that the result is true for any $f \in C(\RR^n)$, i.e. for such functions, and any $x \in \RR$,
    %
    \[ \limsup_k \left| \fint_{t_k E} f(x) - f(x-y) \right| = 0. \]
    %
    Now given any $f \in L^p(\RR^n)$, for $1 \leq p < \infty$, fix $\varepsilon > 0$, and find $g \in C(\RR^n)$ with $\| f - g \|_p \leq \varepsilon$. Then
    %
    \[ |\{ M(f - g) > \lambda \}| \leq C \varepsilon^p \lambda^{-p}. \]
    %
    Thus
    %
    \begin{align*}
        &\left\{ \limsup_k \left| \fint_{t_k E} f(x) - f(x-y)\; dy \right| \geq \delta \right\}\\
        &\quad \subset \left\{ \limsup_k \left| \fint_{t_k E} f(x) - g(x) + g(x-y) - f(x-y)\; dy \right| \geq \delta \right\}\\
        &\quad \subset \left\{ x : |f(x) - g(x)| \geq \delta / 2 \right\} \cup \left\{ \limsup_k \left| \fint_{t_k E} g(x - y) - f(x-y)\; dy \right| \geq \delta/2 \right\}.
    \end{align*}
    %
    Now Markov's inequality implies that
    %
    \[ \left| \left\{ |f(x) - g(x)| \geq \delta / 2 \right\} \right| \leq \| f - g \|_p (\delta/2)^{-p} \lesssim_p \varepsilon \delta^{-p}. \]
    %
    and
    %
    \[ \left| \left\{ x: \limsup_k \left| \fint_{t_k E} g(x - y) - f(x-y)\; dy \right| \geq \delta/2 \right\} \right| \leq \left| \left\{ x: \left| M(g - f)(x) \right| \geq \delta/2 \right\} \right| \lesssim \varepsilon^p \delta^{-p}. \]
    %
    Thus we conclude that
    %
    \[ \left| \left\{ \limsup_k \left| \fint_{t_k E} f(x) - f(x-y)\; dy \right| \geq \delta \right\} \right| \lesssim (\varepsilon + \varepsilon^p) \delta^{-p}. \]
    %
    Taking $\varepsilon \to 0$ shows that for each $\delta > 0$,
    %
    \[ \left| \left\{ \limsup_k \left| \fint_{t_k E} f(x) - f(x-y)\; dy \right| \geq \delta \right\} \right| = 0. \]
    %
    Taking a union bound over $\delta = 1, 1/2, 1/4, \dots$ completes the proof.
\end{solution}




\question (Spring 2020)
  Let $f_{n}:[0,1]\to \R$ be a sequence of Lebesgue measurable functions such that $f_{n}$ converges to $f$ almost everywhere on $[0,1]$ and such that $\norm{f_{n}}_{L^{2}([0,1])}\leq 1$ for all $n$. Show that
  \begin{equation*}
    \limn \norm{f_{n}-f}_{L^{1}([0,1])}=0.
  \end{equation*}

\begin{solution}
  
  \textit{[This proof may border on trivializing the problem by invoking the Vitali convergence theorem, but I seem to have gotten away with a proof like this on my qual, so... -m]}. Since $|E|<\infty$, it follows trivially that $\{f_{n}\}$ is tight. Since $f_{n}\to f$ a.e., the result will then then follow by the Vitali Convergence Theorem provided that the sequence $\{f_{n}\}$ is uniformly integrable.
  We wish to show that $\left\{ f_{n} \right\}$ is uniformly integrable. Let $\epsilon>0$ and let $A\subset [0,1]$ be measurable. We need to show there exists a $\delta>0$ such that $\int_{A}|f_{n}| <\epsilon$ for all $n$ whenever $|A|<\delta$. By Cauchy-Schwarz inequality and the assumption that $\norm{f_{n}}_{L^{2}}\leq 1$ for all $n$,
  \begin{equation*}
    \int_{A}|f_{n}| = \int_{[0,1]}\chi_{A}|f_{n}| \leq |A|^{1/2}\norm{f_{n}}_{L^{2}} \leq |A|^{1/2}.
  \end{equation*}
  It follows that $\int_{A}|f_{n}|<\epsilon $ for all $n$ whenever $|A|<(\epsilon/2)^{2}$. Therefore $\left\{ f_{n} \right\}$ is uniformly integrable. This completes the proof.


Another (similar) proof is given:
\begin{proof}

  We first prove the following claim:
  \vt
  \noindent
  \textit{Claim 1:} $f_{n},f\in L^{1}([0,1])$ and $\norm{f_{n}}_{L^{1}},\norm{f}_{L^{1}}\leq 1$.
  \begin{proof}[Proof of Claim 1:]
    Since $x\mapsto x^{2}$ is convex and $[0,1]$ has measure 1, it follows by Jensen's inequality that
    \begin{equation*}
      \left(\int_{0}^{1}|f_{n}(x)|dx\right)^{2}\leq \int_{0}^{1}|f_{n}(x)|^{2}dx\leq 1.
    \end{equation*}
    Therefore $\norm{f_{n}}_{L^{1}}\leq 1$ and hence $f_{n}\in L^{1}([0,1])$ for all $n$. Since $f_{n}\to f$ a.e., it follows by Fatou's lemma that
    \begin{equation*}
      \int_{0}^{1}|f(x)|dx \leq \liminf_{n\to\infty}\int_{0}^{1}|f_{n}(x)|dx \leq 1.
    \end{equation*}
    Therefore $f\in L^{1}([0,1])$ with $\norm{f}_{L^{1}}\leq 1$  as well.
  \end{proof}



  By Egorov's Theorem, since $[0,1]$ has finite measure, there exists $E\subset [0,1]$ with $|E|<(\epsilon/4)^{2}$ such that $f_{n}\to f$ uniformly on $[0,1]\backslash E$. By uniform convergence, we may choose a positive integer $n_{0}$ such that $\left| f_{n}(x)-f(x) \right|< \epsilon/2$ for all $x\in [0,1]\backslash E$ and all $n\geq n_{0}$. Then for $n\geq n_{0}$,
  \begin{align*}
    \int_{0}^{1}\left| f_{n}-f \right|
    &= \int_{E} |f_{n}-f| + \int_{[0,1]\backslash E} |f_{n}-f|\\
    &< \int_{E} |f_{n}(x)-f(x)|dx + \epsilon/2\\
    &= \int_{[0,1]} \chi_{E}|f_{n}(x)|dx  + \int_{[0,1]}\chi_{E}|f(x)|dx + \epsilon/2 \\
    &\leq |E|^{1/2}\norm{f_{n}}_{L^{2}}+ |E|^{1/2}\norm{f}_{L^{2}} + \epsilon/2 &&\text{by Cauchy Schwarz}\\
    &\leq 2 |E|^{1/2}+ \epsilon/2 &&\text{by Claim 1}\\
    &< \epsilon/2+\epsilon/2\\
    &=\epsilon.
  \end{align*}
  It follows that $\limn \norm{f_{n}-f}_{L^{1}([0,1])}=0$.
\end{proof}
\end{solution}


\question (Spring 2020)
Let $\sum_{n=1}^{\infty}a_{n}$ be a convergent series. Let $b_{n}\in\R$ be an increasing sequence with $\lim_{n\to\infty}b_{n}=\infty$. Show that
\begin{equation*}
  \lim_{n\to\infty} \frac{1}{b_{n}}\sum_{k=1}^{n}b_{k}a_{k} = 0.
\end{equation*}

\begin{solution}
  Let $k_{n}= \max \left\{ k\geq 1: \left| \sum_{i=1}^{k}a_{i}b_{i} \right| < \sqrt{b_{n}}\text{ and }k\leq n\right\}$. Since $\sqrt{b_{n}}\to\infty$ monotonically as $n\to \infty$, $k_{n}$ is defined for sufficiently large $n$, is increasing, and $k_{n}\to\infty$. For $n$ sufficiently large, we may write
\begin{equation*}
  \frac{1}{b_{n}}\sum_{k=1}^{n}b_{k}a_{k} =   \frac{1}{b_{n}}\sum_{k=1}^{k_{n}-1}b_{k}a_{k} +  \frac{1}{b_{n}}\sum_{k=k_{n}}^{n}b_{k}a_{k}
\end{equation*}
By choice of $k_{n}$, $\left|\frac{1}{b_{n}}\sum_{k=1}^{k_{n}-1}b_{k}a_{k}\right|\leq \frac{1}{\sqrt{b_{n}}}\to 0$ as $n\to\infty$, so it suffices to show that
\begin{equation*}
  \limn \frac{1}{b_{n}}\sum_{k=k_{n}}^{n}b_{k}a_{k} = 0.
\end{equation*}
Let $T_{k}= \sum_{i=1}^{k}a_{k}$ and let $a=\sum_{n=1}^{\infty}a_{n}$. Using the summation by parts formula,
\begin{align*}
  \frac{1}{b_{n}}\sum_{k=k_{n}}^{n}b_{k}a_{k}
  &= \frac{1}{b_{n}}\left[ T_{n}b_{n} - T_{k_{n}-1}b_{k_{n}}+ \sum_{k=k_{n}}^{n-1}T_{k}(b_{k}-b_{k+1}) \right]\\
  &=\frac{1}{b_{n}}\Bigg[ (T_{n}-a)b_{n} +ab_{n}- (T_{k_{n}-1}-a)b_{k_{n}}-ab_{k_{n}}\\
  &\quad +\left.\sum_{k=k_{n}}^{n-1}(T_{k}-a)(b_{k}-b_{k+1}) +a\sum_{k=k_{n}}^{n-1}(b_{k}-b_{k+1}) \right]\\
  &= \frac{1}{b_{n}}\left[ (T_{n}-a)b_{n} - (T_{k_{n}-1}-a)b_{k_{n}}+ \sum_{k=k_{n}}^{n-1}(T_{k}-a)(b_{k}-b_{k+1})\right]
\end{align*}
where for the last equality we used the fact that $\sum_{k=k_{n}}^{n-1}(b_{k}-b_{k+1})= b_{k_{n}}-b_{n}$. Let $M_{n}= \max_{k_{n}\leq k <n}|T_{k}-a|$. By the triangle inequality, and using $b_{k_{n}}\leq b_{n}$,
\begin{align*}
  \left| \frac{1}{b_{n}}\sum_{k=k_{n}}^{n}b_{k}a_{k} \right|
  &\leq 2\left| T_{n}-a \right| +  \frac{1}{b_{n}}\left|\sum_{k=k_{n}}^{n-1}(T_{k}-a)(b_{k}-b_{k+1})\right|\\
  &\leq 2\left| T_{n}-a \right| +  \frac{M_{n}}{b_{n}}\sum_{k=k_{n}}^{n-1}|b_{k}-b_{k+1}|\\
  &= 2\left| T_{n}-a \right| +  \frac{M_{n}}{b_{n}}\sum_{k=k_{n}}^{n-1}b_{k+1}-b_{k}\\
  &= 2\left| T_{n}-a \right| +  \frac{M_{n}}{b_{n}}(b_{n}-b_{k_{n}})\\
  &\leq 2\left| T_{n}-a \right| +  2M_{n}
\end{align*}
The result then follows since $T_{n}\to a$ and $M_{n}\to 0$ as $n\to \infty$. 
\end{solution}

\question (Fall 2018)
  Let $1<p\leq \infty$. Let $(X,\mathcal{M},\mu)$ be a finite measure speace. Let $\left\{ f_{n} \right\}$ be a sequence of measurable functions converging $\mu$-a.e. to the function $f$. Assume further that $\norm{f_{n}}_{p}\leq 1$ for all $n$. Prove that $f_{n}\to f$ as $n\to \infty$ in $L^{r}$ for all $1\leq r<p$.
\begin{solution}
  \textit{Case 1.} If $p=\infty$ then since $\norm{f}_{\infty}\leq 1$ and $f_{n}\to f$ a.e., it follows that $\norm{f}_{\infty}\leq 1$. Therefore by the triangle inequality and using the fact that $r\geq 1$, we have $|f_{n}-f|^{r}\leq (|f_{n}| + |f| )^{r}\leq 2^{r}$. Since the constant function $2^{r}$ is integrable on $X$ with respect to $\mu$, it follows by the dominated convergence theorem that $\lim_{n\to\infty}\int_{X} |f_{n}-f|^{r}d\mu = 0$. Therefore $f_{n}\to f$ in $L^{r}$.

  \textit{Case 2.} Suppose $1<p<\infty$. Let $\epsilon>0$, and let $\epsilon_{0} = 2^{-r} \epsilon^{\frac{p}{p-r}}$ (here we use the assumption that $p>r$ so that $\epsilon_{0}>0$ is defined). By Egorov's theorem there exists $A\subset X$ such that $\mu(A^{c})<\epsilon_{0}$ and $f_{n}\to f$ uniformly on $A$. Then
  \begin{equation*}
    \int_{X} |f_{n}-f|^{r} = \int_{A^{c}}|f_{n}-f|^{r}d\mu + \int_{A}|f_{n}-f|^{r}d\mu
  \end{equation*}
  Since $r\geq 1$, it follows that $|f_{n}-f|^{r}\to 0$ uniformly. Therefore $\lim_{n\to\infty }\int_{A}|f_{n}-f|^{r}d\mu =0$. Therefore
  \begin{equation}\label{eq:2}
    \limsup_{n\to\infty} \int_{X} |f_{n}-f|^{r} \leq \limsup_{n\to\infty} \int_{A^{c}}|f_{n}-f|^{r}d\mu
  \end{equation}
  By Holder's inequality,
  \begin{align*}
    \int_{A^{c}}|f_{n}-f|^{r}d\mu
    &= \int_{X} |f_{n}-f|^{r}\chi_{A^{c}} d\mu\\
    &\leq \left( \int_{X}|f_{n}-f|^{p} d\mu\right)^{r/p}\left( \int_{X}\chi_{A^{c}}d\mu  \right)^{\frac{p-r}{p}}\\
    &= \norm{f_{n}-f}_{p}^{r}\mu(A^{c})^{\frac{p-r}{r}}\\
    &= \epsilon\norm{f_{n}-f}_{p}^{r}\\
    &\leq \epsilon ( \norm{f_{n}}_{p}+ \norm{f}_{p})^{r}\\
    &\leq \epsilon \left( 1 + \norm{f}_{p} \right)^{r}
  \end{align*}
  Also, by Fatou's lemma, $\norm{f}_{p}\leq \liminf_{n\to\infty} \norm{f_{n}}_{p}\leq 1$. Therefore
  \begin{equation*}
    \int_{A^{c}} | f_{n}-f|^{r}d\mu \leq \epsilon 2^{r}
  \end{equation*}
  Therefore by \eqref{eq:2},
  \begin{equation*}
     \limsup_{n\to\infty} \int_{X} |f_{n}-f|^{r}d\mu  \leq \epsilon 2^{r}
   \end{equation*}
   Since $\epsilon>0$ was arbitrary, it follows that $\limsup_{n\to\infty} \int_{X} |f_{n}-f|^{r}d\mu=0$. Therefore $f_{n}\to f$ in $L^{r}$.
\end{solution}

\question (Fall 2017) Let $f: \RR \to \RR$ be a compactly supported function that satisfies the H\"{o}lder condition with exponent $\beta \in (0,1)$, i.e. that there exists a constant $A < \infty$ such that for all $x,y \in \RR$, $|f(x) - f(y)| \leq A|x-y|^\beta$. Consider the function $g$ defined by
%
\[ g(x) = \int_{-\infty}^\infty \frac{f(y)}{|x-y|^\alpha}\; dy, \]
%
where $\alpha \in (0,\beta)$.
%
\begin{parts}
    \part Prove that $g$ is a continuous function at zero.
    \begin{solution}
        One way to see this immediately is to apply Young's inequality for convolution. More directly, we can write
        %
        \begin{align*}
            g(x) - g(0) = \int_{-\infty}^\infty f(y) \left( \frac{1}{|x - y|^\alpha} - \frac{1}{|y|^\alpha} \right)\; dy
        \end{align*}
        %
        For $|y| \leq |x|/2$, we have
        %
        \[ \left( \frac{1}{|x - y|^\alpha} - \frac{1}{|y|^\alpha} \right) \lesssim 1/|x|^\alpha. \]
        %
        Thus
        %
        \[ \left| \int_{|y| \leq |x|/2} f(y) \left( \frac{1}{|x - y|^\alpha} - \frac{1}{|y|^\alpha} \right)\; dy \right| \lesssim \| f \|_\infty |x|^{1-\alpha} \]
        %
        For $|y| \geq 2|x|$, we can apply Taylor's theorem / the mean value theorem to conclude that
        %
        \[ \left| \frac{1}{|x - y|^\alpha} - \frac{1}{|y|^\alpha} \right| \lesssim \frac{|x|}{|y|^{\alpha + 1}}. \]
        %
        This bound implies that
        %
        \[ \left| \int_{|y| \geq 2|x|} f(y) \left( \frac{1}{|x - y|^\alpha} - \frac{1}{|y|^\alpha} \right)\; dy \right| \lesssim \| f \|_\infty |x|^{1-\alpha}. \]
        %
        For $|x|/2 \leq |y| \leq 2|x|$, we have
        %
        \[ \left| \int_{|x|/2 \leq |y| \leq 2|x|} f(y) \left( \frac{1}{|x - y|^\alpha} - \frac{1}{|y|^\alpha} \right)\; dy \right| \leq 2 \| f \|_\infty \int_{|y| \leq 3|x|} \frac{1}{|y|^\alpha} \lesssim \| f \|_\infty |x|^{1-\alpha}. \]
        %
        These bounds together imply that
        %
        \[ |g(x) - g(0)| \lesssim \| f \|_\infty |x|^{1-\alpha}. \]
        %
        Thus $g(x) \to g(0)$ as $x \to 0$.
    \end{solution}
    
    \part Prove that $g$ is differentiable at zero. (Hint: Try the dominated convergence theorem).
    \begin{solution}
        Assume first that $f(0) = 0$. We have
        %
        \[ \frac{g(x) - g(0)}{x} = \int_{-\infty}^\infty f(y) \frac{1}{x} \left( \frac{1}{|y - x|^\alpha} - \frac{1}{|y|^\alpha} \right)\; dy. \]
        %
        The integrand here converges pointwise as $x \to 0$ to
        %
        \[ f(y) \alpha / |y|^{\alpha+1}, \]
        %
        which is integrable since the function is compactly supported and continuous away from the origin, and $|f(y)| \lesssim |y|^\beta$ near the origin. The mean value theorem implies that if $x \leq 1$,
        %
        \[ \left| \frac{1}{x} \left( \frac{1}{|y - x|^\alpha} - \frac{1}{|y|^\alpha} \right) \right| \lesssim 1/|y|^{\alpha+1} \]
        
        
        so we might expect that
        %
        \[ g'(0) = \int_{-\infty}^\infty f(y) \alpha / |y|^{\alpha+1}, \]
        %
        at least if the dominated convergence theorem
    \end{solution}
\end{parts}




\newpage
\section{Day 13: Interchanging limits and integrals + Arzela-Ascoli Theorem}

\subsection{Limits and Integrals (DCT and friends)}

\question (Rice, Winter 2011) Let $\{ f_n \}$ be a sequence of Lebesgue measurable functions defined on $[0,1]$ such that $\| f(x) \| \leq 1$ for all $n \geq 1$ and all $0 < x \leq 1$, and
\[ \lim_{n \to \infty} f_n(x) = f(x) \]
%
exists for each $0 \leq x \leq 1$. Prove that
%
\[ \lim_{n \to \infty} \int_0^1 \frac{f_n(x)}{\sqrt{|x-1/n|}} = \int_0^1 \frac{f(x)}{\sqrt{x}}\; dx \]
\begin{solution}
    Let $g_n(x) = f_n(x) / \sqrt{|x - 1/n|}$. We wish to apply the Vitali convergence theorem. To do this, let us first show that the sequence $\{ g_n \}$ is uniformly integrable. Now for a fixed $M > 0$ and $n > 0$, since $|f_n(x)| \leq 1$, if $|g_n(x)| \geq M$, then $|x- 1/n| \leq 1/M^2$, and so
    %
    \[ \int_{|g_n(x)| \geq M} |g_n(x)| \leq \int_{|x - 1/n| \leq 1/M^2} \frac{1}{|x - 1/n|^{1/2}} \leq \int_{|x| \leq 1/M^2} \frac{1}{|x|^{1/2}} \lesssim M^{-1}. \]
    %
    Thus
    %
    \[ \lim_{M \to \infty} \sup_{|g_n(x)| \geq M} |g_n(x)| = 0. \]
    %
    To show $g_n$ converges to $g(x) = f(x) / \sqrt{x}$, it thus suffices to show that $g_n$ converges to $g$ in measure. Now $g_n$ clearly converges to $g$ except when $x = 0$, and thus $g_n$ converges to $g$ almost everywhere. But this implies $g_n$ converges to $g$ in measure (since we are working over a finite measure space), and thus $g_n$ converges to $g$ in $L^1[0,1]$.
\end{solution}

\question (Rice, Spring 2005) Compute
%
\begin{parts}
    \part $\lim_{n \to \infty} \int_0^\infty \frac{x^{n-2}}{1 + x^n}$.
    \begin{solution}
        We apply the dominated convergence theorem. For $0 \leq x \leq 1$, $x^{n-2} / 1 + x^n \leq 1$, and for $1 \leq x \leq \infty$, $x^{n-2} / (1 + x^n) \leq x^{n-2} / x^n \leq 1/x^2$. Thus the family of integrands $x^{n-2} / (1 + x^n)$ is domainated by an integrable function, namely the function
        %
        \[ \mathbf{I}(0 \leq x \leq 1) + \mathbf{I}(1 \leq x) \cdot 1/x^2. \]
        %
        The sequence converges pointwise to zero for $0 \leq x < 1$, and for $x > 1$,
        %
        \[ \lim_{n \to \infty} x^{n-2} / (1 + x^n) = \lim_{n \to \infty} 1 / (x^2 + 1/x^{n-2}) = 1/x^2. \]
        %
        Thus the domainated convergence theorem implies that
        %
        \[ \lim_{n \to \infty} \frac{x^{n-2}}{1 + x^n} = \int_1^\infty \frac{1}{x^2} = 1. \]
    \end{solution}
    
    \part $\lim_{n \to \infty} n \int_0^\infty \frac{\sin y}{y(1 + n^2 y)}\; dy$ (Hint: Substitute $x = ny$).
    \begin{solution}
        We have
        %
        \[ n \int_0^\infty \frac{\sin y}{y(1 + n^2 y)}\; dy = n \int_0^\infty \frac{\sin(y/n)}{y (1 + ny)}\; dy. \]
        %
        For $y \geq 1$,
        %
        \[ n \frac{\sin(y/n)}{y ( 1 + ny) } \leq n \frac{1}{y(ny)} \leq 1/y^2. \]
        %
        For $y \leq 1$, we employ the bound $\sin(y/n) \leq y/n$ to conclude that
        %
        \[ n \frac{\sin(y/n)}{y ( 1 + ny) } \leq \frac{1}{1 + ny} \leq 1. \]
        %
        These bounds imply that the family of integrands is uniformly integrable. Now for all $0 \leq x \leq 1$,
        %
        \[ \lim_{n \to \infty} n \frac{\sin(y/n)}{y(1 + ny)} = \lim_{n \to \infty} \frac{1}{1 + ny} + O \left( n \frac{(y/n)^2}{y(1 + ny)} \right) = \lim_{n \to \infty} \frac{1}{1 + ny} + O \left( \frac{y}{n(1 + ny)} \right) = 0. \]
        %
        Thus the dominated convergence theorem implies that
        %
        \[ \lim_{n \to \infty} n \int_0^\infty \frac{\sin y}{y(1 + n^2 y)}\; dy = 0. \]
    \end{solution}
\end{parts}

\question (Spring 2017)
    Let $f: [0,\infty) \to \RR$ be a continuously differentiable function for which $\| f' \|_\infty < \infty$. Define, for $x > 0$,
    %
    \[ F(x) = \int_0^\infty f(x + yx) \psi(y)\; dy, \]
    %
    where $\psi$ satisfies
    %
    \[ \int_0^\infty |\psi(y)|\; dy \quad\text{and}\quad \int_0^\infty y \cdot |\psi(y)|\; dy < \infty. \]
    %
    Show that $F(x)$ is well defined for all $x \geq 0$, and that $F$ is continuously differentiable.
\begin{solution}
    The fundamental theorem of calculus implies that
    %
    \[ |f(x + yx)| = \left| f(x) + \int_x^{x+yx} f'(t)\; dt \right| = f(x) + O(yx). \]
    %
    Thus combined with the fact that $\int_0^\infty |\psi(y)|\; dy < \infty$ and $\int_0^\infty y \cdot |\psi(y)|\; dy < \infty$, this implies that
    %
    \[ \int_0^\infty |f(x + yx)| |\psi(y)|\; dy \lesssim |f(x)| + |x| < \infty, \]
    %
    so $F$ is well defined.
    
    To show that $F$ is continuously differentiable, we note that for a fixed $x \in [0,\infty)$, we calculate that, for $h$ with $0 \leq x + h$,
    %
    \begin{align*}
        \frac{F(x+h) - F(x)}{h} &= \int_0^\infty \frac{|f(x+yx + h(1 + y)) - f(x + yx)|}{h} \psi(y)\; dy.
    \end{align*}
    %
    Now
    %
    \[ \left| \frac{|f(x+yx + h(1 + y)) - f(x + yx)|}{h} \psi(y) \right| \lesssim (1 + y) |\psi(y)|. \]
    %
    Since $(1 + y) \psi(y)$ is integrable, the dominated convergence theorem implies that $F$ is differentiable at $x$, and
    %
    \[ F'(x) = \int_0^\infty f'(x + yx) (1 + y) \psi(y)\; dy. \]
    %
    Finally, we show $F'$ is continuous. We note that for any $\varepsilon > 0$, there exists $R > 0$ such that
    %
    \[ \int_R^\infty (1 + y) |\psi(y)|\; dy \leq \varepsilon. \]
    %
    Since $f'$ is continuous, it is uniformly continuous on $[0,2R(1 + x)]$. Thus there exists $\delta > 0$ such that for $|h| \leq \delta$, and $0 \leq y \leq R$, $|f'((x + yx) + h(1 + y)) - f(x + yx)| \leq \varepsilon$, and so
    %
    \begin{align*}
        F'(x+h) - F(x) &\lesssim \varepsilon + \int_0^R [f'((x + yx) + h(1 + y)) - f'(x + yx)] (1 + y) \psi(y)\; dy\\
        &\lesssim \varepsilon + \int_0^R \varepsilon (1 + y) \psi(y)\; dy \lesssim \varepsilon.
    \end{align*}
\end{solution}

\subsection{Arzela-Ascoli}


The key part of the Arzela-Ascoli theorem to know for the qual is the following: If $\{f_n\}\subset C[0,1]$ is a sequence which is uniformly bounded and equicontinuous, then $\{f_n\}$ has a uniformly convergent subsequence. 

(Note that we can replace $[0,1]$ by any compact subset of $\R^d$. Also, there is a converse to the theorem, but I haven't seen it used in any qual problems.)

By \textit{uniformly bounded}, we mean that $|f_n(x)|\leq C$ for all $x\in [0,1]$, $n\in \mathbb{N}$.

By \textit{equicontinuous}, we mean that for all $\epsilon>0$, there exists $\delta$ such that  $|f_n(x)-f_n(y)|<\epsilon$ whenever $|x-y|<\delta$ for all $n\in \mathbb{N}$.


\question (From a UBC Math 321 Midterm) Let $\{ f_n \}$ be a sequence of functions in $C[a,b]$ with no uniformly convergent subsequence. Define
%
\[ F_n(x) = \int_a^x \sin(f_n(t))\; dt. \]
%
Does $\{ F_n \}$ has a uniformly convergent subsequence.
\begin{solution}
    The sequence $\{ F_n \}$ does have a uniformly convergent subsequence, by Arzela-Ascoli. Indeed, it is simple to see that $\| F_n \|_\infty \leq b - a$ for all $n > 0$, so the sequence $\{ F_n \}$ is uniformly bounded. The sequence is also equicontinuous, because
    \[ |F_n(x) - F_n(y)| \leq |x - y| \]
    %
    uniformly in $n$. Thus Arzela-Ascoli implies the existence of a uniformly convergent subsequence.
\end{solution}

\question (Fall 2004) Let $f_{n}:[0,1]\to \R$ be a sequence of continuous functions whose derivatives $f'_{n}$ in the sense of distributions belong to $L^{2}(0,1)$. The functions also satisfy $f_{n}(0)=0$.
\begin{parts}
  \part
Assume that
\begin{equation*}
  \lim_{n\to\infty} \int_{0}^{1}f'_{n}(x)g(x)dx
\end{equation*}
exists for all $g\in L^{2}(0,1)$. Show that the $f_{n}$ converge uniformly as $n\to\infty$.
\begin{solution}
  Define $T_n(g) = \int_0^1 f_n'gdx$ for $g\in L^2(0,1)$. Then $T_n$ is clearly linear and by Cauchy-Schwarz inquality, $T_n$ is bounded. Then by the part (a) assumption, we have
  \begin{equation*}
  \sup \{ T_n(g) : n\geq 1 \} <\infty.
  \end{equation*}
  Therefore by the Uniform Boundedness Principle, 
  \begin{equation*}
  C:= \sup_n \norm{f_n'}_{L^2} <\infty
  \end{equation*}
  Uniform boundedness principle to obtain a uniform bound on $f_n'$. Then apply Arzela ascoli to a subsequence to obtain convergence of whole sequence. [Note the uniform boundedness of the derivatives suggests that we have uniform equicontinuity -- and hence maybe we can use the Arzela-Ascoli Theorem.] In particular, since $f'\in L^2[0,1]$, it follows that $f'\in L^1[0,1]$. Therefore
  \begin{equation}\label{newton-leibniz}
  f_n(x)-f_n(y) = \int_y^x f_n(t)dt
  \end{equation}
  Taking $y=0$ and using $f_n(0)=0$, we have 
  \begin{equation*}
  |f_n(x)|\leq \int_0^x |f_n'(t)|dt \leq \int_0^1 |f_n'(t)|dt \leq \(\int_0^1 |f_n'(t)|^2dt\)^{\frac{1}{2}} \leq C.
  \end{equation*}
  Therefore $\{f_n\}_n$ is uniformly bounded. In addition, by \eqref{newton-leibniz} and Cauchy-Schwarz inequality, we have 
  \begin{equation*}
  |f_n(x)-f_n(y)| \leq \left| \int_y^x f_n'(t)dt\right| \leq \sqrt{x-y} \(\int_y^x |f_n'|^2dt \)^{\frac{1}{2}} \leq C \sqrt{x-y}.
  \end{equation*}
  Therefore $\{f_n\}$ is uniformly equi-continuous. Therefore by the Arzela-Ascoli theorem, $\{f_n\}_{n\in \mathbb{N}}$ is a relatively compact subset of $C([0,1])$. 
  
  Recall that by assumption $\lim_{n\to\infty} \int_{0}^{1}f'_{n}(x)g(x)dx$ exists for any $g\in L^2[0,1]$. Taking $g(x) = \chi_{[0,a]}$, we have $\lim_{n\to\infty} \int_{0}^{a}f'_{n}(x)dx=\limn f'_n(a)$ exists. That is, there exists some real-valued function $f$ such that $f_n\to f$ pointwise.  
  
  Suppose that $f_n$ does not converge uniformly to $f$. Then there exists a subsequence $n_k$ and $\epsilon_0>0$ such that 
  \begin{equation*}
  \sup_{x\in[0,1]} |f(x)-f_{n_k}(x)|\geq \epsilon_0
  \end{equation*}
  for all $k$. However, since $\{f_n\}_n$ is relatively compact, there exists a subsequence $f_{n_{k_j}}$ which is uniformly Cauchy--and therefore must converge uniformly to $f$. But this implies that
  \begin{equation*}
  \sup_{x\in[0,1]} |f(x)-f_{n_{k_j}}(x)|\to 0 \quad \text{as }j\to\infty
  \end{equation*}
  This is a contradiction.
\end{solution}

\part Assume that
\begin{equation*}
  \lim_{n\to\infty} \int_{0}^{1}f'_{n}(x)g(x)dx
\end{equation*}
exists for all $g\in C([0,1])$. Do we still have the $f_{n}$ converge uniformly?
\begin{solution}
  Maybe yes by density?
\end{solution}
\end{parts}

\question (Spring 2014)

Consider the following operator

\begin{equation*}
  Af = \frac{1}{x \sqrt{1+|\log x|}} \int_{0}^{x}f(t)dt  
\end{equation*}
Is $A$ bounded as an operator from $L^{2}[0,1]$ to $L^{2}[0,1]$. Is it compact?
\begin{solution}
We first prove the following lemma: 

  \textit{Lemma 1 (Hardy's Inequality):} If $F(x)= \int_{0}^{x}f(t)dt$ then
  \begin{equation*}
    \int_{0}^{1}\left( \frac{F(x)}{x} \right)^{2}dx \leq 4 \int_{0}^{1}f^{2}(x)dx
  \end{equation*}
  \begin{proof}[Proof of Lemma:] 
    \begin{align*}
      \int_{0}^{1}\frac{F(x)^{2}}{x^{2}}dx
      &= - \left.\frac{F(x)^{2}}{x}\right|_{0}^{1} - \int_{0}^{1}\left( \frac{-1}{x} \right)2F(x)f(x)dx &&\text{Integration by parts}\\
      &\leq 2 \int_{0}^{1}\frac{F(x)}{x}f(x)dx\\
      &\leq 2 \left(  \int_{0}^{1}\frac{F(x)^{2}}{x^{2}}dx \right)^{\frac{1}{2}} \left( \int_{0}^{1}f(x)^{2}dx \right)^{\frac{1}{2}}&&\text{Cauchy-Schwarz}
    \end{align*}
    Dividing by $ \left(  \int_{0}^{1}\frac{F(x)^{2}}{x^{2}}dx \right)^{\frac{1}{2}}$ and squaring both sides gives the result.
  \end{proof}
  
  We will use Hardy's inequality to prove that $A$ is bounded. Letting $F(x) = \int_{0}^{x}f(t)dt$, we have
  \begin{align*}
    \norm{Af}_{2}^{2} = \int_{0}^{1}\frac{|F(x)|^{2}}{|x|^{2}} \frac{1}{1+|\log x|}dx \leq \int_{0}^{1}\frac{|F(x)|^{2}}{|x|^{2}}dx \leq 4 \norm{f}^{2}_{2}
  \end{align*}
  where the last inequality is due to Hardy's inequality. Since $A$ is clearly linear, it follows that $A$ is a bounded linear operator.
  
  Next we show that $A$ is a compact operator. Let $\left\{ f_{n} \right\}_{n\geq 1}$ be a bounded sequence in $L^{2}[0,1]$. To show that $A$ is compact, it suffices to show that there exists a subsequence $\left\{ f_{n_{k}} \right\}_{k\geq 1}$ such that $\left\{ Af_{n_{k}} \right\}_{k\geq 1}$ converges in $L^{2}[0,1]$.

  
  \vt
  \textit{Claim 2.} Let $F_{n}(x):=\int_{0}^{x}f_{n}(t)dt$. Then $\{F_{n}\}_{n=1}^{\infty}$ is a relatively compact subset of $C[0,1]$ (with respect to the topology induced by the uniform norm).

  To prove claim 2, we apply the Arzela-Ascoli Theorem. First, since $\{f_{n}\}_{n}$ is bounded in $L^{2}[0,1]$, there exists some $C>0$ such that $\norm{f_{n}}_{L^{2}}\leq C$ for all $n$. Then
  \begin{equation*}
    |F_{n}(x)| \leq \int_{0}^{x}|f_{n}(x)|dx \leq \(\int_{0}^{1}|f_{n}(x)|^{2}dx \)^{\frac{1}{2}} \leq C <\infty.
  \end{equation*}
  Therefore $\left\{ F_{n} \right\}$ is uniformly bounded. It remains to show that $\left\{ F_{n} \right\}$ is equicontinuous. By the Cauchy-Schwarz inequality,
  \begin{equation*}
    \left| F_{n}(x)-F_{n}(y) \right| = \left| \int_{y}^{x}f_{n}(t)dt \right| \leq \norm{f_{n}}_{L^{2}} \sqrt{|x-y|} \leq C \sqrt{|x-y|}.
  \end{equation*}
  Therefore  $\left\{ F_{n} \right\}$ is equicontinuous. Therefore by the Arzela-Ascoli theorem, $\{F_{n}\}_{n=1}^{\infty}$ is a relatively compact. This proves Claim 2.

  By Claim 2, there exists a subsequence $F_{n_{k}}$ such that $\{F_{n_{k}}\}_{k}$ is uniformly Cauchy. We claim that $\{Af_{n_{k}}\}_{k\geq 1}$ converges in $L^{2}$-norm, which will complete the proof. Indeed,
  \begin{align*}
    \norm{Af_{n_{j}}-Af_{n_{k}}}_{L^{2}}^{2}
    &\leq \int_{0}^{1}\frac{|F_{n_{j}}(x)-F_{n_{k}}(x)|^{2}}{|x|^{2}\left( 1+|\log x| \right)} dx\\
    &= \int_{0}^{\delta}\frac{|F_{n_{j}}(x)-F_{n_{k}}(x)|^{2}}{|x|^{2}\left( 1+|\log x| \right)} dx + \int_{\delta}^{1}\frac{|F_{n_{j}}(x)-F_{n_{k}}(x)|^{2}}{|x|^{2}\left( 1+|\log x| \right)} dx\\
    &\leq \frac{1}{1+|\log \delta |} \underbrace{\int_{0}^{\delta}\frac{|F_{n_{j}}(x)-F_{n_{k}}(x)|^{2}}{|x|^{2}} dx}_{\text{apply Hardy's inequality again}} + \norm{F_{n_{j}}-F_{n_{k}}}_{L^{\infty}} \cdot\frac{1}{\delta^{2}}\\
    &\leq \frac{1}{1+|\log \delta |} \norm{f_{n_{j}}-f_{n_{k}}}_{L^{2}} + \frac{1}{\delta^{2}} \norm{F_{n_{j}}-F_{n_{k}}}_{L^{\infty}}\\
    &\leq \frac{2C}{1+|\log \delta|} + \frac{1}{\delta^{2}} \norm{F_{n_{j}}-F_{n_{k}}}_{L^{\infty}}.
  \end{align*}
  Let $\epsilon>0$. Choose $\delta$ sufficiently small that $\frac{2C}{1+|\log \delta|}<\epsilon/2$, and choose $N\geq 0$ sufficiently large that $\norm{F_{n_{j}}-F_{n_{k}}}_{L^{\infty}}\leq \delta^{2}\epsilon/2$ whenever $j,k\geq N$. So $j,k\geq N$ implies $\norm{Af_{n_{j}}-Af_{n_{k}}}_{L^{2}}^{2} \leq \epsilon/2 + \epsilon/2= \epsilon$.
  
  We have shown that $\left\{ Af_{n_{k}} \right\}_{k}$ is Cauchy in $L^2[0,1]$, as required.
\end{solution}


\question (Problem 36 from the 2017 SEP) Consider the Hilbert space $L^{2}([0,1])$ with inner product $(f,g):=\int_{0}^{1}f(t)\bar{g}(t)dt$. Let $\left\{ e_{n} \right\}_{n=1}^{\infty}$ be an orthonormal system of functions in $L^{2}([0,1])$.
\begin{parts}
  \part
Suppose that $e_{n}\in L^{2}([0,1])$ for all $n\in\mathbb{N}$. Show that
\begin{equation*}
  \sup_{n}\max_{x\in[0,1]}|e_{n}'(x)| = \infty.
\end{equation*}

\begin{solution}
  Suppose not. Then there exists some $C<\infty$ such that
  \begin{equation*}
  \sup_n \max_{x\in [0,1]} \left| e'_{n}(x) \right| \leq C.
\end{equation*}
Therefore $|e_{n}(x)-e_{n}(y)| \leq C|x-y|$. Hence $\{e_{n}\}$ is equicontinuous.

By Chebychev's inequality,
\begin{equation*}
  \left| \left\{ x\in [0,1]: |e_{n}(x)|>\alpha \right\} \right|\leq \frac{\int_{0}^{1}e_{n}(x)^{2}dx}{\alpha^{2}}= \frac{1}{\alpha^{2}}.
\end{equation*}
Taking $\alpha=2$, we find that $\left| \left\{ x\in [0,1]: |e_{n}(x)|\leq 2 \right\} \right|>\frac{1}{4}$. Since every set with positive measure has at least one element, there exists some $x_{n}\in [0,1]$ such that $|e_{n}(x)|\leq 2$. Therefore for any $x\in [0,1]$ and any $n\in \mathbb{N}$, we have
\begin{equation*}
  |e_{n}(x)| = \left| e_{n}(x_{n})+\int_{x_{n}}^{x}e'_{n}(t)dt \right|\leq 2 + \left| x_{n}-x \right| \leq 3.
\end{equation*}
Therefore $\left\{ e_{n} \right\}$ is uniformly bounded. Therefore by the Arzela-Ascoli theorem, there exists some subsequence $(e_{n_{k}})$ such that $e_{n_{k}}\to f$ uniformly in $C[0,1]$ as $k\to \infty$. Therefore $e_{n_{k}}\to f$ in $L^{2}[0,1]$. But this contradicts the orthonormality of the sequence $\left\{ e_{n} \right\}_{n=1}^{\infty}$. (In particular, by orthonormality we have for all $m\neq n$: 
\begin{equation*}
  \norm{e_{n}-e_{m}}_{L^{2}} ^{2}= \langle e_{n}-e_{m},e_{n}-e_{m}\rangle = \langle e_{n},e_{n} \rangle - \langle e_{m},e_{n} \rangle - \langle e_{n},e_{m} \rangle + \langle e_{m},e_{m} \rangle =1-0-0+1 =2
\end{equation*}
and this implies that $\{e_{n}\}$ can have no subsequence which is Cauchy in $L^{2}$.)
\end{solution}

\part Suppose that $e_{n}$ is complete, which means $(g,e_{n})=0$ for all $n$ implies $g=0$ almost everywhere. Prove
\begin{equation*}
  \sum_{n=1}^{\infty}|e_{n}(x)|^{2}=\infty, \quad\text{almost everywhere.}
\end{equation*}

\begin{solution}
We first prove the following claim: 

  \textit{Claim 1.} If $E\subset [0,1]$ is measurable and $|E|>0$, then
  \begin{equation*}
    \int_{E} \sum_{n=1}^{\infty} \left| e_{n}(x) \right|^{2}dx \geq 1.
  \end{equation*}
  To prove the claim, observe that
  \begin{equation*}
    |E|= \norm{\chi_{E}}_{L^{2}}^{2}  = \sum_{n=1}^{\infty} (\chi_{E},e_{n})^{2} = \sum_{n=1}^{\infty} \left( \int_0^{1}\chi_{E}(x) e_{n}(x)dx \right)^{2}\leq \sum_{n=1}^{\infty} \(\int_{E} e_{n}^{2}dx \) |E|
  \end{equation*}
  where the second equality is by Completeness of $(e_{n})$ and the inequality is due to Cauchy-Schwarz. Dividing both sides by $|E|$ and applying the Fubini-Tonelli theorem proves the claim.

  Define
  \begin{equation*}
  A_M = \left\{ x:  \sum_{n=1}^{\infty} |e_n(x)|^2 \leq M\right\}.
  \end{equation*}
  Suppose there exists some $M>0$ such that $|A_M|>0$. Let $E_n\subset A_M$ with $0<|E_n|<1/n$. Then by Claim 1,
  
  $$1\leq \int_{E_n} \sum_{n=1}^{\infty} \left| e_{n}(x) \right|^{2}dx \leq \frac{M}{n} \to 0$$
  a contradiction. Therefore $|A_M|=0$ for all $M$. Since $\{A_M\}_M$ is an ascending sequence of sets, it follows by continuity of measure that
  
  $$\left|\left\{x: \sum_{n=1}^{\infty} |e_n(x)|^2 dx <\infty\right\}\right|=\left|\bigcup_{M=1}^{\infty} A_M\right| = \lim_{M\to\infty} |A_M| =0.$$
\end{solution}
\end{parts}





\newpage
\section{Day 14: Misc. Topics}

\question (Rice, Winter 2008) Is it possible to construct a measurable set $E \subset \RR$ of positive measure such that for any pair $a < b$, $|E \cap [a,b]| \leq 0.5 (b - a)$?
\begin{solution}
    No, since the Lebesgue differentiation theorem implies that if $E$ has positive measure, then there exists $x_0$ such that
    %
    \[ \limsup_{x_0 \in I} |E \cap I|/|I| = 1. \]
    %
    However, if $E$ is as above, then $|E \cap I| / |I| \leq 0.5$ for any interval $I$, so $x_0$ cannot possibly exist. Thus any $E$ satisfying the condition above must have measure zero.
\end{solution}

\question (Spring 2010) For $\lambda>0$, set
\begin{equation*}
  F(\lambda)= \int_{0}^{1}e^{-10\lambda x^{4}+\lambda x^{6}}dx
\end{equation*}
Prove there exists constants $A$ and $C>0$, such that $F(\lambda)=\frac{A}{\lambda^{\frac{1}{4}}} + E(\lambda)$ where $|E(\lambda)|\leq \frac{C}{\lambda^{\frac{1}{2}}}$.
\begin{solution}
  Supposing $A$ and $C$ exist, then $A=\lim_{\lambda\to\infty} \lambda^{\frac{1}{4}}F(\lambda)$. To determine the value of $A$, we make the $u$-substitution $u=\lambda^{\frac{1}{4}}x$, which gives
  \begin{equation}\label{eq:u-form-of-F}
    \lambda^{\frac{1}{4}}F(\lambda)= \int_{0}^{\lambda^{\frac{1}{4}}}e^{-10u^{4}+\lambda^{-\frac{1}{2}}u^{6}}du = \int_{0}^{\infty}\chi_{\(0,\lambda^{\frac{1}{4}}\)}e^{-10u^{4}+\lambda^{-\frac{1}{2}}u^{6}}du.
  \end{equation}
  Note that for $0<u<\lambda^{\frac{1}{4}}$, it holds that $\lambda^{-\frac{1}{2}}u^{6} \leq u^{4}$. It follows that
  \begin{equation*}
    \chi_{\(0,\lambda^{\frac{1}{4}}\)}e^{-10u^{4}+\lambda^{-\frac{1}{2}}u^{6}}\leq e^{-9u^{4}}.
  \end{equation*}
  Since the right-hand side is integrable and $\chi_{\(0,\lambda^{\frac{1}{4}}\)}e^{-10u^{4}+\lambda^{-\frac{1}{2}}u^{6}}\to \chi_{(0,\infty)}e^{-10u^{4}}$ pointwise as $\lambda\to\infty$, it follows by the Dominated Convergence Theorem that
  \begin{equation*}
    \lim_{\lambda\to\infty} \lambda^{\frac{1}{4}}F(\lambda) = \int_{0}^{\infty}e^{-10u^{4}}du.
  \end{equation*}
  So let $A:= \int_{0}^{\infty}e^{-10u^{4}}du$. We need to show that there exists some constant $C$ such that
  \begin{equation*}
     \left|\lambda^{\frac{1}{4}}F(\lambda) - A \right| \leq \frac{C}{\lambda^{\frac{1}{4}}}.
   \end{equation*}
In particular, by equation \eqref{eq:u-form-of-F} and the triangle inequality,
   \begin{align*}
     \left|\lambda^{\frac{1}{4}}F(\lambda) - A\right|
     &\leq \underbrace{\left|\int_{0}^{\lambda^{\frac{1}{4}}} e^{-10u^{4}}\left( e^{\lambda^{-\frac{1}{2}}u^{6}}-1 \right)du\right|}_{\text{Call this } g(\lambda)} + \underbrace{\int_{\lambda^{\frac{1}{4}}}^{\infty}e^{-10u^{4}}du}_{\text{Call this }h(\lambda)},
   \end{align*}
   so it will suffice to show that $\lambda^{\frac{1}{4}}g(\lambda)\leq C_{1}$ and $\lambda^{\frac{1}{4}}h(\lambda)\leq C_{2}$ for all $\lambda$.

  To estimate $\lambda^{\frac{1}{4}}g(\lambda)$, we use the inequality $|e^{x}-1|\leq |x|e^{x}$  (which holds for all $x>0$ since $|e^{x}-1|=|\int_{0}^{x}e^{t}dt|\leq xe^{x}$). Using this inequality, we have:
   \begin{align*}
     \lambda^{\frac{1}{4}}g(\lambda)
     &\leq \lambda^{\frac{1}{4}}\int_{0}^{\lambda^{\frac{1}{4}}} e^{-10u^{4}}\lambda^{-\frac{1}{2}}u^{6}e^{\lambda^{-\frac{1}{2}}u^{6}}du\\
     &=\lambda^{-\frac{1}{4}}\int_{0}^{\lambda^{\frac{1}{4}}} u^{6}e^{-10u^{4}+\lambda^{-\frac{1}{2}}u^{6}}du\\
     &\leq \int_{0}^{\lambda^{\frac{1}{4}}} u^{5}e^{-9u^{4}}du &&\text{using }u\leq \lambda^{\frac{1}{4}}\\
     &\leq  C_{1}
   \end{align*}
   where $C_{1} := \int_{0}^{\infty} u^{5}e^{-9u^{4}}du<\infty$.
   
   It remains to estimate $\lambda^{\frac{1}{4}}h(\lambda)$. Using $\lambda^{\frac{1}{4}}\leq u$, we have
   \begin{equation*}
     \lambda^{\frac{1}{4}}h(\lambda)
      = \lambda^{\frac{1}{4}}\int_{\lambda^{\frac{1}{4}}}^{\infty}e^{-10u^{4}}du
     < \int_{\lambda^{\frac{1}{4}}}^{\infty}ue^{-10 u^{4}}du < C_{2}
   \end{equation*}
   where $C_{2} := \int_{0}^{\infty}ue^{-10u^{4}}du<\infty$. This completes the proof.
 \end{solution}

\question (Fall 2010)
Let $I = [0,1]$ and define for $f\in L^{2}(I)$ the Fourier coefficients as $\hat{f}(k) =\int_{0}^{1}f(t)e^{-2\pi i k t}dt$ for any $k\in \Z$. 


\begin{parts}
\part Let $\mathcal{G}$ be the set of all $L^{2}(I)$ functions with the property that  $|\hat{f}(0)|\leq 1$ and  $|\hat{f}(k)|\leq |k|^{-3/5}$ for any $k\in \Z$, $k\neq 0$. Prove that $\mathcal{G}$ is a compact subset of $L^{2}(I)$.


\begin{solution}

  Let $f_{n}\in \mathcal{G}$. It will suffice to show that there exists a subsequence $f_{n_{j}}$ and an $f\in \mathcal{G}$ such that $f_{n_{j}}\to f$ in $L^{2}(I)$.

  We will apply Cantor's diagonal sequence argument to obtain a subsequence $n_{j}$ such that $\lim_{j\to\infty}\hat{f}_{n_{j}}(k)$ exists for all $k\in \Z$. The details of this argument are as follows:

  
  First, since $\{\hat{f}_{n}(0)\}_{n\in\mathbb{N}}$ is a bounded sequence of real numbers, there exists a subsequence $\{s(n,0)\}_{n\in \mathbb{N}}$ of $\mathbb{N}$ such that $\limn \hat{f}_{s(n,0)}(0)= c_{0}$ for some $c_{0}\in \R$.

  Next, since $|\hat{f}_{s(n,0)}(k)|\leq |k|^{-3/5}$ for any $k\in \Z\backslash \left\{ 0 \right\}$, it follows that  $\{\hat{f}_{s(n,0)}(1)\}_{n\in\mathbb{N}}$ and $\{\hat{f}_{s(n,0)}(-1)\}_{n\in\mathbb{N}}$ are both bounded sequences of real numbers. Therefore there exists a subsequence $\left\{ s(n,1) \right\}_{n\in \mathbb{N}}$ of  $\left\{ s(n,0) \right\}_{n\in \mathbb{N}}$ such that $\limn \hat{f}_{s(n,1)}(1) = c_{1}$ and $\limn \hat{f}_{s(n,1)}(-1) = c_{-1}$ for some $c_{1},c_{-1}\in \R$.
  
  In general, suppose we have chosen the subsequence $s(n,m-1)$. Then by the inequality $|\hat{f}(k)|\leq |k|^{-3/5}$, it follows that both $\{\hat{f}_{s(n,m-1)}(m)\}_{n\in\mathbb{N}}$ and $\{\hat{f}_{s(n,m-1)}(-m)\}_{n\in\mathbb{N}}$ are bounded sequences of real numbers, and therefore we may choose a further subsequence $\{s(n,m)\}_{n\in\mathbb{N}}$ of $\{s(n,m-1)\}_{n\in\mathbb{N}}$ such that 
  $\limn \hat{f}_{s(n,m)}(m) = c_{m}$ and $\limn \hat{f}_{s(n,m)}(-m) = c_{-m}$ for some $c_{m},c_{-m}\in \R$.

  We now have in our hands a nested sequence of subsequences
  \begin{equation*}
    \left\{ s(n,0) \right\}_{n\in\mathbb{N}}\supset  \left\{ s(n,1) \right\}_{n\in\mathbb{N}}\supset \left\{ s(n,2) \right\}_{n\in\mathbb{N}}\supset \cdots
  \end{equation*}
  Now consider the diagonal subsequence $\{s(n,n)\}_{n\in \mathbb{N}}$. By construction,
  \begin{equation}
    \label{eq:1}
    \lim_{n\to\infty} \hat{f}_{s(n,n)}(k) =c_{k}\in \R
  \end{equation}
  for all $k\in \Z$.

  Let $f(t)=\sum_{k\in\Z} c_{k}e^{2\pi i k t}$. By Parseval's identity and the inequality $|\hat{f}(k)|\leq |k|^{-3/5}$, we have
  \begin{equation*}
    \norm{f}^{2}_{L^{2}(I)} =  \sum_{k\in\Z}|\hat{f}(k)|^{2} \leq  \left( 1 + 2\sum_{k\geq 1}|k|^{-6/5} \right)<\infty
  \end{equation*}
  Therefore $f$ is well defined and in $L^{2}(I)$.
  
  It remains to show that $f_{s(n,n)}\to f$ in $L^{2}(I)$. To see this, observe that by Parseval's identity,
  \begin{align*}
    \norm{f_{s(n,n)}-f}_{L^{2}}^{2}
    &= \sum_{k\in \Z} \left| \hat{f}_{s(n,n)}(k)-c_{k} \right|^{2}\\
    &= \sum_{|k|\leq N} \left| \hat{f}_{s(n,n)}(k)-c_{k} \right|^{2} + \sum_{|k|> N} \left| \hat{f}_{s(n,n)}(k)-c_{k} \right|^{2}\\
    &\leq \sum_{|k|\leq N} \left| \hat{f}_{s(n,n)}(k)-c_{k} \right|^{2} + \sum_{|k|> N} \left| K \right|^{-6/5}
  \end{align*}
  Therefore
  \begin{align*}
    \limsup_{n\to\infty}  \norm{f_{s(n,n)}-f}_{L^{2}}^{2}
    &\leq \sum_{|k|> N} \left| K \right|^{-6/5} \to 0 \text{ as }N\to\infty
  \end{align*}
  Therefore $f_{s(n,n)}\to f$ in $L^{2}(I)$.
\end{solution}

\part Let $\mathcal{E}$ be the set of all $L^{2}(I)$ functions with the property that $\sum_{k}|\hat{f}(k)|^{5/3}\leq 2016^{-2016}$. Is $\mathcal{E}$ a compact subset of $L^{2}(I)$?

\begin{solution}
  $\mathcal{E}$ is not compact. To see this, let $f_{n}(t)= e^{2\pi i n t}$. Then $\hat{f_{n}}(k) = \delta(k-n)$ where $\delta$ is the Kronecker-delta function. If $\epsilon_{0} \in (0,2016^{-2016})$, then $\left\{ \epsilon_{0}f_{n} \right\}_{n=1}^{\infty}\subset \mathcal{E}$ but it is not compact because it contains no convergent subsequence.
\end{solution}
\end{parts}

\question (Fall 2011) Let $\ell^{2}(\mathbb{N})$ denote the Hilbert space of square summable sequences with inner product $(x,y)= \sum_{n=1}^{\infty}x_{n}y_{n}$, where $x=(x_{1},x_{2},\cdots)$ and $y=(y_{1},y_{2},\cdots)$.
\begin{parts}
\part What are the necessary and sufficient conditions on $\lambda_n>0$ for the set
\begin{equation*}
  S = \left\{ (x_{1},x_{2}\cdots) \in \ell^{2}(\mathbb{N}): |x_{n}| \leq \lambda_{n}, \forall n\right\}
\end{equation*}
to be compact in $\ell^{2}(\mathbb{N})$? 
\begin{solution}
    For any sequence $\{ x(1), x(2), \dots \}$ in $S$, and any fixed $i$, the sequence $\{ x(1)_i, x(2)_i, \dots \}$ is bounded, and thus has a convergent sub-sequence. Assuming that
    %
    \[ \lim_{N \to \infty} \sum_{i = N}^\infty \lambda_i^2 = 0, \]
    %
    we will use diagonalization to construct a convergent subsequence. This is necessary, because the sequence $\{ (\lambda_1,0, \dots), (\lambda_1,\lambda_2,0,\dots), (\lambda_1,\lambda_2,\lambda_3, 0, \dots), \dots \}$ only has a convergent subsequence if this property was true.
    
    We will define a family of elements $x(i,j) \in l^2(\mathbf{N})$ for $0 \leq i < \infty$ and $1 \leq j < \infty$, such that for $0 < k \leq i$, $x(i,j)_k$ converges as $j \to \infty$. Begin by setting $x(0,i) = x(i)$. Given $x(i_0,j)$, the sequence $x(i_0,j)_{i_0 + 1}$ is bounded in $j$, so we can find a sequence $y(j)$ such that $y(j)_{i_0 + 1}$ converges. Let $x_{i_0+1,j}$ be this convergent sub-sequence. Now define $y(k) = x(k,k)$. For each $i$, the sequence $\{ y(k) : k > i \}$ is a subsequence of $\{ x(i,j) \}$, and so $y(k)_i$ converges as $i \to \infty$ for each $i$. Thus for any fixed $K$,
    %
    \[ \limsup_{N \to \infty} \sup_{M > N} \sum_{j \leq K} |y(N)_j - y(M)_j|^2 = 0. \]
    %
    But
    %
    \[ \limsup_{N \to \infty} \sup_{M > N} \sum_{j > K} |y(N)_j - y(M)_j|^2 \leq 4 \sum_{j > K} \lambda_j^2. \]
    %
    Thus for any $K$,
    %
    \[ \limsup_{N \to \infty} \sum_{M > N} \| y(N) - y(M) \|_2 \leq 2 \left( \sum_{j > K} \lambda_j^2 \right)^{1/2} \]
    %
    Taking $K \to \infty$ gives
    %
    \[ \limsup_{N \to \infty} \sum_{M > N} \| y(N) - y(M) \|_2 = 0, \]
    %
    and so $\{ y(k) \}$ is a Cauchy sequence in $l^2(\mathbf{N})$, and is thus convergent.
\end{solution}

\part What are the necessary and sufficient conditions on $\mu_{n}>0$ for the set
\begin{equation*}
  \left\{ (x_{1},x_{2}\cdots) \in \ell^{2}(\mathbb{N}): \sum_{n}\frac{|x_{n}|^{2}}{\mu_{n}^{2}}\leq 1\right\}
\end{equation*}
to be compact in $\ell^{2}(\mathbb{N})$?
\begin{solution}
The necessary and sufficient condition is that $\mu_n\to 0$ as $n\to \infty$.

\vt
\textit{Proof of Necessity:} Suppose that $\mu_n\not\to 0$. Then there exists an $\epsilon>0$ and a subsequence $\{\mu_{n_j}\}$ with $\mu_{n_j}\geq \epsilon$.
Let $x^{(j)} = (0,\cdots,\mu_{n_j},0,\cdots)$. It is easy to check that this sequence has no convergent subsequence.

\vt
\textit{Proof of Sufficiency:} Suppose $\mu_n\to 0$. 


Let $\{x^{(n)}\}$ be a seqeunce with $x^{(n)}\in \left\{ (x_{1},x_{2}\cdots) \in \ell^{2}(\mathbb{N}): \sum_{n}\frac{|x_{n}|^{2}}{\mu_{n}^{2}}\leq 1\right\}$.

By a Cantor diagonalization argument, there exists a subsequence $\{x^{(n_j)}\}_j$ such that $x^{n_j}_k\to y_k\in \R$ for each $k$ as $j\to\infty$. 

We then check that that $y = (y_1,y_2,\cdots) \in \ell^2$ and that $x^{n_j}\to y$ in $\ell^2$.

\end{solution}
\end{parts}


\question Let $f:\R\to\R$ be a convex function, let $E=\left\{ x\in \R : f \text{ is not differentiable at }x\right\}$. Show that $E$ is at most countable.


\begin{solution}
For each $x_{0}\in \R$, define $f_{-}'(x_{0}) := \lim_{x\to x_{0}^{-}} \frac{f(x)-f(x_{0})}{x-x_{0}}$ and $f_{+}'(x_{0}) := \lim_{x\to x_{0}^{+}} \frac{f(x)-f(x_{0})}{x-x_{0}}$.

\vt
\textit{Claim 1:} Both $f_{-}'(x_{0})$ and $f_{+}'(x_{0})$ exist and are finite for all $x_{0}\in \R$. 


To prove this, we will demonstrate that $f_{-}'(x_{0})$ exists and is finite, as the case for $f_{+}'(x_{0})$ is similar. Recall the chordal slope property of convex functions: if $f$ is convex on $\R$, then for any $x_{1}<x_{2}<x_{3}$, the following inequality holds (draw a picture of this)
\begin{equation}\label{eq:chord-inequality}
  \frac{f(x_{2})-f(x_{1})}{x_{2}-x_{1}}\leq \frac{f(x_{3})-f(x_{1})}{x_{3}-x_{1}}\leq \frac{f(x_{3})-f(x_{2})}{x_{3}-x_{2}}.
\end{equation}

The proof of \eqref{eq:chord-inequality} requires only the definition of convexity and a little bit of algebra.  Suppose $x_{1}<x_{2}<x_{3}$. Recall $f$ convex means that for all $a,b\in \R$ and all $0\leq \lambda\leq 1$, we have
  \begin{equation*}
    f(\lambda x + (1-\lambda)y)\leq \lambda f(x) + (1-\lambda)f(y).
  \end{equation*}
  Taking $x=x_{1}, y=x_{3}$ and $\lambda=\frac{x_{3}-x_{2}}{x_{3}-x_{1}}$ (so that $x_{2}= \lambda x_{1} + (1-\lambda)x_{3}$) in the above equation, we obtain 
\begin{equation*}
  \frac{f(x_{2})-f(x_{1})}{x_{2}-x_{1}}\leq \frac{f(x_{3})-f(x_{2})}{x_{3}-x_{2}}.
\end{equation*}
Then, with a little more algebra, we can rewrite this inequalilty as the equivalent forms
\begin{equation*}
  \frac{f(x_{2})-f(x_{1})}{x_{2}-x_{1}}\leq \frac{f(x_{3})-f(x_{1})}{x_{3}-x_{1}} \quad \text{and}\quad \frac{f(x_{1})-f(x_{3})}{x_{1}-x_{3}}\leq \frac{f(x_{2})-f(x_{3})}{x_{2}-x_{3}}.
\end{equation*}
This proves \eqref{eq:chord-inequality}

From \eqref{eq:chord-inequality}, we can deduce the following:
\begin{enumerate}
\item For each fixed $x_{0}\in \R$, the function $x\mapsto \frac{f(x)-f(x_{0})}{x-x_{0}}$ is increasing on $(-\infty,x_{0})$.
\item The family $\left\{   \frac{f(x)-f(x_{0})}{x-x_{0}}:  x\in (-\infty,x_{0})\right\}$ is bounded above by $\frac{f(x_{0}+1)-f(x_{0})}{(x_{0}+1)-x_{0}} = f(x_{0}+1)-f(x_{0})<\infty$.
\end{enumerate}
These two facts together imply that the limit
\begin{equation*}
  f_{-}'(x_{0}):=\lim_{x\to x_{0}^{-}}\frac{f(x)-f(x_{0})}{x-x_{0}}
\end{equation*}
is both increasing and bounded above, and therefore exists as a real number. The case for $f_{+}'(x_{0})$ is similar. This proves Claim 1.



By Claim 1, it follows that $f$ is not differentiable at a point $x\in \R$ if and only if $f'_{+}(x)\neq f'_{-}(x)$. Let $E=\left\{ x\in \R : f'_{-}(x)\neq f'_{+}(x) \right\}$. We need to show that $E$ is at most countable.


To see this, we first make the following observations, all of which follow from \eqref{eq:chord-inequality}:
\begin{enumerate}[(a)]
\item \label{item:3} $f'_{+}$ and $f'_{-}$ are both increasing functions
\item \label{item:5} $f'_{-}(x)\leq f'_{+}(x)$ for all $x\in\R$
\item \label{item:4} $f_{+}'(a) \leq \frac{f(b)-f(a)}{b-a}\leq f'_{-}(b)$ if $a<b$
\end{enumerate}

By \ref{item:3}, $f_{+}'$ and $f'_{-}$ are continuous at all but countably many points. That is, the set  $$D:=\left\{ x\in \R: f_{+}'\text{ is not continuous at }x \right\}\cup \left\{ x\in \R: f_{-}'\text{ is not continuous at }x \right\}$$ is countable. We will show that $E\subset D$, which will complete the proof.

Suppose $x\notin D$. We need to show that $x\notin E$. By \ref{item:5}, $E=\left\{ x\in \R : f'_{-}(x)< f'_{+}(x) \right\}$, so it will suffice to show that $f'_{-}(x)\geq f_{+}'(x)$.

Let $x_{n}$ be a sequence such that $x_{n}>x$ and $x_{n}\to x$. Since $x\notin D$,
\begin{equation*}
  f'_{-}(x)=\limn f'_{-}(x_{n})\geq f'_{+}(x)
\end{equation*}
where the final inequality is justified by \ref{item:4} since $x_{n}>x$.

(This proof was adapted from Royden and Fitzpatrick's \textit{Real Analysis} 4th ed.)
\end{solution}

\question (Fall 2015) Identify all $\alpha\in \R$ such that $\lim_{n\to\infty} \sin(2 \pi n \alpha)$ exists.

\begin{solution}
	Define $f(\alpha)=\sin(2\pi \alpha)$. It will suffice to characterize the $\alpha$ for which $\limn f(n\alpha)$ converges, since $\limn f(n\alpha)$ exists if and only if $\limn \sin(n2\pi \alpha)$ exists. Observe that $f$ is periodic: if $k\in \Z$ then $f(x)=\sin(2\pi x) = \sin(2\pi x +2\pi k) =f(x+k)$. By periodicity it suffices to consider $\alpha\in [0,1)$, since $\limn f(n\alpha)$ converges if and only if $\limn f(n(\alpha+k))$ converges for all $k\in \Z$. 

  \textit{Case 1.} Suppose $\alpha \notin \Q$. Let $\xi_{n}:= n\alpha - \lfloor n\alpha \rfloor$. Then by \emph{Weyl's equidistribution theorem}, the sequence $\{ \xi_{n} \}$ is equidistributed on $[0,1)$ and hence is dense. If we choose a sequence $n_k$ such that $\xi_{n_k} \to 3/4$ as $k \to \infty$, and another sequence $m_k$ such that $\xi_{m_k} \to 1/4$, then we conclude that by continuity,
  %
  \[ \liminf_{n \to \infty} f(n \alpha) \leq \liminf_{k \to \infty} f(n_k \alpha) = -1 \]
  %
  and
  %
  \[ \limsup_{n \to \infty} \sin(2 \pi n \alpha) \geq \limsup_{k \to \infty} \sin(2 \pi m_k) = 1. \]
  %
  This shows the $\liminf$ differs from the $\limsup$, so $\limn \sin(2 \pi n \alpha)$ cannot exist. Therefore $\sin(2\pi n \alpha)$ does not converge for any $\alpha\notin \Q$
  
  \textit{Case 2.} Suppose $\alpha\in\Q$. Write $\alpha = p/q$ where $p,q$ are integers with $\gcd(p,q)=1$. Let $x = 2\pi p/q$. Then the sequence $\xi_{n}:= n\alpha-\lfloor n \alpha \rfloor$ is periodic with $q$ distinct values, one of which is zero. Indeed,
  \begin{equation*}
    \xi_{q}=q \frac{p}{q} - \left\lfloor q \frac{p}{q} \right\rfloor = p-p = 0
  \end{equation*}
  and
  \begin{equation*}
    \xi_{q+1}= (q+1)\frac{p}{q} - \left\lfloor \frac{(q+1)p}{q} \right\rfloor = \frac{p}{q} - \left\lfloor \frac{p}{q}\right\rfloor = \xi_{1}.
  \end{equation*}
  Since $f(n\alpha) = f(\xi_{n})$, it follows that the seuence $\{f(n\alpha)\}_{n=1}^{\infty}$ is periodic and takes up to $q$ distinct values. Therefore, in order for $\limn f(n\alpha)$ to converge, we must have
  %
  \[ f(\xi_{1})=f(\xi_{2})=\ldots=f(\xi_{q})=f(0)=0. \]
  %
  By periodicity of $f$, this is equivalent to
  %
	\begin{equation*}
    f\(\frac{p}{q}\)= f\(\frac{2p}{q}\)=\ldots=f\left(\frac{(q-1)p}{q}\right)=f(p)
  \end{equation*}
  and
  \begin{equation*}
    f(p) = \sin(2\pi p)=0.
  \end{equation*}
  Therefore $\sin(2\pi p/q)=0$. This implies that $q=1$ or $q=2$. So either $\alpha =0$ or $1/2$ (mod 1). That is, $x=k\pi$ for $k\in \Z$.
\end{solution}


\newpage
\section{Day 15: Spring 2021 Final Qualifying Exam}

\question (3) For a Lebesgue measurable subset $E$ of $\R$, denote $\textbf{1}_E$ the indicator function of $E$ (i.e. $\textbf{1}_E(x)=1$ for $x\in E$ and $\textbf{1}_E(x) = 0$ for $x\in E^c$).

Let $\{E_n: n\in \mathbb{N}\}$ be a family of Lebesgue measurable subsets of $\R$ with finite measure and let $f$ be a measurable function such that \begin{equation*}
\limn \int_{\R} | f(x)-\textbf{1}_{E_n}(x)| dx = 0.
\end{equation*}
Prove that $f$ is the indicator function of a measurable set.

\begin{solution}
For each $\epsilon>0$, 
\begin{equation*}
  \epsilon \mu \left\{ x: |f(x)-\mathbf{1}_{E_{n}}(x)| >\epsilon \right\}
  <  \int_{\left[|f-\mathbf{1}_{E_{n}}| >\epsilon \right]} |f(x)-\mathbf{1}_{E_{n}}(x)|dx
  \leq \int_{\R} |f(x)-\mathbf{1}_{E_{n}}(x)|dx \to 0
\end{equation*}
as $n\to \infty$.
Therefore $\mathbf{1}_{E_{n}}\to f$ in measure. By definition of convergence in measure, we may choose a subsequence $n_{k}$ such that
\begin{equation*}
  \mu \left\{ x: |f(x)-\mathbf{1}_{E_{n_{k}}}(x)|>\epsilon \right\} \leq 2^{-k}.
\end{equation*}
Therefore
\begin{equation*}
  \sum_{k\geq 1}\mu \left\{ x: |f(x)-\mathbf{1}_{E_{n_{k}}}(x)|>\epsilon \right\}<\infty.
\end{equation*}
Therefore by the Borel-Cantelli lemma,
\begin{equation*}
  \mu \left\{ x: |f(x)-\mathbf{1}_{E_{n_{k}}}(x)|>\epsilon \text{ for infinitely many }k \right\}=0.
\end{equation*}
In other words, $\lim_{k\to \infty} \mathbf{1}_{E_{n_{k}}}(x) = f(x)$ for a.e. $x\in \R$. Therefore $f$ is equal to zero or one almost everywhere, and as the a.e. limit of measureable functions, $f$ is also measurable. Therefore $f$ is the indicator function of a measurable set.

\end{solution}


\question (Spring 2021) Let $f$ be a $C^1$ function on $[0,\infty)$. Suppose that
%
\[ \int_0^\infty t |f'(t)|^2\; dt < \infty \]
%
and
%
\[ \lim_{T \to \infty} \frac{1}{T} \int_0^T f(t)\; dt = L. \]
%
Show that $f(t) \to L$ as $t \to \infty$.
\begin{solution}
    To relate $f$ and $f'$, we apply the fundamental theorem of calculus. Suppose that
    %
    \[ \int_R^\infty t |f'(t)|^2\; dt \leq \varepsilon. \]
    %
    Then if $S = R e^{1/\varepsilon^{1/2}}$, then for $s \in [R,S]$,
    %
    \[ |f(s) - f(R)| = \left| \int_R^s f'(t)\; dt \right| \leq \left( \int_R^s t |f'(t)|^2\; dt \right)^{1/2} \left( \int_R^s 1/t\; dt \right)^{1/2} \leq \varepsilon^{1/2} \log(s/R)^{1/2} \leq \varepsilon^{1/4}. \]
    %
    If $R$ is suitably large, then
    %
    \[ \left| \frac{1}{R} \int_0^R f(t)\; dt \right| \leq 2L. \]
    %
    Thus
    %
    \[ \left| \frac{1}{R e^{1/\varepsilon^{1/2}}} \int_0^R f(t)\; dt \right| \leq 2L e^{-1/\varepsilon^{1/2}}. \]
    %
    Next,
    %
    \begin{align*}
        \frac{1}{R e^{1/\varepsilon^{1/2}}} \int_R^{Re^{1/\varepsilon^{1/2}}} f(t)\; dt &= \frac{1}{R e^{1/\varepsilon^{1/2}}} \int_R^{Re^{1/\varepsilon^{1/2}}} f(R) + [f(t) - f(R)]\; dt\\
        &= \left( 1 - e^{-1/\varepsilon^{1/2}} \right) f(R) + \int_R^{R e^{1/\varepsilon^{1/2}}} [f(t) - f(R)]\; dt.
    \end{align*}
    %
    Again, for any $\delta > 0$, if $R$ is suitably large, then
    %
    \[ \left| \frac{1}{R e^{1/\varepsilon^{1/2}}} \int_0^{R e^{1/\varepsilon^{1/2}}} f(t)\; dt - L \right| \leq \delta \]
    %
    Combining all these inequalities yields that
    %
    \begin{align*}
        |L - f(R)| &= \left| \frac{1}{R e^{1/\varepsilon^{1/2}}} \int_0^{R e^{1/\varepsilon^{1/2}}} f(t)\; dt - f(R) \right| + \delta\\
        &= \left| \frac{1}{R e^{1/\varepsilon^{1/2}}} \int_R^{R e^{1/\varepsilon^{1/2}}} f(t)\; dt - f(R) \right| + \delta + 2Le^{-1/\varepsilon^{1/2}}\\
        &= \left| \frac{1}{R e^{1/\varepsilon^{1/2}}} \int_R^{R e^{1/\varepsilon^{1/2}}} [f(t) - f(R)]\; dt - e^{-1/\varepsilon^{1/2}} f(R) \right| + \delta + 2Le^{-1/\varepsilon^{1/2}}\\
        &\leq e^{-1/\varepsilon^{1/2}} |f(R)| + \delta + 2L e^{-1/\varepsilon^{1/2}} + \varepsilon^{1/4}.
    \end{align*}
    %
    The fact that
    %
    \[ |f(R)| \leq 2 \left| \frac{1}{R e^{1/\varepsilon^{1/2}}} \int_R^{Re^{1/\varepsilon^{1/2}}} f(t)\; dt \right| + 2 \left| \int_R^{R e^{1/\varepsilon^{1/2}}} [f(t) - f(R)]\; dt \right| \leq 2L + 2 \varepsilon^{1/4} \]
    %
    shows $f$ is bounded, which is sufficient to complete the proof.
\end{solution}


\begin{solution}
(Due to Anuk D.) Integration by parts with $u=f(t)$, $dv=1$ gives
\begin{equation*}
  \frac{1}{T}\int_{0}^{T}f(t)dt = f(T) - \frac{1}{T}\int_{0}^{T}tf'(t)dt.
\end{equation*}
Therefore it suffices to show that $\frac{1}{T}\int_{0}^{T}tf'(t)dt$ converges as $T\to \infty$. Indeed, by the Cauchy Schwarz inequality,
\begin{align*}
  \left|\frac{1}{T}\int_{0}^{T}tf'(t)dt\right|
  &= \left|\frac{1}{T}\int_{0}^{\sqrt{T}}\sqrt{t}\cdot\sqrt{t}f'(t)dt + \frac{1}{T}\int_{\sqrt{T}}^{T}\sqrt{t}\cdot \sqrt{t}f'(t)dt\right|\\
  & \leq \frac{1}{T}\left(\int_{0}^{\sqrt{T}}t dt \right)^{\frac{1}{2}}\left( \int_{0}^{\sqrt{T}}t |f'(t)|^{2}dt \right)^{\frac{1}{2}}+ \frac{1}{T}\left( \int_{\sqrt{T}}^{T}tdt \right)^{\frac{1}{2}}\left( \int_{\sqrt{T}}^{T}t|f'(t)|^{2}dt \right)^{\frac{1}{2}}\\
  &\leq \frac{1}{2 \sqrt{T}} \left(\int_{0}^{\infty}t|f'(t)|^{2} dt \right)^{\frac{1}{2}} + \frac{1}{T}\left( \frac{T^{2}-T}{2} \right)^{1/2}\left(\int_{\sqrt{T}}^{\infty}t|f'(t)|^{2}dt \right)^{\frac{1}{2}}\\
  &\leq \frac{C}{2 \sqrt{T}}  +\left(\int_{\sqrt{T}}^{\infty} t|f'(t)|^{2}dt \right)^{\frac{1}{2}} \to 0 \text{ as }T\to 0
\end{align*}
\end{solution}


\question (6) Let $f: \R\to\R$ be a compactly supported function that satisfies the Holder condition with exponent $\beta\in (0,1)$, i.e., there exists a constant $A<\infty$ such that $\forall x,y\in \R: |f(x)-f(y)|\leq A|x-y|^{\beta}$. Consider the function $g$ defined by
\begin{equation*}
g(x) = \int_{-\infty}^{\infty} \frac{f(y)}{|x-y|^{\alpha}} dy
\end{equation*}
where $\alpha \in (0,\beta).$
\begin{parts}
\part Prove that $g$ is a continuous function at $0$.
\begin{solution}
We will show that $|g(x)-g(0)|\to 0$ as $|x|\to 0$. Without loss of generality, we may assume that $|x|\leq 1$. Performing the $u$-substitution $u=y-x$, we may write
\begin{equation*}
g(x) = \int_{-\infty}^{\infty} \frac{f(x+y)}{|y|^{\alpha}} dy.
\end{equation*}
Therefore, taking $k>0$ sufficiently large that supp$(f)\subseteq [-k,k]$, we have
\begin{align*}
|g(x)-g(0)| 
&= \left|\int_{-\infty}^{\infty} \frac{f(x+y)-f(y)}{|y|^{\alpha}} dy\right|\\
&= \left|\int_{-(k+1)}^{k+1} \frac{f(x+y)-f(y)}{|y|^{\alpha}} dy\right| &&\text{since }|x|\leq 1\\
& \leq \int_{-(k+1)}^{k+1} \frac{|f(x+y)-f(y)|}{|y|^{\alpha}} dy\\
&\leq 2A|x|^{\beta} \int_0^{k+1} y^{-\alpha}dy \\
& = 2A |x|^{\beta} \frac{(k+1)^{1-\alpha}}{1-\alpha} \to 0 \text{ as } |x|\to0
\end{align*}
where we have used the fact that $\alpha<1$ so that the integral is finite.
\end{solution}
\part Prove that $g$ is differentiable at $0$. (Hint: Try the dominated convergence theorem).
\begin{solution}

So the idea is to use the so called generalized dominated convergence theorem, whose proof is exactly the same as the usual dominant convergence theorem. And the statement is the following:

\textit{Let $\{h_n\}$ be a family of $L^1$ function which converge almost everywhere to a $L^1$ function $h$, let $g_n$ be another family of $L^1$ functions such that $g_n\to g$ a.e., $|h_n|\le g_n$ for each $n$ and $|h|\le g$,  Suppose now we have $\lim \int g_n=\int g$, then $\lim\int h_n=\int h$.}

Now come back to our problem. First attempt would be to use the standard test, which means we just naively take the derivative of the inside function and then check integrability. We immediately encounter a problem because we don't have the desired integrability condition. In fact, we have $f(y)\cdot sgn(x-y)|x-y|^{-\alpha-1}$, which is not integrable.

So our second attempt would be to use the Holder condition to balance this singularity. Assume that supp$(f)\subset [-K,K]$ for some $K>1$. Let $(x_n)$ be a sequence of real numbers converging to zero. Without loss of generality, we may assume $|x_n|\leq1$ for all $n$. By the Mean Value Theorem, for each $n$ there exists some $\xi_n$ with $0\le |\xi_n|\le |x_n|$ such that $$\frac{1}{|y-x_n|^\alpha}-\frac{1}{|y|^\alpha}=\alpha\cdot sgn(y-\xi_n)\cdot |y-\xi_n|^{-\alpha-1}\cdot |x_n|.$$
Therefore
\begin{align}
\frac{g(x_n)-g(0)}{x_n} &=\int_{-K}^K \frac{f(y)-f(\xi_n)}{|y-x_n|^\alpha\cdot x_n}-\frac{f(y)-f(\xi_n)}{|y|^\alpha\cdot x_n}dy+E_n \nonumber\\
&=\int_{-K}^K \frac{f(y)-f(\xi_n)}{x_n}\cdot \(\frac{1}{|y-x_n|^{\alpha}}-\frac{1}{|y|^\alpha}\)dy+E_n \nonumber\\
&=\int_{-K}^K (f(y)-f(\xi_n))\cdot \alpha\cdot sgn(y-\xi_n)\cdot |y-\xi_n|^{-\alpha-1}dy+E_n \label{first-deriv-form}
\end{align}
where 
\begin{equation*}
E_{n}=\int_{-K}^K \frac{f(\xi_n)}{x_n}\(\frac{1}{|y-x_n|^\alpha}-\frac{1}{|y|^\alpha}\)dy.
\end{equation*}
In particular, we have 
\begin{align*}
|E_{n}|&=\left|\int_{-K-x_n}^{-K}\frac{f(\xi_n)}{x_n}\frac{1}{|y|^\alpha}dt-\int_{K-x_n}^{K}\frac{f(\xi_n)}{x_n}\frac{1}{|y|^\alpha}dy\right| = \left|\frac{f(\xi_n)}{x_n}\int_{K-x_n}^{K+x_n} \frac{1}{|y|^{\alpha}} dy\right| \leq \frac{C}{K^{\alpha}}.
\end{align*}
for $C>0$ sufficiently large. Therefore by the above estimate for $|E_n|$ and equation \eqref{first-deriv-form} we have:
\begin{equation}\label{deriv-equation}
\left|\frac{g(x_n)-g(0)}{x_n} - \int_{\R} h_n(y)dy\right| \leq \frac{C}{K^\alpha}
\end{equation}
where
$$h_n(y)=\chi_{[-K,K]}\cdot (f(y)-f(\xi_n))\cdot \alpha\cdot sgn(y-\xi_n)\cdot |y-\xi_n|^{-\alpha-1}.$$

Let $$h(y):= \lim_{n\to\infty} h_n(y) = \chi_{[-K,K]}\cdot(f(y)-f(0))\cdot \alpha\cdot sgn(y)\cdot |y|^{-\alpha-1}.$$

We wish to show that $\int_{\R}h_n(y)dy \to  \int_{\R}h(y)dt$ as $n\to\infty$. We now justify this by applying the generalized dominant convergence theorem. Define 
$$g_n=C\cdot |y-\xi_n|^{\beta-\alpha-1}\cdot \chi_{[-K,K]}$$ where $C$ is some large constant. Similarly, define 
$$g=C\cdot |y|^{\beta-\alpha-1}\cdot \chi_{[-K,K]}.$$ 
One easily sees that $|h_n|\le g_n$ and $|h|\le g$ by applying the Holder condition. Moreover, each $g_n$ is in $L^1$. Since $\xi_n\to 0$, we have $\lim \int g_n\to \int g$ and $g_n\to g$ a.e. Finally, invoke the generalized dominated convergence theorem, and it follows that $\limn \int h_n = \int h$, so that by equation \eqref{deriv-equation},
\begin{equation*}\limsup_{n\to\infty} \left|\frac{g(x_n)-g(0)}{x_n} - \int_{\R} h(y)dy\right| \leq \frac{C}{K^\alpha}\end{equation*}

Taking $K\to \infty$, it follows that
\begin{equation*}
\limn \frac{g(x_n)-g(0)}{x_n} = \int_{\R} (f(y)-f(0))\cdot \alpha\cdot sgn(y)\cdot |y|^{-\alpha-1}dy.
\end{equation*}
Since $(x_n)$ was an arbitrary sequence converging to zero, it follows that $$g'(0) = \int_{\R} (f(y)-f(0))\cdot \alpha\cdot sgn(y)\cdot |y|^{-\alpha-1}dy$$
which is finite.
\end{solution}
\end{parts}


\question (7R) Let $f_n \to f$ weakly in $L^2(\R)$ and $\norm{f_n}_2 \to \norm{f}_2$ as $n\to\infty$. Show that $f_n\to f$ strongly in $L^2(\R)$.

\begin{solution}
Since $f_n\to f$ weakly, therefore $\langle f_n, g \rangle \to \langle f, g \rangle$ for any $g\in L^2$. Since $\norm{f}_2 \leq \sup_{n}\norm{f_n}_2 <\infty$, we have $f\in L^2$. Therefore taking $g=f$, we have $\langle f_n,f\rangle \to \norm{f}_2^2$, so that
\begin{equation*}
\norm{f_n-f}_2^2 = \langle f_n-f,f_n-f\rangle = \norm{f_n}_2^2 -2 \langle f_n,f\rangle + \norm{f}^2_2 \to 0
\end{equation*}
as $n\to \infty$.
\end{solution}
\question (8R) 
\begin{parts}
\part Let $H_1$ and $H_2$ be Hilbert spaces, and let $T:H_1\to H_2$ be a continuous linear operator. Give a precise definition of the adjoint operator $T^*$.
\begin{solution}
    The adjoint operator $T^*$ is a bounded operator from $H_2$ to $H_1$. For each $x \in H_2$, $T^*x$ is the unique element of $H_1$ such that for any $y \in H_1$, $\langle T^* x, y \rangle = \langle x, Ty \rangle$.
\end{solution}

\part Let $(a,b)\subset \R$ be a (possibly infinite) open interval. If $f\in L^2(a,b)$, explain what it means that the distributional derivative $f'$ is also in $L^2(a,b)$. 
\begin{solution}
    This means that there exists $g \in L^2(a,b)$, such that for any $\phi \in C_c^\infty(a,b)$,
    %
    \[ \int f(X) \phi'(x)\; dx = - \int g(x) \phi(x)\; dx. \]
\end{solution}

\part Let $\R_+$ denote the positive real axis $[0,\infty)$. Let $H^1(\R)$ (respectively $H^1(\R_+)$) be the space of real-valued functions $f\in L^2(\R)$  (respectively $f\in L^2(\R_+)$ such tha thte distributional derivative $f'$ is also in $L^2(\R)$ (respectively $L^2(\R_+)$). Then $H^1(\R)$ and $H^1(\R_+)$ are Hilbert spaces with inner product given by
\begin{align*}
\langle f, g \rangle_{H^1(\R)} &= \int_{\R} f(x)g(x)dx + \int_{\R} f'(x) g'(x)dx,\\
\langle f, g \rangle_{H^1(\R_+)} &= \int_{\R_+} f(x)g(x)dx + \int_{\R_+} f'(x) g'(x)dx
\end{align*}
Let $T:H^1(\R)\to H^1(\R_+)$ be the mapping given by the restriction. Compute exactly the adjoint operator $T^*$.
\begin{solution}
\begin{comment}
    Let $K$ be the kernel of $T$, and let $V$ be the orthogonal complement of $K$. We claim $T$ is a unitary map when restricted as a map from $V$ to $H^1(\RR_+)$. Indeed, if $f \in V$, then $f \in L^2(\RR)$, and since $\langle f, \phi \rangle = 0$ for any $\phi \in C_c^\infty(\RR)$ supported compactly on $(-\infty,0)$, we conclude via an integration by parts that the distribution $f - f''$ is supported on $x \geq 0$. Thus on $x \leq 0$, we can write $f(x) = A e^x + B e^{-x}$ on $x \leq 0$, and the fact that $f \in L^2(\RR)$ implies that $B = 0$.
    
    
    Any element of $H^1(\RR_+)$ is continuous, and has a well defined value at zero (a trace). Given $f \in H^1(\RR_+)$, define
    %
    \[ T^*f(x) = \begin{cases} f(0) e^x &: x < 0 \\ f(x) &: x > 0. \end{cases} \]
    %
    Then $T^* f$ is obviously square integrable, and
    %
    \begin{align*}
        \int T^*f(x) \phi(x)\; dx &= f(0) \int_{-\infty}^0 e^x \phi'(x)\; dx + \int_0^\infty f(x) \phi'(x)\; dx\\
        &= f(0) \left[ \phi'(0) - \int_{-\infty}^0 e^x \phi(x)\; dx \right] + \int_0^\infty f(x) \phi'(x)\; dx\\
        &= - f(0) \int_{-\infty}^0 e^x \phi(x)\; dx + \int_0^\infty f(x) \phi'(x)\; dx
    \end{align*}
    
    
    One can argue (Morrey's inequality for example, plus the density of $C_c^\infty(\RR)$ in $H^1(\RR)$), that if $g \in K$, then $g(0) = 0$.
\end{comment}
\end{solution}

\end{parts}


\question (9R) A real valued function $f$ defined on $\RR$ belongs to the space $C^{1/2}(\RR)$ if and only if
%
\[ \sup_{x \in \RR} |f(x)| + \sup_{x \neq y} \frac{|f(x) - f(y)|}{\sqrt{|x - y|}} < \infty. \]
%
Prove that a function $f \in C^{1/2}(\RR)$ if and only if there exists a constant $C$ so that for every $\varepsilon > 0$, there is a bounded function $\varphi \in C^\infty(\RR)$ such that
%
\[ \sup_{x \in \RR} |f(x) - \varphi(x)| \leq C \varepsilon^{1/2} \quad\text{and}\quad \sup_{x \in \RR} \varepsilon^{1/2} |\varphi'(x)| \leq C. \]
\begin{solution}
    Suppose $f \in C^{1/2}(\RR)$. Let $\phi$ be a bump function symmetrical about the origin supported on $|x| \leq 1$, set $\phi_\varepsilon(y) = (1/\varepsilon) \phi(y/\varepsilon)$ and define $\varphi = f * \phi_\varepsilon$. Then $\varphi$ is bounded, and lies in $C^\infty(\RR)$. Since $\phi_\varepsilon(y)$ is supported on $|y| \leq \varepsilon$, We have
    %
    \[ f(x) - \varphi(x) = \int [f(x) - f(x-y)] \phi_\varepsilon(y) \lesssim \varepsilon^{1/2}. \]
    %
    We write (using the symmetry of $\phi$)
    %
    \[ \varphi'(x) = (1/\varepsilon)^2 \int_0^\infty \phi'(y/\varepsilon) [f(x-y) - f(x+y)]\; dy. \]
    %
    Now $|f(x-y) - f(x+y)| \lesssim \varepsilon^{1/2}$ for $|y| \leq \varepsilon$, the support of our integrand lies on $0 \leq y \leq \varepsilon$, and $|\phi'(y/\varepsilon)| \lesssim 1$. Thus it follows that $|\varphi'(x)| \lesssim \varepsilon^{1/2}$, which completes the proof.

    Now suppose the second condition. Given $\varphi$ satisfying the condition above, the derivative condition together with the fundamental theorem of calculus implies that
    %
    \[ |f(x) - f(y)| \leq |f(x) - \varphi(x)| + |\varphi(x) - \varphi(y)| + |\varphi(y) - f(y)| \leq 2C \varepsilon^{1/2} + C |x - y| / \varepsilon^{1/2}. \]
    %
    If we choose $\varepsilon = |x - y|$, then we conclude that
    %
    \[ |f(x) - f(y)| \leq 3C |x-y|^{1/2}, \]
    %
    so that
    %
    \[ \sup_{x \neq y} \frac{|f(x) - f(y)|}{|x-y|^{1/2}} \leq 3C. \]
    %
    The fact that $\varphi$ is bounded shows that if the condition holds for any $\varepsilon > 0$, then $f$ is bounded. Thus $f \in C^{1/2}(\RR)$.
\end{solution}









\newpage
\section{Questions that need solutions}

\question (Fall 2017) Let $g: \RR^2 \to \RR$ be a function that has continuous partial derivatives in $\RR^2$. Define $\chi: \RR^3 \to \{ 0, 1 \}$ by $\chi(x) = 1$ if $x_3 > g(x_1,x_2)$, and $\chi(x) = 0$ otherwise. Compute the derivatives $\partial \chi / \partial x^i$ for $i = 1, 2, 3$.

\question Let $\alpha \in (0,1)$, and for $f \in C[0,1]$, and $x \in [0,1]$, define
%
\[ (T_\alpha f)(x) = \int_0^1 \sin(x+y) |x-y|^{-\alpha} f(y)\; dy. \]
\begin{parts}
    \part Prove that $T_\alpha$ extends to a bounded operator on $L^2[0,1]$.
    
    \item For which $\alpha \in (0,1)$ is $T_\alpha: L^2[0,1] \to L^2[0,1]$ a compact operator?
\end{parts}

\end{questions}

\end{document}
