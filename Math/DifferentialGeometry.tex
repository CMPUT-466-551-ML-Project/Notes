\input{../style.tex}

\title{Differential Geometry}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}
\maketitle
\tableofcontents
\pagenumbering{arabic}

\part{Manifold Theory}

\chapter{Topological Considerations}

In a mathematician's heaven, all objects would be linear, and finite dimensional at that. Unfortunately, we live in the real world. When a physicist describes the motion of a robot's arm, rigidity forces the joints to move along curves bounded to a sphere, never linear. When an algebraic geometer studies the solution set of the equation $X^2 + Y^3 - 5$, he must analyze a shape which bends and curves, never straight. Differential geometry gives the mathematician tools to cheat -- Most of the time, on shapes that, though non-linear, are {\it locally linear}, we can recover the methods used in the linear case. The challenge, of course, is to figure out how to put all the locally linear properties together into a nice, global form.

\section{Manifolds}

Topology attempts to describe the properties of space invariant under actions which stretch and squash continuously. Differential geometry extends this description to spatial properties constant when space is stretched and squashed, but not `bent' in some form. Four centuries of calculus have established differentiability in the nice cartesian spaces $\mathbf{R}^n$. A basic environment to extend the notions of `bentness' should then be to consider spaces which are locally similar to $\mathbf{R}^n$. These are the {\bf topological manifolds}, Hausdorff spaces which are `locally euclidean'. In detail, at every point $p$ on a manifold, there exists a neighbourhood $U$ of $p$, and a non-negative integer $n$ such that $U$ is homeomorphic to $\mathbf{R}^n$. We take $\mathbf{R}^0 = \{ 0 \}$, so that discrete point sets are manifolds\footnote{This may seem odd to a fresh reader of differential geometry, but we will soon find that adding `zero dimensional manifolds' to the fold allows us to more easily describe the theory.}. Non-Hausdorff manifolds are far and few between, and most of the theory requires manifolds to be Hausdorff. We only consider Hausdorff manifolds in this volume.

Geometrically, a homeomorphism on a manifold can be seen as a way of assigning coordinates to points on the manifold, because we associate each geometric point with a sequence of numbers, via an element of $\mathbf{R}^n$. One way we can see the study of manifolds is as an extension of analytic geometry, to non-planar systems. Indeed, the technologies developed have immediate applications to projective, hyperbolic, and elliptic geometries, and the language of manifolds has become the common language of modern day geometers.

\begin{example}
    $\mathbf{R}^n$ is a manifold. Any ball in Euclidean space is also a manifold; in these examples, we may simply take the entire space as the neighbourhood of each point, since a ball in $\mathbf{R}^n$ is homeomorphic to $\mathbf{R}^n$.
\end{example}

\begin{example}
    Let $f: \mathbf{R}^n \to \mathbf{R}^m$ be a continuous function, and consider the graph
    %
    \[ \Gamma(f) = \{ (x, f(x)) : x \in \mathbf{R}^n \} \]
    %
    Then $\Gamma(f)$ is a manifold, since it is homeomorphic to $\mathbf{R}^n$ by projection.
\end{example}

The above examples are easily extended to show that any topological space homeomorphic to a manifold is also a manifold! This is a bad omen, for we wish to discuss properties invariant under differentiability, which should not necessarily be invariant under homeomorphism! Clearly we must add additional structure to a manifold to distinguish between continuous and smooth maps. This task will be taken up in the next chapter.

\begin{example}
    Consider the circle $S^1 = \{ x \in \mathbf{R}^2 : \|x\| = 1 \}$. For any proper subset $U$ of $S^1$, an {\bf angle function} is a continuous function $\theta:U \to \mathbf{R}$ such that $e^{i\theta(x)} = x$ for all $x \in U$. This restriction immediately implies $\theta$ is an embedding, with inverse $\theta^{-1}(t) = e^{it}$. Angle functions exist on any proper subset $U$ of $S^1$, and therefore cover $S^1$, which is therefore shown to be a 1-manifold.
\end{example}

The circle is different to $\mathbf{R}^n$ in the sense that we cannot put coordinates over the whole space at once, we must analyze the circle piece by piece to determine the structure on the whole space. This is the main trick to manifold theory -- a manifold might be a big nasty object globally, but locally, the shape is pleasant.

\begin{example}
    There is a less obvious coordinate system on the circle, which permits an easy generalization to higher dimensional space. We will project the open subset $S^n - \{(1,0,\dots,0)\}$ onto the hyperplane $\{-1\} \times \mathbf{R}^{n-1}$, by taking the intersection of this plane with the line that passes through a projected point $x$ and $z = (1,0, \dots, 0)$. To calculate a formula for this projection, we can calculate the set of points that lie on the line generated by $x$ and $z$ to be
    %
    \[ \{ \lambda x + (1 - \lambda) z : \lambda \in \mathbf{R} \} = \{ ((1 - \lambda) + \lambda x_1, \lambda x_2, \dots, \lambda x_n) : \lambda \in \mathbf{R} \} \]
    %
    To find the intersection on the hyperplane, we set the first coordinate equal to $-1$, and find the projection
    %
    \[ f(x_1, \dots, x_n) = \frac{2}{1 - x_1}(x_2, \dots, x_n) \]
    %
    Via a similar calculation, we find that
    %
    \begin{align*}
        f^{-1}(y_1, \dots, y_{n-1}) &= \left(1 - \frac{8}{4 + \| y \|^2}, \frac{4y_2}{4 + \| y \|^2}, \dots, \frac{4y_n}{4 + \| y \|^2} \right)\\
        &= \frac{1}{4 + \| y \|^2} \left( \| y \|^2 - 4, 4y_2, \dots, 4y_n \right)
    \end{align*}
    %
    Note that the inverse function is continuous. If we project from the point $(-1,0,\dots,0)$ to $\{ 1 \} \times \mathbf{R}^{n-1}$, then the homeomorphism defined on $S^1 - \{ (-1,0,\dots,0) \}$ is calculated to be
    %
    \[ g(x_1, \dots, x_n) = \frac{1}{1 + x_1}(x_2, \dots, x_n) \]
    %
    \[ g^{-1}(y_1, \dots, y_{n-1}) = \frac{1}{4 + \| y \|^2} \left( 4 - \| y \|^2, 4y_2, \dots, 4y_n \right) \]
    %
    And we have covered $S^n$ with homeomorphisms: the space is a manifold.
\end{example}

Any open subset of $\mathbf{R}^n$ is a manifold; around any point, we may take an open ball as a neighbourhood, and any open ball is homeomorphic to $\mathbf{R}^n$. In fact, any open subset of a manifold, with the subspace topology, is also a manifold, known as an {\bf open submanifold}. When we analyze manifolds, it is convenient to consider not only homeomorphisms onto $\mathbf{R}^n$, but also maps onto open submanifolds of $\mathbf{R}^n$. We will call a homeomorphism $x: U \to V$, where $V$ is an open subset of Euclidean space a {\bf chart}, and denote it $(x,U)$. The letters $x,y$ and $z$ are often used for charts, so that it is easy to confuse coordinates $(x^1,x^2, \dots, x^n) \in \mathbf{R}^n$ with coordinates $(x^1(p), x^2(p), \dots, x^n(p))$ on a manifold.

\begin{example}
    Consider the set $M_n(\mathbf{R})$ of $n \times n$ matrices with entries in the real numbers. We can identify $M_n(\mathbf{R})$ with $\mathbf{R}^{n \times n}$ by considering only the coefficients of the matrix, and this tells us $M_n(\mathbf{R})$ is a topological manifold of dimension $n^2$. The determinant map $\det:M_n(\mathbf{R}) \to \mathbf{R}$ can be viewed as a polynomial in the entries of the matrix, so the function is continuous, and
    %
    \[ GL_n(\mathbf{R}) = {\det}^{-1}(\mathbf{R} - \{0\}) \]
    %
    hence $GL_n(\mathbf{R})$ is an open submanifold of $M_n(\mathbf{R})$.
\end{example}

\begin{example}
    Let $M(n,m;k)$ be the set of $n$ by $m$ matrices of rank $k$. For any $M_0 \in M(n,m;k)$, there are permutation matrices $P$ and $Q$ such that
    %
    \[ PM_0Q = \begin{pmatrix} A_0 & B_0 \\ C_0 & D_0 \end{pmatrix} \]
    %
    where $A_0$ is an invertible $k$ by $k$ matrix. The map
    %
    \[ L: M \mapsto PMQ = \begin{pmatrix} A(M) & B(M) \\ C(M) & D(M) \end{pmatrix} \]
    %
    is a rank-preserving linear endomorphism on $M(n,m)$, and locally around $M_0$, the resulting $A(M)$ are still invertible. Fix $M$, and consider the rank-preserving linear map
    %
    \[ T: Y \mapsto \begin{pmatrix} I_k & 0 \\ -C(M)A^{-1}(M) & I_{n-k} \end{pmatrix} Y \]
    %
    Notice that
    %
    \[ (T \circ L)(M) = \begin{pmatrix} A(M) & B(M) \\ 0 & -C(M)A^{-1}(M)B(M) + D(XM \end{pmatrix} \]
    %
    It follows that $M$ is rank $k$ if and only if $D(M) = C(M)A^{-1}(M)B(M)$. Thus, locally around $T(M_0)$ we may specify an element of $M(n,m;k)$ via a $k \times k$ invertible $A$, a $k \times (n - k)$ matrix $B$, and a $k \times (m - k)$ matrix $C$. Thus $M(n,m;k)$ is a
    %
    \[ k^2 + k(n-k) + k(m-k) = k(n + m - k) \]
    %
    dimensional manifold. In particular, this verifies that if $k = \min(m,n)$, then the set $M(n,m;k)$ of matrices of full rank is actually an open subset of $M(n,m)$.
\end{example}

$M(n,m;k)$ is an interesting manifold, since it embeds itself in a space in such a way that the canonical charts are essentially expressed in the canonical coordinates of the manifold, forgetting a few coefficents; the directions the space travels in relative to $M(n,m)$ are always along axis of the space. The best way to see this is to consider $M(2,1;1)$, which consists of vectors in $\mathbf{R}^2$ of the form $(a,0)$ and $(0,b)$, with $a,b \neq 0$. This is the $x$ and $y$ axis, with the origin removed, and is a $1$ manifold. At any point you can travel left and right, or up and down, just not too far!

Sometimes, you may wish to place a topological structure on a set, with the purpose of making it into a manifold, without having any topological structure to begin with. In most cases, the set is contained in a bigger space with some topological structure, and you can just surplant the relative topology on the subspace. But in certain cases, for instance in the construction of Moduli spaces in algebraic geometry, there is nothing to start working with. The next theorem provides a useful strategy for placing a topology on such a space.

\begin{theorem}
    Let $M$ be a set covered by a family $U_\alpha$ of sets, together with bijections $x_\alpha$ from $U_\alpha$ onto open subsets of euclidean space, such that
    %
    \begin{itemize}
        \item For any $\alpha, \beta$, $x_\alpha(U_\alpha \cap U_\beta)$ is open, and $x_\alpha \circ x_\beta^{-1}$ is continuous.
        \item If $a, b \in M$, then either there is $U_\alpha$ with $a,b \in U_\alpha$, or disjoint $U_\alpha, U_\beta = \emptyset$ such that $a \in U_\alpha$, $b \in U_\beta$.
    \end{itemize}
    %
    Then there is a unique topological structure on $M$ making it into a Hausdorff manifold, such that each $(x_\alpha, U_\alpha)$ is a chart.
\end{theorem}
\begin{proof}
    Consider the topological structure on $M$ generated by all sets of the form $x_\alpha^{-1}(V)$, where $V$ is an open subset of $x_\alpha(U_\alpha)$. These sets form a base for the topology by the first condition of the theorem; if $x_\alpha(U)$ and $x_\beta(V)$ are open, then
    %
    \[ x_\alpha(U \cap V) = x_\alpha(U) \cap (x_\alpha \circ x_\beta^{-1})(x_\beta(U_\alpha \cap V)) \]
    %
    is open, since it is the intersection of open sets. This also shows that if $V \subset U_\alpha$ is open in our topology, then $x_\alpha(V)$ is open, so that each $x_\alpha$ is a homeomorphism, and the space is Hausdorff since points in a single $U_\alpha$ may be separated by Euclidean open sets, and points in disjoint $U_\alpha$, $U_\beta$ may surely be separated. We conclude that $M$ is a manifold under this new topological structure.
\end{proof}

More generally, we may assume that the charts above are maps into manifolds rather than open subsets of euclidean space, since, locally, these identifications are essentially the same thing. What's more, as will become more important later, if the family of charts is countable, the manifold obtained is second countable.

\begin{example}
    Given an $n$ dimensional vector space $V$, let $G_k(V)$ be the space of all $k$ dimensional subspaces of $V$. If $W$ is $k$ dimensional, then there is a subspace $W'$, of dimension $k - n$, such that $V = W \oplus W'$. The set $L(W,W')$ of linear maps can then be identified with the set $A_{W'}$ of $k$ dimensional subspaces of $V$ which intersect $W'$ trivially, by taking a map $T: W \to W'$ to the subspace $W_T = \{ w + Tw : w \in W \}$. Denote the projection from $V$ onto $W'$ by $\pi$, and the projection onto $W$ by $\psi$. If $U$ is any subspace that intersects $W'$ trivially, then $\psi$ is an isomorphism between $W$ and $U$, and then $U = W_{\pi \circ \psi^{-1}}$. Since $L(W,W')$ has dimension $k(n-k)$, we shall find that with our newly defined topology, $G_k(V)$ is a $k(n-k)$ manifold, using the theorem above.

    Let $W_0$ and $W_1$ be two subspaces of $V$ of dimension $k$. To verify the second condition of the manifold construction theorem, we shall prove that these subspaces lie in some common $A_U$, for some space $U$ of dimension $n - k$. Let $W_0 \cap W_1$ be a space of dimension $x$, so that $W_0 \oplus W_1$ is a space of dimension $2k - x$, and some complement $(W_0 \oplus W_1)'$ is then a space of dimension $n - (2k - x) = (n - k) - (k - x)$. It suffices to prove that $W_0 \oplus W_1$ has a subspace of dimension $k - x$ which intersects $W_0$ and $W_1$ trivially. Note that both
    %
    \[ X = W_0' \cap (W_0 \oplus W_1)\ \ \ \ \ Y = W_1' \cap (W_0 \oplus W_1) \]
    %
    have dimension $k - x$. If we consider any isomorphism $T: X \to Y$, then the space $U = \{ x + Tx : x \in X \}$ is $k-x$ dimensional, and intersects both $W_0$ and $W_1$ trivially. Thus $W_0$ and $W_1$ are both contained in the neighbourhood $A_U$.

    The hard part is verifying the first property of the theorem. Consider the set
    %
    \[ U = A_{W_0'} \cap A_{W_1'} \]
    %
    of spaces which trivially intersect both $W_0'$ and $W_1'$, for some $k$ dimensional spaces $W_0$ and $W_1$. The image of $U$ in $L(W_0,W_0')$ consists of maps $T: W_0 \to W_0'$ such that $\pi \circ (1 + T)$ is an isomorphism. The association $T \mapsto \pi \circ (1 + T)$ is continuous, which shows that the set of maps is open, since the set of isomorphisms between vector spaces is open in the set of all maps between vector spaces. Now how does $U$ transform between the identification maps of $L(W_0,W_0')$ and $L(W_1,W_1')$. Consider a subspace $U$ is mapped to a transformation $T:W_0 \to W_0'$, and a transformation $L: W_1 \to W_1'$. If we let $\pi, \psi, \nu$, and $\eta$ denote the projections onto $W_0$, $W_0'$, $W_1$, and $W_1'$ respectively, then since $U$ intersects $W_0'$ and $W_1'$ trivially, the projection maps $\pi$ and $\nu$ are isomorphisms when restricted to $U$, and $L = (\eta|_U) \circ (\nu|_U)^{-1}$. Note that the map $(1 + T): W_0 \to U$ is an isomorphism, and so
    %
    \[ L = (\eta|_U) \circ (1 + T) \circ (1 + T)^{-1} \circ (\nu|_U)^{-1} = [\eta \circ (1 + T)] \circ (\nu \circ (1 + T))^{-1} \]
    %
    a formula now expressed independently of $U$, which holds over all choices of $T$. The map $T \mapsto [\eta \circ (1 + T)] \circ [\nu \circ (1 + T)]^{-1}$ is continuous, which shows that charts are consistant.

    There is another way to obtain $G_k(V)$, in a way that makes it much less obvious that it is a manifold, but is more useful for constructing continuous maps. Consider the open set $X$ of tuples $(v_1, \dots, v_k) \subset V^k$ which are linearly independant. Then we have a surjective map $\pi: X \to G_k(V)$, which maps a set of vectors to their span in $V$. We claim this is a quotient map. By fixing an arbitrary basis of $V$, $X$ may be topologically identified with the set of $n \times k$ matrices with full rank. We then have a smooth action from $GL_k(\mathbf{R})$ onto $X$, obtained by right multiplication. In terms of the representation of $X$ on $V^k$, this takes the form of the action
    %
    \[ (v_1, \dots, v_k) \cdot M = \left( \sum M_{i1} v_i, \sum M_{i2} v_i, \dots, \sum M_{ik} v_i \right) \]
    %
    In this form, it becomes more obvious that the orbits of this action are precisely the fibres of $\pi$.

    Now note that it suffices that we verify continuity and openess locally, in terms of the coordinate charts on the space. That is, we prove that the inverse image of the domains of each chart on $G_k(V)$ is open in $X$, and locally in terms of each chart, the map has the properties required of a quotient map. Thus consider a partition $V = W \oplus W'$, where $W$ is $k$-dimensional with some basis $w_1, \dots, w_k$. If we let $\pi$ denote projection onto $W$, then $M \in X$ represents a basis of a subspace trivially intersecting $W'$ if and only if $\pi \circ M$ is an invertible matrix. Since the association $M \mapsto \pi \circ M$ is continuous, and the space of invertible matrices is open, the set of such $M$ is also open.

    To verify the topological properties, it it useful to switch to a coordinate basis such that $w_1 = e_1, \dots, w_k = e_k$ are unit vectors, and $W' = \text{span}(e_{k+1}, \dots, e_n)$. Then $L(W,W')$ is just the set of $(n-k) \times k$ matrices, and by changing the coordinate representation of $X$, we may assume that
    %
    \[ \pi \begin{pmatrix} I_k \\ N \end{pmatrix} = N \]
    %
    Every $M \in X$ with $\pi(M)$ intersecting $W'$ trivially is equivalent to a matrix of the form under the action of $GL_k(\mathbf{R})$. Thus if $A \subset M_{(n-k),k}$, then
    %
    \[ \pi^{-1}(A) = \left\{ \begin{pmatrix} I_k \\ N \end{pmatrix} : N \in A \right\} \cdot GL_k(\mathbf{R}) = \left\{ \begin{pmatrix} M \\ NM \end{pmatrix} : N \in A, M \in GL_k(\mathbf{R}) \right\} \]
    %
    The chart
    %
    \[ \begin{pmatrix} M \\ NM \end{pmatrix} \mapsto (M, N) \]
    %
    is then a homeomorphism from $\pi^{-1}(A)$ to $GL_k(\mathbf{R}) \times A$, and we see that $A$ is open if and only if $\pi^{-1}(A)$ is open, so the map is a quotient map.
\end{example}

Many proofs about manifolds use a reliable trick. First, we conjure forth local homeomorphisms to $\mathbf{R}^n$. Then we transport nice properties of $\mathbf{R}^n$ across the homeomorphism, thereby inducing the properties on the manifold. In fact, the general philosophy of manifold theory is that most properties of $\mathbf{R}^n$ will carry across to arbitrary spaces that look locally like $\mathbf{R}^n$ -- we can perform linear algebra on spaces that are not really linear!

\begin{theorem}
    Every manifold is locally compact.
\end{theorem}
\begin{proof}
    Let $p$ be an arbitrary point on a manifold, with a chart $(x,U)$. If we consider any closed ball around $x(p)$ in $x(U)$, the inverse image will be compact, since compactness is preserved under homeomorphisms. Thus $p$ has a compact neighbourhood.
\end{proof}

The same method shows that every manifold is locally path-connected, and thus locally connected. The next problem requires more foresight on the reader, though the basic technique used is exactly the same.

\begin{theorem}
    A connected manifold is path-connected.
\end{theorem}
\begin{proof}
    Let $p$ be a point on a connected manifold $M$, and consider the set $U$ of all points in $M$ path connected to $p$. Local path connectedness shows $U$ is open. Suppose $q$ is a limit point of $U$. Take some path-connected chart $(x,V)$ around $q$. Then $V$ contains some $r \in U$, which is path connected to $p$, and we can then patch this path with a path from $r$ to $q$ obtained in the chart to obtain a path from $p$ to $q$. Thus $U$ is open, closed, and non-empty, so $U = M$.
\end{proof}

Since every manifold is locally connected, any manifold can be split up into the disjoint sum of its connected components. It is therefore interesting to prove theorems about connected manifolds, since any manifold can be built up as a disjoint union of connected manifolds.

\begin{example}
    $GL_n(\mathbf{R})$ is a disconnected manifold, since $\det(GL_n(\mathbf{R}))$ is disconnected. By Corollary (1.3) we should be able to identify the path connected components. In $GL_n(\mathbf{R})$, we shall construct paths reducing matrices to certain canonical forms, which shall identify the path connected components. In our proof, we shall use the fact that $GL_n(\mathbf{R})$ can be described as tuples of $n$ linearly independent vectors in $\mathbf{R}^n$, which simplifies notation. If $v_1, \dots, v_n$ are linearly independent, consider adding one vector to another.
    %
    \[ (v_1, \dots, v_p, \dots, v_q, \dots, v_n) \mapsto (v_1, \dots, v_p + v_q, \dots, v_q, \dots, v_n) \]
    %
    These vectors are path connected in $GL_n(\mathbf{R})$ by the path
    %
    \[ t \mapsto (v_1, \dots, v_p + t v_q, \dots, v_q, \dots, v_n) \]
    %
    Similarily, we may subtract rows from one another. Next, consider multiplying a row by a scalar $\gamma > 0$,
    %
    \[ (v_1, \dots, v_p, \dots, v_n) \mapsto (v_1, \dots, \gamma v_p, \dots, v_n) \]
    %
    These matrices are path connected by
    %
    \[ t \mapsto \begin{pmatrix} v_1 & \dots & [(1-t) + t \gamma]v_p & \dots & v_n \end{pmatrix}^t \]
    %
    We cannot perform this technique if $\gamma < 0$, because then $(1-t) + t \gamma = 0$ for some choice of $t$, and the resulting vectors become linearly dependent. We should not expect to find a path when $\gamma < 0$, since multiplying by a negative number reverses the sign of the determinant, and we know from the continuity of the determinant that the sign of the determinant determines at least two connected components. The same reasoning shows we can't necessarily swap two rows. Fortunately, we don't need these operations -- we may use the path-connected elementary matrices to reduce any matrix to a canonical form. A modification of the Gauss Jordan elimination algorithm (left to the reader as a simple exercise) shows all matrices can be path-reduced to a matrix of the form
    %
    \[ \begin{pmatrix} 1 & 0 & \dots & 0 & 0 \\ 0 & 1 & & 0 & 0 \\ 0 & 0 & \ddots & 0 & 0 \\ 0 & 0 &  & 1 & 0 \\ 0 & 0 & \dots & 0 & \pm 1 \end{pmatrix} \]
    %
    One matrix has determinant greater than zero, the other has determinant less than zero. Thus $GL_n(\mathbf{R})$ consists of two homeomorphic path-connected components: the matrices with determinant greater than zero, and the component with determinant less than zero. This is quite a different situation from $GL_n(\mathbf{C})$, which is always connected.
\end{example}





\section{Products and Quotients}

The category {\bf Man} has useful constructions which make it a nice category to work with. The coproduct (disjoint-union) of two manifolds is easily shown to be a manifold. Here are some more constructions.

\begin{example}[Manifold Products and the Torus]
    If $M$ and $N$ are manifolds, then the product $M \times N$ is also a manifold -- we simply take products of homeomorphisms on the space. Since $S^1$ is a 1-manifold, we obtain a 2-manifold $\mathbf{T}^2 = S^1 \times S^1$, the Torus. More generally, the $n$-torus $\mathbf{T}^n = S^1 \times \dots \times S^1$ is an $n$-manifold.
\end{example}

\begin{example}[Surfaces of Revolution]
    If $M$ is a 1-manifold in the subplane of $\mathbf{R}^3$ defined by
    %
    \[ X = \{ (x,y,0): x,y > 0 \} \]
    %
    Then the space obtained by rotating $M$ around the $y$ axis is a 2-manifold, known as a surface of revolution, and homeomorphic to $M \times S^1$.
\end{example}

Most quotient spaces of manifolds will not be manifolds. Nonetheless, under some restrictions, the quotient space will be a manifold. It shall suffice that if $f:M \to N$ is a locally injective open surjective map, and $M$ is a manifold, then $N$ is a manifold, because we can push sufficiently small charts on $M$ into charts on $N$. This is satisfied for many of the useful examples which occur in the earlier parts of the theory.

\begin{example}[The M\"{o}bius Strip]
    Consider the quotient space $M$ obtained from $[-\infty, \infty] \times (-1,1)$ by identifying $(x,y)$ with $(x + n, (-1)^n y)$, for $n \in \mathbf{Z}$. Then the projection is open and locally injective, so $M$ is a manifold, known as the M\"{o}bius strip. By throwing away points, we find $M$ can also be obtained from the product space $[-1,1] \times (-1,1)$ by identifying $(-1,x)$ with $(1,-x)$, for each $x \in (0,1)$. It only has one edge, even though it exists in three dimensional space, and if you have a paper copy at hand, try cutting it down the middle!
\end{example}

\begin{example}[Projective Space]
    Consider the quotient space of $S^2$ obtained by identifying opposite sides of the sphere: glue each point $x$ to $-x$. The projection is locally injective, so the space is a 2-manifold, denoted  $\mathbf{R} \mathbf{P}^2$ and known as real projective space. In general $\mathbf{R} \mathbf{P}^n$ is created by identifying opposite points on $S^n$. Another way to describe the space is as $\mathbf{R}^{n+1} - \{ 0 \}$, where $x$ and $\lambda x$ are identified, for $\lambda \neq 0$. To obtain explicit charts on $\mathbf{R} \mathbf{P}^n$, define $x: S^n \to \mathbf{R}^n$ by
    %
    \[ x(a_1, \dots, a_{n+1}) = \frac{1}{a_i} (a_1, \dots, \widehat{a_i}, \dots, a_{n+1}) \]
    %
    This map is continuous whenever $a_i \neq 0$. For all points $p$, $x(p) = x(-p)$, so the chart descends to a map on $\mathbf{R} \mathbf{P}^n$ instead. It even has a continuous inverse
    %
    \[ x^{-1}(b_1, \dots, b_n) = \left[ b_1, \dots, 1, \dots, b_n \right] \]
    %
    Since our maps cover the space, $\mathbf{R} \mathbf{P}^n$ is an $n$ dimensional manifold.
\end{example}

\begin{example}
    A similar space can be formed in the case of complex scalars, by viewing $\mathbf{R}^{2n}$ as $\mathbf{C}^n$. We then identify $z$ with $\lambda z$, for $\lambda \in \mathbf{C} - \{ 0 \}$ to form the quotient space $\mathbf{P} \mathbf{C}^n$ from $\mathbf{C}^{n+1} - \{ 0 \}$. For $n = 1$, $\mathbf{P} \mathbf{C}^1$ is just the Riemann sphere, a 2 manifold. In general, $\mathbf{C} \mathbf{P}^n$ is a 2n dimensional real manifold, or $n$ `complex' dimensions. We may consider the chart
    %
    \[ x(z_1, \dots, z_{n+1}) = \frac{1}{z_i} (z_1, \dots, \widehat{z_i}, \dots, z_{n+1}) \]
    %
    with inverse
    %
    \[ x^{-1}(w_1, \dots, w_n) = [w_1, \dots, 1, \dots, w_n] \]
    %
    where we consider complex multiplication instead of real multiplication. The space formed is known as complex projective space.
\end{example}

Even though $\mathbf{R} \mathbf{P}^2$ is locally trivial, the space is very strange globally, and cannot be embedded in $\mathbf{R}^3$. Nonetheless, the geometry our eyes percieve is modelled very accurately by the spherical construction of projective space. We don't see the really weird part of $\mathbf{R} \mathbf{P}^2$, since our eye cannot see the full circumpherence of vision, but the topological problems occur in the theory of computer vision, where we must attempt to find algorithms to patch portions of vision across the whole circumpherence of vision.

\begin{example}[Gluing Surfaces]
    Let $M$ and $N$ be connected $n$-manifolds. We shall define the connected sum $M \# N$ of the two manifolds. There are two sets $B_1$ and $B_2$ in $M$ and $N$ respectively, both homeomorphic to the closed unit ball in $\mathbf{R}^n$. Then there is a homeomorphism $h:\partial B_1 \to \partial B_2$, and we may define the connected sum as
    %
    \[ M \# N = (M - B_1^\circ) \cup_h (N - B_2^\circ) \]
    %
    The topological structure formed can be shown unique up to homeomorphism, but this is non-trivial to prove. The $n$-holed torus $T \# T \# \dots \# T \# T$ is an example of such a structure.
\end{example}

\section{Euclidean Neighbourhoods are Open}

In these notes, we consider a neighbourhood as in the French school, as any subset containing an open set, regardless of whether it is open or not. Nonetheless, let $M$ be a manifold, and take a point $p$ with neighbourhood $U$ homeomorphic to $\mathbf{R}^n$, lets say, by some continuous function $f: U \to \mathbf{R}^n$. Then $U$ contains an open set $V$, and $f(V)$ is open in $\mathbf{R}^n$, so that $f(V)$ contains an open ball $W$ around $f(x)$. But then $W$ is homeomorphic to $\mathbf{R}^n$, and $f^{-1}(W)$ is a neighbourhood of $x$ open in $V$ (and therefore open in $M$) homeomorphic to $\mathbf{R}^n$. This complicated discussion stipulates that we may always choose open neighbourhoods in the definition in a manifold. Remarkably, it turns out that all neighbourhoods homeomorphic to $\mathbf{R}^n$ {\it must} be open; to prove this, we require an advanced theorem of algebraic topology.

% Draw construction above

\begin{theorem}[Invariance of Domain]
    If $f:U \to \mathbf{R}^n$ is a continuous, injective function, where $U$ is an open subset of $\mathbf{R}^n$, then $f(U)$ is open, so that $f$ is a homeomorphism.
\end{theorem}

A domain is a connected open set, and this theorem shows that the property of being a domain is invariant under continuous, injective maps from $\mathbf{R}^n$ to itself. In multivariate calculus, the inverse function theorem shows this for differentiable mappings with non-trivial Jacobian matrices across its domain; invariance of domain stipulates that the theorem in fact holds for any such continuous map $f$ on an open domain. The theorem can be proven in an excursion in some basic algebraic topology (homology theory, to be {\it exact}). In an appendix to this chapter, we shall prove the theorem based on the weaker assumption of the Jordan curve theorem.

\begin{lemma}
    If $U \subset \mathbf{R}^n$ and $V \subset \mathbf{R}^m$ are open, then $U \cong V$ implies $n = m$.
\end{lemma}
\begin{proof}
    If $n < m$, consider the projection $\pi: \mathbf{R}^n \to \mathbf{R}^m$
    %
    \[ \pi(x_1, \dots, x_n) = (x_1, \dots, x_n, 0, \dots, 0) \]
    %
    Clearly no subset of $\pi(\mathbf{R}^n)$ is open. But if $f: V \to U$ is a homeomorphism, then $\pi \circ f: V \to \mathbf{R}^m$ is continuous and injective, so $\pi(V) \subset \pi(\mathbf{R}^n)$ is open by invariance of domain.
\end{proof}

The {\bf dimension} of a point on a manifold is the dimension of the euclidean space which is locally homeomorphic to a neighbourhood of the point. When a manifold is connected, one can show simply that the dimension across the entire space is invariant, and we may call this the {\bf dimension of the manifold}. An $n$-dimensional manifold $M$ is often denoted $M^n$.

\begin{corollary}
    The dimension of a point on a manifold is unique.
\end{corollary}
\begin{proof}
    Let $U$ and $V$ be two non-disjoint neighbourhoods of a point homeomorphic to $\mathbf{R}^n$ and $\mathbf{R}^m$ by $f:U \to \mathbf{R}^n$ and $g:V \to \mathbf{R}^m$. Then $U \cap V$ is also open, and homeomorphic to open sets of $\mathbf{R}^n$ and $\mathbf{R}^m$. We conclude $n = m$.
\end{proof}

\begin{theorem}
    Any subset of a manifold locally homeomorphic to Euclidean space is open in the original topology.
\end{theorem}
\begin{proof}
    Let $M$ be a manifold, and $U \subset M$ homeomorphic to $\mathbf{R}^n$ by a function $f$. Let $x \in U$ be arbitrary. There is an open neighbourhood $V$ of $x$ that is homeomorphic into $\mathbf{R}^n$ by a function $g$. Since $V$ is open in $M$, $U \cap V$ is open in $U$, so $f(U \cap V)$ is open in $\mathbf{R}^n$. We obtain a one-to-one continuous function from $f(U \cap V)$ to $g(U \cap V)$ by the function $g \circ f^{-1}$. It follows by invariance of domain that $g(U \cap V)$ is open in $\mathbf{R}^n$, so $U \cap V$ is open in $V$, and, because $V$ is open in $M$, $U \cap V$ is open in $M$. In a complicated manner, we have shown that around every point in $U$ there is an open neighbourhood contained in $U$, so $U$ itself must be open.
\end{proof}

Really, this theorem is just a generalized invariance of domain for arbitrary manifolds -- since the concept of a manifold is so intertwined with Euclidean space, it is no surprise we need the theorem for $\mathbf{R}^n$ before we can prove the theorem here.

\section{Equivalence of Regularity Properties}

Many important results in differentiable geometry require spaces with more stringent properties than those that are merely Hausdorff. At times, we will want to restrict ourselves to topological manifolds with these properties. Fortunately, most of these properties are equivalent.

\begin{theorem}
    For any manifold, the following properties are equivalent:
    %
    \begin{enumerate}
        \item[(1)] Every component of the manifold is $\sigma$-Compact.
        \item[(2)] Every component of the manifold is second countable.
        \item[(3)] The manifold is metrizable.
        \item[(4)] The manifold is paracompact (so every compact manifold is metrizable).
    \end{enumerate}
\end{theorem}

\begin{lemma}[$1) \to (2$]
    Every $\sigma$-compact, locally second countable space is globally second countable.
\end{lemma}
\begin{proof}
    Let $X$ be a locally second countable space, equal to the union of compact sets $\bigcup_{i = 1}^\infty A_i$. For each $x$, there is an open neighbourhood $U_x$ with a countable base $\mathcal{C}_x$. If, for some $A_i$, we consider the set of $U_x$ for $x \in A_i$, we obtain a cover, which therefore must have a finite subcover $U_{x_1}, U_{x_2}, \dots, U_{x_n}$. Taking $\bigcup_{i = 1}^n \mathcal{C}_{x_i}$, we obtain a countable base $\mathcal{C}_i$ for all points in a neighbourhood of $A_i$. Then, taking the union $\bigcup_{i = 1}^\infty \mathcal{C}_i$, we obtain a countable base for $X$.
\end{proof}

\begin{lemma}[$2) \to (3$]
    If a manifold is second countable, then it is metrizable.
\end{lemma}
\begin{proof}
    This is a disguised form the Urysohn metrization theorem, proved in a standard course in general topology. If you do not have the background, you will have to have faith that this lemma holds. All we need show here is that a second countable manifold is regular, and this follows because every locally compact Hausdorff space is Tychonoff.
\end{proof}

\begin{lemma}[$3) \to (1$]
    Every connected, locally compact metrizable space is $\sigma$-compact.
\end{lemma}
\begin{proof}
    Consider any connected, locally compact metric space $(X,d)$. For each $x$ in $X$, let
    %
    \[ r(x) = \frac{\sup \{ r \in \mathbf{R} : \overline{B}_r(x)\ \text{is compact} \}}{2} \]
    %
    Since $X$ is locally compact, this function is well defined and positive for all $x$. If $r(x) = \infty$ for any $x$, then $\{ \overline{B}_n(x) : n \in \mathbf{Z} \}$ is a countable cover of the space by compact sets. Otherwise, $r(x)$ is finite for every $x$. Suppose that
    %
    \[ d(x,y) + r' < 2r(x) \]
    %
    By the triangle inequality, this tells us that $\overline{B}_{r'}(y)$ is a closed subset of $\overline{B}_{r(x)}(x)$, which is hence compact. This shows that, when $d(x,y) < r(x)$,
    %
    \[ r(y) \geq r(x) - \frac{d(x,y)}{2} \]
    %
    Put more succinctly, this equation tells us that the function $r:X \to \mathbf{R}$ is continuous:
    %
    \[ |r(x) - r(y)| < \frac{d(x,y)}{2} \]
    %
    This has an important corollary. Consider a compact set $A$, and let
    %
    \[ A' = \bigcup_{x \in A} \overline{B}_{r(x)}(x) \]
    %
    We claim that $A'$ is also compact. Consider some sequence $\{ x_i \}$ in $A'$, and let $\{ a_i \}$ be elements of $A$ for which $x_i \in \overline{B}_{r(a_i)}(a_i)$. Since $A$ is compact, we may assume $\{ a_i \}$ converges to some $a$. When $d(a_i, a) < r(a)/2$,
    %
    \[ r(a_i) < r(a) + r(a)/4 \]
    %
    and so
    %
    \[ d(a,x_i) \leq d(a,a_i) + d(a_i,x_i) < r(a)/2 + [r(a) + r(a)/4] = 7r(a)/4 \]
    %
    Since we chose $r(a)$ to be half the supremum of compact sets, the sequence $x_k$ will eventually end up in the compact ball $B_{3r(a)/4}(a)$, and hence will converge.

    If $A$ is a compact set, we will let $A'$ be the compact set constructed above. Let $A_0$ consist of an arbitrary point $x_0$ is $X$, and inductively, define $A_{k+1} = A_k'$, and $A = \bigcup_{i = 0}^\infty A_k$. Then $A$ is the union of countably many compact sets. $A$ is obviously open. If $x$ is a limit point of $A$, then there is some sequence $\{ x_i \}$ in $A$ which converges to $x$, so $r(x_i) \to r(x)$. If $|r(x_i) - r(x)| < \varepsilon$, and also $d(x_i,x) < r(x) - \varepsilon$, then $x$ is contained in $B_{r(x_i)}(x_i)$, and hence if $x_i$ is in $A_k$, then $x$ is in $A_{k+1}$. Thus $A$ is non-empty and clopen, so $X = A = \bigcup A_k$ is $\sigma$-compact.
\end{proof}

\begin{lemma}[$4) \to (1$]
    A connected, locally compact, paracompact space is $\sigma$ compact.
\end{lemma}
\begin{proof}
    Consider a locally-finite cover $\mathcal{C}$ of precompact neighbourhoods in a space $X$. Fix $x \in X$. Then $x$ intersects finitely many elements of $\mathcal{C}$, which we may label $U_{1,1}, U_{1,2}, \dots, U_{1,n_1}$. Then
    %
    \[ U_1 = \overline{U_{1,1}} \cup \overline{U_{1,2}} \cup \dots \cup \overline{U_{{1,n_1}}} \]
    %
    intersects only finitely more elements of $\mathcal{C}$, since the set is compact, and we need only add finitely more open sets $U_{2,1}, \dots, U_{2,n_2}$, obtaining
    %
    \[ U_2 = \overline{U_{2,1}} \cup \dots \cup \overline{U_{2,n_2}} \]
    %
    Continuing inductively, we find an increasing sequence of compact neighbourhoods. Then $U = \bigcup U_i$ is open because a neighbourhood of $y \in U_k$ is contained in $U_{k+1}$. If $y$ is a limit point of $U$, take a neighbourhood $V \in \mathcal{C}$, which must intersect some $U_k$. Then $y \in U_{k+1}$, so $U$ is closed. We conclude $X = U$ is $\sigma$ compact.
\end{proof}

\begin{lemma}[$1) \to (4$]
    A $\sigma$ compact, locally compact Hausdorff space is paracompact.
\end{lemma}
\begin{proof}
    Let $X = \bigcup C_i$ be a locally compact, $\sigma$-compact space. Since $C_1$ is compact, it is contained in an open precompact neighbourhood $U_1$. Similarily, $C_2 \cup \overline{U_1}$ is contained in a precompact neighbourhood $U_2$ with compact closure. We find $U_1 \subset U_2 \subset \dots$, each with compact closure, and which cover the entire space. Now let $\mathcal{U}$ be an arbitrary open cover of $X$. Each $V_k = U_{k} - \overline{U_{k-2}}$ (letting $U_{-2} = U_{-1} = U_0 = \emptyset$) is open, and its closure $\overline{V_k}$ is a closed subset of compact space, hence compact. Since $\mathcal{U}$ covers $\overline{V_k}$, it has a finite subcover $U_1, \dots, U_n$, and we let
    %
    \[ \mathcal{V}_1 = (U_1 \cap V_1), (U_2 \cap V_1), \dots, (U_n \cap V_1) \]
    %
    be a collection of refined open sets which cover $V_1$. Do the same for each $V_k$, obtaining $\mathcal{V}_2, \mathcal{V}_3, \dots$, and consider $\mathcal{V} = \bigcup \mathcal{V}_i$. Surely this is a cover of $X$, and each point is contained only in some $\mathcal{V}_k$ and $\mathcal{V}_{k+1}$, so this refined cover is locally finite.
\end{proof}

\section{Boundaries}

There is an addition family of `manifolds with sides' which often occurs in the theory of differential geometry. A {\bf manifold with boundary} is a space also containing points that are {\it locally bounded}. Points in such a space must either be locally homeomorphic to some $\mathbf{R}^n$, or be a point on the `boundary' of the manifold. That is,a point $x$ lies on the {\bf boundary} of the manifold if it has a neighbourhood homeomorphic to a `halfspace' $\mathbf{H}^n = \{ (x_1, \dots, x_n) \in \mathbf{R}^n: x_1 \geq 0 \}$. If $M$ is a manifold with boundary, then we denote the set of points on its boundary by $\partial M$. This is well defined by the invariance of domain theorem, in the sense that a point on a manifold with boundary {\it either} has a neighbourhood homeomorphic to some $\mathbf{R}^n$, or to some $\mathbf{H}^m$, but not both. The non boundary points are known as the interior, denoted $M^\circ$.

\begin{theorem}
    If $M^n$ is a manifold with boundary, then $\partial M$, considered as a subspace of $M$, is a manifold (without boundary) of dimension $n-1$.
\end{theorem}
\begin{proof}
    Let $x$ be a point in $\partial M$, and let $U$ be a neighbourhood homeomorphic to $\mathbf{H}^n$ by a map $f:U \to \mathbf{H}^n$. Consider the points in $U$ that map to the boundary plane under $f$,
    %
    \[ V = \{ y \in U : f(y) = (0,x_2, \dots, x_n) \} \]
    %
    We contend that $V = U \cap \partial M$, so that $V$ is a neighbourhood of $x$ in the relative topology, and since $V$ is homeomorphic to $\mathbf{R}^{n-1}$, this will show that $\partial M$ is an $n-1$ dimensional manifold. It is easy to see that $V \subset U \cap \partial M$. Conversely, the other points in $U - V$ have a neighbourhood homeomorphic to $\mathbf{R}^n$, so that they do not lie in $\partial M$. Thus $U - V \subset M^\circ \cap U$, and this completes the proof.
\end{proof}

\begin{example}
    $\mathbf{H}^n$ is the easiest example of a manifold with boundary. It's boundary consists of $\{ 0 \} \times \mathbf{R}^{n-1}$, which is an $n - 1$ manifold. Another manifold with boundary is the unit disc $D^n = \{ x \in \mathbf{R}^n : \|x\| \leq 1 \}$. We have already shown that the discs boundary, $\partial D^n = S^{n-1}$, is an $n - 1$ manifold.
\end{example}

We will see later that considering a manifold boundary as a disjoint structure is very useful. Stoke's theorem provides a direct application. But for now, let's move onto introducing calculus into the picture.








\chapter{Differentiable Structures}

As a topological space, we know when a map between manifolds is continuous, but when is a map differentiable? What we seek is a definition abstract enough to work on any manifold, yet possessing the same properties of differentiable functions on $\mathbf{R}^n$.

\section{Defining Differentiability}

Let us be given a map $f:M \to N$ between manifolds. Given a correspondence $b = f(a)$, a reasonable inquiry would be to consider two charts $(x,U)$ and $(y,V)$, where $U$ is a neighbourhood of $a$ and $V$ is a neighbourhood of $b$. We obtain a map $y \circ f \circ x^{-1}$, defined between open subsets of Euclidean space. We have `expressed $f$ in coordinates'. $f$ shall then be differentiable at $a$ if $y \circ f \circ x^{-1}$ is differentiable at $x(a)$. Unfortunately, this idea is doomed to fail, for we can hardly expect that the statement holds for all charts when it holds for a pair of them.

\begin{example}
    Consider the chart $y: \mathbf{R} \to \mathbf{R}$, $y(t) = t^3$, and let $x$ be the identity chart. If $f(x) = \sin(x)$, then $x \circ f \circ x^{-1} = f$ is differentiable, yet
    %
    \[ (y \circ f \circ y^{-1})(t) = \sin(\sqrt[3]{t})^3 \]
    %
    is not differentiable at the origin.
\end{example}

If we are to stick with this definition, we either need to define differentiability in terms of the charts $g$ and $h$ used, or identify additional structure to manifolds. The latter option is clearly more elegant. Our method will be to identify charts which are `correct', and ignore `non-differentiable' charts. Two charts $(x,U)$ and $(y,V)$ are {\bf $\mathbf{C^\infty}$ related}, if either $U$ and $V$ are disjoint, or
%
\[ y \circ x^{-1} : x(U \cap V) \to y(U \cap V) \]
%
\[ x \circ y^{-1} : y(U \cap V) \to x(U \cap V) \]
%
are $C^\infty$ functions. One can see a chart as laying a blanket down onto a manifold. Two charts are $C^\infty$ related if, when we lay them down over each other, they contain no creases! The fact that manifolds do not have a particular preference for coordinates is both a help and a hindrance. On one side, it forces us to come up with elegant, coordinate free approaches to geometry. On the other end, these coordinate free approaches can also be incredibly abstract!

A {\bf smooth} or {\bf $\mathbf{C^\infty}$ atlas} for a manifold is a family of $C^\infty$ charts whose domains cover the entire manifold. A maximal atlas is called a {\bf smooth structure} on a manifold, and a manifold together with a smooth structure is called a {\bf smooth} or {\bf differentiable manifold}. In the literature, each map $y \circ x^{-1}$ is known as a {\bf transition map}, so that we say an atlas for a manifold has $C^\infty$ transition maps. From now on, when we mention a chart on a differentiable manifold, we implicitly assume the chart is the member of the smooth structure of the manifold.

A $f:M \to N$ be a map between two smooth manifolds. $f$ is differentiable at $p \in M$ if it is continuous at $p$, and if for some chart $x:U \to \mathbf{R}^n$ whose domain contains $p$, and for some chart $y:V \to \mathbf{R}^m$ whose domain contains $f(p)$, the map $y \circ f \circ x^{-1}:x(f^{-1}(V) \cap U) \to \mathbf{R}^m$ is differentiable at $x(p)$. $f$ itself is {\bf differentiable} if it is differentiable at every point on its domain, or correspondingly, if $y \circ f \circ x^{-1}$ is differentiable for any two charts $x$ and $y$. Since differentiability is a {\it local} condition on Euclidean spaces, and manifolds are locally Euclidean spaces, the smooth structure, which preserves the differentiability across all local charts, allows us to define differentiability on arbitrary manifolds.

It is uncomfortable to construct a maximal atlas explicitly on a manifold. Fortunately, we do not need to specify every single valid chart in our manifold.

\begin{lemma}
    Every atlas extends to a unique smooth structure.
\end{lemma}
\begin{proof}
Let $\mathcal{A}$ be an atlas for a manifold $M$, and consider the set $\mathcal{A}'$, which is the union of all atlases containing $\mathcal{A}$. We shall show that $\mathcal{A}'$ is also an atlas, and therefore necessarily the unique maximal one. Let $x:U \to \mathbf{R}^n$ and $y:V \to \mathbf{R}^n$ be two charts in $\mathcal{A}'$ with non-disjoint domain, containing a point $p$. Let $z:W \to \mathbf{R}^n$ be a chart in $\mathcal{A}$ containing $p$. Then, on $U \cap V \cap W$, an open set containing $p$, we have
%
\[ x \circ y^{-1} = (x \circ z^{-1}) \circ (z \circ y^{-1}) \]
%
and by assumption, each component map is $C^\infty$ on this domain, so $x \circ y^{-1}$ is smooth in a neighbourhood of $p$. The proof for $y \circ x^{-1}$ is exactly the same. Since the point $p$ was arbitrary, we conclude that $x$ and $y$ are $C^\infty$ related across their domains.
\end{proof}

\begin{corollary}
    If $x$ is a chart defined on a differentiable manifold $M$, and is $C^\infty$ related to each map in a generating atlas $\mathcal{A}$, then $x$ is in the smooth structure generated by $\mathcal{A}$.
\end{corollary}

The next few theorems are justified by the fact that $C^\infty$ related charts play nicely with one another.

\begin{lemma}
    If a map $f$ is differentiable at a point $p$ in charts $x$ and $y$, it is differentiable at $p$ for any other charts containing $p$ and $q$.
\end{lemma}
\begin{proof}
    Suppose $y \circ f \circ x^{-1}$ is differentiable at a point $x(p)$, and consider any other charts $y'$ and $x'$. Then
    %
    \[ y' \circ f \circ x'^{-1} = (y' \circ y^{-1}) \circ (y \circ f \circ x^{-1}) \circ (x \circ x'^{-1}) \]
    %
    On a smaller open neighbourhood than was considered. Nonetheless, since differentiability is a local concept, we need only prove the theorem for this map on a reduced domain. This follows since the component maps are differentiable.
\end{proof}

\begin{example}
    Let $M$ be a manifold, and $U$ an open submanifold. Define a differentiable structure on $U$ consisting of all charts defined on $M$ whose domain is a subset of $U$. This is a maximal atlas, and is the unique such structure such that
    %
    \begin{enumerate}
        \item If $f: M \to N$ is differentiable, then $f|_U: U \to M$ is differentiable.
        \item The inclusion map $i:U \to M$ is differentiable.
        \item If $f: N \to M$ is differentiable, and $f(N) \subset U$, then $f: N \to U$ is differentiable.
    \end{enumerate}
\end{example}

\begin{example}
    Consider the manifold $\mathbf{R}^n$, and define a smooth structure by considering the generating atlas containing only the identity map $id_{\mathbf{R}^n}$. This defines a smooth structure on $\mathbf{R}^n$, such that
    %
    \begin{enumerate}
        \item $x$ is a chart on $\mathbf{R}^n$ if and only if $x$ and $x^{-1}$ are $C^\infty$.
        \item A map $f:\mathbf{R}^n \to \mathbf{R}^m$ is differentiable in the sense of a manifold if and only if it is differentiable in the usual sense.
        \item A map $f:M \to \mathbf{R}^n$ is differentiable if and only if each coordinate $f_i:M \to \mathbf{R}$ is differentiable.
        \item A chart $x:U \to \mathbf{R}^n$ is a diffeomorphism from $U$ to $x(U)$.
    \end{enumerate}
    %
    Our definition has naturally extended calculus to arbitrary manifolds.
\end{example}

\begin{example}
    On $\mathbf{R}^2$, we have the polar coordinate system $(\theta, \mathbf{R}^2 - \{0\})$, defined by
    %
    \[ \theta^{-1}(r,u) = re^{iu} \]
    %
    This chart is injective and has full rank, so that $\theta$ is $C^\infty$ by the inverse function theorem. Thus the polar coordinate system truly is in the smooth structure generated by the identity. On $\mathbf{R}^3$, we have the spherical and cylindrical coordinate systems to work with.
\end{example}

\begin{example}
    The differentiable structure on $S^n$ is defined by the stereographic projection maps. We may also define this structure on $S^1$ by the angle functions. If $(\theta,U)$ and $(\psi,V)$ are angle functions, then $\theta \circ \psi^{-1}$ is just a translation by a multiple of $2\pi$ on each connected component of $\psi(U \cap V)$, hence $C^\infty$. Indeed, if
    %
    \[ \psi^{-1}(t) = e^{it} = \theta^{-1}(t') \]
    %
    then $t = t' + 2 \pi n$ for some unique integer $n$. Define $f(t) = n$, giving us a map $f: U \cap V \to \mathbf{Z}$. This map is continuous because if $t_i \to t$, then
    %
    \[ (\theta \circ \psi^{-1})(t_i) \to (\theta \circ \psi^{-1})(t) \]
    %
    so $t_i + 2 \pi f(t_i) \to t + 2 \pi f(t)$, hence $f(t_i) \to f(t)$. The continuity of $f$ implies that $f$ is constant on every connected component of $U \cap V$.
\end{example}

\begin{example}
    Smooth structures on manifolds are {\it not} unique. Let $\mathbf{R}_1$ be the canonical smooth manifold on $\mathbf{R}$. Let $\mathbf{R}_2$ be the smooth structure on $\mathbf{R}$ generated by the map $x$, such that $x(t) = t^3$. Then $\mathbf{R}_1$ and $\mathbf{R}_2$ are diffeomorphic. Let $x:\mathbf{R}_2 \to \mathbf{R}_1$ be our diffeomorphism. It is surely bijective. Let $y$ be a chart on $\mathbf{R}_2$. We must verify that $y = z \circ x$, where $z$ is a chart on $\mathbf{R}_1$. We may show this by verifying that $y \circ x^{-1} = z$, and $x \circ y^{-1} = z^{-1}$ is $C^\infty$ on $\mathbf{R}_1$. But this was exactly why $y$ was a chart on $\mathbf{R}_2$ in the first place, hence the map is a diffeomorphism.
\end{example}

The $C^\infty$ maps are not the only classes of maps occuring in calculus, and analogously, $C^\infty$ manifolds are not the only types of manifolds. More generally, we can consider a $C^k$ atlas, whose transition maps are $C^k$, and form a $C^k$ manifold. Clearly any $C^\infty$ manifold can be made into a $C^k$ manifold in a unique way by extending the atlas of transition maps, but it is less clear that a $C^k$ manifold can be specialized to a $C^\infty$ manifold. Hassler Whitney proved, using advanced tools from differential topology, that every $C^k$ can be specialized into a $C^\infty$, such that any two specializations are diffeomorphic. We can even specialize $C^\infty$ further by requiring that the transition maps are (real) analytic, in which case we obtain a $C^\omega$ or analytic manifold. If the dimension of the manifold is even, we can identify our charts on $\mathbf{R}^{2n}$ with $\mathbf{C}^n$, and require the transition maps be complex analytic. This gives us the family of complex manifolds. We will not discuss these types of manifolds in detail, but they occur in applications and in more general contexts.

\section{The Function Space $C^\infty(M)$}

The set of all real-valued differentiable maps defined on a manifold $M$ form the algebra $C^\infty(M)$, contained with the algebra $C(M)$. Note that a continuous map $f: M \to N$ induces an algebra homomorphism $f^\#: C(N) \to C(M)$ defined by $f^\#(g) = g \circ f$. If $f$ and $g$ are $C^\infty$, then $g \circ f$ is $C^\infty$, and so we may restrict $f^\#$ to a map from $C^\infty(N)$ to $C^\infty(M)$. We see therefore that the `map' $C^\infty$ defines a contravariant functor from the category of differential manifolds to the category of algebras.

\begin{lemma}
    If a continuous map $f:M \to N$ satisfies $f^\#(C^\infty(N)) \subset C^\infty(M)$, then $f$ is smooth.
\end{lemma}
\begin{proof}
    Let $(y,V)$ be a chart on $N$ at a point $q$, and let $(x,U)$ be a chart on $M$ at $p \in f^{-1}(p)$. By assumption, each $y^i \circ f = f^\#(y^i)$ is differentiable, so that $y^i \circ f \circ x^{-1}$ is differentiable. But this implies $y \circ f \circ x^{-1}$ is differentiable, so $f$ is differentiable.
\end{proof}

\begin{theorem}
    A homeomorphism $f:M \to N$ is a diffeomorphism if and only if $f^\#$ is an isomorphism between $C^\infty(N)$ and $C^\infty(M)$.
\end{theorem}
\begin{proof}
    Given $g = f^{-1}$, we note that $(g \circ f)^\# = f^\# \circ g^\#$ is just the identity map. Thus if $f$ is a diffeomorphism, Then $f^\# \circ g^\#$ is the identity, so that $f^\#$ is invertible, and hence an isomorphism. Conversely, if $f^\#$ is a bijection between $C^\infty(N)$ and $C^\infty(M)$, then $f^\#(C^\infty(N)) = C^\infty(M) \subset f^\#(C^\infty(M))$, so $f$ is differentiable, and $g^\#(C^\infty(M)) = C^\infty(N) \subset C^\infty(N)$, so $g$ is also differentiable, hence $f$ is a diffeomorphism.
\end{proof}

Suppose we know $C^\infty(M)$. Then we may recover the smooth structure on $M$, which is the set of diffeomorphisms from open subsets of $M$ to open subsets of euclidean space. This is the foundation of the algebraic viewpoint of manifold theory, which attempts to uncover the nature of manifolds solely by analyzing the commutative algebra $C^\infty(M)$. You can actually get pretty far with this approach (Nestruev's book ``Smooth Manifolds and Observables'' attempts to introduce differential geometry solely in this manner), but we prefer the geometric approach.

\section{Partial and Covariant Derivatives}

In calculus, when a function is differentiable, we obtained a derivative, a measure of a function's local change. On manifolds, determining an analogous object is difficult due to the coordinate invariant definition required. For now, we shall stick to structures corresponding to some particular set of coordinates. Consider a differentiable map $f \in C^1(M)$. We have no conventional coordinates to consider partial derivatives on, but if we fix some chart $x:U \to \mathbf{R}^n$ on $M$, we obtain a differentiable map $f \circ x^{-1}$, and we define, for a point $p \in U$,
%
\[ \left. \frac{\partial f}{\partial x^k} \right|_p = D_k(f \circ x^{-1})(x(p)) \]
%
Geometrically, this is the change in the function $f$ when we trace the function along the coordinate lines from the map $x$; literally, if we define a curve $c(t) = (f \circ x^{-1})(x(p) + te_k)$, then
%
\[ c'(0) = \left.\frac{\partial f}{\partial x^k}\right|_p \]
%
Sometimes we use the notation $\partial_{x^k} f$ for $\frac{\partial f}{\partial x^k}$, which tends to make notation simpler in calculations. Our new partial derivative still satisfies the familiar chain rule.

\begin{theorem}
    If $x$ and $y$ are coordinate systems at a point $p$, and $f \in C^1(M)$, then
    %
    \[ \left. \frac{\partial f}{\partial x^i} \right|_p = \sum_j \left. \frac{\partial y^j}{\partial x^i} \right|_p \left. \frac{\partial f}{\partial y^j} \right|_p \]
\end{theorem}
\begin{proof}
    We just apply the chain rule in Euclidean space.
    %
    \begin{align*}
        \left.\frac{\partial f}{\partial x_i}\right|_p = D_i(f \circ x^{-1})(x(p)) &= D_i((f \circ y^{-1}) \circ (y \circ x^{-1}))(x(p))\\
        &= \sum D_j(f \circ y^{-1})(y(p)) D_i(y_j \circ x^{-1})(x(p))\\
        &= \sum \left.\frac{\partial f}{\partial y_j}\right|_p \left.\frac{\partial y_j}{\partial x_i}\right|_p
    \end{align*}
\end{proof}

\begin{example}
    Let us compute the laplacian on $\mathbf{R}^2$ in polar coordinates.
    %
    \[ \bigtriangleup f = \frac{\partial f^2}{\partial x^2} + \frac{\partial f^2}{\partial y^2} \]
    %
    To do this, we need to relate partial differentives by the chain rule. If $(r,\theta)$ is the polar coordinate chart, and $(x,y)$ the standard chart on $\mathbf{R}^2$, then
    %
    \[ x = r \cos(\theta)\ \ \ \ \ y = r \sin(\theta) \]
    %
    (note that this is a relation between functions, and can be applied pointwise at any point on the charts). Thus the matrix of partial derivatives is
    %
    \[ \begin{pmatrix} \frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \\ \frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta} \end{pmatrix} = \begin{pmatrix} \cos \theta & -r \sin \theta \\ \sin \theta & r \cos \theta \end{pmatrix} \]
    %
    We can invert this matrix to obtain the partial derivatives with respect to $x$ and $y$. We have
    %
    \[ \begin{pmatrix} \frac{\partial r}{\partial x} & \frac{\partial r}{\partial y} \\ \frac{\partial \theta}{\partial x} & \frac{\partial \theta}{\partial y} \end{pmatrix} = \frac{1}{r} \begin{pmatrix} r \cos(\theta) & r \sin(\theta) \\ -\sin(\theta) & \cos(\theta) \end{pmatrix} = \begin{pmatrix} \cos(\theta) & \sin(\theta) \\ -\frac{\sin(\theta)}{r} & \frac{\cos(\theta)}{r} \end{pmatrix} \]
    %
    Now we apply the chain rule. We have
    %
    \[ \frac{\partial}{\partial x} = \cos(\theta) \frac{\partial}{\partial r} - \frac{\sin(\theta)}{r} \frac{\partial}{\partial \theta}\ \ \ \ \ \ \ \ \ \ \frac{\partial}{\partial y} = \sin(\theta) \frac{\partial}{\partial r} + \frac{\cos(\theta)}{r} \frac{\partial}{\partial \theta} \]
    %
    So
    %
    \begin{align*}
        \frac{\partial^2 f}{\partial x^2} &= \left( \cos(\theta) \frac{\partial}{\partial r} - \frac{\sin(\theta)}{r} \frac{\partial}{\partial \theta} \right) \left( \cos(\theta) \frac{\partial f}{\partial r} - \frac{\sin(\theta)}{r} \frac{\partial f}{\partial \theta} \right)\\
        &= \cos^2(\theta) \frac{\partial^2 f}{\partial r^2} + \frac{\cos(\theta) \sin(\theta)}{r^2} \frac{\partial f}{\partial \theta} - \frac{\cos(\theta) \sin(\theta)}{r} \frac{\partial^2 f}{\partial r \partial \theta}\\
        &\ \ \ \ \ \ \ + \frac{\sin^2(\theta)}{r} \frac{\partial f}{\partial r} - \frac{\sin(\theta) \cos(\theta)}{r} \frac{\partial^2 f}{\partial \theta \partial r} + \frac{\sin(\theta) \cos(\theta)}{r^2} \frac{\partial f}{\partial \theta} + \frac{\sin^2(\theta)}{r^2} \frac{\partial^2 f}{\partial \theta^2}
    \end{align*}
    %
    \begin{align*}
        \frac{\partial^2 f}{\partial y^2} &= \left( \sin(\theta) \frac{\partial}{\partial r} + \frac{\cos(\theta)}{r} \frac{\partial}{\partial \theta} \right) \left( \sin(\theta) \frac{\partial f}{\partial r} + \frac{\cos(\theta)}{r} \frac{\partial f}{\partial \theta} \right)\\
        &= \sin^2(\theta) \frac{\partial^2 f}{\partial r^2} - \frac{\cos(\theta) \sin(\theta)}{r^2} \frac{\partial f}{\partial \theta} + \frac{\cos(\theta) \sin(\theta)}{r} \frac{\partial^2 f}{\partial r \partial \theta}\\
        &\ \ \ \ \ \ \ + \frac{\cos^2(\theta)}{r} \frac{\partial f}{\partial r} + \frac{\sin(\theta) \cos(\theta)}{r} \frac{\partial^2 f}{\partial \theta \partial r} - \frac{\sin(\theta) \cos(\theta)}{r^2} \frac{\partial f}{\partial \theta} + \frac{\cos^2(\theta)}{r^2} \frac{\partial^2 f}{\partial \theta^2}
    \end{align*}
    %
    It follows that, by use of the trigonometric identities,
    %
    \[ \bigtriangleup f = \frac{\partial f}{\partial r^2} + \frac{1}{r} \frac{\partial f}{\partial r} + \frac{1}{r^2} \frac{\partial^2 f}{\partial \theta^2} \]
\end{example}

Partial derivatives also satisfy a nice `derivation' property, which we lead to the reader to calculate. It is essentially the product rule for partial derivatives. We shall see later that this property characterizes the partial derivative maps in the space of linear maps on $C^\infty(M)$.

\begin{lemma}
    For $f,g \in C^\infty(M)$,
    %
    \[ \left.\frac{\partial fg}{\partial x^i}\right|_p = f(p)\left.\frac{\partial g}{\partial x^i}\right|_p + g(p)\left.\frac{\partial f}{\partial x^i}\right|_p  \]
\end{lemma}

The coordinate operators allow us to extend differentiation to functions in $C^\infty(M)$, for any manifold $M$. However, defining the derivative of a differentiable map $f: M \to N$ between arbitrary manifolds is more tricky. We could define the derivative coordinatewise at a point $p$ by considering the derivative $D(y \circ f \circ x^{-1})(x(p))$, for some coordinate systems $x$ and $y$. The trouble is that in this way we can only talk of properties of the derivative that are invariant under the coordinate systems chosen. The trouble here is that there is no `definite' space the operator is defined over -- as we change the coordinate systems, the space changes. To obtain a `universal' coordinate independent map, we need to form a `universal' space which represents all coordinates at the same time, upon which the derivative operators are invariant. This space is known as the tangent bundle, and there are many deep and elegant constructions. For now, we only require the basics, which we'll expand upon later.

One of the first rules we learn in calculus is that
%
\[ f(x + t) = f(x) + f'(x) t\]
%
where $t$ is an `infinitisimally small' number. In multivariate calculus, this rule expands to
%
\[ f(v + w) = f(v) + Df(v)(w) \]
%
where $w$ is now instead an infinitisimally small vector. Later, we are taught that this rule isn't really true, that $f'(x)$ and $Df(v)$ are really just the best linear approximations of $f$, and that the rule only holds approximately, to greater and greater precision as $w$ and $t$ tend to zero. But we can still make this formally true; if $f: \mathbf{R}^n \to \mathbf{R}^m$, and we consider vectors $v_p$, for $v \in \mathbf{R}^n$ and $p \in \mathbf{R}^n$, which we view as an `infinitisimal' vector at the point $p$, then we can define the differential map, denoted by $f_*$, or $df$ where it is more elegant, as
%
\[ f_*(v_p) = Df(p)(v)_{f(p)} \]
%
Thus an infinitisimal vector at a point $p$ is mapped to an infinitisimal vector at $f(p)$ linearly. We denote these linear maps at each fibre by $df_p: \mathbf{R}^n_p \to \mathbf{R}^m_{f(p)}$, where $\mathbf{R}^n_p$ is the space of infinitisimal vectors at $p$. What we learn in analysis is that if we project $v_p$ to $v + p$ under some map $\pi$, then $(f \circ \pi)(v_p)$ is equal to $(\pi \circ f_*)(v_p)$ up to a quadratic factor of $v$. The advantage to this approach becomes much more evident on manifolds, where we cannot consider `linear approximations' to differentiable functions $f: M \to N$, but since $M$ is locally linear, we can still consider a space of infinitismal vectors which can still be mapped under $f$ to other infinitismally small vectors.

We shall denote the space of all $v_p$, by $T\mathbf{R}^n$, which is really just $\mathbf{R}^n \times \mathbf{R}^n$. We have shown that an $f: \mathbf{R}^n \to \mathbf{R}^m$ induces a linear approximation $df: T\mathbf{R}^n \to T\mathbf{R}^m$, and if $g: \mathbf{R}^m \to \mathbf{R}^k$, then we can consider the composition $g_* \circ f_*$, and we have a beautiful relationship
%
\[ g_* \circ f_* = g_* \circ f_* \]
%
which succinctly represents the chain rule.

There are many complex and elegant ways to form the tangent bundle $TM$ on any manifold $M$. We shall discuss them all in turn, but for now we shall construct the space in the most dirty and quick manner. Given a point $p$ on a manifold $M$, with dimension $n$ at $p$, the tangent space at $p$ will consist of $(x,v)_p$, for charts $x$ containing $p$ and vectors $v \in \mathbf{R}^n$, such that $(x,v)_p$ and $(y,w)_p$ are identified if $D(y \circ x^{-1})(p)(v) = w$. The equivalence classes form the space of infinitismals at $p$, denoted $M_p$. Putting together all these fibres, we obtain the space $TM$. Now given a map $f: M \to N$, we have a map $f_*: TM \to TN$, defined by
%
\[ f_*((x,v)_p) = (y,D(y \circ f \circ x^{-1})(p)(v))_{f(p)} \]
%
which maps infinitismals at $p$ to infinitisimals at $f(p)$, such that the map $df_p: M_p \to N_{f(p)}$ is linear. We shall find that the rank of $df_p$ is essentially all the information that we need to obtain information about the local behaviour of the function $f$, which will tell us  the freedom of movement of the mapping locally around the point $p$. Note that for linear maps, this is certainly true, for all matrices of the same rank are equivalent up to a change of coordinates. We shall say that $f$ is rank $k$ at a point $p$ if $df_p$ is a rank $k$ operator.

\section{The Rank Theorems}

\begin{theorem}
    If $f$ is rank $k$ at a point $p$, there is a coordinate system $x$ at $p$ and $y$ at $f(p)$ such that for $1 \leq i \leq k$,
    %
    \[ y_i \circ f \circ x^{-1}(a_1, \dots, a_n) = a_i \]
    %
    or
    %
    \[ (y \circ f \circ x^{-1})(a_1, \dots, a_n) = (a_1, \dots, a_k, *,\dots, *) \]
    %
    where the asterixes denotes arbitrary functions.
\end{theorem}
\begin{proof}
    Let $(x,U)$ and $(y,V)$ be arbitrary coordinate systems around $p$ and $f(p)$. By a permutation of the coordinates, we may, by arranging coordinates, guarantee that the matrix
    %
    \[ \left( \left.\frac{\partial y_i \circ f}{\partial x_j}\right|_p \right)_{i,j = 1}^k \]
    %
    is invertible. Define a map $z:U \cap f^{-1}(V) \to \mathbf{R}^n$ around $p$ by $z_i = y_i \circ f$, for $1 \leq i \leq k$, and $z_i = x_i$ otherwise. The matrix
    %
    \[ D(z \circ x^{-1})(p) = \begin{pmatrix} \left( \left.\frac{\partial y_i \circ f}{\partial x_j}\right|_p \right) & X \\ 0 & I \end{pmatrix} \]
    %
    is invertible, hence, by the inverse function theorem, $x \circ z^{-1}$ is a diffeomorphism in a neighbourhood of $z(p)$. It follows that $z$ is a coordinate system at $p$, and for $1 \leq i \leq k$,
    %
    \[ y_i \circ f \circ z^{-1}(a_1, \dots, a_n) = a_i \]
    %
    and so we have found the required coordinate system.
\end{proof}

\begin{corollary}
    If $f$ is rank $k$ in a neighbourhood of a point $p$, then we may choose coordinate systems $x$ and $y$ such that
    %
    \[ y \circ f \circ x^{-1}(a_1, \dots, a_n) = (a_1, \dots, a_k, 0, \dots, 0) \]
\end{corollary}
\begin{proof}
    Choose $x$ and $y$ from the theorem above. Then
    %
    \[ D(y \circ f \circ x^{-1})(p) = \begin{pmatrix} I & 0 \\ X & \left.\left( \frac{\partial y_i \circ f}{\partial x_j} \right)\right|_p \end{pmatrix} \]
    %
    Since $f$ is rank $k$, the matrix in the bottom right corner must vanish in a neighbourhood of $p$. Therefore, for $i > k$, $y_i \circ f \circ x^{-1}$ can be viewed only as a function of the first $k$ coordinates. Define $z_i = y_i$, for $i < k$, and
    %
    \[ z_i = y_i - (y_i \circ f)(y_1 \dots y_k) \]
    %
    We have an invertible change of coordinate matrix,
    %
    \[ D(z \circ y^{-1}) = \begin{pmatrix} I & 0 \\ X & I \end{pmatrix} \]
    %
    So $z$ is a coordinate system, and
    %
    \[ z \circ f \circ x^{-1}(a_1, \dots, a_n) = (a_1, \dots, a_k, 0, \dots, 0) \]
    %
    we have constructed a coordinate system as needed.
\end{proof}

\begin{corollary}
    If $f: M^n \to N^m$ is rank $m$ at $p$, then for any coordinate system $y$, there exists a coordinate system $x$ such that
    %
    \[ y \circ f \circ x^{-1} (a_1, \dots, a_n) = (a_1, \dots, a_m) \]
\end{corollary}
\begin{proof}
    In the proof of the theorem, we need not rearrange coordinates of $y$ in the case that the matrix is rank $m$, just the coordinates of $x$.
\end{proof}

\begin{corollary}
    If $f: M^n \to N^m$ is rank $n$ at $p$, then for any coordinate system $x$, there exists a coordinate system $y$ such that
    %
    \[ y \circ f \circ x^{-1} (a_1, \dots, a_n) = (a_1, \dots, a_n, 0, \dots, 0) \]
\end{corollary}
\begin{proof}
    If $f$ is rank $n$ at a point, then it is rank $n$ on a neighbourhood, since the set of full rank matrices is open. Choose coordinate systems $u$ and $v$ such that $u \circ f \circ v^{-1}(a_1, \dots, a_n) = (a_1, \dots, a_n, 0, \dots, 0)$. Define a map $\lambda$ on $\mathbf{R}^m$ by $\lambda(a_1, \dots, a_m) = (x \circ v^{-1}(a_1, \dots, a_n), a_{n+1}, \dots, a_m)$. Then $\lambda$ is a diffeomorphism, hence $\lambda \circ y$ is a coordinate system, and
    %
    \begin{align*}
        (\lambda \circ y) \circ f \circ x^{-1} (a_1, \dots, a_n) &= \lambda \circ (y \circ f \circ v^{-1}) \circ (v \circ x^{-1}) (a_1, \dots, a_n)\\
        &= \lambda (v \circ x^{-1} (a_1 \dots a_n), 0 \dots 0)\\
        &= (a_1 \dots a_n, 0 \dots 0)
    \end{align*}
    %
    and we have found the chart required.
\end{proof}

\section{Immersions, Submersions, and Covering Maps}

Finding such charts is purely local, but we can use global topological properties to infer properties of an entire map. Call a map $f:M \to N$ an {\bf immersion} if $df_p$ is injective for all points $p$, and a {\bf submersion} if $df_p$ is always surjective. In terms of the rank discussion we have been having, if $f$ is an immersion if the rank of $f$ at $p \in M$ is the same as the dimension of $M$ at $p$, and a submersion if the rank of $f$ is the same as the rank of $N$ at $f(p)$.

\begin{theorem}
    Let $f: M \to N$ be a map between smooth manifolds. Then
    %
    \begin{enumerate}
        \item[(a)] If $f$ is injective and has locally constant rank, then $f$ is an immersion.
        \item[(b)] If $f$ is surjective, has constant rank, $M$ is second-countable, and $N$ has the same dimension throughout, then $f$ is a submersion.
    \end{enumerate}
\end{theorem}
\begin{proof}
    If $p$ is arbitrary, if the dimension of $N$ is $m$ at $f(p)$, and if $f$ has rank $k < m$ at $p$, then there are coordinate systems $x$ and $y$ such that
    %
    \[ (y \circ f \circ x^{-1})(a_1, \dots, a_n) = (a_1, \dots, a_k, 0, 0, \dots) \]
    %
    which clearly isn't injective if $k < n$. This proves (a). To prove (b), consider a cover of $f$ by charts $(x_\alpha, U_\alpha)$, where $f(U_\alpha) \subset V_\alpha$ for some chart $(y,V_\alpha)$ on $N$. Since $M$ is second-countable, it is also Lindel\"{o}f, and we may therefore assume the cover is countable. Let $N$ have dimension $m$, and $f$ has rank $k < n$. The coordinate systems can be chosen so that
    %
    \[ (y \circ f \circ x^{-1})(a_1, \dots, a_n) = (a_1, \dots, a_k, 0, \dots, 0) \]
    %
    This shows us that $(y \circ f \circ x_\alpha^{-1})(x(U_\alpha))$ is nowhere dense in $y(V_\alpha)$, and thus, $f(U_\alpha)$ is nowhere dense in $N$. It then follows by the Baire category theorem that $\bigcup f(U_\alpha) = f(M)$ is nowhere dense on $N$, which implies $f$ is not surjective, a contradiction.
\end{proof}

Variants of this proof can be adapted to various other cases, where $f$ may not have constant rank, and $N$ has non-constant dimension. We avoid discussing them in detail, because they are complicated to state, but obvious to adapt to any particular situation by working out the details there.

\begin{theorem}
    If $f:M^n \to N^m$ is a submersion, then $f$ is an open map.
\end{theorem}
\begin{proof}
    If $p \in M$, pick a neighbourhood $x$ around $p$ and $y$ around $f(p)$ such that
    %
    \[ x \circ f \circ y^{-1}(a_1, \dots, a_n) = (a_1, \dots, a_m) \]
    %
    This map is open, showing $f$ is locally open, and thus open on its entire domain, since openness is a local property.
\end{proof}

\begin{corollary}
    A submersion from a compact manifold to a connected manifold with the same dimensions is surjective.
\end{corollary}
\begin{proof}
    If $M$ is compact, and $f: M \to N$ an immersion, then $f(M)$ is compact, hence closed, and since $f$ is open, $f(M)$ is also an open subset of $N$. But then $f(M) = N$, since it is an open, closed, non-empty set.
\end{proof}

All this goes to show that submanifolds defined by differentiable functions have a nice structure. If $M$ is a manifold, and $N \subset M$ also has (under the relative topology) the structure of a differentiable manifold, then we say $N$ is a {\bf submanifold} of $M$ if the embedding $i: N \to M$ is an immersion. If such a differentiable structure exists, it is unique, for if $(x,U)$ is a chart in an atlas $\mathcal{A}$ on $N$, there is a chart $(\tilde{x},V)$ on $M$ such that $(\tilde{x} \circ x^{-1})(a_1, \dots, a_n) = (a_1, \dots, a_n, 0, \dots, 0)$, so that $(\tilde{x}^1, \dots, \tilde{x}^n)$ is equal to $x$ when restricted to $N$. But then if $\mathcal{B}$ is another atlas on $\mathcal{B}$, then any chart $(y,U) \in \mathcal{B}$ can be extended into a chart $\tilde{y}$ in $M$, and then $y \circ x^{-1} = \tilde{y} \circ \tilde{x}^{-1}$ is differentiable, so that $\mathcal{A} = \mathcal{B}$.

An important method of finding submanifolds of $\mathbf{R}^n$ is by specifying the submanifolds as a level set of Euclidean space. For instance, the sphere $S^n$ can be defined as the level set $f^{-1}(1)$ of the map $f(x) = \| x \|$. Provided that $\nabla f$ does not vanish on the domain level set, the level set is always a manifold, and this fact can be generalized to maps between arbitrary manifolds.

\begin{theorem}
    If $f: M^n \to N^m$ has constant rank $k$ in a neighbourhood of the points mapping to $q \in N$, then $f^{-1}(q)$ is a closed $n - k$ submanifold of $M$.
\end{theorem}
\begin{proof}
    If $f(p) = q$, and $f$ has rank $k$ at $p$, then we may write
    %
    \[ y \circ f \circ x^{-1}(a_1, \dots, a_n) = (a_1, \dots, a_k, 0 ,\dots, 0) \]
    %
    for some charts $(x,U)$ and $(y,V)$, with $y$ centred at $q$. This implies that if we adjust the last $n - k$ coordinates around $p$, the resulting point will still be in $f^{-1}(q)$, so that $x(U \cap f^{-1}(q)) = \{ (a_1, \dots, a_k) \} \times \mathbf{R}^{n-k}$. This means that $(x^{k+1}, \dots, x^n)$, restricted to $f^{-1}(q)$, is a homeomorphism onto an open subset of $\mathbf{R}^{n-k}$. We can take all possible $\tilde{x} = (x^{k+1}, \dots, x^n)$, over all possible points $p \in f^{-1}(q)$, as an atlas, because if $\tilde{y} = (y^{k+1}, \dots, y^n)$ is also formed by this process, then $\tilde{y} \circ \tilde{x}^{-1}$ is equal to the restriction of $y \circ x^{-1}$ to $\{ (a_1, \dots, a_k) \} \times \mathbf{R}^{n-k}$, hence differentiable. Thus $f^{-1}(q)$ is a closed $n - k$ submanifold of $M$.
\end{proof}

\begin{example}
    The special linear group $SL_n(\mathbf{R})$ is the set of invertible matrices with determinant one. Since the determinant is a multilinear function, we can find the determinant via the formula
    %
    \[ D(\det)(v_1, \dots, v_n)(w_1, \dots, w_n) = \sum_{k = 1}^n \det(v_1, \dots, w_k, \dots, v_n) \]
    %
    where we see $M_n(\mathbf{R})$ as $(\mathbf{R}^n)^n$. Then for any $(v_1, \dots, v_n) \in GL_n(\mathbf{R})$,
    %
    \[ D(\det)(v_1, \dots, v_n)(v_1, \dots, v_n) = n \det(v_1, \dots, v_n) \neq 0 \]
    %
    So $\det$ has rank 1 at every point (full rank), and $SL_n(\mathbf{R})$ is a (closed) submanifold of $GL_n(\mathbf{R})$ dimension $n^2 - 1$.
\end{example}

\begin{example}
    The orthogonal group $O_n(\mathbf{R})$ is the set of matrices $M$ such that $MM^t = I$. Then $O_n(\mathbf{R})$ is closed, for the map $\psi: M \mapsto MM^t$ is continuous, and $O_n(\mathbf{R}) = \psi^{-1}(I)$. $\psi$ maps $M_n(\mathbf{R})$ into the set of symmetric matrices, which is a subspace of $M_n(\mathbf{R})$ of dimension $n(n+1)/2$. If we take the $i$'th diagonal entry of $MM^t = I$, we obtain that
    %
    \[ v_{i1}^2 + v_{i2}^2 + \dots + v_{in}^2 = 1 \]
    %
    This implies that $M$ lies on $(S^{n-1})^n \subset \mathbf{R}^{n^2}$, so $O_n(\mathbf{R})$ is closed and bounded, and therefore compact. Consider the diffeomorphism $R_A: B \mapsto BA$ of $GL_n(\mathbf{R})$, for a fixed $A \in GL_n(\mathbf{R})$. We also have $\psi \circ R_A = \psi$, for $A \in O_n(\mathbf{R})$. We conclude that
    %
    \[ D(\psi)(A) \circ D(R_A)(I) = D(\psi \circ R_A)(I) = D(\psi)(I) \]
    %
    Since $R_A$ is a diffeomorphism, $D(\psi)(A)$ has the same rank as $D(\psi)(I)$. Let us find this rank. Explicitly, we may write the projections of $\psi$ as
    %
    \[ \psi^{ij}(M) = \sum_{k = 1}^n M_{ik}M_{jk} \]
    %
    Then
    %
    \[ \left.\frac{\partial \psi^{ij}}{\partial M^{lk}}\right|_M = \begin{cases} 2 M_{ik} & i = j = l \\ M_{ik} & j = l \\ M_{jk} & i = l \\ 0 & \text{elsewise} \end{cases} \]
    %
    In particular, at the identity,
    %
    \[ \left.\frac{\partial \psi^{ij}}{\partial M^{lk}}\right|_I = \begin{cases} 2 & i = j = k = l \\ 1 & j = k = l \\ 1 & i = k = l \\ 0 & \text{elsewise} \end{cases} \]
    %
    It follows that $d\psi(TGL_n(\mathbf{R}))$ is the space of all symmetric matrices, which has dimension $n(n+1)/2$. This $\psi$ has constant rank $n(n+1)/2$, and we find the space of orthogonal matrices has dimension
    %
    \[ n^2 - n(n+1)/2 = n(n-1)/2 \]
    %
    as a differentiable manifold.

    Every orthogonal matrix has determinant $\pm 1$. The special orthogonal group $SO_n(\mathbf{R})$ is the set of orthogonal matrices of determinant one, and is an open submanifold of $O_n(\mathbf{R})$. It's actually a connected component. To see this, consider the (Lie-group) action of $SO_n$ on $S^{n-1}$ defined by matrix multiplication. For $n \geq 3$, it is easy to see that the action is transitive -- we can use the Gram-Schmidt orthogonalization process to insert $v \in S^{n-1}$ into an orthonormal basis $(w_1, \dots, w_{n-1}, v)$. If we consider the orthogonal matrix which maps $e_i$ to $w_i$ and $e_n$ to $v$, and it is does not have positive determinant, we need only to swap two $w_i$ with each other. For $n = 2$, $SO(2)$ is isomorphic to the torus $\mathbf{T}$ in such a way that the action on $\mathbf{T} \cong S^1$ is equivalent to complex multiplication, which is verified to be transitive. Thus, for a fixed $p \in S^{n-1}$, the action $M \mapsto Mp$ is surjective, hence open (it in fact must be a submersion). In the 1 dimensional case, the map is not surjective, but in this case $SO(1) = (0)$, the action is trivial, and $S^0$ is discrete, so the maps $M \mapsto Mp$ are open. In any dimension $\geq 2$, the stabilizer of $e_n$ is the subgroup of matrices of the form
    %
    \[ \begin{pmatrix} X & 0 \\ 0 & 1 \end{pmatrix} \]
    %
    where $X \in SO(n-1)$. Thus the stabilizer is actually (Lie-group) isomorphic to $SO(n-1)$. Using the orbit stabilizer theorem, we find that for $n \geq 2$, $S^{n-1}$ is homeomorphic to $SO(n)/SO(n-1)$ (the space of cosets). Using induction, we may assume that $SO(n-1)$ is connected. But for $n \geq 2$, $S^{n-1}$ is connected. The theorem then follows from the general fact that if $H$ is a closed subgroup of $G$, and $H$ and $G/H$ are connected, then $G$ is connected.
\end{example}

\begin{example}
    If $M$ is a 1-submanifold of $\mathbf{R}^2$, specified as the level set of a function $f: \mathbf{R}^2 \to \mathbf{R}$,
    %
    \[ M = \{ (x,y) \in \mathbf{R}^2: f(x,y) = 0 \} \]
    %
    then the surface of revolution about the $z$-axis related to $M$ can be identified as the space
    %
    \[ N = \left\{ (x,y,z) \in \mathbf{R}^3: f \left( \sqrt{x^2 + y^2}, z \right) = 0 \right\} \]
    %
    It is a surface, because the differential of the defining level set mapping has rank 1 at any point in a neighbourhood of $N$ (because we can use the chain rule, and the fact that the map $(x,y,z) \mapsto (\sqrt{x^2 + y^2}, z)$ has full rank at every point).
\end{example}

Similar results can be obtained for manifolds with boundary.

\begin{theorem}
    If $f \in C^\infty(M)$, and $df_p \neq 0$ for any $p$ with $f(p) = a$ or $f(p) = b$, where $a,b \in \mathbf{R}$, then $f^{-1}([a,b])$ is a submanifold of $M$ with boundary.
\end{theorem}

Like the smooth immersions discussed here, a {\bf topological immersion}, which is a continuous map $f: X \to Y$ between topological spaces which is locally an embedding. Certainly a smooth immersion is a topological immersion, but a smooth topological immersion may not be a smooth immersion (indeed, consider the map $t \mapsto t^3$).

Injective immersions are mostly well behaved, apart from the odd inconsistency of dropping differentiable maps to subdomains. Let $g:M \to N$ be a differentiable function with $g(M)$ contained in the image $f(N')$ of an immersed submanifold $N'$. If $f$ is globally injective, we may consider the function $g:M \to N'$. A suitable question to ask is whether this function is differentiable. In most cases, the answer is yes.

\begin{example}
    Immerse $(-\pi, \pi)$ in $\mathbf{R}^2$ via the map
    %
    \[ f(t) = (\sin 2t, \sin t) \]
    %
    The image is known as a lemniscate. The map $g:(-\pi, \pi) \to (-\pi, \pi)$ defined by
    %
    \[ g(x) = \begin{cases} \pi + x &: x < 0 \\ 0 &: x = 0 \\ x - \pi &: x > 0 \end{cases} \]
    %
    is not even continuous, yet $f \circ g$ is differentiable.
\end{example}

Continuity is all that can go wrong in this situation.

\begin{theorem}
    Let $f:M^n \to N^m$ be an injective immersion of $M$ in $N$, and suppose $g: N' \to N$ is differentiable, and $g(N') \subset N$. If $g$ is continuous considered as a map into $M$, then $g$ is differentiable considered as a map into $M$.
\end{theorem}
\begin{proof}
    Consider an arbitrary point $p \in N'$. There is a coordinate system $((y_1, \dots, y_m),U)$ around $g(p)$ such that $(y_1, \dots, y_n)$ is a coordinate system around $f^{-1}(g(p))$. Since $g$ is continuous into $M$, there is a coordinate system $x$ around $p$ which maps into $U \cap f(M)$. $y \circ g \circ x^{-1}$ is differentiable, so each $y_i \circ g \circ x^{-1}$, which we have constructed a coordinate system at $f^{-1}(g(p))$ at, is differentiable. Thus $g$ is differentiable mapping into $M$.
\end{proof}

Thus we see that, for submanifolds whose topological structure is induced by the relative topology on the subspace, we can always descend to differentiable maps when needed. These submanifolds are known as {\bf imbedded manifolds} (an immersion that is also an embedding).

\begin{theorem}
    The following are sufficient to conclude that an injective immersion $f: M \to N$ is an imbedding.
    %
    \begin{enumerate}
        \item[(a)] $f$ is open or closed.
        \item[(b)] $f$ is proper (the inverse image of compact sets are compact).
        \item[(c)] $M$ is compact.
        \item[(d)] The dimension of $M$ is equal to the dimension of $N$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    An injective open or closed continuous map is a topological embedding, so (a) is fairly trivial. If $M$ is compact or $f$ is proper, $f$ is closed, and if the dimension of $M$ is equal to the dimension of $N$, $f$ is open (by looking at the coordinate charts), so (b), (c) and (d) follow from (a).
\end{proof}

The idea of a submersive map $f: M \to N$ is very closely related to its family of smooth sections $\sigma: N \to M$, maps such that $f \circ \sigma = \text{id}_N$. More precisely, a submersive map is closely related to its local smooth sections over neighbourhoods of $N$.

\begin{theorem}
    A smooth map $f: M \to N$ is a submersion if and only if every $p \in M$ is in the image of some local section.
\end{theorem}
\begin{proof}
    If $f$ is a submersion.
\end{proof}

\section{Manifolds and Measure}

The Lebesgue measure on $\mathbf{R}^n$ gives us a nice way to calculate the volume of various sorts of objects. A subset $A$ has measure zero if, for any $\varepsilon$, there is a cover of $A$ by open sets $B_1, B_2, \dots$ with
%
\[ \sum \lambda(B_i) < \varepsilon \]
%
Manifolds may not necessarily have a measure like Lebesgue's, but we can still define a subset $A \subset M$ to be of {\bf measure zero} if $A = \bigcup C_i$ where $C_i \subset U_i$, and there are charts $(x_i,U_i)$ in which $x_i(C_i)$ has measure zero. Indeed, if we place any measure on the manifold, which locally behaves like the Lebesgue measure, then $A$ really will have measure zero, as a countable union of sets of measure zero.

We shall show that there are few problems with differentiable points on manifolds. Call a point $p$ on $M$ {\bf criticial} for a map $f:M \to N$ if $f$ has rank less than the dimension of $p$ at $p$. Call $f(p)$ a critical value. Recall Sard's theorem, that if $f: \mathbf{R}^n \to \mathbf{R}^n$ is $C^1$, then the critical values form a set of measure zero in $\mathbf{R}^n$. It follows by localization that the critical values of every $C^1$ map between manifolds of the same dimension form a set of measure zero. Sard's theorem can actually be generalized to the following:

\begin{theorem}[Sard]
    If $f:M^n \to N^m$ is $C^k$, and $k \geq n-m$, with $k \geq 1$ then the set of critical values from a set of measure zero in $N$.
\end{theorem}

The case we shall need is where $n = m$. It is easy to prove the case where $m > n$. The tricky case is where $m < n$.

\begin{theorem}
    If $f:M^n \to N^m$ is $C^1$, $M^n$ is connected, and $n < m$, then $f(M)$ has measure zero in $N^m$.
\end{theorem}
\begin{proof}
    Consider the map $g: M \times \mathbf{R}^{m-n} \to N$, defined by $g(p,x) = f(p)$. Then all values of $g$ are critical, and so $g(M \times \mathbf{R}^{m-n}) = f(M)$ has measure zero.
\end{proof}




\section{The $\mathbf{C^\infty}$ Category}

We now show that the topological category of manifolds naturally restricts to the category of differentiable manifolds.

\begin{example}[Differentiable Product]
    If $M$ and $N$ are differentiable manifolds, we may consider an atlas on $M \times N$ with the differentiable structure generated by all maps $x \times y$, where $x$ is a chart on $M$ and $y$ is a chart on $N$. From this definition, we have the property that $(f,g): X \to M \times N$ is differentiable if and only if $f: X \to M$ and $g: X \to N$ are differentiable. This is the unique differentiable structure on $M \times N$ which has this property.
\end{example}

\begin{example}[Differentiable Quotients and $\mathbf{P}^n$]
    If $N$ is a quotient space of a differentiable manifold $M$ whose projection $\pi:M \to N$ is locally injective, then we may ascribe a differentiable structure to it. We take all charts $x:U \to \mathbf{R}^n$ on $M$ such that $U$ is homeomorphic to $\pi(U)$ by $\pi$. We may then push the chart onto $N$, and all the charts placed down on $N$ will be $C^\infty$ related. As a covering, this can be extended to a maximal atlas. In fact, this is the unique structure on $N$ which causes $f: N \to L$ to be differentiable if and only if $f \circ \pi$ is differentiable. It allows us to consider $\mathbf{P}^n$ a differentiable manifold, taking the differentiable structure on $S^n$, as does the M\"{o}bius strip, taking the projection from $(-\infty, \infty) \times (0,1)$.
\end{example}




\section{Defining $C^\infty$ functions: Partitions of Unity}

The use of $C^\infty$ functions relies on the fact that manifolds possess them in plenty. The following theorem gives us our first plethora. First, we detail some $C^\infty$ functions on $\mathbf{R}^n$.

\begin{enumerate}
    \item The map $f:\mathbf{R} \to \mathbf{R}$, defined by
    %
    \[
    g(t) =
    \begin{cases}
        e^{-t} & : t > 0\\
        0 & : \text{elsewhere}
    \end{cases}
    \]
    %
    is $C^\infty$. We have $0 < f(t) < 1$ on $(0,\infty)$, and $f^{(n)}(0) = 0$ for all $n$.
    \item The $C^\infty$ map $g(t) = f(t-1)f(t+1)$ is positive on $(-1,1)$, and zero everywhere else. Similarily, for any $\varepsilon$, there is a map $g_\varepsilon$ which is positive on $(-\varepsilon, \varepsilon)$ and zero elsewhere.
    \item The map 
    %
    \[ l(t) = \begin{cases}
        \left(\int_{-\varepsilon}^t g_\varepsilon \right)/\left(\int_{-\varepsilon}^\varepsilon g_\varepsilon \right) & : t \in (0, \infty) \\
        0 & : \text{elsewise}
    \end{cases} \]
    %
    is $C^\infty$, is zero for negative $t$, increasing on $(0, \varepsilon)$, and one on $[\varepsilon, \infty)$.
    \item There is a differentiable map $h:\mathbf{R}^n \to \mathbf{R}$ defined by $h(x_1, \dots, x_n) = g(x_1) g(x_2) \dots g(x_n)$ which is positive on $(-1, 1)^n$, and zero elsewhere.
\end{enumerate}

With these nice functions in hand, we may form them on arbitrary manifolds.

\begin{theorem}
    If $M$ is a differentiable manifold, and $C$ is a compact set contained in an open set $U$, then there is a differentiable function $f:M \to \mathbf{R}$ such that $f(x) = 1$ for $x$ in $C$, and whose support $\overline{\{ x \in M : f(x) \neq 0 \}}$ is contained entirely within $U$.
\end{theorem}
\begin{proof}
    For each point $p$ in $C$, consider a chart $(x,V)$ around $p$, with $\overline{V} \subset U$, and $x(V)$ containing the open unit square $(-1,1)^n$ in $\mathbf{R}^n$. We may clearly select a finite subset of these charts $(x_k,V_k)$ such that the $x_k^{-1}((-1,1)^n)$ cover $C$. We may define a map $f_k:V_k \to \mathbf{R}$ equal to $h \circ x_k$, where $h$ is defined above. It clearly remains $C^\infty$ if we extend it to be zero outside of $V_k$. Then $\sum f_k$ is positive on $C$, and whose closure is contained within $\bigcup \overline{V_k} \subset U$. Since $C$ is compact, and the function is continuous, $\sum f_k$ is bounded below by $\varepsilon$ on $C$. Taking $f = l \circ (\sum f_k)$, where $l$ is defined above, we obtain the map needed.
\end{proof}

To enable us to define $C^\infty$ functions whose support lie beyond this range, we need to consider a technique to extend $C^\infty$ functions defined locally to manifolds across the entire domain. A {\bf partition of unity} on a manifold $M$ is a family of $C^\infty$ functions $\{ \phi_i : i \in I \}$, and such that the following two properties hold:
%
\begin{enumerate}
    \item The supports of the functions forms a locally finite set.
    \item For each point $p \in M$, the finite sum $\sum_{i \in I} \phi_i(p)$ is equal to 1.
\end{enumerate}
%
If $\{ U_i \}$ is an open cover of $M$, then a partition of unity is subordinate to this cover if it also satisfies (3):
%
\begin{enumerate}
    \item[3.] The closure of each function is contained in some element of the cover.
\end{enumerate}
%
It is finally our chance to use the topological `niceness' established in the previous chapter.

\begin{lemma}[The Shrinking Lemma]
    If $M$ is a paracompact manifold, and $\{ U_i \}$ is an open cover, then there exists a refined cover $\{ V_i \}$ such that for each $i \in I$ there exists $i'$ such that $\overline{V_i} \subset U_{i'}$.
\end{lemma}
\begin{proof}
    Without loss of generality, we may assume $\{ U_i \}$ is locally finite, and $M$ is connected. Then $M$ is also $\sigma$-compact, $M = \bigcup C_i$. Since $C_i$ is compact, and each $p \in C_i$ locally intersects only finitely many $U_i$, then $C_i$ intersects only finitely many $U_i$. Therefore $\bigcup C_i$ intersects only countably many $U_i$, and thus our locally finite cover is countable. Consider an ordering $\{ U_1, U_2, \dots \}$ of $\{ U_i \}$. Let $C_1$ be the closed set $U_1 - (U_2 \cup U_3 \cup \dots)$. Let $V_1$ be an open set with $C_1 \subset V_1 \subset \overline{V_1} \subset U_1$. Inductively, let $C_k$ be the closed set $U_k - (V_1 \cup \dots \cup V_{k-1} \cup U_{k+1} \cup \dots)$, and define $V_k$ to be an open set with $C_k \subset V_k \subset \overline{V_k} \subset U_k$. Then $\{ V_1, V_2 \dots \}$ is the desired refinement.
\end{proof}

\begin{theorem}
    Any cover on a paracompact manifold induces a subordinate partition of unity.
\end{theorem}
\begin{proof}
    Let $\{ U_i \}$ be an open cover of a paracompact manifold $M$. Without loss of generality, we may consider $\{ U_i \}$ locally finite. Suppose that each $U_i$ has compact closure. Choose $\{ V_i \}$ satisfying the shrinking lemma. Apply Theorem (2.13) to $\overline{V_i}$ to obtain functions $\psi_i$ that are 1 on $\overline{V_i}$ and zero outside of $U_i$. These functions are locally finite, and thus we may define $\phi_i$ by
    %
    \[ \phi_i(p) = \frac{\psi_i(p)}{\sum_j \psi_j(p)} \]
    %
    Then $\phi_i$ is the partition of unity we desire.

    This theorem holds for any $\{ U_i \}$ provided Theorem (2.13) holds on any closed set, rather than just a compact one. Let $C$ be a closed subset of a manifold, contained in an open subset $U$, and for each $p \in C$, choose an open set $U_p$ with compact closure contained in $U$. For each $p \in C^c$, choose an open subset $V_p$ contained in $C^c$ with compact closure. Then our previous compact case applies to this cover, and we obtain a subordinate partition of unity $\{ \zeta_i \}$. Define $f$ on $M$ by
    %
    \[ f(p) = \sum_{\overline{\zeta_i} \subset U_p} \zeta_i(p) \]
    %
    Then the support of $f$ is contained within $U$, and $f = 1$ on $C$.
\end{proof}

Partitions of unity allow us to extend local results on a manifold to global results. The utility of these partitions is half the reason that some mathematicians mandate that manifolds must be paracompact -- otherwise many nice results are lost.

\begin{theorem}
    In a $\sigma$-compact manifold $M$, there exists a smooth, real-valued function $f$ such that $f^{-1}((-\infty, t])$ is compact for each $t$.
\end{theorem}
\begin{proof}
    Let $M$ be a $\sigma$-compact manifold with $M = \bigcup B_i$, Where $\overline{B_i}$ is compact, $B_i$ is diffeomorphic to a ball, and the $B_i$ are a locally finite cover. Consider a partition of unity $\{\psi_i\}$ subordinate to $\{B_i\}$, and take the sum
    %
    \[ f(x) = \sum k \psi_k(x) \]
    %
    Then $f$ is smooth, since locally it is the finite sum of smooth functions. If $x \not \in B_1, \dots, B_n$, then
    %
    \[ f(x) = \sum_{k = 1}^\infty k \psi_k(x) = \sum_{k = n+1}^\infty k \psi_k(x) \geq (n+1) \sum_{k = n+1}^\infty \psi_k(x) = (n+1) \]
    %
    Therefore if $f(x) < n$, $x$ is in some $B_i$. Thus $f^{-1}((-\infty, n])$ is a closed subset of $\overline{B_1} \cup \dots \cup \overline{B_n}$, and is therefore compact.
\end{proof}

Other existence proofs also follow naturally.

\begin{lemma}
    If $A$ is a closed subset of a paracompact manifold $M$, there is a differentiable function $f: M \to [0,1]$ with $f^{-1}(0) = A$.
\end{lemma}
\begin{proof}
    Let us begin proving this for $M = \mathbf{R}^n$. Let $\{ U_i \}$ be a countable cover of $\mathbf{R}^n - A$ by open unit balls. For each $U_i$, pick a smooth $f_i : \mathbf{R}^n \to [0,1]$ positive on $U_i$, and equal to zero on $\mathbf{R}^n - U_i$. Define
    %
    \[ \alpha_j = \min \left\{ \left\| \frac{\partial^n f_i}{\partial x_{i_1} \dots \partial x_{i_n}} \right\|_\infty : i \leq j, n \leq j \right\} \]
    %
    Each $\alpha_j$ is well defined, because $f_i$ is $C^\infty$ and tends to zero as we leave $U_i$. Define
    %
    \[ f = \sum_{k = 1}^\infty \frac{f_k}{\alpha_k 2^k} \]
    %
    Then $f$ is differentiable, since if $k \geq n$,
    %
    \[ \frac{1}{\alpha_k 2^k} \frac{\partial^n f_k}{\partial x_{i_1} \dots \partial x_{i_n}} \]
    %
    so the sums of all partial derivatives converge uniformly, and $f^{-1}(0) = C$ because the function is positive for some coefficient everywhere else.

    To address the general case, let $M$ be paracompact, and find a cover of coordinate balls $\{ B_\alpha \}$ for $M - A$. Then we may find a partition of unity $\{ \psi_\alpha \}$ for this family, and find smooth $f_\alpha: M \to \mathbf{R}$ with $f_\alpha^{-1}(0) = A \cap B_\alpha$. But then $f = \sum \psi_\alpha f_\alpha$ is smooth (by local finiteness), and satisfies $f^{-1}(0) = A$.
\end{proof}

\begin{corollary}
    If $A$ and $B$ are closed on a paracompact manifold $M$, then there is a function $h: M \to [0,1]$ with $h^{-1}(0) = A$, $h^{-1}(1) = B$.
\end{corollary}
\begin{proof}
    Modifying the function obtained in the last theorem, we can find $f: M \to [0,1]$ with $f^{-1}(1) = B$. Denote $f^{-1}(0)$ by $C$. If $g: M \to [0,1/2]$ satisfies $g^{-1}(0) = A$, and if $\psi: M \to [0,1]$ is a bump function on $B$, vanishing on $A \cup C$, then
    %
    \[ h = \psi f + (1 - \psi) g \]
    %
    satisfies $h^{-1}(1) = B$, because for $p \not \in B$, $f(p), g(p) < 1$. Now certainly $h(p) = 0$ for $p \in C$. If $p \in A - C$, $\psi(p) = 0$, so $h(p) = g(p) > 0$, and if $p \not \in A \cup C$, $f(p), g(p) > 0$, so $h(p) > 0$. Thus $h^{-1}(0) = A$, and we have shown $h$ is the needed function in the theorem.
\end{proof}

\section{Differentiable Manifolds with Boundary}

Recall that we may extend differentiability on subsets of Euclidean space in the following way. In general, a map $f: A \to B$ between arbitrary subsets of Euclidean space is differentiable if $f$ can be extended to a differentiable map on an open subset of Euclidean space containing $A$.

\begin{theorem}
    If $f:\mathbf{H}^n \to \mathbf{R}$ is locally differentiable (every point has a neighbourhood on which $f$ can be locally extended to be differentiable), then $f$ is differentiable in the sense defined above.
\end{theorem}
\begin{proof}
    Let $\{ U_\alpha \}$ be a open cover of $\mathbf{H}^n$ in $\mathbf{R}^n$ with smooth functions $g_\alpha:U \to \mathbf{R}$ agreeing with $f$ where the two are jointly defined. Consider a partition of unity $\{ \Phi_\alpha \}$ subordinate to $\{ U_\alpha \}$. Define $g = \sum g_k \Phi_k$, defined on $\bigcup U_\alpha$, a open extension of $\mathbf{H}^n$. Each pair $g_k$ and $\Phi_k$ are differentiable, so the multiplication of the two is differentiable. Since these functions are locally finite, we also have $g$ differentiable across its domain. If $p \in \mathbf{H}^n$, then $g_k(p) = f(p)$. Thus
    %
    \[ g(p) = \sum g_k(p) \Phi_k(p) = \sum f(p) \Phi_k(p) = f(p) \]
    %
    since the $\Phi_k$ sum up to one. Thus $g|_{\mathbf{H}^n} = f$, and we have extended $f$ to be differentiable.
\end{proof}

This allows us to define a notion of differentiable structure for a manifold with boundary. We can extend the notion of two charts being $C^\infty$ consistent, because we have extended differentiability on non open subsets of $\mathbf{H}^n$ to a non-local criterion. Similarily, a map $f: M \to N$ can also be considered differentiable if $y \circ f \circ x^{-1}:x(U) \to y(V)$ can be extended to be a differentiable function on an open subset of Euclidean space. Thus manifolds with boundary have effectively the same theory as manifolds without boundary.

\begin{example}[Differentiable structures on the boundary of a manifold]
    Given a differentiable manifold with boundary $M$, we can assign a unique differentiable structure to $\partial M$ such that the inclusion map is an imbedding. We can generate it from the atlas consisting of the restriction of charts on $M$ to the boundary, projected into an $(n-1)$ dimensional subspace of $\mathbf{R}^n$. The transition maps are easily verified to be $C^\infty$.
\end{example}

One issue with manifolds with boundary is that the rank theorem does not hold. The problem is that, at the boundary, we are restricted in how we reapply coordinates -- we can only consider smooth automorphisms of $\mathbf{H}^n$ rather than $\mathbf{R}^n$. However, for immersions $f$ of manifolds with boundary in manifolds without, we do have coordinate charts for which
%
\[ (y \circ f \circ x^{-1})(a_1, \dots, a_n) = (a_1, \dots, a_n, 0, \dots, 0) \]
%
because in the theorem we currently have, we need not change the coordinates on the manifold with boundary, only on the manifold without. We obtain similar results for submersions of manifolds without boundary in manifolds with boundary.




\chapter{The Tangent Bundle}

Historically, calculus was the subject of infinitisimals, differentiable functions which are `infinitisimally linear'. It took over 200 years to make precise the analytical notions defining the field; in the process, infinitisimals vanished from sight, replaced by linear approximations, epsilons and deltas. On manifolds, we cannot discuss global linear approximations, since the space is not globally linear. Thus we must return to using infinitisimals, which lie on a structure called the tangent bundle.

Geometrically, $\mathbf{R}^n$ is just a system of points in space. Identification of these points with coordinates gives us numerical facts about the space we live in. Algebraically, $\mathbf{R}^n$ is a system of arrows about an origin point, which can be added, subtracted, and scaled. But why do these arrows have to start at the origin? A differentiable curve $c:(a,b) \to \mathbf{R}^n$ has a tangent vector $c'(x) \in \mathbf{R}^n$, canonically pictured as eminating from the point $c(x)$. The idea of a vector space eminating from each point on a manifold is the key idea which enables us to linearize differentiable functions on the manifold.

$\mathbf{R}^n$ shall provide the prototype of the vector space construction we shall construct on all manifolds. Here, the introduction of tangent vectors is easy, because the space is linear to begin with. We consider the space $\mathbf{R}^n \times \mathbf{R}^n$, where we denote $(p,v)$ by $v_p$, viewed as the vector $v$ eminating from the point $p$. We then have the projection map $\pi(v_p) = p$, which projects a vector to the point it was emitted from. On each of the `tangent spaces' $\pi^{-1}(p)$, which we denote $\mathbf{R}^n_p$, we have a vector space structure defined by $v_p + w_p = (v + w)_p$ and $\lambda (v_p) = (\lambda v)_p$. We shall denote the space of all the tangent vectors as $T\mathbf{R}^n = \mathbf{R}^n \times \mathbf{R}^n$, known as the tangent bundle.

The nicest part about the tangent bundle is that we can express the collection of all the derivatives $Df(p)$ of a differentiable map $f: \mathbf{R}^n \to \mathbf{R}^m$ as a single mathematical object. Indeed, we may define $f_*: T\mathbf{R}^n \to T\mathbf{R}^m$ by
%
\[ f_*(v_p) = Df(p)(v)_{f(p)} \]
%
which is essentially a way of putting all the derivatives $Df(p)$ together, acting on the `infinitisimal' vector spaces at each point. Another feature of the tangent bundle is that if we have two functions $h: \mathbf{R}^n \to \mathbf{R}^m$ and $g: \mathbf{R}^m \to \mathbf{R}^l$, we may consider not only their composition $g \circ h$, but also the composition of their derivatives $g_* \circ h_*$. The chain rule can then be expressed as the elegant formula
%
\[ (g \circ h)_* = g_* \circ h_* \]
%
an obvious fact, once you carry out the computations.

If $M$ is an $m$-dimensional submanifold of $\mathbf{R}^n$, we can obtain a bundle $TM$ as the set of vectors in $T\mathbf{R}^n$ which lie tangent to $M$. Precisely, for each point $p$, we consider a chart $(x,U)$ at $p$, and consider the map $i \circ x^{-1}: x(U) \to \mathbf{R}^n$, then defining the tangent space at $p$ to be
%
\[ M_p = (i \circ x^{-1})_*\left(\mathbf{R}^m_{x(p)}\right) \subset \mathbf{R}^n_p \]
%
This set of vectors is invariant of the particular coordinate system $x$ we use, because if $y$ is any other chart, $(y \circ x^{-1})$ is a diffeomorphism, so
%
\[ (y \circ x^{-1})(T\mathbf{R}^m_{x(p)}) = T\mathbf{R}^m_{y(p)} \]
%
and therefore
%
\[ (i \circ y^{-1})_*\left(T\mathbf{R}^m_{y(p)}\right) = (i \circ y^{-1})_* \circ (y \circ x^{-1})_* \left( T\mathbf{R}^m_{x(p)} \right) = (i \circ x^{-1})_* \left( T\mathbf{R}^m_{(x(p))} \right) \]
%
we can then put all these vectors together to form the tangent space $TM$. If $M$ and $N$ are both submanifolds of Euclidean space, and $f: M \to N$ is differentiable, around each point $p \in M$ there is a chart $(x,U)$ on $\mathbf{R}^n$ such that $(x^1, \dots, x^m)$ is a chart on $M$, and $x^{m+1}(p), \dots, x_n(p) = 0$ for $p \in M \cap U$. We can thus extend $f$ locally around $M$ to a function $\tilde{f}: U \to N$, and then define $f_*$ on $M_p$ as the restriction of $\tilde{f}_*$ (which is invariant of the extension chosen, since the partial derivatives $\partial \tilde{f}^i/\partial x^j$ are invariant, and are sufficient to describe the operation on $M$).

Since all manifolds can be imbedded in some dimension of Euclidean space, this is technically a method strong enough to define the tangent space on all manifolds, though it is not entirely elegant because the imbedding is not unique, so there are many different candidates for the bundle (though all the candidates will essentially be equivalent). There is a much better approach, found in the abstract theory of vector bundles.

\section{Vector Bundles}

The abstract definition of a vector space eminating from points on a topological space comes from the theory of vector bundles. A {\bf vector bundle} over a topological space $X$ is a topological space $B$ together with a continuous, surjective projection $\pi: B \to X$ such that for each $x \in X$, the set $B_x = \pi^{-1}(\{x\})$ has a vector space structure, and such that we have local triviality; for each point $x \in X$, there is a neighbourhood $U$ and a homeomorphism $\phi: \pi^{-1}(U) \to U \times \mathbf{R}^n$ which is linear on each fibre. If $X$ is connected, the $n$ chosen here is unique, and $B$ is known as an $n$-dimensional bundle. If $X$ and $B$ are both differentiable manifolds, and the projection maps $\pi$ and triviality maps $\phi$ are differentiable, then we shall call the vector bundle a {\bf differentiable bundle}.

\begin{example}
    For any topological space $X$, we have trivial $n$ dimensional vector bundles $\varepsilon^n(X) = U \times \mathbf{R}^n$, where the projection map is the obvious one.
\end{example}

\begin{example}
    The M\"{o}bius strip can be identified as a vector bundle over the circle. Indeed, the circle can be topologically identified with $\mathbf{T} = \mathbf{R}/\mathbf{Z}$, and the M\"{o}bius strip can also be defined as the quotient of $\varepsilon^1(\mathbf{R}) = \mathbf{R}^2$ where $(x,v)$ is identified with $(x + n, (-1)^n v)$. The projection induced onto $\mathbf{T}$ from the projection onto $\mathbf{R}$ gives us the M\"{o}bius bundle.
\end{example}

If $X$ and $Y$ are spaces with bundles $\pi: B \to X$ and $\phi: B' \to Y$, then a {\bf bundle map} is a pair $(f,f^\sharp)$ of continuous maps, with $f:X \to Y$ and $f^\sharp:B \to B'$, where each $f^\sharp_p = (f^\sharp)|_{E_p}$ is a linear map from $B_p$ to $B'_{f(p)}$. Such a map makes the diagram below commute.
%
\begin{center}
\begin{tikzcd}
    E \arrow[r, "f_\sharp"] \arrow[d, "\pi"] & F \arrow[d, "\phi"] \\ A \arrow[r, "f"] & B
\end{tikzcd}
\end{center}
%
An isomorphism in the category of bundle maps is known as an equivalence. Since $f$ can be easily obtained from $f^\sharp$, we can describe a bundle map solely by the map $f^\#$ between bundles, or more particularly by the linear maps $f^\#_p: X_p \to Y_p$, which change continuously with respect to $p$. Note that the local triviality condition of a bundle can be expressed by saying that every vector bundle is locally equivalent to the trivial bundle by a tangent map preserving base points.

If $f: X \times \mathbf{R}^n \to X \times \mathbf{R}^n$ is a bundle map, then $f$ can be identified with a continuous map $x \mapsto M^x$ from $X$ into $L(\mathbf{R}^n, \mathbf{R}^m)$. Certainly there are unique $M^x$ such that $f(x,v) = (x, M^x v)$. If we let $x_i \to x$ while fixing $v$, then $(x_i,M^{x_i} v) \to (x, M^x v)$, so that $M^x$ is the pointwise limit of the $M^{x_i}$. Since the weak topology on $L(\mathbf{R}^n, \mathbf{R}^m)$ corresponds to the standard topology, $M^{x_i}$ actually converges to $M^x$ in $L(\mathbf{R}^n, \mathbf{R}^m)$. Thus a bundle map is locally just a continuous way of assigning linear maps to points in $X$. We also note that if $f: X \times \mathbf{R}^n \to X \times \mathbf{R}^n$ is a differentiable bundle map, then the map $x \to M^x$ is also differentiable, since $M^x_{ij}$ is equal to the projection of the vector at $f(x,e_j)$ onto the $i$'th coordinate, which varies differentiably as $x$ varies.

Our current aim is to define, for each manifold $M$, a bundle $TM$ over $M$, such that if $f:M \to N$ is differentiable, then it induces a map $f_*:TM \to TN$, which accurately represents the `infinitismal action' of the function, like the derivative did on $\mathbf{R}^n$. There are many candidates, and all are useful at some point or another. To recapitulate, we let each $M_p$ consist of equivalence classes of tuples $(x,v)$, where $(x,U)$ is a chart containing $p$, $v \in \mathbf{R}^n$, and $(x,v)$ and $(y,w)$ are identified if $(x \circ y^{-1})_*(v_{x(p)}) = w_{y(p)}$. Then $M_p$ is seen to be $n$ dimensional, since every element in $M_p$ can be represented as some $(x,v)$, for a fixed chart $x$ around $p$. We shall denote the equivalence class of some $(x,v)$ in $M_p$ by $[x,v]_p$. Given a map $f: M \to N$ differentiable at $p$, we define $f_*: TM \to TN$ by
%
\[ f_*([x,v]_p) = [y, D(y \circ f \circ x^{-1})(x(p))(v)]_{f(p)} \]
%
$f_*$ is well defined by the equivalence relation imposed on the tangent bundle. It is obviously a linear map. We obtain a tangent bundle structure if we let each $x_*$ be a local trivialization of the space. Our identity $(f \circ g)_* = f_* \circ g_*$ still holds, and $(\text{id}_M)_* = \text{id}_{TM}$. In fact, our new definition of the tangent bundle agrees with the previously defined definition on $T\mathbf{R}^n$, which effectively corresponds to choosing representatives $(\text{id}_{\mathbf{R}^n}, v)_p$ on each equivalence class. $f_*$ is known as the {\bf covariant derivative} of the map $f$.

The tangent bundle of a smooth manifold is a smooth manifold in its own right, since the charts $x_*$ in an atlas are $C^\infty$ related to one another. This makes the bundle map $\pi: TM \to M$ differentiable, so that $TM$ is actually a smooth vector bundle. Introducing notation, we write
%
\[ x_*(v) = (\dot{x}^1(v), \dots, \dot{x}^n(v))_{(x \circ \pi)(v)} \]
%
so that the `coordinates' related to $x_*$ are $(x^1 \circ \pi, \dots, x^n \circ \pi, \dot{x^1}, \dots, \dot{x^n})$, though often we abuse notation and just write $x^k \circ \pi$ as $x^k$.

\section{The Space of Derivations}

The algebraists found another characterization of the tangent bundle, which, though more elegant, is much more abstract then the definition by coordinates. Note that on $\mathbf{R}^n$, the space of vectors $v \in T\mathbf{R}^n_p$ can be identified with the space of directional derivatives, functionals $D_v$ on $C^\infty(M)$ defined by
%
\[ D_v(f)(p) = \lim_{h \to 0} \frac{f(p + hv) - f(p)}{h} \]
%
This map satisfies the product rule
%
\[ D_v(fg)(p) = f(p) D_v(g)(p) + g(p) D_v(f)(p) \]
%
The idea of the algebraic tangent bundle is to identify tangent vectors with certain functionals on $C^\infty(M)$. This is compatible with the interpretation of tangent vectors as velocities over the manifold. If we are moving across a surface, then the velocity we are travelling at determines how the surface below us changes, and the measurement of this change can be identified in the velocity we are travelling at.

Since $C^\infty(M)$ is a vector space, we may consider the dual space $C^\infty(M)^*$, which is a monstrous vector space consisting of all linear functionals from $C^\infty(M)$ to $\mathbf{R}$. A derivation at a point $p \in M$ is a linear functional $l \in C^\infty(M)^*$ satisfying $l(fg) = f(p) l(g) + g(p) l(f)$. A $C^\infty$ map $f: M \to N$ induces a linear map $f^*: C^\infty(N) \to C^\infty(M)$, defined by
%
\[ f^*(g) = g \circ f \]
%
which further induces a map $f_*: C^\infty(M)^* \to C^\infty(N)^*$, defined by
%
\[ [f_*l](g) = l(f^*(g)) \]
%
If $l$ is a derivation at $p$, then
%
\begin{align*}
    f_*(gh) = l(f^*(gh)) &= l((gh) \circ f) = l((g \circ f)(h \circ f))\\
    &= g(f(p)) l(h \circ f) + h(f(p)) l(g \circ f)\\
    &= g(f(p)) f_*(h) + h(f(p)) f_*(g)
\end{align*}
%
so $f_*(l)$ is a derivation at $f(p)$. We can directly calculate that $(f_* \circ g_*) = (f \circ g)_*$, so that if $f$ is a diffeomorphism, $f_*$ is an isomorphism. We shall soon find that we can identify the space of derivations at a point $p$ as the space of tangent vectors at $p$. The identification will match the $f_*$ defined here with the $f_*$ defined on the tangent space, providing an elegant algebraic definition of the covariant derivative.

The differential operators
%
\[ \left. \frac{\partial}{\partial x^1} \right|_p, \dots, \left. \frac{\partial}{\partial x^n}\right|_p \]
%
are all derivations. We will show, in fact, that these operators span the space of all derivations, so that the space is $n$ dimensional, and can be identified with the tangent bundle under the map
%
\[ \left. \sum v^i \frac{\partial}{\partial x^i} \right|_p \mapsto [x,v]_p \]
%
and so the set of all derivations {\it is} the tangent bundle, up to a change in notation.

\begin{lemma}
    If $f \in C^\infty(M)$ is constant, $l(f) = 0$ for any derivation $l$.
\end{lemma}
\begin{proof}
    By scaling, assume without loss of generality that $f = 1$ for all $p$. Then
    %
    \[ l(f) = l(f^2) = l(f) + l(f) = 2 l(f) \]
    %
    We then just subtract $l(f)$ from both sides of the equation.
\end{proof}

\begin{lemma}
    If $l$ is a derivation, and $f(p) = g(p) = 0$, then $l(fg) = 0$.
\end{lemma}
\begin{proof}
    \[ l(fg) = f(p) l(g) + g(p) l(f) = 0 + 0 = 0 \]
    %
    We verified the proof by direct calculation.
\end{proof}

If $V$ is the subspace of $C^\infty(M)$ consisting of all functions $f$ with $f(p) = 0$, and $W$ is the subspace of $V$ generated by all products of functions in $V$, then the lemma above shows that any derivation on $C^\infty(M)$ vanishes on $W$. Conversely, if there is a functional $l: V \to \mathbf{R}$ which vanishes on $W$, then it can be extended to a unique derivation on $C^\infty(M)$, by defining, for arbitrary $f \in C^\infty(M)$,
%
\[ l'(f) = l(f - f(p)) \]
%
an equation which must hold for any derivation which extends $l$. It then follows that $l'$ is a linear operator, and $l'$ annihilates constant functions, from which it then follows that
%
\begin{align*}
    l'(fg) &= l(fg - f(p)g(p))\\
    &= l([f - f(p)][g - g(p)]) + f(p) l'(g) + g(p) l'(f) - 2 f(p) g(p) l(1)\\
    &= f(p) l'(g) + g(p) l'(f)
\end{align*}
%
so derivations on $C^\infty(M)$ can be identified with $(V/W)^*$.

\begin{theorem}
    The space of derivations at the origin on $\mathbf{R}^n$ is $n$ dimensional, with basis
    %
    \[ \left.\frac{\partial}{\partial x^1}\right|_0, \dots, \left.\frac{\partial}{\partial x^n}\right|_0 \]
\end{theorem}
\begin{proof}
    Let $f \in C^\infty(\mathbf{R}^n)$. Using Taylor's theorem, we may write
    %
    \[ f(x) = f(0) + \sum_{k = 1}^n x^i \left.\frac{\partial f}{\partial x^i}\right|_0 + \sum_{i = 1,j = 1}^n \frac{x^i x^j}{2} g_{ij}(x) \]
    %
    where
    %
    \[ g_{ij}(x) = \int_0^1 (1 - t) \left. \frac{\partial^2 f}{\partial x^i x^j} \right|_{tx} \]
    %
    and so $g_{ij} \in C^\infty(\mathbf{R}^n)$. If $l$ is an arbitrary derivation at zero,
    %
    \begin{align*}
        l(f) &= l(f(0)) + \sum_{k = 1}^n \left.\frac{\partial f}{\partial x^i}\right|_0 l(x^i) + \sum_{i = 1, j =1}^n l(x^i x^j g_{ij})\\
        &= \sum_{k = 1}^n \left.\frac{\partial f}{\partial x^i}\right|_0 l(x^i)
    \end{align*}
    %
    Since the higher order terms of the series are the product of the functions $x^i$ and $x^j g_{ij}$, which both vanish at the origin. But this implies that $l$ is determined by its values on $l(x^i)$, and so
    %
    \[ l = \sum_{k = 1}^n l(x^i) \left.\frac{\partial}{\partial x^i}\right|_0 \]
    %
    so partial derivatives form a spanning set in the space of derivatives. The independence of the partial derivatives is left to the reader.
\end{proof}

We can extend this theorem to arbitrary smooth manifolds.

\begin{lemma}
    If $l$ is a derivation at $p$, and $f$ and $g$ are equal in a neighbourhood of $p$, then $l(f) = l(g)$.
\end{lemma}
\begin{proof}
    We shall prove that if $f = 0$ in a neighbourhood $U$ of $p$, then $l(f) = 0$. Consider a bump function $\psi \in C^\infty(M)$ such that $\psi = 1$ at $p$, and $\psi = 0$ outside of $U$. Then $\psi f = 0$, and
    %
    \[ 0 = l(0) = l(\psi f) = \psi(p) l(f) + f(p) l(\psi) = l(f) \]
    %
    hence $l(f) = 0$.
\end{proof}

If $f$ is only defined in a neighbourhood $U$ of $p$, we may still compute a well-defined value $l(f)$. Consider a function $\psi = 1$ in $V \subset U$, and equal to zero outside of $U$. Then $\psi f \in C^\infty(M)$, and $l(\psi f)$ is invariant of the bump function chosen, by the last lemma. This implies that we can identify the space of derivations at $p \in U$ in $C^\infty(U)$ with $C^\infty(M)$, and we find that derivations really act on the {\bf germ} of functions defined in a neighbourhood of $p$. When we are working with analytic or complex manifolds, we no longer have bump functions to work with, so we must begin by working on the space of derivations defined on germs of analytic or holomorphic functions from the very beginning.

\begin{theorem}
    The space of derivations at $p$ is $n$-dimensional, and if $(x,U)$ is a chart centered at $p$, then a basis for the space are teh partial derivatives
    %
    \[ \left.\frac{\partial}{\partial x^1}\right|_p, \dots, \left.\frac{\partial}{\partial x^n}\right|_p \]
\end{theorem}
\begin{proof}
    By restriction, we can identify derivations at $p$ on $M$ with derivations at $p$ on $U$, which can be identified by diffeomorphism with the derivations at the origin on $x(U) \subset \mathbf{R}^n$, and therefore with the space of derivations at the origin in $\mathbf{R}^n$. This identification maps the partial derivatives at $p$ with respect to the coordinates $x$ onto the partial derivatives at $x(p)$, and this is all that is needed to verify the proof.
\end{proof}

If we collect all derivations at all points on a manifold together, we obtain a structure corresponding exactly to $TM$. The correspondence is
%
\[ [x,v]_p \mapsto \sum_{k = 1}^n v_i \left.\frac{\partial}{\partial x^i}\right|_p \]
%
which induces a topology (and smooth structure) on derivations making the correspondence a homeomorphism (diffeomorphism). We will rarely distinguish between the two sets. We will often speak of a tangent vector operating on functions, or of a derivation as an element of the tangent space. Ultimately, they are the same mathematical objects, viewed from two different lenses.

\section{Curves as Vectors}

There is a third important view of the tangent bundle on a manifold, which is perhaps the most geometrically visual. To construct $M_p$, we consider curves $c: (a,b) \to M$ which pass through $p$ at some time $t$. Given our previous construction of the tangent bundle, we can consider the tangent to the curve $c_*(1_t)$ in $M_p$, which represents the speed of the curve passing through the point; any tangent vector can be put in this form for some curve $c$. Classically, $c_*(1_t)$ is often denoted by the Leibnitzian
%
\[ \frac{dc}{dt} \]
%
where the variable $t$ is obscured. We could have defined $M_p$ by these curves, provided we identify $c$ and $c'$ if $dc/dt = dc'/dt$. Of course, without the original tangent bundle to work with, stating this isn't so elegant. We have to fix some coordinate system $(x,U)$ at $p$, and identify two curves $c$ and $c'$ with $c(t) = p$, $c(t') = p$ if $(x \circ c)'(t) = (x \circ c')(t')$. Our new tangent bundle is obviously equivalent to our original tangent bundle.

This is the closest we can get to visualizing the tangent vectors on an abstract manifold not lying in Euclidean space. If a manifold does not lie in $\mathbf{R}^n$, it isn't really fair to see tangent vectors as vectors lying `off' the manifold, because there the tangent vectors don't point to anything. For instance, in Einsteinian physics, we consider a 4-dimensional manifold, and seeing the tangent bundle as vectors lying `outside of the universe' seems particularly strange.

\section{Vector Fields}

A {\bf section} on a vector bundle $(E,B,\pi)$ is a continuous map $f:B \to E$ for which $\pi \circ f = \text{id}_B$. It is a continuous choice of a vector assigned to each point in space. The main purpose of introducing sections is that we want to consider {\bf vector fields}, which are sections from a manifold to its tangent bundle, especially the differentiable ones, for which the the section is differentiable (when $TM$ is given it's standard differentiable structure). We denote a vector field by capital letters like $X$, $Y$, and $Z$, and denote the value of a vector field $X$ at a point $p$ by $X_p$. Vector fields are so important that we often denote a single vector in the tangent bundle as $X$, as well, which could get confusing if we forget to distinguish the two, but often vector fields act just like vectors in the same was that real valued functions `act' like numbers, so this doesn't cause a problem. Vector fields form a vector space. Locally, around a chart $(x,U)$, we may express the vector field in terms of the basis
%
\[ X_p = \sum a^i(p) \left.\frac{\partial}{\partial x^i}\right|_p \]
%
This vector field is differentiable or continuous if and only if the functions $a^i$ are differentiable or continuous. The space of all differentiable vector fields is itself a vector space, an algebra over $C^\infty(M)$. If $X$ is a vector space, and $f \in C^\infty(M)$, then we define a new function $X(f) \in C^\infty(M)$, defined by
%
\[ X(f)(p) = X_p(f) \]
%
Since $X_p$ is a derivation at $p$, which acts on functions in $C^\infty(M)$. What's more, we have the elegant equation
%
\[ X(fg) = f X(g) + g X(f) \]
%
which expresses the pointwise derivation property globally. In general, a {\bf derivation} is a map $F: A \to A$ on an algebra such that
%
\[ F(ab) = a F(b) + b F(a) \]
%
If $F: C^\infty(M) \to C^\infty(M)$ is any derivation, then we may then define a vector field $X$ by the equation
%
\[ X_p(f) = F(f)(p) \]
%
and it is easily verified that $X_p \in M_p$. This vector field is the unique field which generates a derivation on $C^\infty(M)$, for if locally,
%
\[ X = \sum a^i \frac{\partial}{\partial x^i} \]
%
Then $a^i(p) = X_p(x^i) = F(x^i)(p)$. The derivation corresponding to $X$ is $F$, so $C^\infty$ vector fields and derivations on $C^\infty(M)$ are in one to one correspondence.

Sections of a tangent bundle provide an interesting technique for forming trivializations of the bundle. Define a {\bf frame} on vector bundle $\pi: E \to B$ to be a collection $s_1, \dots, s_n$ of sections such that $s_1(p), \dots, s_n(p)$ spans $B_p$ at each point $p \in B$. It is effectively a continuous way to choose a basis at each point. Local frames are defined only over subsets of the space $B$, and are a continuous way to choose a basis in a neighbourhood of some point. If $t: \pi^{-1}(U) \to U \times \mathbf{R}^n$ is a local trivialization, then we obtain a local frame defined by $s_i(p) = t^{-1}(p,e_i)$. Normally we will only be able to choose local frames, because the existence of frames is strongly connected to triviality.

\begin{theorem}
    A bundle $\pi: E \to B$ is trivial if and only if there exists a global frame $(s_1, \dots, s_n)$ on the space.
\end{theorem}
\begin{proof}
    Consider the map $f: B \times \mathbf{R}^n \to E$ defined by
    %
    \[ f \left(p,\sum a_i e_i \right) = \sum a_i s_i(p) \]
    %
    If we fix some local trivialization $t: \pi^{-1}(U) \to U \times \mathbf{R}^n$, then the maps $(t \circ s_i)$ form a series of continuous maps from $U$ to $\mathbf{R}^n$, which we can put together as columns to form a continuous map $F: U \to GL_n(\mathbf{R})$ such that $F(p)(e_1) = s_1(p)$. We then have
    %
    \[ (t \circ f)\left(p,v \right) = \left( p, F(p)(v) \right) \]
    %
    which is continuous where defined, and actually a homeomorphism, because
    %
    \[ (t \circ f)^{-1}(p,v) = (p, F(p)^{-1}(v)) \]
    %
    and inversion is continuous on $GL_n(\mathbf{R})$. But by locality, this shows that $f$ is in fact a homeomorphism, and therefore an equivalence.
\end{proof}

Sections also provide eaasy ways to form of {\bf subbundles} $E' \subset E$ of bundles $\pi: E \to B$, which are subspaces such that $\pi^{-1}(p) \cap E'$ is a subspace of $E$, and $E'$ also has local trivializations in a neighbourhood of each point.

\begin{theorem}
    A subset $E' \subset E$ is an $m$ dimensional subbundle of $E$ if and only if there are $m$ linearly independant sections $s_1, \dots, s_m$ into $E'$ in a neighbourhood of each point in the base space $B$.
\end{theorem}
\begin{proof}
    A modification to the last theorem gives us equivalence maps into $U \times \mathbf{R}^m$ for every linearly independent section $s_1, \dots, s_m : U \to E'$. This is sufficient to prove the theorem, for the fact that linearly independant sections exist locally on every manifold is trivial.
\end{proof}

\begin{example}
    Let $f: E \to E'$ be a bundle map between two tangent bundles $\pi: E \to B$ and $\pi': E' \to B'$, such that the dimensional of the kernel of each linear map $f_p: B_p \to B'_{f(p)}$ is locally constant as $p$ varies. Then the collection of kernels of each $f_p$ forms a subbundle $\text{Ker} f$ of $E$. Indeed, if we have a local trivializations $t: \pi^{-1}(U) \to U \times \mathbf{R}^n$ and $t': \pi^{-1}(V) \to V \times \mathbf{R}^m$, then locally $f$ is a way of continuously assigning a matrix $F(p) \in M_{n,m}(\mathbf{R})$ to each point $p$, such that
    %
    \[ (t' \circ f \circ t^{-1})(p,v) = (f(p), F(p) v) \]
    %
    Suppose that $F(p)$ has rank $k$ on $U$. Our method will consist of finding a continuous choice of matrices $N(p) \in GL_n(\mathbf{R})$ such that the last $n-k$ columns of $F(p) N(p)$ are zero. Then the kernel of $f$ is locally spanned by the vector fields $s_i(p) \mapsto t^{-1}(p, N(p)e_{k+i})$, for $1 \leq i \leq n-k$, which give the required trivialization to verify that the kernel is a subbundle.

    To construct $N(p)$, we apply column reduction operations continuously, reducing $F(p)$ to column echelon form continuously in a neighbourhood of each point. Since column reduction operations correspond to multiplication on the right by invertible matrices, this process will suffice to finding a continuous $N(p)$ such that $F(p) N(p)$ is as required above, since a matrix in reduced column echelon form surely has $n-k$ vanishing columns to the right. In constructing $N(p)$, we just have to be a bit careful how we perform the column operations. If $F_{ij}(p) \neq 0$, then $F_{ij} \neq 0$ in a neighbourhood of $p$, which justifies scaling the column by $F_{ij}^{-1}$ so that the coeffient is equal to one in a neighbourhood of $p$. Then we must subtract column $F_j$ from every column $F_k$ by a factor of $F_{ik}$, so that $F_{ik} = 0$ in a neighbourhood of $p$. Then the matrix $F(p) N(p)$ after column reduction has $n-k$ blank columns, and these columns remain blank in a neighbourhood of $p$, for the rows corresponding to the unital positions in the non-zero columns must be zero in a neighbourhood, and if other the entries in other rows became nonzero, the rank of the function would increase, contradicting the fact that $F$ is rank $k$ in a neighbourhood of $p$. This completes the proof, for the $n-k$ sections spanning the kernel of each $f_p$ suffice to construct $n-k$ dimensional trivialization maps in a neighbourhood of each point.
\end{example}

\begin{example}
    Similarily, if $f: E \to E'$ is a bundle map whose rank is locally constant, then the image of $E$ in $E'$ is a subbundle. To obtain local trivializations, we first trivialize $E$ locally by some $t$, trivialize $E'$ locally by some $t'$, and then form the matrices $N(p)$, such that the last $n-k$ columns of $F(p)N(p)$ are blank, from which it follows that the first $k$ columns are linearly independent, and then $s_i(p) = (f \circ t^{-1})(e_i)$ are linearly independent for $i = 1, \dots, k$, and span the image of $f$.
\end{example}

\begin{example}
    If $(f,f^\sharp): (E,B) \to (E',B')$ is an bundle map such that $f$ is injective, and the rank is locally constant, then we can also make the cokernel of $f$, formed by the union of the cokernels of each $f_p$ and given the quotient topology, into a vector bundle, because we can extend the sections in the last example to local frames, and the added sections, when projected into the cokernel, give trivializations of the bundle.
\end{example}

\section{Universality of the Tangent Bundle}

For a map $f:M \to N$, the map $f_*: TM \to TN$ is meant to be a sufficient generalization of the derivative operator on Euclidean space. The fact that this is a `universal' generalization can in fact be proved from a categorical viewpoint. Note that the association of $M$ with $TM$ and $f$ with $f_*$ is a {\it functor} from the category of $C^\infty$ manifolds to the category of vector bundles, such that
%
\begin{itemize}
    \item The bundle $T\mathbf{R}^n$ is isomorphic to $\varepsilon^n(\mathbf{R}^n)$ by a trivialization $t_n$ such that for any map $f: \mathbf{R}^n \to \mathbf{R}^m$, the diagram
    %
    \begin{center}
    \begin{tikzcd}
        T\mathbf{R}^n \arrow{d}{f_*} \arrow{r}{t_n} & \varepsilon^n(\mathbf{R}^n) \arrow{d}{\text{old\ $f_*$}}\\
        T\mathbf{R}^m \arrow{r}{t_m} & \varepsilon^m(\mathbf{R}^m)
    \end{tikzcd}
    \end{center}
    %
    commutes, where $\text{old $f_*$}$ is defined by $(p,v) \mapsto (f(p), Df(p)(v))$.
    \item If $U \subset M$ is an open submanifold, there are equivalences
    %
    \[ u_{U,M}: TU \to (TM|_U) \]
    %
    such that if $i: U \to M$ is the inclusion map, then
    %
    \begin{center}
    \begin{tikzcd}
        & (TU|_M) \arrow{rd} &\\
        TU \arrow{ru}{u_{U,M}} \arrow{rr}{i_*} & & TM
    \end{tikzcd}
    \end{center}
    %
    commutes, and for any differentiable $f:M \to N$,
    %
    \begin{center}
    \begin{tikzcd}
        & TU \arrow{ld}{i_*} \arrow{rd}{(f|_U)_*} &\\
        TM \arrow{rr}{f_*} & & TN
    \end{tikzcd}
    \end{center}
    %
    commutes.
\end{itemize}
%
The first bullet says that the functor, restricted to Euclidean spaces, is naturally equivalent to the functor which associates $\mathbf{R}^n$ with $\varepsilon^n(\mathbf{R}^n)$ and $f: \mathbf{R}^n \to \mathbf{R}^m$ with the old definition of the differential. The second says that the family of maps $u_{U,M}$ is a natural transformation between the tangent functor and the restriction of the functor to open submanifolds.

We shall verify that any functor satisfying these properties is unique up to a natural equivalence. We shall first explore the properties of functors satisfying the properties above. So until necessary, we will let $M \to TM$ stand for such a functor. These properties will be obvious for the standard tangent functor, but as long as we don't use any facts about our constructed tangent bundle, the theorem will remain true for any of the other functors.

Given a chart $(x,U)$ on a manifold $M$, we have a chain of isomorphisms
%
\[ (TM)|_U \xleftarrow{u_{U,M}} TU \xrightarrow{x_*} Tx(U) \xrightarrow{u_{x(U), \mathbf{R}^n}} (T\mathbf{R}^n)|_{x(U)} \xrightarrow{t_n|_{x(U)}} \varepsilon^n(x(U)) \]
%
Define $\alpha_x: (TM)|_U \to \varepsilon^n(x(U))$ to be the chain of compositions. We shall try and understand the properties of $\alpha_x$ independent of the properties of our tangent bundle, because if we have some other functor $M \mapsto T'M$, with other equivalences $t_n'$ and $u_{U,M}'$, then we have $\beta_x: (T'M)|_U \to \varepsilon^n(x(U))$ defined exactly as $\alpha_x$ is defined, and we can consider $\beta_x^{-1} \circ \alpha_x: (TM)|_U \to (T'M)|_U$. Provided that this map is independent of $x$, we can put the maps together for all $x$, and obtain an equivalence between $TM$ and $T'M$.

\begin{lemma}
    If $V \subset U$ is open, and $y = x|_V$, then $\alpha_y = (\alpha_x)|_V$.
\end{lemma}
\begin{proof}
    Denote the inclusion maps by $i: U \to M$, $\iota: V \to M$, $j : V \to U$, and $k : y(V) \to x(U)$. Then consider the diagram
    %
    \begin{center}
    \begin{tikzcd}
        (TM)|_U \arrow[bend left=20]{rrrr}{\alpha_x} & TU \arrow{l}[above]{u_{U,M}} \arrow{r}{x_*} & T(x(U)) \arrow{r}{u_{x(U), \mathbf{R}^n}} & (T\mathbf{R}^n)|_{x(U)} \arrow{r}{(t_n)|_{x(U)}} & \varepsilon^n(\mathbf{R}^n)|_{x(U)}\\
        (TM)|_V \arrow[bend right=20]{rrrr}{\alpha_y} \arrow{u} & TV \arrow{u}{j_*} \arrow{l}{u_{V,M}} \arrow{r}{y_*} & T(y(V)) \arrow{u}{k_*} \arrow{r}{u_{y(V), \mathbf{R}^n}} & (T\mathbf{R}^n)|_{y(V))} \arrow{r}{(t_n)|_{y(V)}} \arrow{u}{} & \varepsilon^n(\mathbf{R}^n)|_{y(V)} \arrow{u}{}\\
    \end{tikzcd}
    \end{center}
    %
    We verify commutativity by verifying the commutativity of each square. The first square follows by breaking the diagram into triangles.
    %
    \begin{center}
    \begin{tikzcd}
        TM|_U \arrow{rd} & & TM|_V \arrow{ll} \arrow{ld}\\
        & TM &\\
        TU \arrow{ru}{i_*} \arrow{uu}{u_{U,M}} & & TV \arrow{lu}[above]{\iota_*} \arrow{ll}{j_*} \arrow{uu}[right]{u_{V,M}}
    \end{tikzcd}
    \end{center}
    %
    These subtriangles commutes by the universal properties of the functor, and these gives us the commutativity of the square. The second square follows by functoriality, because $x \circ j = k \circ y$. The third square commutes for the same reason the first square commutes, and the last square obviously commutes.
\end{proof}

\begin{lemma}
    If $A \subset \mathbf{R}^n$ and $B \subset \mathbf{R}^m$ are open, and $f: A \to B$ is $C^\infty$, then the diagram
    %
    \begin{center}
    \begin{tikzcd}
    TA \arrow{r}{u_{A,\mathbf{R}^n}} \arrow{d}{f_*} & (T\mathbf{R}^n)|_A \arrow{r}{t_n|_A} & \varepsilon^n(\mathbf{R}^n)|_A \arrow{d}{\text{old $f_*$}} \\
    TB \arrow{r}{u_{B,\mathbf{R}^m}} & (T\mathbf{R}^m)|_B \arrow{r}{t_m|_B} & \varepsilon^m(\mathbf{R}^m)|_B
    \end{tikzcd}
    \end{center}
    %
    commutes.
\end{lemma}
\begin{proof}
    First, assume that $f$ can be extended to a map $g: \mathbf{R}^n \to \mathbf{R}^m$. Denoting inclusions by $i: A \to \mathbf{R}^n$ and $j : B \to \mathbf{R}^m$. Then we have a huge diagram, whose subtriangles and subsquares all obviously commute.
    %
    \begin{center}
    \begin{tikzcd}
       & (T\mathbf{R}^n)|_A \arrow{r}{t_n|_A} \arrow{d} & \varepsilon^n(\mathbf{R}^n)|_A \arrow{d} \arrow[bend left=80]{dd}{\text{old}\ f_*}\\
    TA \arrow{r}[below]{i_*} \arrow{ru}{u_{A,\mathbf{R}^n}} \arrow{d}{f_*} & T\mathbf{R}^n \arrow{d}[left]{g_*} \arrow{r}{t_n} & \varepsilon^n(\mathbf{R}^n) \arrow{d}{\text{old $g_*$}}\\
    TB \arrow{r}{j_*} \arrow{dr}[below left]{u_{B,\mathbf{R}^m}} & T\mathbf{R}^m \arrow{r}{t_m}     & \varepsilon^m(\mathbf{R}^m)\\
       & (T\mathbf{R}^m)|_B \arrow{r}{t_m|_B} \arrow{u} & \varepsilon^m(\mathbf{R}^m)|_B \arrow{u}
    \end{tikzcd}
    \end{center}
    %
    and this diagram contains the other diagram as a subdiagram, since we know that $\text{old}\ g_*$ and $\text{old}\ f_*$ agree on $A$ (since $A$ is an open subset), so that the theorem is proved in this case.

    In general, we might not be able to extend the entire map $f$ to all of $\mathbf{R}^n$, but we can at least find a function $g$ for each $p \in A$ such that $g$ agrees with $f$ on a neighbourhood $A' \subset A$ of $p$. Then we have a commutative diagram
    %
    \begin{center}
    \begin{tikzcd}
        TA \arrow{rr}{u_{A,\mathbf{R}^n}} \arrow[bend right=50]{ddd}[left]{f_*} & & (T\mathbf{R}^n)|_A \arrow{r}{t_n|_A} & \varepsilon^n(\mathbf{R}^n)|_A \arrow[bend left=50]{ddd}[right]{\text{old $g_*$}}\\
        & (TA)|_{A'} \arrow{lu}\\
        TA' \arrow{ru}{u_{A',A}} \arrow{uu}{i_*} \arrow{d}{(f|_{A'})_*} \arrow{rr}{u_{A', \mathbf{R}^n}} & & (T\mathbf{R}^n)|_{A'} \arrow{uu} \arrow{r}{t_n|_{A'}} & \varepsilon^n(\mathbf{R}^n)|_{A'} \arrow{uu} \arrow{d}[left]{\text{old}\ (f|_{A'})_*}\\
        TB \arrow{rr}{u_{B,\mathbf{R}^m}} & & (T\mathbf{R}^m)|_{B} \arrow{r}{t_m|_B} & \varepsilon^m(\mathbf{R}^m)|_B
    \end{tikzcd}
    \end{center}
    %
    where $i: A' \to A$ is the inclusion map. Every subshape but the top left and bottom rectangle obviously commutes. First, note that the bottom rectangle is just an instance of this theorem, but where we can extend our map to all of $\mathbf{R}^n$, so we have already argued it's commutativity. To see that the top left square commutes, we extend it to a larger diagram. Defining $j: A \to \mathbf{R}^n$ to be the inclusion, we have a diagram
    %
    \begin{center}
    \begin{tikzcd}
        & T\mathbf{R}^n\\
        TA \arrow{ru}{j_*} \arrow{rr}{u_{A,\mathbf{R}^n}} &  & (T\mathbf{R}^n)|_A \arrow{lu} \\
        TA' \arrow{u}{i_*} \arrow{rr}{u_{A', \mathbf{R}^n}} & & (T\mathbf{R}^n)|_{A'} \arrow{u}
    \end{tikzcd}
    \end{center}
    %
    To prove that $u_{A,\mathbf{R}^n} \circ i_* = u_{A',\mathbf{R}^n}$, it suffices to prove that $j_* \circ i_* = u_{A', \mathbf{R}^n}$, because $j_*$ is equal to $u_{A,\mathbf{R}^n}$ when viewed as functions without a specified codomain. But $j \circ i$ is just the inclusion of $A'$ in $\mathbf{R}^n$, so this fact is obvious. We have verified enough commutativity to conclude that the composition
    %
    \[ TA \xrightarrow{f_*} TB \xrightarrow{u_{B,\mathbf{R}^m}} (T\mathbf{R}^m)|_B \xrightarrow{t_m|_B} \varepsilon^m(\mathbf{R}^m)|_B \]
    %
    is equal to the composition
    %
    \[ TA \xrightarrow{u_{A,\mathbf{R}^n}} (T\mathbf{R}^n)|_A \xrightarrow{t_n|_A} \varepsilon^n(\mathbf{R}^n)|_A \xrightarrow{\text{old\ $g_*$}} \varepsilon^m(\mathbf{R}^m)|_B \]
    %
    on $TA|_{A'}$, and since $\text{old}\ g_*$ is equal to $\text{old}\ f_*$ on $(TA)|_{A'}$, we have verified the theorem over $A'$. Since $A'$ was arbitrary, we obtain commutativity over all of $A$.
\end{proof}

\begin{lemma}
    If $(x,U), (y,V)$ are two coordinates charts with $p \in U \cap V$, then $\beta_y^{-1} \circ \alpha_y$ and $\beta_x^{-1} \circ \alpha_x$ agree at $p$.
\end{lemma}
\begin{proof}
    We may assume $U = V$, because our first lemma shows the theorem is true in general otherwise. Assuming this is true, we consider the diagram
    %
    \begin{center}
    \begin{tikzcd}
        & & T(x(U)) \arrow{r}{u_{x(U),\mathbf{R}^n}} \arrow{dd}{(y \circ x^{-1})_*} & (T\mathbf{R}^n)_{x(U)} \arrow{r}{t_n|_{x(U)}} & \varepsilon^n(\mathbf{R}^n)|_{x(U)} \arrow{dd}{\text{old}\ (y \circ x^{-1})_*}\\
        (TM)|_U & TU \arrow{l}{u_{U,M}} \arrow{ru}{x_*} \arrow{rd}{y_*}\\
        & & T(y(U)) \arrow{r}{u_{y(U),\mathbf{R}^n}} & (T\mathbf{R}^n)|_{y(U)} \arrow{r}{t_n|_{y(U)}} & \varepsilon^n(\mathbf{R}^n)|_{y(U)}
    \end{tikzcd}
    \end{center}
    %
    The triangle obviously commutes, and the rectangle commutes by the second lemma. This implies that $\alpha_y = \text{old}\ (y \circ x^{-1})_* \circ \alpha_x$, and $\beta_y = \text{old}(y \circ x^{-1})_* \circ \beta_x$. The desired result follows immediately.
\end{proof}

Putting together all $\beta_x^{-1} \circ \alpha_x: (TM)|_U \to (T'M)|_U$, we now have a well defined equivalence $e_M$ from $TM$ to $T'M$. Now we need only prove that this is in fact a natural equivalence -- that is, for any $f: M \to N$, $e_N \circ f_* = f_\sharp \circ e_M$. Let's prove this first for $f: \mathbf{R}^n \to \mathbf{R}^m$. First, note that $e_{\mathbf{R}^n} = (t_n')^{-1} \circ t_n$, because most of the maps involved in the construction of the equivalence are trivial. The properties of the tangent map imply that the squares and triangles of the diagram
%
\begin{center}
\begin{tikzcd}
    T\mathbf{R}^n \arrow{r}{t_n} \arrow[bend left=30]{rr}{e_{\mathbf{R}^n}} \arrow{d}{f_*} & \varepsilon^n(\mathbf{R}^n) \arrow{d}{\text{old}\ f_*} & T'\mathbf{R}^n \arrow{l}[above]{t'_n} \arrow{d}{f_\sharp}\\
    T'\mathbf{R}^m \arrow[bend right=30]{rr}{e_{\mathbf{R}^m}} \arrow{r}{t'_m} & \varepsilon^m(\mathbf{R}^m) & T\mathbf{R}^m \arrow{l}[above]{t'_m}
\end{tikzcd}
\end{center}
%
commute, and this shows the naturality equation holds. Second, note that $e_M \circ i_* = i_\sharp \circ e_U$, where $i: U \to M$ is the inclusion of an open submanifold of $M$, where $(x,U)$ is a chart, because $e_M$ is essentially formed by putting together all $i_\sharp \circ e_M \circ i_*^{-1}$. Similarily, we find that if $(x,U)$ is a chart, then $e_{x(U)} \circ x_* = x_\sharp \circ e_U$. Putting all this together, we prove that, given $f:M \to N$, $e_N \circ f_* = f_\sharp \circ e_M$ holds at a neighbourhood of each point. Fixing some point $p$, we choose a chart $(x,U)$ containing the point, and a chart $(y,V)$ containing $f(p)$, such that $f(U) \subset V$. We may assume $x(U) = \mathbf{R}^n$ and $y(V) = \mathbf{R}^m$. If $g = y \circ f \circ x^{-1}$, then
%
\begin{center}
\begin{tikzcd}
    T\mathbf{R}^n \arrow[bend left=20]{rrrrr}{g_*} \arrow{d}{e_{\mathbf{R}^n}} & TU \arrow{l}{x_*} \arrow{r}{i_*} \arrow{d}{e_U} & TM \arrow{d}{e_M} \arrow{r}{f_*} & TN \arrow{d}{e_N} & TV \arrow{d}{e_V} \arrow{l}{j_*} \arrow{r}{y_*} & T\mathbf{R}^m \arrow{d}{e_{\mathbf{R}^m}}\\
    T'\mathbf{R}^n \arrow[bend right=20]{rrrrr}{g_\sharp}  & T'U \arrow{l}{x_\sharp} \arrow{r}{i_\sharp} & T'M \arrow{r}{f_\sharp} & T'N & T'V \arrow{l}{j_\sharp} \arrow{r}{y_\sharp} & T'\mathbf{R}^m
\end{tikzcd}
\end{center}
%
Then we have justified that all the squares in the diagram but the middle one commute, as do the top and bottom triangles, and the rectangle as a whole (in the sense that $e_{\mathbf{R}^m} \circ g_* = g_\sharp \circ e_{\mathbf{R}^n}$). But a final diagram chase justifies that $f_\sharp \circ e_M$ is equal to $e_N \circ f_*$ on $TM|_U$. Since $(x,U)$ was arbitrary, we have shown that the property is a natural equivalence in full. Thus

\begin{theorem}
    There is a unique functor from the category of $C^\infty$ manifolds to the category of vector bundles which satisfies the bullet point properties we denoted at the beginning of the section.
\end{theorem}

The beauty of this categorical proof is that it depends on very little of the structure of $C^\infty$ manifolds. The proof easily extends to $C^k$ manifolds, and even to $C^\omega$ manifolds. It also shows uniqueness of association to $C^\infty$ vector bundles, and effectively settles the question of how well the tangent bundle represents the linear property of differentiable maps on spaces.

\section{Orientation}

The key idea of differential geometry is that classical geometric concepts (which do not `really' hold ground rigorously) can be given a rigorous standing when reinterpreted as some structure on the tangent bundle. The first, easiest concept to introduce, is orientation. In a {\it real} vector space $V$ (in complex vector spaces no such problem arises), each tuple $(v_1, \dots, v_n)$ gives rise to a linear isomorphism $T: \mathbf{R}^n \to V$ such that $T(e_i) = v_i$. Given another basis $(w_1, \dots, w_n)$, we obtain another isomorphism $S: \mathbf{R}^n \to V$ with $S(e_i) = v_i$, and therefore a linear operator $T \circ S^{-1}: \mathbf{R}^n \to \mathbf{R}^n$. The determinant of this operator is non-zero and therefore positive or negative. We say these tuples are {\it equally oriented} if the determinant of this operator is positive. This divides the bases of $V$ into two equivalence classes, and an {\bf orientation} for $V$ is a choice of one of these classes. The equivalence class of some basis $(v_1, \dots, v_n)$ shall be denoted $[v_1, \dots, v_n]$, so that if $V$ is an oriented vector space with fixed orientation $\mu$, then $(v_1, \dots, v_n)$ is oriented if and only if $[v_1, \dots, v_n] = \mu$. Given two vector spaces $V$ and $W$ with a fixed orientation, a linear isomorphism $T: V \to W$ is orientation preserving if $T$ maps oriented bases $(v_1, \dots, v_n)$ in $V$ to an oriented bases $(Tv_1, \dots, Tv_n)$ in $W$. Either an isomorphism is orientation preserving or orientation reversing -- it maps oriented bases $(v_1, \dots, v_n)$ to unoriented bases $(Tv_1, \dots, Tv_n)$.

The key reason that orientation exists is that the determinant of an operator is always positive or always negative. We see the same phenomenon occur when considering equivalences $f: \varepsilon^n(\mathbf{R}^n) \to \varepsilon^m(\mathbf{R}^n)$, which can be written
%
\[ f(p,v) = \left(p, \sum a_{ij}(p) v_i e_j \right) \]
%
where the $a_{ij}: \mathbf{R}^n \to \mathbf{R}$ change continuously, and since the matrix $(a_{ij})$ is always invertible, it is either always positive or always negative. Thus we may define an orientation to the bundle $\varepsilon^n(\mathbf{R}^n)$ as a choice of orientation on each fibre, such that every equivalence is either orientation preserving on all fibres, or orientation reversing. Our reasoning shows that there are only two choices of orientation on $\varepsilon^n(\mathbf{R}^n)$.

Now on an arbitrary vector bundle, defining such a vector bundle may be impossible, because we have to patch local equivalences up across the whole space. An orientation for an arbitrary bundle $\pi: E \to B$ is a choice of orientation $\mu_p$ on each fibre $B_p$, such that any local trivialization $t: \pi^{-1}(U) \to U \times \mathbf{R}^n$ around a connected set $U$ is either orientation preserving on all fibres, or orientation reversing on all fibres. To verify that a particular orientation is `consistant', we need only verify it for a set of connected open sets which cover the bundle, because the trivializations are already orientation reversing or preserving when they are locally trivialized, as we already noted. A bundle is called orientable if it has an orientation, and a differentiable manifold is called orientable if its tangent bundle is orientable. An oriented manifold is a manifold with a fixed orientation on its tangent bundle, and if $f: M \to N$ is a local diffeomorphism between oriented manifolds of the same dimension, we say $f$ is orientation preserving if $f_*: TM \to TN$ is orientation preserving on each fibre.

Since $T\mathbf{R}^n$ is equivalent to $\varepsilon^n(\mathbf{R}^n)$, Euclidean space is an orientable manifold, with a canonical orientation induced by the unit vectors $\mu_p = [(e_1)_p, \dots, (e_n)_p]$. Similarily, any open submanifold of an orientable manifold is orientable. The spheres $S^n$ are all orientable -- to obtain the orientation, we note that, viewing $TS^n$ as a subset of $\varepsilon^{n+1}(\mathbf{R}^{n+1})$, we see that $p_p \not \in TS^n$ for any $p \in S^n$. We may then define an orientation on $S^n$ by letting $[v_1, \dots, v_n]$ be an oriented basis at $p$ if $[p,v_1, \dots, v_n]$ is oriented in $\mathbf{R}^n$.


For any manifold $M$, $TM$ is always orientable as a differentiable manifold. We calculate the transition map between two charts induced by the coordinate maps $(x,U)$ and $(y,V)$ on $M$ to be
%
\begin{align*}
    (y^1, \dots, &y^n, \dot{y}^1, \dots, \dot{y}^n) \circ (x^1, \dots, x^n, \dot{x}^1, \dots, \dot{x}^n)^{-1}[p, v]\\
    &= \left[ (y \circ x^{-1})(p), \sum_{i,j} v^j \left. \frac{\partial y^i}{\partial x^j} \right|_p e_i \right]
\end{align*}
%
and so the matrix of partial derivatives is
%
\[ M = \begin{pmatrix} \left( \frac{\partial y^i}{\partial x^j} \right) & 0 \\ X & \left( \frac{\partial y^i}{\partial x^j} \right) \end{pmatrix} = \begin{pmatrix} N & 0 \\ X & N \end{pmatrix} \]
%
Essentially, we have found that
%
\[ \frac{\partial \dot{y}^i}{\partial \dot{x}^j} = \frac{\partial y^i}{\partial x^j} \]
%
Now
%
\[ \begin{pmatrix} N^{-1} & 0 \\ 0 & I_n \end{pmatrix} \begin{pmatrix} N & 0 \\ X & N \end{pmatrix} \begin{pmatrix} I_n & 0 \\ 0 & N^{-1} \end{pmatrix} = \begin{pmatrix} I_n & 0 \\ X & I_n \end{pmatrix} \]
%
Taking determinants on both sides, we find
%
\[ \frac{\det(M)}{\det(N)^2} = 1 \]
%
and so $\det(M) = \det(N)^2 > 0$. This implies that the family of charts $(x,\dot{x})$ have consistant orientations, and so together they give us an orientation on $T(TM)$. Here we have used the fact that if we have a family of chart $\{ (x_\alpha, U_\alpha) \}$ covering a manifold $M$, and
%
\[ \det \left(\frac{\partial x^i_\beta}{\partial x^j_\alpha}\right) \]
%
is always positive, then the orientations induced from the $(x_\alpha)_*$ combine to give an orientation on $TM$.

\section{Tangents on Manifolds with Boundary}

Given a manifold $M$ with boundary, it is easy to define the tangent bundle on the interior of the manifold, but it is less clear what the fibres of the bundle should look like on the boundary -- should they have the same dimension as on the interior, or one dimension less?

The safest option is to look at the space of derivations at points on the boundary of the manifold, which are defined exactly the same as on any manifold without boundary. The locality properties apply, and so it suffices to determine the space $V$ of derivations on $\mathbf{H}^n$. Surely we have the partial derivatives
%
\[ \frac{\partial}{\partial x^1}, \dots, \frac{\partial}{\partial x^{n-1}} \]
%
So the space is at least $n-1$ dimensional. But perhaps suprisingly, the partial derivative operator $\frac{\partial}{\partial x^n}$ is also still well defined -- to calculate it, we take some $f \in C^\infty(M)$, and extend to some differentiable $\tilde{f}: \mathbf{R}^n \to \mathbf{R}$, and then take partial derivatives. The value we obtain is independent of extension, because we can also calculate the partial derivative as the limit
%
\[ \lim_{t \to 0^+} (f \circ x^{-1})(te_n) \]
%
Arguing the same way as in our calculation of the space of derivations in $\mathbf{R}^n$, we may take Taylor series to determine that these partial derivatives span the space of all derivations. Thus we find that, for $p \in \partial M$, $M_p$ is $n$-dimensional.

An additional feature of the tangent bundle on the boundary of a manifold is we can identify some vectors as `outward' pointing, `inward' pointing, and parallel to the manifold at the boundary. For a chart $(x,U)$ at the boundary, we can write any derivation as
%
\[ \sum a_i \frac{\partial}{\partial x^i} \]
%
If $a_n < 0$, then we say the vector is outward pointing, if $a_n > 0$ we say the vector is inward pointing, and if $a_n = 0$ then the vector is parallel to the manifold. This is well defined even when we vary coordinate systems, because if $y \circ x^{-1}: U \to V$ is a diffeomorphism, where $U,V \subset \mathbf{H}^n$, then by invariance of domain we know that
%
\[ y \circ x^{-1}(t_1, \dots, t_{n-1}, 0) = (f^1(t_1, \dots, t_{n-1}), \dots, f^{n-1}(t_1, \dots, t_{n-1}), 0) \]
%
so that $\left.\frac{\partial y^n}{\partial x^i}\right|_p = 0$ if $i \neq n$ and $p \in \partial \mathbf{H}^n$, and $\left. \frac{\partial y^n}{\partial x^n}\right|_p > 0$ because $y^n$ is always non-negative, and must be non-zero in order for $y \circ x^{-1}$ to be a diffeomorphism. Thus we find that if a vector $v$ can be written
%
\[ \sum a_i \frac{\partial}{\partial x^i} = \sum b_j \frac{\partial}{\partial y^j} \]
%
then
%
\[ b_n = \sum a_k \frac{\partial y^n}{\partial x_k} = a_n \frac{\partial y^n}{\partial x^n} \]
%
and we see that the sign of $b_n$ and $a_n$ agree.

If $M$ is a manifold with boundary, we can consider orientability exactly how we considered it for manifolds without boundary. Given a fixed orientation $\mu$ on $M$, there is actually a natural orientation on $\partial M$ which makes it orientable. We say that a basis $(v_1, \dots, v_{n-1})$ of $\partial M_p$ is orientable if $[w, v_1, \dots, v_{n-1}] = \mu$ for any outward pointing vector $w \in M_p$. This is exactly how we defined the orientation on $S^n$, as the boundary of the closed, unit ball in $\mathbf{R}^{n+1}$. If we consider the orientation on $\mathbf{R}^{n-1}$ as a subset of $\mathbf{H}^n$, we see that we obtain $(-1)^n$ times the standard orientation. The reason for this choice will become clear when we talk about integration on manifolds, in which orientation plays a key role.

\chapter{The Cotangent Bundle}

\section{The Method of Lagrangian Multipliers}

The theory we have developed is enough to obtain an incredibly practical theorem, which has usage almost everywhere in applied mathematics, and has theoretical usage as well. Let $M$ be a differentiable manifold, and let $N$ be a $C^\infty$ submanifold. Given a function $f \in C^\infty(M)$, we may wish to consider maximizing $f$ over $N$. One method of finding the maximum over $f$ is to find a cover of $N$ by coordinate charts $(x_\alpha,U_\alpha)$, and then find points where
%
\[ \frac{\partial f}{\partial x^i_\alpha} = 0 \]
%
for all $i$, which are candidates for extrema. In terms of our notation, if $h = f|_N$, then candidates for extrema are points $p$ for which $dh(p) = 0$. This may prove impractical, especially when the equations for the partial derivatives are cumbersome to solve. The method of Lagrangian multipliers applies when $N$ is specified as the level set of some specifying function $g: M \to M'$, where $N = g^{-1}(p)$.

\begin{theorem}
    If $N = g^{-1}(p)$ is a differentiable manifold, where $g$ has locally constant rank on $N$, then the tangent bundle $TN$ can be defined as the set of vectors in $TM|_N$ for which $g_*(v) = 0$.
\end{theorem}
\begin{proof}
    If $g: M \to M'$ is rank $k$ at $p$, then choose some coordinate system $(x,U)$ on $M$ and $(y,V)$ centred at $f(p)$ such that
    %
    \[ (y \circ g \circ x^{-1})(t^1, \dots, t^n) = (t^1, \dots, t^k, 0, \dots, 0) \]
    %
    Then $(x^{n-k+1}, \dots, x^n)$ is a coordinate system on $N$, and
    %
    \[ g_* \left( \sum a^i \frac{\partial}{\partial x^i} \right) = \sum_{i,j} a^i \frac{\partial y^j \circ g}{\partial x^i} \frac{\partial}{\partial y^j} = \sum_{i = 1}^k a^i \frac{\partial}{\partial y^i} \]
    %
    and so $g_* \left(\sum a^i \frac{\partial}{\partial x^i} \right) = 0$ if and only if $a^i = 0$ for $i = 1, \dots, k$, in which case $\sum a^i \frac{\partial}{\partial x^i}$ is a vector in $TN$.
\end{proof}

This was essentially the way we defined tangent vectors on $S^n$, as vectors $v_p$ such that $\langle p, v \rangle = 0$. This is the correct definition, because the manifold can be defined by the equation $\| p \|^2 = \langle p, p \rangle = 1$, and if $f(v) = \langle v, v \rangle$ defines the inner product, then
%
\[ \left. \frac{\partial f}{\partial x^i} \right|_p = 2p^i \]
%
so a vector $v_p$ is in $TS^n$ if and only if $2 \sum v^i p^i = 2 \langle v, p \rangle = 0$.

Now consider the special case where $g: M \to \mathbf{R}^n$. Then we can consider the covector fields $dg^i$, and the vectors in $TN$ are essentially also those vectors for which $dg^i(v) = 0$, since $dg^i$ is obtained from $g^i_*$ by `forgetting the base point'. That is
%
\[ g_*(v_p) = (dg^1(v), \dots, dg^n(v))_{g(p)} \]
%
The kernel of the reduction $M^*_p \to N^*_p$ is a space of dimension $k$, because $\sum a_i dx^i(p)$ annihilates all vectors in $N_p$ if and only if $a_i = 0$ for all $i > k$, and the $dg^i(p)$ span a space of dimension $k$, because
%
\[ dg^i(p) = \sum \frac{\partial g^i}{\partial x^j} dx^j(p) \]
%
and $\left( \frac{\partial g^i}{\partial x^j} \right)$ is rank $k$. This implies that the $dg^i$ are actually a spanning set of the kernel. Now if $p$ maximizes $f$ on $N$, then $df(p)(v) = 0$ for all vectors $v \in N_p$, implying $df(p)$ is in the kernel of the reduction, and implying the existence of $\lambda_i$ such that
%
\[ df(p) = \sum \lambda_i dg^i(p) \]
%
This gives us a different system of equations to solve, with an additional set of unknowns $\lambda_i$. If $g: M \to \mathbf{R}$, then we need only find $\lambda$ such that $df(p) = \lambda dg(p)$.

\begin{example}
    Consider finding extrema of the function $f(x,y) = 5x - 3y$, subject to the constraint $x^2 + y^2 = 136$. Then the constraint function is $g(x,y) = x^2 + y^2$, and
    %
    \[ df = 5dx - 3dy\ \ \ \ \ dg = 2x dx + 2y dy \]
    %
    Extrema occur at points $(x,y)$ such that
    %
    \[ 5dx - 3dy = 2 \lambda x dx + 2 \lambda y dy \]
    %
    Which implies that $2 \lambda x = 5$, $2 \lambda y = -3$, and in order for the point $(x,y)$ to occur in the constraint region, we require $25/4 \lambda^2 + 9/4 \lambda^2 = 17/2\lambda^2 = 136$, so $1/16 = \lambda^2$, and so $\lambda = \pm 1/4$. If $\lambda = 1/4$, then we have an extrema $(10, -6)$ and $(-10, 6)$. Since the constraint region is compact, we must have a maxima and minima, and since the function is non-constant, one of these points must be the maixmum, and one the minimum. Since $f(10,-6) = 68$, $f(-10,6) = -68$, $(10,-6)$ is the maxima of $f$, and $(-10,6)$ the minima.
\end{example}

\begin{example}
    Let $T: \mathbf{R}^n \to \mathbf{R}^n$ be a self-adjoint operator, and consider maximizing $\langle Tv, v \rangle$ over $S^{n-1}$. If $f(v) = \langle Tv, v \rangle$, then
    %
    \[ f(v) = \sum_{i,j} T_{ij} v^i v^j \]
    %
    so we may take differentials,
    %
    \[ df(v) = \sum_{i = 1}^n \left( \sum_{j \neq i} (T_{ij} + T_{ji}) v^j + 2 T_{ii} v^i \right) dx^i = 2 \sum_{i,j} T_{ij} v^j dx^i \]
    %
    and the constraint region is defined by the function $g(v) = \sum (v^i)^2$, so $dg(v) = 2 \sum v^i dx^i$. Since $S^{n-1}$ is compact, extrema exist, and at this extremum point $v$ there is $\lambda$ such that
    %
    \[ \sum_{i,j} T_{ij} v^j dx^i = \lambda \sum v^i dx^i \]
    %
    so for each $i$
    %
    \[ \sum_{j} T_{ij} v^j = \lambda v^i \]
    %
    Implying that $v$ is an eigenvector of $T$, with eigenvalue $\lambda$.

    Now if $V$ is the orthogonal complement of the span of $v$, then $T(V) \subset V$, because if $\langle w, v \rangle = 0$, then $\langle Tw, v \rangle = \langle w, Tv \rangle = \lambda \langle w, v \rangle = 0$, and $T: V \to V$ is still self-adjoint, implying the existence of another eigenvector in $V$. Continuing this process, we find a sequence of orthogonal eigenvectors $v_1, \dots, v_n$ for $T$, which enable us to diagonalize $T$.

    The space of self-adjoint matrices is actually a submanifold of $M_n(\mathbf{R})$, of dimension $n(n+1)/2$. Given such a matrix $M$, we have shown that $M$ can be diagonalized by some orthogonal matrix $N \in O(n)$. This will allow us to prove a structure theorem for $GL_n(\mathbf{R})$, allowing us to write any invertible matrix $A$ uniquely as $A_1 A_2$, where $A_1$ is orthogonal and $A_2$ is positive definite. The uniqueness is easy, because if $A = A_1 A_2 = A_1' A_2'$, then $A^t A = A_2^2 = (A_2')^2$, and this implies by positive definiteness that $A_2 = A_2'$, and from this we conclude that $A_1 = A_1'$. The existence is more tricky.

    Given an arbitrary matrix $M \in GL_n(\mathbf{R})$, the matrix $M^t M$ is self-adjoint and positive definite, so that there is a positive definite matrix $N$ such that $M^t M = N^2$. Then $MN^{-1}$ is orthogonal, because $N^{-1}$ is also positive definite, so
    %
    \[ (MN^{-1})^t (MN^{-1}) = N^{-1} M^t M N^{-1} = I \]
    %
    so $M = (MN^{-1}) N$ is the required decomposition.

    The multiplication map $(A_1, A_2) \mapsto A_1 A_2$ is a continuous bijection between the product space of $O(n)$ and orthogonal matrices and positive definite matrices. It is actually a homeomorphism. To see this, suppose $A^\alpha_1 A^\alpha_2 \to A$, where $A$ has a decomposition $A_1 A_2$. Then $(A^\alpha_2)^2 \to A^t A = A_2^2$, and therefore $A_2^\alpha \to A_2$. This implies $A_1^\alpha \to A_1$.

    Finally, we note that the space of positive definite $n \times n$ matrices is an open subset of the symmetric matrices, which is an $n(n+1)/2$ dimensional vector space. The space is a convex subset, which implies that it is homeomorphic to $\mathbf{R}^{n(n+1)/2}$. We conclude with the general fact that $GL_n(\mathbf{R})$ is homeomorphic to $O(n) \times \mathbf{R}^{n(n+1)/2}$. This is another way to prove that $GL_n(\mathbf{R})$ has two connected components, since it is easy to verify that $O(n)$ has 2 connected components.
\end{example}

\section{Tensors}

\begin{theorem}
    If $\sum \nu_i \mu^i$ is an invariant, for any covector field $\sum \nu_i dx^i$, then $\sum \mu^i \frac{\partial}{\partial x_i}$ is a well defined vector field.
\end{theorem}
\begin{proof}
    For any covector field $\sum \nu_i dx^i = \sum (\nu')_i dy^i$, we have
    %
    \[ \sum \mu^i \nu_i = \sum \mu'^i \nu'_i \]
    %
    and we have the relation $(\nu')_i = \sum \nu_j \frac{\partial x^j}{\partial y_i}$. If we choose $\nu_\alpha = 1$, and $\nu_i = 0$ otherwise, we find
    %
    \[ \mu^\alpha = \sum \mu^i \nu_i = \sum_i \mu'^i \nu_i' = \sum_{i,j} \mu'^i \frac{\partial x^j}{\partial y_i} \nu_j = \sum_i (\mu')^i \frac{\partial x^\alpha}{\partial y_i} \]
    %
    so
    %
    \[ \sum_i \mu^i \frac{\partial}{\partial x^i} = \sum_{i,j,k} (\mu')^k \frac{\partial x^i}{\partial y_k} \frac{\partial y^j}{\partial x^i} \frac{\partial}{\partial y^j} = \sum_{j,k} (\mu')^k \frac{\partial y^j}{\partial y^k} \frac{\partial}{\partial y^j} = \sum_k (\mu')^k \frac{\partial}{\partial y^k} \]
    %
    is a well defined vector field, and is uniquely determined since we can identify $\mu^i$ from the function $\sum \mu^i \nu_i$, where $\nu_i = 1$.
\end{proof}

\section{The Classical Viewpoint}

The classical viewpoint of vectors in the tangent bundle, and tensors in general, is obsessed with the coefficients associated to these objects and the way they transform with respect to coordinate systems. For instance, for a coordinate chart $(x,U)$ an $(n,m)$ tensor field $\omega$ may be written on $U$ as
%
\[ \omega = \sum_{\substack{i_1, \dots, i_n\\j_1, \dots, j_m}} a_{i_1, \dots, i_n}^{j_1, \dots, j_m} dx^{i_1} \otimes \dots \otimes dx^{i_n} \otimes \frac{\partial}{\partial x^{j_1}} \otimes \dots \otimes \frac{\partial}{\partial x^{j_m}} \]
%
If $(y,V)$ is another coordinate system, then we also have
%
\[ \omega = \sum_{\substack{i_1, \dots, i_n\\j_1, \dots, j_m}} b_{i_1, \dots, i_n}^{j_1, \dots, j_m} dy^{i_1} \otimes \dots \otimes dy^{i_n} \otimes \frac{\partial}{\partial y^{j_1}} \otimes \dots \otimes \frac{\partial}{\partial y^{j_m}} \]
%
and on $U \cap V$, we have the monstrous transformation rule
%
\[ a_{i_1, \dots, i_n}^{j_1, \dots, j_m} = \sum_{\substack{\alpha_1, \dots, \alpha_n\\\beta_1, \dots, \beta_m}} b_{\alpha_1, \dots, \alpha_n}^{\beta_1, \dots, \beta_m} \frac{\partial y^{\alpha_1}}{\partial x^{i_1}} \dots \frac{\partial y^{\alpha_n}}{\partial x^{i_n}} \frac{\partial x^{j_1}}{\partial y^{\beta_1}} \dots \frac{\partial x^{j_m}}{\partial y^{\beta_m}} \]
%
Classically, one defines a $(n,m)$ tensor on a $k$ dimensional manifold as an assignment of $k^{n + m}$ functions to each chart $(x,U)$, defined on $U$, such that the transformation above is satisfied on $U \cap V$, where $(y,V)$ is another coordinate system.











\chapter{Lie Derivatives and Differential Equations}

So far, we've managed to construct a manifold, and a coordinate-independant way to measure the rate of change of functions between manifolds. However, a problem results when we wish to measure the rate of change of a rate of change. But it's difficult to measure the rate of change of vector field $X$, because we cannot compare $X_p$ and $X_q$ for $p \neq q$, as the elements lie in different vector spaces. We can of course switch to a coordinate system for close enough $p$ and $q$, associating with each $X$ a function $F(x) : \mathbf{R}^n \to \mathbf{R}^n$ we can differentiate, but this doesn't really apply to a global perspective in any way. On their own, we shouldn't expect there to be a coordinate independent way to measure the rate of change of these vector fields, because if we can choose coordinate arbitrarily, then the rate of change of $X$ can take the form of any derivative we like.

Thus we must add additional structure to our problem -- in this case, a direction with which to differentiate. This will turn out to require the theory of differential equations. In particular, we require the following result from the theory of differential equations, which requires only an elementary familiarity with functional analysis.

\begin{theorem}
    If $F: \mathbf{R}^n \to \mathbf{R}^n$ is a differentiable function, then at any point $p \in \mathbf{R}^n$, there is a suitably small $\varepsilon$ and a unique curve $x: (-\varepsilon, \varepsilon) \to \mathbf{R}^n$ with $x(0) = p$, and $x' = F \circ x$, and if we extend $x$ to have the maximum possible interval domain, then either $x$ is defined everywhere, or $x$ is defined on some interval $(a,b)$, and $\lim_{t \to a} \| x(t) \| = \lim_{t \to b} \| x(t) \| = \infty$.
\end{theorem}

The theorem has the following interpretation in the theory of differentiable manifolds. Given any differentiable vector field $X: M \to TM$, and $p \in M$, there is a suitably small $\varepsilon$ and a unique differentiable curve $x: (-\varepsilon,\varepsilon) \to M$ such that
%
\[ \frac{dx}{dt} = X_{x(t)} \]
%
We shall call a curve satisfying this equation an {\bf integral curve} for $X$. this results obviously follows because we can switch to any particular coordinate system. The unique curves imply that we can define a partial function on $M \times \mathbf{R}$ by
%
\[ \phi_t(p) = x(t) \]
%
where $x$ is the unique integral curve for $X$ with $x(0) = p$, where we assume such an $x$ exists in the definition of the function. If $\phi_{t+h}(p)$, and $\phi_t(\phi_h(p))$ are all well defined, then
%
\[ \phi_t(\phi_h(p)) = \phi_{t + h}(p) \]
%
because if $x: (-\varepsilon_0,\varepsilon_0) \to M$ is an integral curve satisfying $x(0) = p$, and $y: (-\varepsilon_1, \varepsilon_1) \to M$ is an integral curve with $y(0) = x(h)$, then the uniqueness theorem tells us that $y(u) = x(h + u)$ where these functions are defined, because $z(u) = x(h + u)$ is also an integral curve with $z(0) = x(h)$.

We shall require another theorem of the theory of differential equations, which requires some functional analysis of a much more difficult quality. First, note that the domain of the function $\phi$ forms an open subset of $M \times \mathbf{R}$, because if we switch back to coordinates, if $F: \mathbf{R}^n \to \mathbf{R}^m$ satisfies $\| DF(p) \| \leq M$ for $\| p \| < \varepsilon$, then any integral curve in $x$ with $\| x(0) \| < \varepsilon/2$ is defined for provided $\| x(p) \| < \varepsilon$
%
\[ |x'| = \| F'(x) \| < M \]
%
hence $x(t) \leq Mt$, so that $x$ is well defined on $(-\varepsilon/2M,\varepsilon/2M)$. Thus $\phi$ is well defined on $(-\varepsilon/2M, \varepsilon/2M) \times B_{\varepsilon/2}$, a neighbourhood of the origin, and by transporting coordinates, we obtain the result for arbitrary points on manifolds. 

Thus we find $\phi$ is defined on an open {\it submanifold} of $M \times \mathbf{R}$, and hence we can consider differentiability. It is a very difficult theorem of differential calculus on Banach spaces to show the following, very important theorem.

\begin{theorem}
    If $X$ is a $C^k$ vector field, then $\phi$ is a $C^k$ function.
\end{theorem}

Since $\phi_{-t} = \phi_t^{-1}$ on a small enough neighbourhood of every domain, we conclude that each $\phi_t$ is `almost' a diffeomorphism, and is certainly a local diffeomorphism. For compact manifolds, we can obtain a global family of diffeomorphisms.

\begin{theorem}
    If $X$ is a vector field with compact support, then $\phi$ is defined everywhere.
\end{theorem}
\begin{proof}
    If $K$ is the support of $X$, then $\phi_t$ is defined on $K^c$ for all $t$, because if $p \not \in K$, then $x(p) = p$ is an integral curve of $X$. If we cover $K$ by finitely many coordinate charts $(x_1,U_1), \dots, (x_n,U_n)$, such that $\phi_t$ is defined for $|t| < \varepsilon_i$ on $U_i$, then we have define $\phi_t$ on $M$ for all $|t| < \min(\varepsilon_1, \dots, \varepsilon_n)$, and we can then define $\phi$ for arbitrarily large real numbers by considering the composition $\phi^n_t = \phi(nt)$.
\end{proof}

There is also a very useful theorem in both theory, and for the development of the method of proof.

\begin{theorem}
    If $X$ is a differentiable vector field on a manifold $M$ with $X_p \neq 0$, then there is a coordinate chart $x$ around $p$ such that in a neighbourhood of $p$, $X_p = \frac{\partial}{\partial x^1}$.
\end{theorem}
\begin{proof}
    It obviously suffices to prove this for $p = 0$, and $M = \mathbf{R}^n$. We may also assume that $X_0 = \frac{\partial}{\partial x^1}$ Consider the induced diffeomorphism $\phi$ from the vector field $X$, and take the map $y: (-\varepsilon, \varepsilon) \times U$, where $U \subset \mathbf{R}^{n-1}$ is a neighbourhood defined by
    %
    \[ y(t,x) = \phi_t(0,x) \]
    %
    Clearly the map is $C^\infty$, and if $p = (0,x)$,
    %
    \begin{align*}
        y_* \left( \left. \frac{\partial}{\partial x^i} \right|_p \right)(f) &= \frac{\partial f \circ y}{\partial x^i} = \lim_{h \to 0} \frac{f(0,x + he_i) - f(0,x)}{h}\\
        &= \left. \frac{\partial f}{\partial x^i} \right|_p
    \end{align*}
    %
    and conversely, for any $p = (0,x)$,
    %
    \begin{align*}
        y_* \left( \left. \frac{\partial}{\partial t} \right|_p \right) (f) &= \lim_{h \to 0} \frac{f(\phi_{t + h}(0,x)) - f(\phi_t(0,x))}{h}\\
        &= \lim_{h \to 0} \frac{(f \circ \phi_h)(\phi_t(0,x)) - (f \circ \phi_0)(\phi_t(0,x))}{h}\\
        &= \frac{d (f \circ \phi_t)}{dt} = X_{\phi_t(0,x)}(f)
    \end{align*}
    %
    Hence $y$ has a non-vanishing derivative in a neighbourhood of the origin, and is therefore a coordinate system with
    %
    \[ y_* \left( \sum a^i \left. \frac{\partial}{\partial x^i} \right|_{(t,x)} + b \left. \frac{\partial}{\partial t} \right|_{(t,x)} \right) = \sum a^i \left. \frac{\partial}{\partial y^i} \right|_{\phi_t(0,x)} + b (y_*X)_{\phi_t(0,x)}  \]
    %
    hence $x := y^{-1}$ is a coordinate system around the origin with $X = \frac{\partial}{\partial x^1}$.
\end{proof}

Note that here we have used the fact that
%
\[ (X_p f) = \lim_{h \to 0} \frac{f(\phi_h(p)) - f(p)}{h} \]
%
which suggests that the translation maps $\phi_h$ can be used to measure the rate of change along the vector $X_p$. As foreshadowed at the beginning of the chapter, we can use this process to measure the rate of change of the many objects defined on a differentiable manifold.

First, we switch notations, we denote $X(f)$ by $L_X(f)$, and call it the {\bf Lie derivative} of $f$ along $X$. We can also consider the derivatives of covariant vector fields $\omega$, obtaining a new covariant vector field
%
\[ (L_X \omega)(p) = \lim_{h \to 0} \frac{(\phi_h^*\omega)(p) - \omega(p)}{h} \]
%
So that, `up to first order', $(L_X \omega)(\phi_h(p)) = \omega(p) + h (L_X \omega)(p)$. Similarily, and most importantly, we consider the derivatives of other vector fields, defined by
%
\[ (L_X Y)(p) = \lim_{h \to 0} \frac{Y_p - ((\phi_h)_* Y)_p}{h} \]
%
Where the order of terms is switched because $((\phi_h)_* Y)_p$ is really looking at the point on the vector field at $Y_{\phi_{-h}}$. We can define $(\phi_h^* Y)_p = ((\phi_{-h})_* Y)_{\phi_h(p)}$, and then we can interchange the equation, but we'll stick with this definition.

\begin{theorem}
    If $f, \omega$ $X$, and $Y$ are given, then
    %
    \begin{itemize}
        \item[(i)] $L_X(f \omega) = (L_X f) \omega + f (L_X \omega)$.
        \item[(ii)] $L_X(f Y) = (L_X f) X + f (L_X Y)$.
        \item[(iii)] $L_X(\omega(Y)) = \omega(L_X Y) + \omega(L_X Y)$.
    \end{itemize}
\end{theorem}
\begin{proof}
    We write
    %
    \begin{align*}
        L_X(f \omega)(p)(v) &= \lim_{h \to 0} \frac{\phi_h^*(f \omega)(p)(v) - (f \omega)(p)(v)}{h}\\
        &= \lim_{h \to 0} \frac{f(\phi_h(p)) \omega(\phi_h(p))((\phi_h)_*(v)) - f(p) \omega(p)(v)}{h}\\
        &= (FG)'(0)
    \end{align*}
    %
    Where $F(t) = f(\phi_t(p))$, and $G(t) = \omega(\phi_t(p))((\phi_t)_*(v))$. Since $F(0)' = (L_X f)(p)$, and $G(0)' = (L_X \omega)(p)(v)$, we find that
    %
    \[ L_X(f \omega)(p)(v) = f(p) (L_X \omega)(p)(v) + (L_X f)(p) \omega(p)(v) \]
    %
    and this completes the proof of (i). The propositions (ii) and (iii) are proved in essentially the same way.
\end{proof}

The Lie derivative is obviously linear, and we can obtain a formula for the derivatives in the coordinates. First, let's compute the Lie derivative for a covariant vector field. If
%
\[ X = \sum a^i \frac{\partial}{\partial x^i} \]
%
Then
%
\begin{align*}
    (L_X dx^i)(p) \left( \frac{\partial}{\partial x^j} \right) &= \lim_{h \to 0} \frac{(\phi_h^* dx^i)(p) \left( \frac{\partial}{\partial x^j} \right) - dx^i(p) \left( \frac{\partial}{\partial x^j} \right)}{h}\\
    &= \lim_{h \to 0} \frac{dx^i(\phi_h(p)) \left((\phi_h)_* \frac{\partial}{\partial x^j} \right) - \delta_i^j}{h}\\
    &= \lim_{h \to 0} \frac{1}{h} \left( \left. \frac{\partial x^i \circ \phi_h}{x^j} \right|_{\phi_h(p)} - \delta_i^j \right)\\
    &= \left. \frac{\partial^2 (x^i \circ \phi_h)}{\partial h \partial x^j} \right|_p = \left. \frac{\partial a^i}{\partial x^j} \right|_p
\end{align*}
%
Thus if $\omega = \sum b_i dx^i$, then
%
\[ L_X \omega = \sum \left( b_i \frac{\partial a^i}{\partial x^j} + a_i \frac{\partial b_i}{\partial x^j} \right) dx^j \]
%
To calculate $(L_X Y)$ in coordinates, let $X = \sum a^i \frac{\partial}{\partial x^i}$, and $Y = \sum b^i \frac{\partial}{\partial x^i}$. Using (iii) from the last proposition, we find
%
\begin{align*}
    0 = (L_X \delta_i^j) &= L_X \left( dx^i \left(\frac{\partial}{\partial x^j} \right) \right)\\
    &= dx^i \left( L_X \frac{\partial}{\partial x^j} \right) + (L_X dx^i) \left( \frac{\partial}{\partial x^j} \right)\\
    &= dx^i \left( L_X \frac{\partial}{\partial x^j} \right) + \frac{\partial a^i}{\partial x^j}
\end{align*}
%
Hence $\left( L_X \frac{\partial}{\partial x^j} \right) = \sum - \frac{\partial a^i}{\partial x^j} \frac{\partial}{\partial x^i}$, and so
%
\[ (L_X Y) = \sum \left( a^i \frac{\partial b^j}{\partial x^i}  - b^i \frac{\partial a^j}{\partial x^i} \right) \frac{\partial}{\partial x^j} \]
%
We can use this theorem to find a much simpler expression for $L_X Y$. Note that if $f$ is a function, then $Y(f)$ is also a function, so we can consider $X(Y(f))$.

\begin{theorem}
    $(L_X Y)(f) = X(Y(f)) - Y(X(f))$.
\end{theorem}
\begin{proof}
    If $X = \sum a_i \frac{\partial}{\partial x^i}$, and $Y = \sum b_i \frac{\partial}{\partial x^i}$, then
    %
    \begin{align*} X(Y(f)) - Y(X(f)) &= \sum a_i \frac{\partial Y(f)}{\partial x^i} - b_i \frac{\partial X(f)}{\partial x^i}\\
    &= \sum a_i \frac{\partial b_j}{\partial x_i} \frac{\partial f}{\partial x_j} - a_i b_j \frac{\partial^2 f}{\partial x_i \partial x_j} - b_i \frac{\partial a_j}{\partial x_i} \frac{\partial f}{\partial x_j} + b_i a_j \frac{\partial^2 f}{\partial x_i \partial x_j}\\
    &= \sum \left( a_i \frac{\partial b_j}{\partial x_i} - b_i \frac{\partial a_j}{\partial x_i} \right) \frac{\partial f}{\partial x_j}\\
    &= (L_X Y)(f)
    \end{align*}
\end{proof}







\chapter{Lie Groups}

A {\bf Lie group} is a group with a differentiable action. That is, it is a group with a differentiable structure such that multiplication and inversion are differentiable functions on the space. It suffices to verify that the map $(x,y) \mapsto xy^{-1}$ is differentiable. We have actually already considered many of the important Lie groups. The simplest Lie groups are abelian, like the group $\mathbf{R}^n$ under addition, the circle group $S^1 = \mathbf{R}/\mathbf{Z}$, and more generally, the torus groups $\mathbf{T}^n = \mathbf{R}^n/\mathbf{Z}^n$. The main noncommutative exmamples occur as matrix groups, like $GL_n(\mathbf{R})$, $SL_n(\mathbf{R})$, $O_n(\mathbf{R})$ and $SU_n(\mathbf{R})$.

Any subgroup of a Lie group which is also a submanifold is a Lie group, because the group structure on the subgroup is just the restriction of the group structure on the entire group, which is differentiable. This is essentially the argument we used to show that $SL_n(\mathbf{R})$, $O_n(\mathbf{R})$, and $SU_n(\mathbf{R})$ are Lie groups. We could have also used the fact that $S^1$ is a subgroup of the multiplicative group of non-zero complex numbers to show it was a Lie group. $S^3$ is a Lie group, because it is a subgroup of the Lie group of quaternions, consisting of elements of norm one. More generally, we define a {\bf Lie subgroup} of a Lie group to be a subgroup, with some $C^\infty$ structure making the operations of the subgroup differentiable, and such that the $C^\infty$ structure makes the inclusion of the subgroup an immersion. As an example of a subgroup that is not an imbedded submanifold, consider the set of points $(x,cx) \in \mathbf{T}^2$, where $c$ is an irrational number.

Lie groups are incredibly useful in geometry, because we often want to consider some symmetries which occur in a problem, which often turn out to be a group with some Lie structure. As an example, suppose we are discussing the metric structure of $\mathbf{R}^n$. In this situation, it is natural to discuss the Euclidean group $E_n$, which is the collection of all isometries of $\mathbf{R}^n$. The easiest case to analyze is the group $E_1$. For each $x \in \mathbf{R}$, and $z \in \{ -1, 1 \}$, define $T_{xz}(t) = x + zt$. Every $T \in E_1$ can be written as $T_{xz}$ for some $x$ and some $z$. To see this, let $x = T(0)$. Since $T$ is an isometry, either $T(1) = x + 1$ or $T(1) = x - 1$, because $|T(1) - x| = 1$. If $T(1) = x + 1$, then $T(y) = x + y$, because $x + y$ is the only number satisfying
%
\[ |T(y) - x| = |y|\ \ \ \ \ |T(y) - (x + 1)| = |y - 1|  \]
%
Similarily, if $T(1) = y - 1$, then $T(x) = y - x$. Since
%
\[ T_{x_0z_0} \circ T_{x_1z_1} = T_{(x_0 + z_0x_1)(z_0z_1)} \]
%
and so $E_1$ can be described as the {\it semidirect product} of the multiplicative group $\{ -1, 1 \}$ and $\mathbf{R}$ under the representation $\rho: \{ -1, 1 \} \to \text{Aut}(\mathbf{R})$ defined by $\rho(x)(y) = -y$. For $E_2$, we note that if $T: \mathbf{C} \to \mathbf{C}$ is an isometry such that $T(0) = 0$, and $T(1) = 1$, then $|T(i)| = 1$ and $|T(i) - 1| = \sqrt{2}$, so either
%
\begin{itemize}
    \item $T(i) = 1$, which implies $T(z) = z$ for all $z \in \mathbf{C}$, because $z$ is uniquely specified by the values $|z|$, $|z - 1|$, and $|z - i|$.
    \item $T(i) = -1$, which implies $T(z) = \overline{z}$ for all $z \in \mathbf{C}$, because $\overline{z}$ is the unique point with $|\overline{z}| = |z|$, $|\overline{z} - 1| = |z - 1|$, and $|\overline{z} - (-i)| = |z - i|$.
\end{itemize}
%
If $z \in \mathbf{C}$, and $w \in \mathbf{T}$, we define $T_{zw}$ to be the isometry $T(u) = z + wu$. If $T \in E_2$ is arbitrary, and if $T(0) = u$, $T(1) = w$, then $T_{uw}^{-1} \circ T$ maps 0 to 0, and 1 to 1, hence either $T = T_{uw}$ or $T = \overline{T_{uw}}$. Note that $\overline{T_{uw}}(z) = T_{\overline{uw}}(\overline{z})$, so that the set of all $T_{uw}$ is a normal subgroup of $E_2$. Since the group of all $T_{uw}$ is isomorphic to the semidirect product $\mathbf{C} \rtimes \mathbf{T}$, because $T_{u_0w_0} \circ T_{u_1w_1} = T_{(u_0 + w_0u_1)(w_0w_1)}$, and therefore $E_2$ is isomorphic to the semidirect product $(\mathbf{C} \rtimes \mathbf{T}) \rtimes \{ -1, 1 \}$ with multiplication law
%
\[ (z_0,w_0,t_0)(z_1,w_1,t_1) = \begin{cases} (z_0 + \overline{w_0z_1},w_0\overline{w_1},t_0t_1) & t_0 = -1 \\ (z_0 + w_0z_1,w_0w_1, t_0t_1) & t_0 = 1 \end{cases} \]
%
The fact that $\{ -1, 1 \}$ is isomorphic to $O_1$, and $\{ -1, 1 \} \rtimes \mathbf{T}$ is isomorphic to $O_2$ hints at a more general fact about the structure of the Euclidean groups, but we need to know some structure of the metric of $\mathbf{R}^n$ first. Say a point $x$ lies {\it between} two points $y$ and $z$ if $x = \lambda y + (1 - \lambda)z$ for $0 \leq \lambda \leq 1$. This holds if and only if $\| y - x \| + \| x - z \| = \| y - z \|$, because if $x = \lambda y + (1 - \lambda) z$, then
%
\[ \| y - x \| + \| x - z \| = \|(1 - \lambda)y - (1 - \lambda)z \| + \| \lambda y - \lambda z \| = \| y - z \| \]
%
and the Cauchy Schwartz inequality implies that this inequality occurs only when $y - x = \lambda (x - z)$ for some $\lambda > 0$, in which case we find
%
\[ x = \left( \frac{1}{1 + \lambda} \right) y + \left( \frac{\lambda}{\lambda + 1} \right) z \]
%
We say $x,y,z$ are colinear if one point lies between the other pair of points. This occurs if and only if $y - x$ and $z - x$ are linearly dependant, because if $\lambda (y - x) = (z - x)$, then
%
\begin{itemize}
    \item If $\lambda < 0$, then $\| y - x \| + \| x - z \| = \| y - z \|$, so $x$ lies between $y$ and $z$.
    \item If $0 < \lambda < 1$, then $z$ lies between $x$ and $y$, because $z = \lambda y + (1 - \lambda) x$.
    \item If $\lambda > 1$, then $y$ lies between $x$ and $z$, because
    %
    \[ y = \frac{1}{\lambda} z + \frac{\lambda - 1}{\lambda} x \]
\end{itemize}
%
This tells us that an isometry maps straight lines to straight lines, because betweenness and colinearity are purely metric conditions, hence preserved by an isometry, and a line can be described by a set of points such that any triple of points is colinear. Furthermore, an isometry maps planes to planes, because a plane can be described as the smallest set containing a triple of non-colinear points $x,y,z$, and also containing the line generated by any points in the set. We claim that this plane is the set of points
%
\[ \{ x + \lambda (y - x) + \gamma (z - x) : \lambda, \gamma \in \mathbf{R} \} \]
%
If $x + \lambda_0 (y - x) + \gamma_0 (z - x)$ and $x + \lambda_1 (y - x) + \gamma_1 (z - x)$ are two points in this plane, then the set of points on the line between these two points is exactly $x + (\lambda_0 + t \lambda_1) (y - x) + (\gamma_0 + t \gamma_1) (z - x)$, as $t$ ranges over all real numbers, and these points all lie in the set above. Conversely, if $X$ is any colinearily closed set containing $x$, $y$, and $z$, then $x + t_0 (y - x)$ and $x + t_1 (z - x)$ are elements of $x$, for all $t \in \mathbf{R}$, and therefore
%
\[ x + t_0 (y - x) + t_2 (t_1 (z - x) - t_0 (y - x)) = x + t_0 (1 - t_2) (y - x) + t_2 t_1 (z - x) \]
%
are also points in $X$, for all $t_0,t_1,t_2 \in \mathbf{R}$. This implies that the set of all points $x + \lambda (y - x) + \gamma (z - x)$ are contained in $X$. Since colinearity is a metric notion, an isometry maps planes to planes.

Now suppose $T: \mathbf{R}^n \to \mathbf{R}^n$ is an isometry, with $T(0) = 0$. Our discussion implies that $T$ maps lines through the origin to lines through the origin. Thus $T(cx) = cT(x)$, because $cT(x)$ is the only point on the line through the origin and $x$ which lies at a distance $|c|\|x\|$ from the origin and a distance $|c - 1|\|x\|$ from $x$. Similarily, for a fixed $x,y \in \mathbf{R}^n$, if we assume that $T$ maps the plane generated by $x$ and $y$ to itself, then since $T(0) = 0$ we find $T(x + y) = T(x) + T(y)$. Otherwise, we consider a linear isometry $S$ which projects the plane generated by $T(x)$ and $T(y)$ to the plane generated by $x$ and $y$, and then it follows that $(S \circ T)(x + y) = S(Tx + Ty)$, hence $T(x + y) = Tx + Ty$, because $S$ is a bijection. It follows that $T(0) = 0$ holds if and only if $T$ is an element of the orthogonal group of isometric linear transformations $O_n$. If $T \in E_n$ is any linear transformation, and if $T(0) = x$, then the isometry $T_x^{-1} \circ T$ maps zero to zero, hence $T_x^{-1} \circ T \in O_n$, and we find that we can write any Euclidean transformation as a rotation and a translation, and by normality we find the Euclidean group is actually the semidirect product of $O_n$ and $\mathbf{R}^n$, since if $M,N \in O_n$,
%
\[ (T_x \circ M) \circ (T_y \circ N) = T_{x + My} \circ MN \]
%
$E_n$ can be given the structure of a Lie group if we take the topology corresponding to $\mathbf{R}^n \times O_n$, in which case
%
\[ (x,M)(y,N)^{-1} = (x,M)(-Ny,N^{-1}) = (x - MNx,MN^{-1}) \]
%
which is differentiable, since the multiplication map on $O_n$ is differentiable, and the map $x - MNx$ is differentiable since the action of $M_n$ on $\mathbf{R}^n$ defined by $(M,x) \mapsto Mx$ is differentiable.

The left and right translation maps $L_x(y) = xy$ and $R_x(y) = yx$ are diffeomorphisms on any Lie group $G$, so they induce bundle equivalences $(L_x)_*: TG \to TG$ and $(R_x)_*: TG \to TG$. We say a vector field $X$ is {\bf left-invariant} if $(L_x)_* X = L_x \circ X$ for all $x \in G$, i.e. if $(L_x)_*(X_y) = X_{xy}$. It suffices to show that $(L_x)_*(X_e) = X_x$, because then
%
\[ (L_x)_*(X_y) = (L_x \circ L_y)_*(X_e) = (L_{xy})_*(X_e) = X_{xy} \]
%
Given any $v \in G_e$, we can define a unique left invariant vector field $X$ with $X_e = v$ by setting $X_x = (L_x)_*(v)$.

\begin{theorem}
    Any left-invariant vector field is automatically $C^\infty$.
\end{theorem}
\begin{proof}
    We need only verify that the map $X_p = (L_p)_*(v)$ is $C^\infty$ for any $v \in G_e$, and it suffices to prove this in a neighbourhood of the origin. Let $(x,U)$ be a chart around a neighbourhood of the origin. Let $V \subset U$ be a neighbourhood chosen such that $ab^{-1} \in U$ if $a,b \in U$. Then
    %
    \[ Xx_i \]
\end{proof}

\begin{corollary}
    A Lie group always has trivial tangent bundle.
\end{corollary}

Since a Lie group is differentiable, we should be able to `linearly approximate' the group multiplication action. 

\begin{thebibliography}{10}
    \bibitem{intro} Michael Spivak,
    \emph{A Concise Introduction to Differential Geometry: Vol. One}

    \bibitem{leesmooth} James Lee,
    \emph{An Introduction to Smooth Manifolds}

    \bibitem{halm} Paul Halmos,
    \emph{Naive Set Theory}

    \bibitem{wiki} Wikipedia,
    \emph{Lie Groups}
\end{thebibliography}

\end{document}











\section{* A Non Metrizable Manifold}

In this chapter, we will, for completeness, provide an example of a non-metrizable manifold. Recall that a {\bf well-ordered set} is a set $X$ together with a linear ordering such that every subset has a least element. A subset $Y$ of a well-ordered segment is an {\bf initial segment} if $y \in Y$ and $x < y$ imply $x \in Y$.

\begin{definition}
    An {\bf order morphism} between two well-ordered sets $X$ and $Y$ is a map $f:X \to Y$ such that if $x < y$, $f(x) < f(y)$. A bijective order morphism is called an {\bf order isomorphism}, and all order morphisms are order isomorphisms onto their codomains. An {\bf ordinal} is an equivalence class of order isomorphic well ordered sets.
\end{definition}

It is helpful to visualize ordinals as the well-ordered set they represent, since we need no further properties of well ordered sets other than the ordering they possess. We will often (to our convenience) confuse the two. One key feature of ordinals is that they allow us to measure the size of infinite sets. It should come as no surprise then, that ordinals will allow us to construct a manifold too large to be metrizable.

The most well known ordinals are the natural numbers. 0 can be considered the equivalence class containing the empty set. 1 can be considered the equivalence class of well ordered sets consisting of a single element (which obviously must be order isomorphic). In general, the number $n$ can be considered the equivalence class of well ordered sets consisting of $n$ elements (which, less obviously, must be order isomorphic). It doesn't stop here though, for we can consider the equivalence class containing $\mathbf{N}$ of all natural numbers, which is also a well ordered set. By custom, this ordinal is denoted $\omega$. Then we may consider $\omega + 1$, the equivalence class of the well ordered set obtained by taking $\mathbf{N}$ and popping a greatest element on the end, and so on and so forth. There's many more ordinals in this magnificant menagerie, and they form a beautiful transfinite chain:

\[ 0, 1, 2, 3, \dots, \omega, \omega + 1, \dots, \omega 2, \omega 2 + 1, \dots, \omega 3, \dots, \omega^2, \dots \omega^\omega, \dots  \]

\begin{lemma}
    If $X$ and $Y$ are well ordered sets, and if for $A \subset B \subset X$ there are two order morphisms $f:A \to Y$ and $g:B \to Y$ whose ranges are initial segments of $Y$, then $g|_A = f$.
\end{lemma}
\begin{proof}
    Consider the set of all elements in $B$ that do not agree on $f$ and $g$. If this set is non-empty, there must be a least such element $b$, so either $f(b) < g(b)$, or $g(b) < f(a)$. In the first case, there must be $b'$ such that $g(b') = f(b)$ (since $g$ maps onto an initial segment). We also must have $b' < b$, and so $f(b') = g(b') = f(b)$. All order isomorphisms are injective, so we reach a contradiction. The latter case is similar, and shows by contradiction that there can be no elements that disagree on the domains of the functions.
\end{proof}

\begin{corollary}
    There is at most one map $f:X \to Y$ which maps onto an initial segment of $Y$.
\end{corollary}

\begin{lemma}
    If $X$ and $Y$ are well ordered sets, there either exists a unique order morphism from $X$ to an initial segment of $Y$, or a unique order morphism from $Y$ to an initial segment of $X$. What's more, this map is unique.
\end{lemma}
\begin{proof}
    Consider the set $A$ of all initial segments of $X$ which have order morphisms $f_A$ (which are necessarily unique) onto initial segments of $Y$. If we have a linear chain $\{A_k\}$ of such sets, we may by the last corollary take the union $\bigcup f_A$ of order morphisms to form an order morphism on $\bigcup A_k$. By Zorn's lemma, we must have a maximal initial segment $A$. If $A = X$, we are done. If $A \neq X$, and $f_A(A) = Y$, then we may invert the domain of $f_A$ to obtain an order morphism from $Y$ to $A$, and initial segment of $X$. These are all of the possibilities, since if $f_A(A) \neq Y$, we may consider the least element $y$ in $f_A(A)^c$ and $x$ in $A^c$, and extend the map $f_A$ by defining $f_A(x) = y$, contradicting the fact that $A$ is maximal.
\end{proof}

We say $X \leq Y$ if there is an order morphism from $X$ to an initial segment of $Y$. Because of the above theorem, we can visualize any ordinal as an initial segment of an ordinal of a larger size. In fact, with the above ordering, any ordinal is the equivalence class of the set of ordinals less than itself. From this, we can also see than any set of ordinals is well ordered, and that any set of ordinals is contained within an ordinal.

\begin{lemma}
    If $A$ is an initial segment which is a proper subset of a well ordered set $B$, there is no order isomorphism from $B$ to $A$.
\end{lemma}
\begin{proof}
    Let $f:A \to B$ be an order isomorphism from $A$ to $B$. Consider the smallest element $a \in A$ such that $f(a) \neq a$. There must be one such $a$, since $f$ is surjective, and there are some $b \in B$ which are not in $A$. We cannot have $f(a) < a$, since $f$ is injective, and this would imply $f(f(a)) \neq f(a)$, and $f(a)$ an element of $A$ since $A$ is an initial segment. We also cannot have $f(a) > a$, since there is $a' \in A$ such that $f(a') = a$, and since $f(a') < f(a)$, we have $a' < a$. By contradiction, there cannot be an order isomorphism $f$.
\end{proof}

If two well-ordered sets are order isomorpic, they have the same cardinality, and therefore it makes sense to discuss the cardinality of an ordinal. The well ordering theorem stipulates that any set can be well ordered. Therefore, taking the equivalence class of a well-ordering of $\mathbf{R}$, we obtain an uncountable ordinal. All countable ordinals can be considered initial segments of $X$, and we may therefore consider the set $\Omega$ of all countable ordinals.

\begin{theorem}
    $\Omega$ is uncountable.
\end{theorem}
\begin{proof}
    Suppose $\Omega$ is countable, Then $\Omega$ itself represents a countable ordinal $\alpha \in \Omega$. But $\alpha$ is order isomorphic to the set of ordinals less than $\alpha$, and so $\Omega$ is order isomorphic to a proper initial segment of itself, contradicting the above lemma.
\end{proof}

After this development, we can now release our non-metrizable manifolds.

\begin{example}[The Long Line]
    Take the set $\Omega$ of all countable ordinals. Then $\Omega$ is itself an ordinal, and we may consider the space $L = \Omega \times [0,1)$ together with the dictionary order. The order topology established forms a space, the long ray. Now take two copies of the long ray, and attach them at the smallest elements. This create a one-manifold -- the long line. Obviously, the space isn't metrizable -- it contains an uncountable discrete subset, so none of the other nice properties that we considered above hold.
\end{example}

\begin{example}[Long 2-Manifolds]
    The two-manifold $L \times S^1$ is called the long cylinder, and is also non-metrizable, and the long plane $L \times L$ is the same. A 2-manifold that is long only in one direction is the long strip $L \times \mathbf{R}$.
\end{example}

We'll encounter more unmetrizable manifolds in later chapters.




















\section{* A Proof of Invariance of Domain}

For this section, we will prove invariance of domain, relying on two unproved (but `obviously true') theorems. It sure takes a lot to build up to this theorem, but the result is worth every penny.

\begin{theorem}[The Generalized Jordan Curve Theorem]
    Every subspace $X$ of $\mathbf{R}^n$ homeomorphic to $S^{n-1}$ splits $\mathbf{R}^n - X$ into two components, and $X$ is the boundary of each.
\end{theorem}

\begin{theorem}
    If a subspace $Y$ of $\mathbf{R}^n$ is homeomorphic to the unit disc $\mathbf{D}^n$, then $\mathbf{R}^n - Y$ is connected.
\end{theorem}

We'll put on the finishing touches to Invariance of Domain now. Hopefully this will give you intuition to why the theorem is true.

\begin{lemma}
    One of the components of $\mathbf{R}^n - X$ is bounded, and the other is unbounded. We call the bounded component the {\bf inside} of $X$, and the unbounded component the {\bf outside}.
\end{lemma}
\begin{proof}
    Since $X$ is homeomorphic to $S^n$, it is a compact set, and therefore contained in some ball $B$. $\mathbf{R}^n - B$ is connected, so therefore one component of $\mathbf{R}^n - X$ is contained within $B$. Since $B$ is bounded, this component is bounded. If both components are bounded, we conclude that the union of the two components plus $X$ is bounded, a contradiction. Therefore the other component is unbounded.
\end{proof}

\begin{lemma}
    If $U \subset \mathbf{R}^n$ is open, $A \subset U$ is homeomorphic to $S^n$, $f:U \to \mathbf{R}^n$ is one-to-one and continuous, and $A \cup (\text{inside of}\ A)$ is homeomorphic to $\mathbf{D}^n$, then $f(\text{inside of}\ A) = \text{inside of}\ f(A)$.
\end{lemma}
\begin{proof}
    Since $f$ is continuous, $f(\text{inside of}\ A)$ is connected, and is therefore contained either entirely within the outside of $f(A)$ or the inside of $f(A)$. The same is true of $f(\text{outside of}\ A)$. The difference is that, due to compactness, $f(A \cup (\text{inside of}\ A))$ is homeomorphic to $A \cup (\text{inside of}\ A)$, and in connection, homeomorphic to $\mathbf{D}^n$. Therefore $\mathbf{R}^n - f(A \cup \text{inside of}\ A)$ is connected. It follows that $f(\text{inside of}\ A)$ is a component of $\mathbf{R}^n - f(A)$, so it is equal to either the inside of outside of space. Since $f(\text{inside of}\ A)$ is contained within a bounded ball, we conclude that it is equal to the inside.
\end{proof}

\begin{theorem}[Invariance of Domain]
    If $f:U \to \mathbf{R}^n$ is an injective continuous function, where $U$ is an open subset of $\mathbf{R}^n$, then $f(U)$ is open, and therefore $f$ is homeomorphic onto its image.
\end{theorem}
\begin{proof}
    Let $V$ be an arbitrary open subset of $U$. We must show $f(V)$ is also open. Let $x \in V$ be arbitrary, and consider a closed ball $\overline{B}$ containing $x$, and contained in $V$. The boundary of $\overline{B}$ is homeomorphic to $S^{n-1}$, and the interior $B$ is equal to the inside of $\overline{B}$. By the lemma (2.4) above, we conclude that
    %
    \[ f(B) = \text{inside of}\ f(\partial B) \]
    %
    Since $\partial B$ is closed in $\mathbf{R}^n$, the inside is open, hence $f(B)$ is open. By an extension of this argument, we have shown the the image of any open set is open, so invariance of domain is proved.
\end{proof}

The unproved theorems we rely on here require quite advanced techniques in algebraic topology. Hopefully, the results seem intuitive enough that the theorem now should be `correct' in your mind, so we'll leave the algebra for a different set of notes, and seek other interests.