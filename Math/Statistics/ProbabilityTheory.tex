\input{../../style.tex}

\title{Probability Theory}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}
\maketitle
\tableofcontents
\pagenumbering{arabic}

\chapter{Foundations}

These notes outline the basics of probability theory, the mathematical framework which allows us to interpret the statement that we are {\it 80\% more likely} to develop lung disease if you are smoker than if you are a non-smoker, or that there is a {\it 50-50 chance} of rain on Saturday? These statements seem intuitive, and we use them naturally in everyday conversation, but a closer analysis of these statements reveals a couple difficulties with understanding such a statement. For instance, on Saturday, it will either rain, or not rain, so it is difficult to believe that there is a reasonable universal `chance' of such an event happening except for absolute certainty. The mathematician has a rigorous abstraction of these statements interpreted in the language of measure theory. Nonetheless, in order to justify our intuition and to apply the theory in the sciences, we require a deeper understanding of what a probabilistic statement means. In this chapter, we will explore the two major interpretations of probability theory in real life, each of which use the same underlying mathematical theory to make judgements about the world. Regardless of which of the common interpretations you have, we will find there are certain properties your probabilistic statements possess, which can be taken as axiomatic properties of a synthetic study of probability.

\section{Frequentist Probability}

Classical probability theory was developed according to the intuitions of what is now known as the frequentist school of probability theory, and is the simplest interpretation of probability to understand. It is easiest to understand from the point of view of a scientist. Suppose you are repeatedly performing some well-controlled experiment, in the sense that you do not expect the outcome of the experiment to change drastically between trials. Even under rigorously controlled conditions, the experiment will not always result in the same outcome. Slight experimental error results in slight changes in the outcome of the experiment. Nonetheless, some outcomes will occur more frequently than others.

Let us perform an experiment as often as desired, obtaining an infinite sequence of outcomes $\omega_n$, for $n = 1$, $n = 2$, and so on. Let $D$ be a certain question, or {\it proposition} about the outcome of the experiment. For instance, $D$ may ask whether a flipped coin lands heads up when flipping a coin repeatedly. Mathematically, we can represent the proposition as a subset of the set of all outcomes in an experiment -- the outcomes for which the proposition is true. We define the {\it relative frequency} of $D$ being true in $n$ trials by the equation
%
\[ P_n(D) := \frac{\# \{ k \leq n : \omega_k \in D \}}{n} \]
%
The key assumption of the frequentist school of probability is that, if our experiments are suitably controlled, then regardless of the specific sequence of measured outcomes, our relative frequencies will always converge to a well defined invariant ratio, which we define to be the probability of a certain event:
%
\[ \prob(D) := \lim_{n \to \infty} P_n(D) \]
%
Let's explore some consequences of this doctrine. First, $0 \leq P_n(D) \leq 1$ is true for any $n$, so for any proposition $D$, $0 \leq \mathbf{P}(D) \leq 1$. If we let $\Omega$ denote the set of all possible outcomes to the experiment (a proposition true for all outcomes of the experiment), then
%
\[ P_n(\Omega) = \frac{\# \{ k \leq n : \omega_k \in \Omega\}}{n} = \frac{\# \{ 1, 2, \dots, n \}}{n} = 1 \]
%
Thus we conclude $\mathbf{P}(\Omega) = 1$. If $A_1, A_2, \dots$ is a sequence of disjoint propositions, in the sense that no more than one outcome is true in each instance of the experiment, then
%
\[ P_n \left( \bigcup_i A_i \right) = \frac{\# \{ k \leq n : \omega_k \in \bigcup A_i \}}{n} = \frac{\sum_i \# \{ k \leq n : \omega_k \in A_i \}}{n} = \sum_i P_n(A_i) \]
%
Hence $\mathbf{P}( \bigcup A_i ) = \sum \mathbf{P}(A_i)$, when the events are disjoint \footnote{There are problems if we take uncountably many events $A_i$, rather than a countable set, but this needn't concern us now.}. There is no real generality here, because only countably many disjoint propositions can be true in the sequence of experimental outcomes, hence the probability of only countably many propositions is nonzero.

The properties we have so described turn out to be sufficient to describe all the mathematically important rules of frequentist probability. What's more, we can use these rules to {\it prove} that the probability of a sequence of controlled experiments eventually settles down, commonly called the strong and weak laws of probability, which justifies the thought process of the frequentist school in the first place.

\section{Bayesian Probability}

The frequentist school is sufficient to use probability theory to model scientific experiments, but in everyday life we make a more expansive use of probabilistic language. If you turn on the news, it's common to hear that ``there is an 80\% chance of downpour this evening''. It is difficult to interpret this result in the frequentist definition of probability. Even if we see each night's temperament as an experimental trial, it is hard to convince yourself that these experiments are controlled enough to converge to a probabilistic result. The Bayesian school of probability redefines probability theory to be attuned to a person's individual beliefs, so that we can interpret ``there is an 80\% chance of downpour this evening'' as an individual's belief that they think it will rain this evening rather than not rain.

You might argue that, if probability is a personal belief in an unknown event, we can choose probabilities however we want, and this would break down the logical structural required for a mathematical theory of probability. However, the probabilities that the Bayesian school studies are forced to be `logically consistant'. Consistancy can be formulated in various ways; here we discuss what is known as the Dutch book method, developed by the Italian probabilist Bruno de Finetti; if you assign to a certain unknown event $D$ a probability $\prob(D)$, then you are willing to make a bet at $[\prob(D):1 - \prob(D)]$ odds, playing the following game: If $D$ occurs, you win $1 - \prob(D)$ dollars, but if $D$ does not occur, you have to pay up $\prob(D)$ dollars. You {\it must} also be willing to play the game where you lose $1 - \prob(D)$ dollars if $D$ occurs, and gain $\prob(D)$ dollars if $D$ does not occur, so that you think the bets are `fair' to both sides. For instance, you might be willing to bet a dollar against a dollar that a coin will turn up heads, which is $[1:1] = [1/2:1/2]$ odds, so we would assign the probability that a coin will turn up heads as $1/2$, because then we win or lose an equal amount depending on the outcome of the bet. A person's probability function is inconsistant if it possible to make a series of bets that will guarantee a profit regardless of the outcome; this is known as a dutch book.

Here's an example of how the Dutch book method can be employed to obtain general rules of probability. We claim that for any event $D$, $0 \leq \prob(D) \leq 1$. If a person believed that $\prob(D) < 0$, then I could make a bet that person that $D$ occured, and I would make money regardless of the outcome. Similar results occur from betting against $D$ if $\prob(D) > 1$. It can be shown, via similar arguments, that the laws of probability hold for any logically consistant Bayesian choice of probabilities. As an aside, Definetti would have only allowed finitely many bets at once, which means that he would only accent $\mathbf{P}(\bigcup A_i) = \sum \mathbf{P}(A_i)$ for finite sums, but here we allow countably many bets to be made at once. Allowing limit operations is too useful to eschew! What this means is that, regardless of whether you think that probabilities are a measure of `degrees of belief' in an event happening, or the experiment frequencies of an experiment, then you still believe in the same laws of probability. Regardless of which philosophy you agree with, the fundamental principles of probability theory remain the same. We shall take the three laws we derived, and use it to make a rigorous model so that we can avoid future philosophical controversies, and this is where mathematical probability theory takes its form.

\section{Axioms of Probability}

Mathematically ,rigorous probability theory is defined under the banner of measure theory. The framework enables us to avoid some paradoxes which can be found if we aren't careful when analyzing experiments with infinitely many outcomes. Note, however, that the focus of probability theory is on events and random quantities (what we will soon refer to as random variables), rather than on focusing on a particular measure space under questions. Probability theorists focus on studying these {\it concepts}, and the framework provides the formality to understand these concepts. A {\bf probability space} is a measure space $\Omega$ with a positive measure $\mathbf{P}$ such that $\mathbf{P}(\Omega) = 1$. To the non-initiated, this means that there is a function $\mathbf{P}$, mapping certain subsets of $X$ to numbers in $[0,1]$, satisfying $\mathbf{P}(\bigcup A_i) = \sum \mathbf{P}(A_i)$ for disjoint events $A_i$, and $\mathbf{P}(\Omega) = 1$. $\Omega$ is known as the {\bf sample space}, and $\mathbf{P}$ is known as the {\bf probability distribution} or {\bf probability measure}. We interpret $\Omega$ as the space of outcomes to some random phenomena, and $\mathbf{P}$ measuring the likelyhood of each outcome happening. Classically, probability theory was the study of certain techniques used to calculate $\mathbf{P}(E)$ for certain outcomes $E \subset \Omega$, which encouraged the development of the modern fields of combinatorics and integration theory. Nowadays, probability theory tends to focus more on general principles underlying probability spaces.

\begin{example}
    Suppose we flip a coin. There is a certain chance of flipping a heads, or flipping a tails. Since the coin is essentially symmetric, we should expect that the chance of a heads is as equally likely as a chance of tails. We can encode the set of outcomes in the sample space $\{ H, T \}$, and then model the probability distribution as $\mathbf{P}(H) = \mathbf{P}(T) = 1/2$. More generally, if we have a finite sample space $S$, we can put a distribution on $S$ which considers all points equally known as the {\bf uniform distribution}, with distribution $\mathbf{P}(s) = 1/|S|$, for each $s \in S$.
\end{example}

\begin{example}
    If $\omega \in \Omega$ is fixed, the {\bf point mass distribution} $\delta_\omega$ at $\omega$ is the probability distribution defined by
    %
    \[ \delta_\omega(E) = \begin{cases} 1 & \omega \in E \\ 0 & \omega \not \in E \end{cases} \]
    %
    The distribution represents an event where a single outcome is certain to occur, and all other situations are impossible.
\end{example}

We remark that there is no restriction in mathematical probability theory on {\it how} particular probabilistic events are obtained. All we need is that every agrees on a single value of the probability of each event, and that such values obey the laws of probability encompassed by the axioms of probability spaces. In the study of classical statistical mechanics, one can often use a discrete model consisting of placing a certain number of indistinguishable molecules in a certain number of positions. If the positions are indistinguishable, it is reasonable to assume that each molecules independently occurs in any position with equal probability, an assumption leading to the Maxwell-Boltzmann theory of statistical mechanics. Given $n$ points to place in $m$ positions, combinatorics tells us the probability that $k_1$ points occur in the first position, $k_2$ in the second, and so on up to $k_m$ in the $m$'th position, is
%
\[ \frac{1}{m^n} {n \choose k_1\ k_2\ \dots\ k_m} = \frac{1}{m^n} \frac{n!}{k_1! k_2! \dots k_m!} \]
%
Thus more evenly spread states are more likely to occur. However, in the 20th century Bose and Einstein found that in the study of certain particles, any such configuration $(k_1, \dots, k_m)$ has an {\it equal} chance of occuring, leading to a completely different assignment of probabilities. To the mathematician, both theories are an equally applicable area of study.

\begin{example}
    If $\Omega$ is a countable set, then we can view a probability measure on $\Omega$ as a member of the set
    %
    \[ \left\{ v: \Omega \to [0,1]: \sum_{\omega \in \Omega} v(\omega) = 1 \right\} \]
    %
    This is a convex subset of the unit ball under the $l^\infty$ norm on $\Omega$, which leads to some interesting linear analysis.
\end{example}

The above example shows that the $\sigma$ algebra of an at most countable probability space plays no real role in the theory. This allows us to get away with discussing most of the basic principles of probability theory without running into too many technicalities. Nonetheless, even in the study of discrete phenomena understanding probability spaces with uncountably many points becomes necessary. For instance, in the study of the limiting average of a sequence of discrete coins flips, our sample space must consist of the space of infinite sequences of coin flips $\{ 0, 1 \}^\omega$, which is an infinite dimensional space. And it is often necessary in applications to select a point in an interval uniformly at random.

\begin{example}
    Consider a five digit number $X$ selected uniformly at random. Then the sample space consists of the numbers $[0,99999]$, and the probability that a particular number is selected is one in a 100,000. There are $10!/5! = 30240$ numbers all of whose digits are different, so the probability that a number is selected all of whose digits are different is $0.3024$. If we look at the first 800 digits of the decimal expansion of the number $e$, and we take each 5 digit consecutive sequence in this expansion, we end up with approximately the same frequency of unique digits, leading us to believe the digits occuring in the number $e$ are essentially random.
\end{example}

\begin{example}
    An interesting application of combinatorial probability theory is in the so called Birthday paradox. Given a number of $n$ points to place uniformly in $m$ boxes, the probability that no single one of them lies in the same box is $m!/(m-n)!m^n = \prod_{k = 1}^n (1 - k/m)$. In particular, if $m = 365$, and $n = 23$, then we calculate the probability that two points lie in the same box exceeds one half. To determine this approximately, we take logarithms, using the fact that $\log(1 - x) = -x + O(x^2)$, so
    %
    \[ \log \left( \prod_{k = 1}^n (1 - k/m) \right) = \sum_{k = 1}^n k/m + O(k^2/m^2) = \frac{n(n+1)}{2m} + O(n^3/m^2) \]
    %
    Thus
    %
    \[ 1 - \prod_{k = 1}^n (1 - k/m) = 1 - \exp \left( \frac{n(n+1)}{2m} + O(n^3/m^2) \right) = \frac{n(n+1)}{2m} + O(n^4/m^2) \]
    %
    Thus we should expect it to be more likely for two points to lie in the same box then unlikely if $n \geq \sqrt{2m}$. In particular, if $m = 365$, then this estimate says we should expect that two people share the same birthday in a group of more than 27 people.
\end{example}

The first immediately obvious fact from the axioms is $\prob(E^c) = 1 - \prob(E)$, since $E$ and $E^c$ are disjoint events whose union is $\Omega$. A similar discussion shows that $\prob(E \cup F) = \prob(E) + \prob(F) - \prob(E \cap F)$ because $E \cup F$ can be written as the union of the three disjoint events $E \cap F$, $E \cap F^c$, and $E^c \cap F$, and
%
\[ \prob(E) = \prob(E \cap F) + \prob(E \cap F^c)\ \ \ \ \ \prob(F) = \prob(E \cap F) \cup \prob(E^c \cap F) \]
%
This process can be generalized to unions of finitely many events. We have
%
\[ \prob(E \cup F \cup G) = \prob(E) + \prob(F) + \prob(G) - \prob(E \cap F) - \prob(E \cap G) - \prob(F \cap G) + \prob(E \cap F \cap G) \]
%
which can be reasoned by looking at the number of times each element of $E \cup F \cup G$ is `counted' on the right hand side. In general, we have the inclusion-exclusion principle
%
\[ \prob \left( \bigcup_{k = 1}^n E_k \right) = \sum_{S \subset \{ 1, \dots, k \}} (-1)^{|S|} \mathbf{P} \left( \bigcap_{k \in S} E_k \right) \]
%
This can be proven by a clumsy inductive calculation. More interestingly, but less useful, we often want to calculate the probability of an infinite union of sets $E_k$ occuring. The inclusion-exclusion principle can be taken `in the limit' to conclude that
%
\[ \prob \left(\bigcup_{k = 1}^\infty E_k \right) = \lim_{n \to \infty} \prob \left( \bigcup_{k = 1}^n E_k \right) = \sum_{\substack{S \subset \mathbf{N}\\|S| < \infty}} (-1)^{|S|} \prob \left( \bigcap_{k \in S} E_k \right) \]
%
where the sum on the right is taken as the limit of the partial sums where $S \subset \{ 1, \dots, n \}$ (the sum need not convergence absolutely, so it is important to take the limit in the precise ordering given).

The inclusion-exclusion formula can be tricky to calculate in real examples, so we often rely on estimates to upper bound or lower the probability of a particular event occuring. The trivial {\bf union bound}
%
\[ \prob \left( \bigcup E_i \right) \leq \sum \prob(E_i) \]
%
can be applied. This is a good inequality to apply if the $E_i$ are `nearly disjoint', or each have a neglible probability of occuring. On the other hand, the bound is shockingly bad if all the $E_i$ are equal to one another.

Another useful fact to consider is that $\prob(E_k) \to \prob(E)$ if the sets $E_k$ `tend to' $E$ in one form of another. If the $E_k$ are an increasing sequence whose union is $E$, then we can certainly conclude $\prob(E_k) \to \prob(E)$. Similarily, if $E_k$ is a decreasing sequence whose intersection is $E$, then $\prob(E_k) \to \prob(E)$. To obtain general results, we say that $E_k \to E$ if $\limsup E_k = \liminf E_k = E$, where
%
\begin{align*}
    \limsup_{k \to \infty} E_k &= \bigcap_{n = 1}^\infty \bigcup_{k \geq n} E_k = \{ \omega : \omega \in E_k\ \text{for infinitely many $k$} \}\\
    \liminf_{k \to \infty} E_k &= \bigcup_{n = 1}^\infty \bigcap_{k \geq n} E_k = \{ \omega: \omega \in E_k\ \text{for sufficiently large $k$} \}
\end{align*}
%
We can then conclude that $\prob(E_i) \to \prob(E)$, since once can show
%
\[ \limsup \prob(E_k) \leq \prob \left( \limsup E_k \right) \]
\[ \liminf \prob(E_k) \geq \prob \left( \liminf E_k \right) \]
%
so we can apply the squeeze theorem. This already enables us to prove a very interesting theorem which can guarantee an event can `never occur'.

\begin{lemma}[Borel-Cantelli Lemma]
    If $E_1, E_2, \dots$ is a sequence of events with $\sum \prob(E_k) < \infty$, then $\mathbf{P} \left( \limsup E_k \right) = 0$. Thus none of the events $E_k$ can happen infinitely often.
\end{lemma}
\begin{proof}
    Because
    %
    \[ \prob \left( \bigcup_{k \geq n} E_k \right) \leq \sum_{k \geq n} \prob(E_k) \]
    %
    for any $\varepsilon > 0$ we can find an $N$ such that for $n \geq N$, $\prob \left( \bigcup_{k \geq n} E_k \right) < \varepsilon$. But for any $n$, $\limsup E_k \subset \bigcup_{k \geq n} E_k$, and so we conclude $\prob(\limsup E_k) < \varepsilon$. We then let $\varepsilon \to 0$ to conclude $\prob(\limsup E_k) = 0$.
\end{proof}

The next example shows that the hypothesis $\sum \prob(E_k) < \infty$ cannot be relaxed without further analysis of the events $E_k$ beyond their probabilities.

\begin{example}
    Take the Haar measure measure on $\mathbf{T} = \mathbf{R}/\mathbf{Z}$. Consider a sequence of positive numbers $x_1, x_2, \dots$, define $S_N = \sum_{n = 1}^N x_n$, and $E_n = [S_{n-1},S_n]$, considered modulo $\mathbf{Z}$ of course. Then $\mathbf{P}(E_n) = x_n$, and $\sum x_n = \infty$ happens if and only if every point in $\mathbf{T}$ is contained in infinitely many of the $E_n$.
\end{example}

\begin{theorem}
    If $E_1, E_2, \dots$ are events with $\inf \mathbf{P}(E_k) > 0$, then infinitely many of the $E_i$ occur at once with positive probability.
\end{theorem}
\begin{proof}
    The event that infinitely many of the $E_1, E_2, \dots$ occur is the complement of the event that all but finitely many of the $E_i$ {\it do not} occur, i.e. $\liminf E_i^c$, and it suffices to show $\mathbf{P}(\liminf E_i^c) < 1$. But by Fatou's lemma,
    %
    \[ \mathbf{P} \left(\inf_{k \geq n} E_k^c \right) \leq \inf_{k \geq n} \mathbf{P}(E_k^c) = 1 - \sup_{k \geq n} \mathbf{P}(E_k) \leq 1 - \delta \]
    %
    and so, letting $n \to \infty$, we conclude $\mathbf{P}(\liminf E_k^c) \leq 1 - \delta$. Alternatively, if we consider the functions $S_n = \chi_{E_1} + \dots + \chi_{E_n}$, then
    %
    \[ S_n \leq m \mathbf{I}(S_n \leq m) + n \mathbf{I}(S_n > m) = m + (n - m) \mathbf{I}(S_n > m) \]
    %
    so if $\delta = \inf \mathbf{P}(E_i)$, then
    %
    \[ \delta n \leq \mathbf{E}(S_n) \leq m + (n - m) \mathbf{P}(S_n > m) \]
    %
    which leads to the upper bound
    %
    \[ \mathbf{P}(S_n > m) \geq \frac{\delta n - m}{n - m} \]
    %
    As $n \to \infty$, the events on the left hand side increasing to $\mathbf{P}(S_\infty > m)$, where we define $S_\infty$ as the sum of all $\chi_{E_k}$. Thus
    %
    \[ \mathbf{P}(S_\infty > m) \geq \limsup_{n \to \infty} \frac{\delta n - m}{n - m} = \delta \]
    %
    But we can then let $m \to \infty$ to conclude that $\mathbf{P}(S_\infty = \infty) = \delta$.
\end{proof}

\section{Conditional Probabilities}

In the Bayesian interpretation of probability theory, it is natural for probabilities to change over time as more information is gained about the system in question. That is, given that we know some proposition $F$ holds over the sample space, we obtain a new probability distribution over $\Omega$, denoted $\prob( \cdot | F)$, which represents the ratio of winnings from the bet which is only played out if $F$ occurs. That is
%
\begin{itemize}
    \item You win $1-\prob(E|F)$ dollars if $E$ occurs, and $F$ occurs.
    \item You lose $\prob(E|F)$ dollars if $E$ does not occur, and $F$ occurs.
    \item No money exchanges hands if $F$ does not occur.
\end{itemize}
%
It then follows from a dutch book argument that $\prob(F) \prob(E | F) = \prob(E \cap F)$ TODO: Fill in this argument. In the emperical interpretation, $\prob(E | F)$ is the ratio of times that $E$ is true in experiments, where we only count experiments in which $F$ also occurs. That is, we define $\prob(E | F)$ as the limit of the ratios
%
\[ P_n(E|F) = \frac{\# \{ k \leq n: \omega_k \in E, \omega_k \in F \}}{\# \{ k \leq n: \omega_k \in F \}} \]
%
But it is easy to calculate, by dividing the numerator and denominator by $n$, that $P_n(E|F) = P_n(E \cap F)/P_n(F)$, so by taking limits, we find
%
\[ \prob(E|F) = \lim_{n \to \infty} \frac{P_n(E \cap F)}{P_n(F)} = \frac{\prob(E \cap F)}{\prob(F)} \]
%
which gives us the formula $\prob(F) \prob(E | F) = \prob(E \cap F)$. We must of course assume that $\prob(F) \geq 0$, since overwise we are almost certain that $F$ will never occur, and then we can almost guarantee that the limit of the values $P_n(E|F)$ does not exist.

Thus we have motivation to define conditional probabilities by the formula $\prob(F) \prob(E|F) = \prob(E \cap F)$, provided that $\prob(F) > 0$. It enables us to model the information gained by restricting our knowledge to a particular subset of sample space. In particular, we can use the definition to identify events which contain information `useless' to learning about another event. We say two events $E$ and $F$ are independant if $\prob(E \cap F) = \prob(E) \prob(F)$, or, provided $\prob(F) > 0$, $\prob(E | F) = \prob(F)$; knowledge of $F$ gives us no foothold over knowledge of the likelihood of $E$.

\begin{example}
    The Monty Hall problem is an incredible example of how paradoxical probability theory can seem. We are on a gameshow. Suppose there are three doors in front of you. A car (brand new!) is placed uniformly randomly behind one of the doors. After we pick a door (the first door, for instance), the gameshow host then opens the second door, which you didn't pick, revealing the car isn't behind the door. It is important to note that he picked randomly from the remaining doors which you didn't pick and don't have a car behind them. What is the chance that the door you picked has the brand new car? You likely would think the two doors have a 50-50 chance of containing the car given this info, but you'd be wrong. Let $X \in \{ 1, 2, 3 \}$ denote the door chosen uniformly at random where the car lies, and let $Y \in \{ 1, 2, 3 \}$ denote the door that the host randomly chose to open. We know $Y \neq 1$, because the gameshow host would never open the door we picked; that would give the game away! If $X = 1$, then $Y$ is picked from $\{ 2, 3 \}$ with uniform possibility. However, if $X = 2$, something interesting occurs -- the gameshow is forced to open door number $3$, because that's the only door that (he thinks) won't give any information to the player, and similarily, if $X = 3$, then $Y = 2$. Now we know that since $X$ is chosen uniformly at random $\mathbf{P}(X = k) = 1/3$ for each $k$. Similarily, we know that $Y$ is then chosen uniformly at random from $\{ 2, 3 \}$, given that $X = 1$, so assuming $X$ and $Y$ are independent, we conclude
    %
    \[ \mathbf{P}(X = 1, Y = 2) = \mathbf{P}(X = 1) \mathbf{P}(Y = 2) = 1/6 \]
    \[ \mathbf{P}(X = 1, Y = 3) = 1/6 \]
    %
    But we also know that if $X = 2$, then $Y = 3$, so
    %
    \[ \mathbf{P}(X = 2, Y = 3) = \mathbf{P}(X = 2) = 1/3 \]
    \[ \mathbf{P}(X = 3, Y = 2) = \mathbf{P}(X = 3) = 1/3 \]
    %
    It follows that
    %
    \begin{align*}
        \prob(\text{door}\ &1\ \text{has a car} | \text{door}\ 2\ \text{was opened})\\
        &= \frac{\prob(\text{door}\ 1\ \text{has a car}, \text{door}\ 2\ \text{was opened})}{\prob(\text{door 2 was opened})}\\
        &= \frac{\mathbf{P}(\{ (X = 1,Y = 2) \})}{\mathbf{P}( \{ (X = 1,Y = 2), (X = 3,Y = 2) \} )} = \frac{1/6}{1/6 + 1/3} = 1/3
    \end{align*}
    %
    This means we should definitely change our minds about which door we were going to pick! The argument above causes a great media uproar when it was published in 1990 in a popular magazine, because of how convincing the fallacious argument below is. The total number of possibilities is
    %
    \[ (X = 1,Y = 2),(X = 1,Y = 3),(X = 2,Y = 3),(X = 3,Y = 2) \]
    %
    and the car seems to be in door one half of the possibilities. However, these events do not have the same probability of occuring. However, if the host changes his strategy, the conditional probabilities fall more in line with intuition -- if the host always picks door number 2 to open if door number 1 was picked and had the car behind it, then the two remaining doors have an equal chance of being picked.
\end{example}

We end this chapter with a final probability rule which is important in statistical analysis. If $B$ is partitioned into a finite sequence of disjoint events $A_1, \dots, A_n$, then we have the formula $\prob(B) = \sum_i \prob(B | A_i) \prob(A_i)$. This easily gives us Bayes rule
%
\[ \prob(A_j | B) = \frac{\prob(B | A_j)}{\sum_i \prob(B | A_i) \prob(A_i)} \]
%
If we view $A_j$ as a particular hypothesis from the set of all hypotheses, and $B$ as some obtained data, then Bayes rule enables us to compute the probability that $A_j$ is the true hypothesis from the probability that $B$ is the data generated given the hypothesis is true. This is incredibly important if you can interpret these probabilities correctly (if you are a Bayesian), but not so useful if you are an empiricist (in which case we assume there is a `true' result we are attempting to estimate from trials, so there is no probability distribution over the correctness hypothesis, other than perhaps a point mass, in which case Bayes rule gives us no information). We reiterate that Bayes rule is a theorem of probability theory, so is true in any interpretation, but can be used by Bayesians in a much more applicable way to their statistical analysis.

\section{Kolmogorov's Zero-One Law}

s

\chapter{Random Variables}

The formality of probability theory is ironic, because even though we require the theory of measures and real analysis to place the foundations of the theory, in the probabilistic way of thinking we try to eschew as much of this foundation as possible; studying properties of random variables which aren't `independent' of the sample space considered is avoided. As a rough approximation, if $T: X \to Y$ is a surjective measure preserving map between probability spaces ($X$ is an extension of the space $Y$, allowing more outcomes), then the random variable $Y \circ T$ is considered the `same' as the random variable $Y$, and the concepts studied in probability theory should be preserved under this extension. As we reach further and further into statistical theory, samples spaces will soon become a distant memory, brought back only for the most technical of arguments. The irony of introducing the sample space is unfortunate, because while the space is in the background, in the probabilistic way of thinking about problems we try and eschew the sample space as much as possible.

\section{Expectation}

\begin{theorem}
    For any $X \geq 0$,
    %
    \[ \mathbf{E}[X] = \int \mathbf{P}(X \geq x) dx \]
\end{theorem}
\begin{proof}
    Applying Fubini's theorem,
    %
    \begin{align*}
        \int_0^\infty \mathbf{P}(X \geq x) dx &= \int_0^\infty \int_x^\infty d \mathbf{P}_*(y)\ dx\\
        &= \int_0^\infty \int_0^y dx\ d\mathbf{P}_*(y)\\
        &= \int_0^\infty y d\mathbf{P}_*(y) = \mathbf{E}[X]
    \end{align*}
\end{proof}

\section{Distributions}

Given a random variable $X$ into $\mathbf{R}$. Then $X$ induces a pushforward measure on the Borel $\sigma$ algebra of $\mathbf{R}$, which we call the {\bf law}, or {\bf distribution} of $X$, denoted $\mu_X$. By definition, this means that
%
\[ \mu_X(E) = \mathbf{P}(X \in E) \]
%
The distribution captures the size and shape of the random variable, but not it's relation to other random variables. As examples, we note that if $X$ is uniformly distributed on $[0,1]$, then $\mu_X$ is the Lebesgue measure on $[0,1]$, and if $X$ is a {\it discrete random variable}, in the sense that the range of $X$ takes on only countably many values, then $\mu_X$ is a discrete measure with $\mu_X(\{ a \}) = \mathbf{P}(X = a)$.

We say two random variables $X$ and $Y$ are {\bf identically distributed}, sometimes written
%
\[ X \overset{(d)}{=} Y \]
%
if $\mu_X = \mu_Y$. Note that $X$ and $Y$ need not even be defined on the same sample space, let alone be actually equal to one another. For instance, if $\Omega = \{ 0, \dots, 6 \}^2$, with the uniform measure, and $X$ and $Y$ are random variables with $X(a,b) = a$ and $Y(a,b) = b$, then $X$ and $Y$ are identically distributed, but they are not equal to one another.

\chapter{Inequalities}

It is often to calculate explicitly the probability values of a certain random variable, but it often suffices to bound these values, especially when discussing convergence results, and doing other analytical calculations.

\section{Convexity}

The first classical inequality we discuss, Jensen's inequality, allows us to upper bound functions of an average with averages of a function. It depends in an essential way on the {\it convexity} of the function in question.

\begin{theorem}
    Given a convex $f: \mathbf{R} \to \mathbf{R}$ and random $X$, $f(\mathbf{E} X) \leq \mathbf{E}(f(X))$.
\end{theorem}
\begin{proof}
    Define
    %
    \[ \beta = \sup_{s < \mathbf{E}X} \frac{f(\mathbf{E}X) - f(s)}{\mathbf{E} X - s} \]
    %
    Convexity shows that $f(u) \geq f(\mathbf{E}X) - (u-\mathbf{E}X) \beta$, and by definition, we find $f(s) \geq f(\mathbf{E}X) - \beta (\mathbf{E} X - s)$ for every $s < \mathbf{E}X$. But this means the inequality holds for all $s$, and so, in particular, $f(X) - f(\mathbf{E} X) - \beta(f(X) - \mathbf{E}X) \geq 0$. Integrating both sides of this expression gives $\mathbf{E} f(X) \geq f(\mathbf{E} X)$, completing the proof.
\end{proof}

A simple consequence is obtained if we define the $L^p$ norms by $\| X \|_p = (\mathbf{E} |X|^p)^{1/p}$.

\begin{corollary}
    If $p \leq q$, $\| X \|_p \leq \| X \|_q$ if $p \leq q$.
\end{corollary}
\begin{proof}
    If we set $f(t) = t^{q/p}$, which is convex for $p \leq q$, then applying Jensen's inequality to $|X|^p$, we conclude that $(\mathbf{E} |X|^p)^{q/p} = f(\mathbf{E} |X|^p) \leq \mathbf{E} |X|^q$, and we can then take $q$'th roots.
\end{proof}

Another simple corollary is H\"{o}lder's inequality for the $L^p$ norms.

\begin{corollary}
    For $1 \leq p,q \leq \infty$, with $1/p + 1/q = 1/r$,
    %
    \[ \| X Y \|_r \leq \| X \|_p \| Y \|_q \]
\end{corollary}
\begin{proof}
    By trading powers of coefficients in the equation above, it suffices to prove the theorem when $r = 1$. Furthermore, by scaling the inequality we can assume $\mathbf{E}|X|^p = \mathbf{E}|Y|^q = 1$, and it suffices to prove that $\mathbf{E}|XY| \leq 1$. By convexity, we find
    %
    \[ |XY| \leq \frac{|X|^p}{p} + \frac{|Y|^q}{q} \]
    %
    Now integrating this equation gives the claim.
\end{proof}

\begin{corollary}
    If $p \geq 1$, $\| X + Y \|_p \leq \| X \|_p + \| Y \|_p$.
\end{corollary}
\begin{proof}
    By replacing $X$ and $Y$ with $\lambda X$ and $\lambda Y$, for an appropriate $\lambda$, we may assume $\| X \|_p + \| Y \|_p = 1$, and it suffices to show $\| X + Y \|_p \leq 1$. A simple application of convexity shows that if $p \geq 1$, and $x,y \geq 0$,
    %
    \[ (x + y)^p = (x^p + y^p) \left( \frac{x + y}{x^p + y^p} \right)^p \leq \frac{1}{x^p + y^p} \]
    %
    Integrating this gives $(\mathbf{E} |X + Y|^p \leq 2^{1-1/p}$

    $(x + y)^p/2^p \leq (x^p + y^p)/2$. Thus $\mathbf{E} |X+Y|^p \leq 2^{p-1} \mathbf{E} |X|^p + |Y|^p = 2^{p-1}$. Taking $p$'th roots gives $\| X + Y \|_p \leq 2^{1 - 1/p} \leq 1$. 
    %
    \[ (x + y)^p \leq x^p + y^p \]

     of Jensen's inequality on the sample space on two points gives that $(x + y)^p / 2^p \leq x^p + y^p / 2$. Thus
    %
    \[ \mathbf{E} |X + Y|^p \leq 2^{p-1}  \mathbf{E}|X|^p + \mathbf{E}|Y|^p \leq 2^{p-1} \]
\end{proof}

\section{Deviation From the Mean}

The most important inequality bounds the chance that a probability will deviate from it's mean. For instance, if $X$ has mean $\mu < \infty$, then the Lebesgue integral calculates
%
\[ \mu = \int X d\mathbf{P} \]
%
as the supremum of step functions. In particular, if we take the step function $x \mathbf{I}(X \geq x) \leq X$, then we find

\begin{theorem}[Markov's Inequality]
    If $X$ has finite mean $\mu$, then
    %
    \[ \prob(X \geq x) \leq \frac{\mathbf{E}(X)}{x} \]
\end{theorem}

The bound is trivial, and is therefore very rough. Nonetheless, it suffices for many purposes. One can obtain better estimates by taking a more detailed step function bounded by $X$, but the payoff isn't normally that great. We obtain a somewhat sharper estimate if $X$ has a finite variance $\sigma$.

\begin{theorem}[Chebyshev's Inequality]
    If $X$ has mean $\mu$ and variance $\sigma^2$, then
    %
    \[ \prob(|X - \mu| \geq x) \leq \frac{\sigma^2}{x^2} \]
    %
    If $Z = (X - \mu)/\sigma$, then
    %
    \[ \prob(|Z| \geq x) \leq \frac{1}{x^2} \]
\end{theorem}
\begin{proof}
    Applying Markov's inequality, we find
    %
    \[ \prob(|X - \mu| \geq x) = \prob(|X - \mu|^2 \geq x^2) \leq \frac{\mathbf{E}|X - \mu|^2}{x^2} = \frac{\sigma^2}{x^2} \]
    %
    We obtain the inequality for $Z$ by carrying out coefficents and applying Chebyshev's inequality.
\end{proof}

We can continue this process. When $X$ has an $n$'th moment, then
%
\[ \prob(|X - \mu| \geq x) \leq \frac{\mathbf{E}|X - \mu|^n}{x^n} \]
%
which shows that the existence of moments guarantees the decay of $X$. It is often difficult to calculate high degree moments, however, so this inequality does not occur as often.

\begin{example}
    Let $X_1, \dots, X_n \sim \Ber(p)$ by independant and identically distribution, where $p$ is an unknown value. A good way to estimate $p$ is via the random variable
    %
    \[ \widehat{p} = \frac{X_1 + \dots + X_n}{n} \]
    %
    which has a binomial distribution. We measure the utility of $\widehat{p}$ by minimizing the probability that $\widehat{p}$ deviates far from the mean. That is, $\prob(|\widehat{p} - p| \geq x)$ is small for large values of $x$. We find $\widehat{p}$ has mean $p$ and variance $p(1-p)/n$, so we may apply Chebyshev's inequality to conclude
    %
    \[ \prob(|\widehat{p} - p| \geq x) \leq \frac{p(1-p)}{nx^2} \leq \frac{1}{4nx^2} \]
    %
    so even for the worst possible choice of $p$, we still obtain inversely linear decay; not great, but still enough to guarantee that $\widehat{p}$ converges in distribution to the point mass measure at $p$ as $n \to \infty$. This is the weak law of large numbers for the Bernoulli distribution.
\end{example}

Measure theory gives us general bounds, which are just special results of more general inequalities. We have the Cauchy Schwarz inequality, which says $\expect(XY) \leq \sqrt{\expect(X^2) \expect(Y^2)}$, and Jensen's inequality, which says that is $f$ is convex, then $f(\expect(X)) \leq \expect(f(X))$. If $f$ is concave, $f(\expect(X)) \geq \expect(f(X))$. In particular, Jensen's inequality shows
%
\[ \expect(X^2) \geq [\expect(X)]^2\ \ \ \ \ \expect(1/X) \geq 1/\expect(X)\ \ \ \ \ \expect(\log x) \leq \log \expect(X) \]
%
which is used in the more advanced theory to obtain deeper inequalities.

Hoeffding's inequality is similar to Markov's inequality, but is generally much sharper. It therefore has a more complicated formula.

\begin{theorem}[Hoeffding's Inequality]
    Let $X_1, \dots, X_n$ be centrally distributed i.i.d random variables, with $a_i \leq X_i \leq b_i$, then for any $t > 0$,
    %
    \[ \prob \left( \sum X_i \geq x \right) \leq e^{-tx} \prod_{i = 1}^n e^{t^2 (b_i - a_i)^2/8} \]
\end{theorem}
\begin{proof}
    For any $t > 0$, Markov's inequality implies
    %
    \begin{align*}
        \prob \left( \sum X_i \geq x \right) &= \prob \left( t \sum X_i \geq tx \right)\\
        &= \prob \left( e^{t \sum X_i} \geq e^{tx} \right)\\
        &\leq e^{-tx} \prod \mathbf{E}[e^{t X_i}]
    \end{align*}
    %
    We can write
    %
    \[ X_i = \Lambda a_i + (1 - \Lambda) b_i \]
    %
    for some function $0 \leq \Lambda \leq 1$, and by applying the convexity of the exponential function, we find
    %
    \[ e^{tX_i} \leq \Lambda e^{t a_i} + (1 - \Lambda) e^{t b_i} \]
    %
    Hence
    %
    \[ \expect(e^{tX_i}) \leq \expect(\Lambda) e^{t a_i} + (1 - \expect(\Lambda)) e^{t b_i} \]
    %
    Now we may explicitly calculate $\Lambda = (X_i - a_i)/(b_i - a_i)$, so that
    %
    \[ \expect(e^{tX_i}) \leq \frac{a_i}{a_i - b_i} e^{t a_i} + \frac{b_i}{b_i - a_i} e^{t b_i} = e^{F(t(b_i - a_i))} \]
    %
    Where $F(x) = - \lambda x + \log(1 - \lambda + \lambda e^x)$, where $\lambda = a_i/(a_i - b_i)$. Note that $F(0) = F'(0) = 0$, and $F''(x) \leq 1/4$ for $x > 0$, so that by Taylor's theorem, there is $y \in (0,x)$ such that
    %
    \[ F(x) = \frac{x^2}{2} g''(y) \leq \frac{x^2}{8} \]
    %
    Hence $\expect(e^{tX_i}) \leq e^{t^2(b_i - a_i)^2/8}$, and this completes the proof.
\end{proof}

\begin{example}
    If $\widehat{p} \sim \Bin(n,p)$, and we take $X_i = (\widehat{p} - p)/n$, then $\mathbf{E}(X_i) = 0$, and $-p/n \leq X_i \leq 1/n-p/n$, and since $\sum X_i = \widehat{p} - p$, we find
    %
    \[ \prob(\widehat{p} - p \geq x) \leq e^{t^2/8n - tx} \]
    %
    For $t = 4nx$, we find $\prob(\widehat{p} - p \geq x) \leq e^{-2nx^2}$. By symmetry, we can calculate the absolute deviance as $\prob(|\widehat{p} - p| \geq x) \leq 2e^{-2nx^2}$. This gives us a much sharper rate of convergence than our last result.
\end{example}

\section{Subgaussian Random Variables}

Hoeffding's inequality only applies to bounded random variables. In the general case, we can't apply the inequality (which relies on the bounded intervals to use convexity), and Chebyshev's inequality often does not suffice. We should still obtain fast tail decay in most circumstances, say, for instance a Gaussian distribution with variance $\sigma^2$. Calculating, we find
%
\begin{align*}
    \mathbf{P}(X - \mu \geq y) &= \int_{y}^\infty \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{x^2}{2 \sigma^2}} dx\\
    &\leq \frac{1}{y \sqrt{2 \pi \sigma^2}} \int_{y}^\infty x e^{- \frac{x^2}{2\sigma^2}} dx\\
    &= \frac{\sigma e^{- \frac{y^2}{2 \sigma^2}}}{y \sqrt{2 \pi}}
\end{align*}
%
This quantity is almost always better than Chebyshev's inequality, since the ratio $x/y$, which measures the inaccuracy of our inequality, is nullified by the exponential function. We can find similar equalities for random variables which are `bounded' by normal distributions.

We shall say a random variable $X$ is $\sigma^2$-{\bf subgaussian} if for all $\lambda \in \mathbf{R}$,
%
\[ \mathbf{E}[e^{\lambda X}] \leq e^{\lambda^2 \sigma^2/2} \]
%
where we assume $e^{\lambda X}$ is integrable for all $\lambda$. Pointwise, we have
%
\[ e^{\lambda X} = \sum_{k = 0}^\infty \frac{\lambda^k X^k}{k!} \leq \sum_{k = 0}^\infty \frac{\lambda^{2k} \sigma^{2k}}{2^k k!} = e^{\lambda^2 \sigma^2/2} \]
%
since $e^{|\lambda| |X|}$ is integrable ($|X| = X^+ + X^-$, and the Cauchy Schwarz equality implies)
%
\[ \mathbf{E}[e^{|\lambda| X^+} e^{|\lambda| X^-}]^2 \leq \mathbf{E}[e^{2 |\lambda| X^+}] \mathbf{E}[e^{2 \lambda X^-}] < \infty \]
%
Thus we may apply the dominated convergence theorem to conclude
%
\[ \mathbf{E}[e^{\lambda X}] = \sum_{k = 0}^\infty \frac{\lambda^k}{k!} \mathbf{E}[X^k] \]
%
and for any $\lambda$,
%
\[ \sum_{k = 0}^\infty \frac{\mu^k}{k!} \mathbf{E}[X^k] \leq \sum_{k = 0}^\infty \frac{\mu^{2k} \sigma^{2k}}{2^k k!} \]
%
If $\mathbf{E}[X] > 0$, we may subtract by one and divide by $\mu \mathbf{E}[X]$ to conclude that for $\mu > 0$
%
\[ 1 + \mu \frac{\mathbf{E}[X^2]}{2 \mathbf{E}[X]} + \dots \leq \mu \frac{\sigma^2}{2} + \dots \]
%
and if we take $\mu \to 0$, we obtain $1 \leq 0$, a contradiction. If $\mathbf{E}[X] < 0$, the same equation holds for $\mu < 0$, so we must have $\mathbf{E}[X] = 0$. Similarily, the bound $\mathbf{V}[X] \leq \sigma^2$ is obtained by comparing coefficients.

\begin{example}
    If $X$ is a symmetric Bernoulli random variable with
    %
    \[ \mathbf{P}(X = -1) = \mathbf{P}(X = -1) = 1/2 \]
    %
    We have
    %
    \[ \mathbf{E}[e^{\lambda X}] = \frac{e^\lambda + e^{-\lambda}}{2} = \sum_{k = 0}^\infty \frac{\lambda^{2k}}{(2k)!} \leq \sum_{k = 0}^\infty \frac{(\lambda^2)^k}{2^k k!} = e^{\lambda^2/2} \]
    %
    so $X$ is a $1$ subgaussian random variable.
\end{example}

\begin{example}
    If $X$ is uniformly distributed on $[-n,n]$, then
    %
    \[ \mathbf{E}[X^k] = \int_{-n}^n \frac{x^k}{2n} dx = \frac{n^{k+1} - (-n)^{k+1}}{(k+1)2n} = \begin{cases} \frac{n^k}{k+1} & k\ \text{even} \\ 0 & k\ \text{odd} \end{cases} \]
    %
    So
    %
    \[ \mathbf{E}[e^{\lambda X}] = \sum_{k = 0}^\infty \frac{n^{2k} \lambda^{2k}}{(2k+1)(2k)!} \leq \sum_{k = 0}^\infty \frac{n^{2k} \lambda^{2k}}{2^k k!} = e^{n^2\lambda^2/2} \]
    %
    so $X$ is $n$-subgaussian.
\end{example}

\begin{example}
    In general, if a centrally distributed random variable $X$ satisfies $|X| \leq M$ almost surely, then $X$ is $M^2$ subgaussian. Assume without loss of generality that $M = 1$. Set $Y = X+1$, and
    %
    \[ f(t) = \frac{e^{2t} + 1}{2} - \mathbf{E}(e^{tY}) \]
    %
    Since $\mathbf{E}(Y) = 1$,
    %
    \[ f'(t) = \mathbf{E}(Y[e^{2t} - e^{tY}]) \]
    %
    since $Y \leq 2$ almost surely, $f'(t) \geq 0$, and so $f$ is increasing. In particular, $f(0) = 1 - 1 = 0$, so that for $t \geq 0$,
-    %
    \[ \mathbf{E}(e^{tX}) = e^{-t}\ \mathbf{E}(e^{tY}) \leq \frac{e^{t} + e^{-t}}{2} \leq e^{t^2/2} \]
    %
    Since we can perform the same argument for $-X$, we see that $X$ is $1$ subgaussian.
\end{example}

The set of subgaussian random variables form a vector space. If $X$ is a $\sigma^2$ subgaussian random variable, then $cX$ is $(c\sigma)^2$ subgaussian. If $Y$ is $\tau^2$ subgaussian, then, using the H\"{o}lder inequality, we find that for $p^{-1} + q^{-1} = 1$,
%
\[ \mathbf{E}[e^{\lambda(X+Y)}] = \mathbf{E}[e^{\lambda X} e^{\lambda Y}] \leq \mathbf{E}[e^{p \lambda X}]^{p^{-1}} \mathbf{E}[e^{q \lambda X}]^{q^{-1}} \leq e^{\frac{\lambda^2 \sigma^2}{2}} e^{\frac{\tau^2 q}{2}} = e^{\frac{\lambda^2}{2}(p \sigma^2 + q \tau^2)} \]
%
This value is minimized for $p = 1 + \tau/\sigma$, where
%
\[ p \sigma^2 + q \tau^2 = \sigma^2 + 2 \tau \sigma + \tau^2 = (\sigma + \tau)^2 \]
%
so $X + Y$ is $(\sigma + \tau)^2$ subgaussian. If $X$ and $Y$ are independant, then we actually have $X+Y$ a $\sigma^2 + \tau^2$ subgaussian variable. We can even make the set of subgaussian random variables into a Banach space, under the norm
%
\[ \sigma(X) = \inf \{ \sigma \geq 0 : X\ \text{is $\sigma^2$ subgaussian} \} \]
%
By continuity, $X$ is a $\sigma(X)$ subgaussian variable. The main reason for studying subgaussian random variables is that we obtain very good tail bounds for the distribution.

\begin{theorem}
    If $X$ is $\sigma^2$-subgaussian, then $\mathbf{P}(X \geq x) \leq e^{-x^2/2\sigma^2}$.
\end{theorem}
\begin{proof}
    Using Markov's inequality,
    %
    \begin{align*}
        \mathbf{P}(X \geq x) &= \mathbf{P}(e^{\lambda X} \geq e^{\lambda x})\\
        &\leq \mathbf{E}[e^{\lambda X}] e^{- \lambda x}\\
        &\leq e^{(\lambda^2 \sigma^2/2) - \lambda x}
    \end{align*}
    %
    The value of $\lambda$ which minizes this quantity $\lambda = x/\sigma^2$, which gives us the bound in question.
\end{proof}

The exponential decay of tails is exactly what specifies a subgaussian random variable. To prove this, note that if $\mathbf{P}(X \geq x) \leq e^{-x^2/2\sigma^2}$ holds, though we do not prove this.

\chapter{Existence Theorems}

In certain fields of probability theory, we wish to discuss collections of random variables defined over the same sample space. For instance, given a sequence $\mathbf{P}_1, \mathbf{P}_2, \dots, \mathbf{P}_n$ of probability distributions defined over a space $Y$, we may want to talk about a sequence of independent random variables $X_i: \Omega \to Y$, such that $\mathbf{P}(X_i \in U) = \mathbf{P}_i(U)$. The construction here is simple; we take $\Omega = Y^n$, let $X_i = \pi_i$ be the projection on the $i$'th variable, and let $\mathbf{P}$ be the probability measure induced by
%
\[ \mathbf{P}(U_1 \times U_2 \dots \times U_n) = \mathbf{P}_1(U_1) \mathbf{P}_2(U_2) \dots \mathbf{P}_n(U_n) \]
%
The construction here is simple because we have finitely many distributions, but the problem becomes much harder when we need to talk about an infinite family of distributions $\mathbf{P}_i$, or when we need to talk about non-independent random variables, with some specified relationships between the variables. The problem is to show there exists a sample space $\Omega$ `big enough' for the random variables to all be defined on the space.

\chapter{Entropy}

Let $\mu$ be a probability distribution. We would like to measure the expected `amount of information' contained in the distribution -- in essence, the average information entropy of $\mu$. It was Claude Shannon who found the correct formula to measure this.

Shannon considered the problem of efficient information transfer. Suppose there was a channel of communication between two friends $A$ and $B$. The friends have agreed on a standard dictionary $X$ of possible messages, along with a probability distribution $\mu$ over the dictionary, and we would like to encode these messages into bits, in such a way that the average length of the message is smallest. We then define this to be the information entropy of $\mu$. Shannon showed that if $\mu$ is discrete with probabilities $p_1, \dots, p_n$, then the entropy can be calculated as
%
\[ H(\mu) = \sum p_n \log_2 \left( \frac{1}{p_n} \right) \]
%
where the entropy is measured in bits, we can define the entropy in terms of the natural logarithm, in which case the entropy is said to be measured in nats. We assume that $p_i \log 1/p_i = 0$ for $p_i = 0$, which makes sense by the continuity of $x \log (1/x)$.

The entropy of a distribution also tells us 

Now suppose that we were attempting to optimize a message with respect to a discrete distribution $\mu$, and we instead encounter a distribution $\nu$. Then the policy we have used for messages will be less optimal than if we had known that $\nu$ was the distribution in the first place. We define the relative difference in information between $\mu$ and $\nu$ as the difference between the encoding of $\nu$ with respect to $\mu$, and the encoding of $\mu$ with respect to $\mu$. This is not a linearly ordered relation, $\nu$ does not possess more information than $\mu$, just different information. If $\mu$ takes probabilities $p_i$ and $\nu$ takes relative probabilities $q_i$, the difference in information is calculated to be
%
\[ D(\mu, \nu) = \sum p_i \log(1/q_i) - \sum p_i \log(1/p_i) = \sum p_i \log(p_i/q_i) \]
%
This is known as the {\bf Kullback Leibler distance} between $\mu$ and $\nu$.

Now suppose we are viewing independent samples $X_1, \dots, X_n$, but we do not know where the samples are drawn from $\mu$ or $\nu$. The larger $D(\mu, \nu)$ is, the less time we should take to make an accurate decision that the distribution is $\mu$ or $\nu$. Indeed, if $p_i > 0$ and $q_i = 0$, then $D(\mu, \nu) = \infty$, and we can conclude with certainty that the distribution is $\mu$ if we ever view the outcome corresponding to $p_i$.

It is necessary to define the `entropy' of an arbitrary distribution, but it is then not clear how to interpret the entropy, since an encoding of uncountably many values will always have an infinite expected number of bits. However, we can defined the relative entropy by performing a discretization; Let $\mu$ and $\nu$ be distributions on some sample space $X$. Consider function $f: X \to \{ 1, \dots, n \}$, and define
%
\[ D(\mu, \nu) = \sup_f D(f_* \mu, f_* \nu) \]
%
where $f_*$ pushes measures on $X$ onto discrete measures on $\{ 1, \dots, n \}$. For a fixed $f$, $D(\mu, \nu)$ upper bounds the difference in information we expect to see over a particular discretization. One can then calculate that
%
\[ D(\mu, \nu) = \begin{cases} \infty &: \mu \not \ll \nu \\ \int \log(\frac{d\mu}{d\nu}) d\mu &: \mu \ll \nu \end{cases} \]
%
The relative entropies of well known distributions are easy to compute. Normal distributions, for instance, have
%
\[ D(N(\mu_1, \sigma^2), N(\mu_2, \sigma^2)) = (\mu_1 - \mu_2)^2/2\sigma^2 \]
%
For Bernoulli distributions, we have
%
\[ D(B(p), B(q)) = p \log(p/q) + (1-p)\log \left( \frac{1-p}{1-q} \right) \]
%
Which is true except perhaps at boundary conditions.

The Kullback Leibler distance gives us certain bounds which are essential to information theoretic lower bounds. The bound is useful, for it relates the probabilities of distributions by the difference in information contained within.

\begin{theorem}[The High Probability Pinsker Bound]
    If $\mu$ and $\nu$ are probability measures on the same space $X$, and $U \subset X$ is measurable, then
    %
    \[ \mu(A) + \nu(A^c) \geq \frac{1}{2} e^{- D(\mu, \nu)} \]
\end{theorem}

Suppose we have a decision procedure which attempts to distinguish between events in probability distributions. If we choose an event $A$ upon which the decision procedure fails to make the correct decision on the measure $\mu$, and $A^c$ measures the decision to fail under the measure $\nu$, then the bound above shows the decision procedure cannot work reliably on both $\mu$ and $\nu$.




















\chapter{Appendix: Uniform Integrability}

Uniform integrability provides stronger conditions on controlling convergence in the $L^1$ norm. For $p > 1$, inequalities often have `smoothing' properties that are not apparent for the $p = 1$ case, so uniform integrability provides particular techniques to help us. We start with a basic result in measure theory, specialized to probabilistic language.

\begin{lemma}
    If $X \in L^1(\Omega)$ is a random variable, then for any $\varepsilon > 0$, there is $\delta > 0$ such that for any event $E$ with $\mathbf{P}(E) \leq \delta$,
    %
    \[ \int_E |X| < \varepsilon \]
\end{lemma}
\begin{proof}
    Suppose that there exists some $\varepsilon$, and events $E_1, E_2, \dots$ with $\mathbf{P}(E_k) \leq 1/2^k$ but with
    %
    \[ \int_{E_k} |X| \geq \varepsilon \]
    %
    By taking successive unions, we may assume the $E_i$ are a decreasing family of sets, and then
    %
    \[ \int_{\bigcap_{k = 1}^\infty E_k} |X| = \lim_{k \to \infty} \int_{E_k} |X| \geq \varepsilon \]
    %
    and $\mathbf{P}(\bigcap E_k) = 0$, which is impossible.
\end{proof}

\begin{corollary}
    If $X \in L^1(\Omega)$, and $\varepsilon > 0$, then there is $K \in [0,\infty)$ with
    %
    \[ \int_{|X| > K} |X| < \varepsilon \]
\end{corollary}

A family of random variables $\{ X_\alpha \}$ is called {\bf uniformly integrable} if given $\varepsilon > 0$, there is $K \in [0,\infty)$ such that
%
\[ \int_{|X_\alpha| > K} |X_\alpha| < \varepsilon \]
%
so that we can uniformly control the integral of $X_\alpha$ over large sets. We note that
%
\[ \mathbf{E} |X_\alpha| = \int_{|X_\alpha| > K} |X_\alpha| + \int_{|X_\alpha| \leq K} |X_\alpha| \leq \varepsilon + K \]
%
so a family of uniformly integrable random variables is automatically in $L^1(\Omega)$, and {\it bounded} in $L^1(\Omega)$. However, a family of random variables bounded in $L^1(\Omega)$ is {\it not} necessarily uniformly integrable.

\begin{example}
    Let $\Omega$ be $[0,1]$ together with the Lebesgue measure. Let $E_n = (0,1/n)$, and set $X_n = n \chi_{E_n}$. Then the $X_n$ are bounded in $L^1(\Omega)$, but for $n \geq K$,
    %
    \[ \int_{X_n > K} X_n = 1 \]
    %
    and so the random variables are not uniformly integrable.
\end{example}

These `concentrating bumps' are essentially the only reason why we cannot always exchange expectations and limits, and require the application of the dominated convergence theorem. The condition of uniform integrability removes the ability for concentrating bumps to hide within the expectation of a family of random variables, and we find it also gives us conditions that guarantee we can exchange limits with integration. First, note that if we take a concentrated bump function $X = n \chi_{E_n}$, then $\mathbf{E}|X| = 1$ is bounded uniformly over $n$, but $\mathbf{E}|X|^{1+\varepsilon} = n^\varepsilon$ is unbounded, reflecting the fact that boundedness in $L^p(\Omega)$ for $p > 1$ removes concetrated bump functions by magnifying their effect.

\begin{theorem}
    Suppose that $\{ X_\alpha \}$ is a class of random variables bounded in $L^p$ for $p > 1$, then $\{ X_\alpha \}$ is uniformly integrable.
\end{theorem}
\begin{proof}
    Consider some $A \in [0,\infty)$ which gives a uniform bound $\mathbf{E} |X_\alpha|^p < A$. Applying H\"{o}lder's inequality, we conclude
    %
    \[ \int_{|X_\alpha| > K} |X_\alpha| \leq \int_{|X_\alpha| > K} \frac{|X_\alpha|^p}{K^{p-1}} \leq \frac{A}{K^{p-1}} \]
    %
    This is a uniform bound, and we may let $K \to \infty$ to let the bound go to zero. Thus the family $\{ X_\alpha \}$ is uniformly integrable.
\end{proof}

\begin{corollary}
    If $|X_\alpha| \leq Y$ is a uniform bound over a family $\{ X_\alpha \}$ of random variables, where $Y \in L^1(\Omega)$, then $\{ X_\alpha \}$ is uniformly integrable.
\end{corollary}
\begin{proof}
    We find
    %
    \[ \int_{|X_\alpha| > K} |X_\alpha| \leq \int_{|X_\alpha| > K} Y \leq \int_{Y > K} Y \]
    %
    and as $K \to \infty$, $\mathbf{P}(Y > K) \to 0$, and we can apply the continuity result to conclude that
    %
    \[ \int_{Y > K} Y \to 0 \]
    %
    and so we obtain a uniform bound.
\end{proof}

We recall that a sequence $X_1, X_2, \dots$ of random variables {\bf converges in probability} to a random variable $X$ if, for every $\varepsilon$, $\mathbf{P}(|X_n - X| > \varepsilon) \to 0$. If $X_i \to X$ almost surely, then $X_i \to X$ in probability, because we can apply the reverse Fatou lemma to conclude
%
\begin{align*}
    0 &= \mathbf{P} \left( \liminf_{n \to \infty} |X_n - X| > \varepsilon \right)\\
    &= \mathbf{P} \left( \limsup \{ |X_n - X| > \varepsilon \} \right) \geq \limsup \mathbf{P}(|X_n - X| > \varepsilon)
\end{align*}
%
Hence $\mathbf{P}(|X_n - X| > \varepsilon) \to 0$. The bounded convergence theorem links $L^1$ convergence to convergence in probability using uniform integrability.

\begin{theorem}
    If $X_n$ is a sequence of bounded random variables which tend to a random variable $X$ in probability, then $X_n \to X$ in the $L^1$ norm.
\end{theorem}
\begin{proof}
    Let us begin by proving that if $|X_n| \leq K$, then $|X| \leq K$ almost surely. This follows because for any $k$,
    %
    \[ \mathbf{P}(|X| > K + 1/k) \leq \mathbf{P}(|X - X_n| > 1/k) \to 0 \]
    %
    so $\mathbf{P}(|X| > K + 1/k) = 0$, and letting $k \to \infty$ gives $\mathbf{P}(|X| > K) = 0$. Let $\varepsilon > 0$ be given. Then if we choose $n$ large enough that
    %
    \[ \mathbf{P}(|X_n - X| > \varepsilon) \leq \varepsilon \]
    %
    then
    %
    \begin{align*}
        \mathbf{E}|X_n - X| &= \int_{|X_n - X| > \varepsilon} |X_n - X| + \int_{|X_n - X| \leq \varepsilon} |X_n - X|\\
        &\leq 2K\varepsilon + \varepsilon
    \end{align*}
    %
    we can then let $\varepsilon \to 0$ to obtain $L^1$ convergence.
\end{proof}

All this discussion concludes with a sufficient condition for $L^1$ convergence, showing that uniform integrability is really the right condition which removes the pathologies which prevent us from exchanging expectation with pointwise limits.

\begin{theorem}
    Let $X_n$ be a sequence of integrable random variables, and $X$ is another integrable random variable. Then $X_n \to X$ in the $L^1$ norm if and only if $X_n \to X$ in probability, and $\{ X_n \}$ is uniformly integrable.
\end{theorem}
\begin{proof}
    Fix $K > 0$, and consider
    %
    \[ f_K(x) = \begin{cases} K &: x > K \\ x & :|x| \leq K\\ -K & x < -K \end{cases} \]
    %
    Then for every $\varepsilon > 0$, we can choose $K$ such that $\| f_K(X_n) - X_n \|_1 \leq \varepsilon$, $\| f_K(X) - X \|_1 \leq \varepsilon$ {\it uniformly across n} (adding a single variable to a uniformly integrable random variable keeps it uniformly integrable). But it is easy to see that $f_K(X_n) \to f_K(X)$ in probability also, so by the bounded dominated convergence theorem, we conclude that $\| f_K(X_n) - f_K(X) \| \to 0$. A triangle inequality result gives the general result because the behaviour of $X$ for large values is bounded by the uniform integrability.

    To verify the reverse condition, note that if $\mathbf{E}|X_n - X| \to 0$, then Markov's inequality gives
    %
    \[ \mathbf{P}(|X_n - X| \geq K) \leq \frac{\mathbf{E}|X_n - X|}{K} \to 0 \]
    %
    to obtain uniform integrability, note that for each $n$, $\{ X_1, \dots, X_n, X \}$ is uniformly integrable, then for each $\varepsilon > 0$, there is $\delta$ such that if $\mathbf{P}(E < \delta)$,
    %
    \[ \int_E |X_n| < \varepsilon\ \ \ \ \ \int_E |X| < \varepsilon \]
    %
    Since the entire set of $X_n$ are bounded in $L^1(\Omega)$, we can choose $K$ such that $\sup \mathbf{E} |X_k| < \delta K$, and then for $m > n$, $\mathbf{P}(|X_m - X| > K) < \delta$, and so
    %
    \[ \int_{|X_m| > K} |X_m| \leq \int_{|X_m| > K} |X| + \mathbf{E}|X - X_m| \leq 2\varepsilon \]
    %
    where we assume we have chosen $n$ large enough that $\mathbf{E}|X - X_m| \leq \varepsilon$. The fact that for $m \leq n$,
    %
    \[ \int_{|X_m| > K} |X_m| \leq \varepsilon \]
    %
    follows from uniform integrability of the family $\{ X, X_1, \dots, X_n \}$, so we have shown the entire infinite sequence is uniformly integrable.
\end{proof}




















\chapter{Percolation Theory}

Let us consider the two dimensional theory of percolation. The two examples we have in mind are the lattice $\mathbf{Z}^2$, and the triangular lattice $\mathbf{T}$. For any $p \in [0,1]$, we define a graph structure on $\mathbf{Z}^2$, adding an edge between two adjacent elements of the lattice with independant probability $p$. On $\mathbf{T}$, we instead consider {\it site percolation}, where we keep a hexagon with probability $p$.

\begin{theorem}[Russo-Seymour-Welsh]
    If $p = 1/2$, then for any $a,b > 0$, there exists $c$ such that if $A_n$ denotes the event that we can travel from the left edge to the right edge of the lattice $[0,a \cdot n] \times [0, b \cdot n] \cap \mathbf{Z}^2$, then $c < \mathbf{P}(A_n) < 1 - c$.
\end{theorem}

One of the main problems in percolation theory is to determine how likely it is to find an infinite connected set of vertices, or cluster, in the randomly selected graph. As the probability of each edge becomes more likely, the graph becomes more and more connected. We find that for $p > 1/2$, there is almost surely an infinite cluster, and for $p < 1/2$, there is almost surely {\it not} a cluster. The value $p = 1/2$ is therefore called the {\it phase transition} point. A very related value to the phase transition problem is the percolation density function $\theta$, which for each $p$, gives the probability $\theta(p)$ of the origin being in an infinite cluster of the graph. As an example, it is known that on $\mathbf{T}$, $\theta(p) = (p - 1/2)^{5/36 + o(1)}$, as $p \downarrow 1/2$. Determining phase transition points is the main focus of this chapter's notes.

\section{Duality}

Note an important duality in these geometric scenarios. Given any graph on $\mathbf{Z}^2$, we can obtain another graph, the dual graph, by taking the vertices as unit squares with corners on $\mathbf{Z}^2$, and with an edge between adjacent squares if there is no edge separating the two squares. Then the probability that there is an edge between two squares is the same as the probability that we do {\it not} select the corresponding separating edge, i.e. with probability $1 - p$. This will be useful.

The book says the dual graph of $\mathbf{T}$ is $\mathbf{T}$, but I don't quite understand why?

\section{Boolean Functions and Sharp Thresholds}

If $G$ is a graph, then the family of all graph structures on these vertices can be identified with $\{ 0, 1 \}^E = \{ 0, 1 \}^{O(V^2)}$. Thus a function $f$ on the set of graphs can be identified with a boolean function, and we can apply boolean function techniques. For the subgraphs of $[0, a \cdot n] \times [0, b \cdot n]$, we can define $f_n(G) = \mathbf{I}(\text{There is a path from left to right in $G$})$. Then $f_n$ is a boolean function, and so we can use boolean analysis. Here we introduce some basics, which will help us get the job done.

If $f$ is a boolean function, we say it is monotone if $x_i \leq y_i$ for each $i$ implies $f(x) \leq f(y)$. An index $i$ is pivotal for an input $x$ if $f(x) \neq f(x^i)$, where $i$ is $x$ with the bit flipped at the $i$'th position. The influence $\mathbf{I}_i(f)$ of $f$ in the variable $i$ is then the probability that for a randomly chosen $x$, $i$ is pivotal. If we instead choose an input to be equal to one with probability $p$, and zero with probability $1 - p$, then the probability that $i$ is pivotal is denoted $\mathbf{I}_i^p(f)$. The sum of the influence over all influences $i$ is known as the {\it total influence}. Now if $E$ is a monotone event, then it is obvious that as $p$ increases, $\mathbf{P}(E)$ should increase. The degree to which it increases is quantified by the Margulis-Russo formula.

\begin{theorem}[Margulis-Russo]
    Let $E$ be monotone. Then $d\mathbf{P}(E)/dp = \mathbf{I}^p(E)$.
\end{theorem}
\begin{proof}
    Temporarily, let $\mathbf{I}_i^{p_1, \dots, p_n}(E)$ denote the chance that $X^i \neq X$, where the $X_j \in \{ 0, 1 \}$ are chosen uniformly at random with $\mathbf{P}(X_j = 1) = p_i$. Define $\mathbf{I}^{p_1, \dots, p_n}(E) = \sum \mathbf{I}^{p_1, \dots, p_n}_i(E)$. It suffices to show $\partial \mathbf{P}(E) / \partial p_i = \mathbf{I}_i^{p_1,\dots,p_n}(E)$, from which we can use the chain rule. We can write $E$ as the union of two disjoint events $E_0$ and $E_1$, where $E_0 = E \cap \{ \chi_E(X^i) \neq \chi_E(X) \}$, and $E_1 = E \cap \{ \chi_E(X^i) = \chi_E(X) \}$. Now $E_1$ does not depend on the value of $X_1$ at all, so $\mathbf{P}(E_1)$ is independant of $p_i$, and so
    %
    \[ \frac{\partial \mathbf{P}(E_1)}{\partial p_i} = 0 \]
    %
    On the other hand, by monotonicity, $E_0$ then equals the probability that $\chi_E(X^i) \neq \chi_E(X)$, intersected with the event $X_i = 1$. These two events are independant, so $\mathbf{P}(E_0) = p_1 \mathbf{P}(\chi_E(X^i) \neq \chi_E(X))$. The latter probability does not depend on the index $1$, so
    %
    \[ \frac{\partial \mathbf{P}(E_0)}{\partial p_i} = \mathbf{P}(\chi_E(X^i) \neq \chi_E(X)) = \mathbf{I}^{p_1, \dots, p_n}_1(E) \]
    %
    This completes the proof.
\end{proof}

To analyze the critical exponent, we rely on two results we will prove later on using Fourier analysis, which allow us to upper bound the influence by the variance of a function.

\begin{theorem}[Bourgain, Kahn, Kalai]
    For any $f$ and $p$, there exists $i$ such that $\mathbf{I}_i^p(f) \gtrsim \mathbf{V}_p(f) [\log n / n]$, and $\mathbf{I}^p(f) \gtrsim \mathbf{V}_p(f) \log(1/\max \mathbf{I}_i^p(f))$.
\end{theorem}

We also rely on a fact that, for exponentially many boolean inputs, the probability that a monotone event happens jumps from being unlikely to being likely in a very small small range of $p$ values, i.e. of length approximately $1/\log n$. Think like the majority function. Once $p > 1/2$, the chance of a vote passing grows rapidly in $p$.

\begin{theorem}[Friedgut, Kalai]
    If $A$ is a monotone event, whose influences are the same for each index, and for $p = p_0$, if $\mathbf{P}(A) > \varepsilon$, then for $p \geq p_0 + c \log(1/\varepsilon)/\log n$, $\mathbf{P}(A) > 1 - \varepsilon$.
\end{theorem}

\begin{proof}
    If all the influences are the same, we find the total variance is
    %
    \[ \mathbf{I}^p(E) \gtrsim \mathbf{V}_p(\chi_E) \log n = \min (\mathbf{P}(E), 1 - \mathbf{P}(E)) \log n \]
    %
    Now the Margulis-Russo formula yields that if $\mathbf{P}(E) \leq 1/2$,
    %
    \[ \frac{d(\log \mathbf{P}(E))}{dp} = \frac{\mathbf{I}^p(E)}{\mathbf{P}(E)} \gtrsim \log n \]
    %
    Thus if $p \geq p_0 + c/\log n$, $\mathbf{P}(E) \geq 1/2$. Now if $\mathbf{P}(E) \geq 1/2$, then
    %
    \[ \frac{d(\log(1 - \mathbf{P}(E)))}{dp} = - \frac{\mathbf{I}^p(E)}{1 - \mathbf{P}(E)} \lesssim - \log n \]
    %
    In order to make $\mathbf{P}(E) \geq 1-\varepsilon$, we need $\log(1 - \mathbf{P}(E)) \leq \log \varepsilon$. To move from $\log(1/2)$ to $\log \varepsilon$, we need $p \geq p_0 + c/\log n + c \log(1/\varepsilon)/\log n = p_0 + c \log(1/\varepsilon)/\log n$.
\end{proof}

We now try and prove the critical exponent for the square lattice is $1/2$. First, we show $\theta(1/2) = 0$. Consider the `square annulus' between $4^n$ and $3 \cdot 4^n$, and let $E_n$ be the event that there is a ring  in this annulus. Then the $E_n$ are independant and the probability of each one happenining is bounded below (I don't see the exact proof, but it makes sence since the rings are thicker to balance out the distance you must travel, why are we able to apply the last point?). Thus infinitely many occur almost surely, so $\theta(1/2) = 0$.

To form a contradiction, we assume the critical point is $1/2 + \delta$ instead of $1/2$. Given $n$, we form a $2n \times n$ box, and we let $J_n$ denote the event that there is a crossing in the box, i.e a path from left to right and an edge from bottom to top.

\begin{lemma}
    As $n \to \infty$, $\max \mathbf{I}_e^{p}(J_n) \to 0$, where the maximum is taken over $1/2 \leq p \leq 1/2 + \delta/2$ and all edges $e$.
\end{lemma}
\begin{proof}
    If $e$ is pivotal on a given input for $J_n$, that means there is a path from a vertex adjacent to $e$ to a vertex a distance $n/2$ away. Thus by translation invariance, $\mathbf{I}_e^p(J_n)$ is bounded by half the probability that there is a path of length $n/2$ from the origin, which is uniformly bounded for $p \leq 1/2 + \delta/2$. As $\theta(1/2 + \delta/2) = 0$, these values must tend to zero as $n \to \infty$.
\end{proof}

\begin{lemma}
    For some $n$, and with $p = 1/2 + \delta/2$, $\mathbf{P}(J_n) \geq 0.98$.
\end{lemma}
\begin{proof}
    We have already seen that $\inf \mathbf{P}(J_n) > 0$ when $p = 1/2$. If $\mathbf{P}(J_n) < 0.98$ for all $n$, then BKS shows that $d\mathbf{P}(J_n)/dp$ must tend to $\infty$ uniformly for all $1/2 \leq p \leq 1/2 + \delta/2$ as $n \to \infty$, by setting $\varepsilon < 0.02$. This means $\mathbf{I}^p(J_n) \to \infty$ uniformly. But why is that a contradiction?
\end{proof}

I'm unsure how to boost the last point to get the infinite clustering criterion. The proof says to use FKG, which only works when we change the $p$ values, yet the $p$ value remains fixed in the proof. And I don't see how to place blocks intelligently since the paths could take any form probabilistically.

\section{Conformal Invariance}

Brownian motion is the limit of a random walk, and in the plane, is conformally invariant, in the sense that the image of a path of Brownian motion under an analytic map looks like the path of Brownian motion, up to a change in the measurement of time. Thus a random walk is conformally invariant `in the limit'. In some sense, percolation should also look `asymptotically' conformally invariant in the limit.

Let us describe what this principle should look like when discretized. We consider a conformal map $\phi$ from the unit disk to some other simply connected domain $D$ fixing the origin, and with $\phi'(0) > 0$. Now for any $\delta$, we can consider the lattice $\delta \mathbf{Z}^2$, restricted to the interior of the unit disk. If $C_\delta$ is the cluster around the origin in the interior. Similarily, we define $C_\delta'$ to be the cluster around the origin in $\delta \mathbf{Z}^2$ restricted to $D$. Now $\phi(C_\delta)$ and $C_\delta'$ don't even lie on the same lattice, but as $\delta \to 0$, they should still asymptotically describe the same law on space.

The simplest precise statement of conformal invariance was proved by Smirnov in 2001. Scale the hexagonal percolation problem by $\delta$ at the critical percolation value $p = 1/2$. If four points $A,B,C$, and $D$ are chosen on the boundary of $D$, then the probability that there is a path from points on the boundary of $D$ between $A$ and $B$ to points on the boundary of $D$ between $C$ and $D$ converges as $\delta \to 0$. Furthermore, this convergent value is invariant under conformal mappings. In the case where $D$ is a sidelength one equilateral triangle, $A$, $B$, and $C$ are the three corner points, and $D$ is on the line between $A$ and $C$ with distance $x$ from $C$, the probability converges to $x$. By conformal invariance, this gives a general way to calculate the limiting probability. On $\mathbf{Z}^2$, even this statement is still open.





\begin{thebibliography}{10}
    \bibitem{intro} Larry Wasserman,
    \emph{All of Statistics}

    \bibitem{papa} Walter Rudin,
    \emph{Real and Complex Analysis}
\end{thebibliography}

\end{document}











\begin{example}
    The uniform distribution works well in modelling a fair coin, but what if we are flipping an unfair coin? The Bernoulli distribution assigns a probability of $p$ to getting a heads, and $1-p$ to getting a tails, for some parameter $p$. We write $\mu \sim \Ber(p)$ for this distribution. We normally rename the elements of the sample space by letting $\text{Heads} = 1$ and $\text{Tails} = 0$, so that the probability distribution counts the number of heads and tails from one coin flip. This permits an easy generalization to the Binomial distribution $\Bin(n,p)$, which counts the number of times we get a head in $n$ coin flips from a $\Ber(p)$ distribution. It is defined on $\mathbf{Z}$ by letting
    %
    \[ \prob(m) = \binom{n}{m} p^m (1-p)^{n-m} \]
    %
    for $0 \leq m \leq n$, and $\prob(m) = 0$ otherwise. If $\mu \sim \Bin(n,p)$, then we have a representation $\mu = \nu * \dots * \nu$ as an $n$-ary convolution, where $\nu \sim \Ber(p)$. In general, if $\mu$ and $\nu$ are probability measures on a group $G$, then $\mu * \nu$ is a probability measure, since
    %
    \[ (\mu * \nu)(X) = \int_{X^2} d\mu(x)\ d\nu(y) = \int_X d\mu(x) \ \int_X d\nu(Y) = 1 \]
    %
    which represents a certain `sum' over the two distributions, especially in the case $G = \mathbf{Z}$.
\end{example}

\begin{example}
    The Geometric distribution on $\mathbf{N}$ gives us the probability distribution of the number of flips required to get a heads from a weighted coin. If $\mu \sim \Geo(p)$, then
    %
    \[ \mu(m) = p (1-p)^m \]
    %
    The infinite sum over all $m \geq 0$ is easily evaluated to be one, so that the distribution truly is a probability measure.
\end{example}

\begin{example}
    The Poisson distribution on $\mathbf{N}$ is defined, for $\mu \sim \Poisson(\lambda)$, by letting
    %
    \[ \mu(n) = \lambda^n \frac{e^{-\lambda}}{n!} \]
    %
    This distribution is usually used to count rare, independant events, like for radioactive decay and traffic accidents. If $\mu \sim \Poisson(\lambda)$ and $\nu \sim \Poisson(\gamma)$, then $\mu * \nu \sim \Poisson(\lambda + \gamma)$.
\end{example}

To obtain measures on uncountable sample spaces, we rely on integration theory.

\begin{example}
    If $f \geq 0$ is a measurable function on a measure space $X$ with measure $\mu$ with $\| f \|_1 = 1$, then we may define a probability measure $\nu$ on $X$ by letting
    %
    \[ \nu(A) = \int_A f d\mu \]
    %
    The measure $\nu$ is often denoted $f d\mu$ because of the integral formula we use to obtain it.
\end{example}

The method in the last example gives us a number of canonical distributions, giving us a wide number of examples. A theorem of measure theory (the Radon-Nikodym theorem) tells us why almost all the important probability distributions can be given by integration (or summation, those measure theory tells us these are essentially the same processes).

\begin{example}
    We would like to have a probability distribution which models picking a point `uniformly' from an interval $[a,b]$. What we really mean is that the probability measure is translation invariant, in the sense that $\prob(A + t) = \prob(A)$, for all $A$ and $t$ for which this should be reasonably defined. What we are looking for is exactly the normalized Lebesgue measure restricted to $[0,1]$, which is rigorously defined in measure theory. We take the lebesgue measure $dx$, and then normalize to obtain a probability measure $dx/(b-a) \sim \Uni(a,b)$. It seem paradoxical that the probability that any particular point is chosen is zero, yet in an experiment we must choose a particular point. However, to obtain probabilities we take the limit of ratios over an infinite number of experiments, and it is entirely possible for the limit of the ratios to converge to zero, even if a point is chosen in some experiment.
\end{example}

\begin{example}
    Define a function $f:\mathbf{R} \to \mathbf{R}$ by the formula
    %
    \[ f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}} \]
    %
    the integral of this function over $\mathbf{R}$ is one, and is proved in multivariate calculus courses. The probability measure generated by $f$ is known as the {\bf normal distribution} with mean $\mu$ and standard deviation $\sigma$. We write $\prob \sim N(\mu, \sigma^2)$ if $\prob$ for such a measure.
\end{example}

\begin{example}
    The exponential distribution $\Exp(\beta)$ is defined on the positive real numbers by the measure induced by the lebesgue measure, and the function
    %
    \[ f(x) = \frac{e^{-x/\beta}}{\beta} \]
    %
    It measures the waiting times between rare events that occur at continuous times.
\end{example}

\begin{example}
    The Gamma distribution is defined with respect to the remarkable function known as the Gamma function
    %
    \[ \Gamma(x) = \int_0^\infty y^{x-1} e^{-y} dy \]
    %
    The Gamma distribution $\Gamma(\alpha, \beta)$ with positive parameters $\alpha$ and $\beta$ is induced from the Lebesgue measure by the function
    %
    \[ f(x) = \frac{x^{\alpha-1} e^{-x/\beta}}{\beta^\alpha \Gamma(\alpha)} \]
    %
    We have $\Exp(\beta) = \Gamma(1,\beta)$.
\end{example}

\begin{example}
    The Beta distribution $\Beta(\alpha, \beta)$ for positive $\alpha, \beta > 0$ is induced by the function
    %
    \[ f(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha-1} (1 - x)^{\beta - 1} \]
    %
    It is a continuous version of the Binomial distribution.
\end{example}

\begin{example}
    The $t$ distribution with $\nu$ degrees of freedom, denoted $t_\nu$, if it is the measure taken by
    %
    \[ f(x) = \frac{\Gamma(\frac{\nu + 1}{2})}{\Gamma(\frac{\nu}{2})} \frac{1}{(1 + x^2/\nu)^{(\nu+1)/2}} \]
    %
    It is similar to the normal distribution, but has thicker tails. In fact, as $\nu \to \infty$, the distributions corresponding to $t_\nu$ look more and more like the normal distribution. The Cauchy distribution is the $t$ distribution with $\nu = 1$, defined by
    %
    \[ f(x) = \frac{1}{\pi(1 + x^2)} \]
\end{example}

\begin{example}
    The $\chi^2$ distribution with $p$ degrees of freedom, denoted $\chi_p^2$, has distribution induced on the positive real numbers by
    %
    \[ f(x) = \frac{x^{(p/2) - 1} e^{-x/2}}{\Gamma(p/2) 2^{p/2}} \]
    %
    if $\mu \sim N(0, 1)$, and $\nu$ is a measure defined on the positive real numbers by letting $\nu(A) = \mu(B)$, where $B = \{ x \in \mathbf{R}: x^2 \in A \}$, then the $p$-adic convolution $\nu * \dots * \nu \sim \chi_p^2$.
\end{example}