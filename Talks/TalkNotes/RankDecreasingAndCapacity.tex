% LaTeX template for Summer Schools
%
% !!!!!!!!!!!!   INSTRUCTIONS  !!!!!!!!!!!!!!

% 0) Submit your work in tex format (not pdf). It will be pasted into the proceedings file.
%
% 1) please name your file yourlastname.tex, e.g. thiele.tex.
%
% As several files will be concatenated, please 
% use some discipline as to the following:
%
% 2) Whenever you use \label{} to get automatic cross references
% through \ref{} (this is the preferred option for cross references)
% please add your initials to the label such as 
% \label{SLEct} for the Stochastic Loewner equation
% with initials of author Christoph Thiele
% Same with other citations such as in bibitem.
%
% 3) Please STRONGLY avoid using \def or \newcommand unless really necessary.
% We do have macros for black board bold. If you use your favorite
% macros while preparing your summary, please expand them
% (replace by the original definition) everywhere in your file.
% This will save me the work of doing the very same thing.
% Thank you! If you have to use \def, please also add your
% initials to the definition.
%
% 4) if you want to compile the header of the document, 
% uncomment remove the corresponding paragraph signs below
%
% 5) There is a sample lecture below. For the header 
% it is best to keep most of the
% commands and just change the name, title, text. etc
%
% 6) Please follow the conventions below in terms of capitalization of 
% headings etc:
% Only the beginning of a sentence and names are capitalized.


\documentclass[12pt]{article}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{esint}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{comment}
\usepackage{bbm,dsfont}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{proposition}[theorem]{Proposition}

\newcommand{\talktitle}[1]{\section{#1}}
\newcommand{\talkafter}[1]{\textbf{After #1} \addcontentsline{toc}{subsection}{after #1}}
\newcommand{\talkspeaker}[2]{\begin{center}
\textit{A summary written by #1}
\end{center}
\addcontentsline{toc}{subsection}{#1, #2}
}

\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[hidelinks]{hyperref}

\newcommand*{\Z}{\mathbb{Z}}
\newcommand*{\Q}{\mathbb{Q}}
\def\C{\mathbb{C}}
\newcommand*{\N}{\mathbb{N}}
\newcommand*{\R}{\mathbb{R}}

% Absolute values and norms using mathtools.
% \\[lr][vV]ert produces correct spacing, as opposed to | and \|.
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}

\title{Brascamp--Lieb inequalities}

% \author{Summer School, Kopp}
% \thanks{supported by Hausdorff Center for Mathematics, Bonn}

% \date{September 2021}



\begin{document}

% \maketitle

% {
% \center{Organizers:}
% \center{
% Christoph Thiele, Universit\"at Bonn}
% \center{
% Pavel Zorin-Kranich, Universit\"at Bonn}
% \center{$\ $}
% }
% \newpage
% \tableofcontents

% \newpage


\talktitle{Capacity of Rank Decreasing Operators}
\talkafter{Gurvits \cite{gurv2004}}
\talkspeaker{Jacob Denson}{University of Madison, Wisconsin}

\setcounter{equation}{0}
\setcounter{theorem}{0}

\begin{abstract}
{ We describe the theory of the capacity of rank-decreasing positive operators, and Gurvit's operator rescaling algorithm to compute the capacity of such operators, with a brief discussion of the connection between this theory and the computation of Brascamp-Lieb constants. }
\end{abstract}

% Operator Scaling Preserves the Rank Non-Decreasing Property
% Polynomial Identity Testing

Recall a theorem of Lieb \cite{lieb1990}, which shows that any inequality of the form
%
\[ \int_{\mathbf{R}^n} \prod_{i = 1}^m |f_i(B_i x)|^{p_i}\; dx \leq \text{BL}(B,p) \cdot \prod_{i = 1}^m \| f_i \|_{L^1(\mathbf{R}^{n_i})}^{p_i}, \]
%
has an optimal Brascamp-Lieb constant $\text{BL}(B,p)$ satisfying
%holds over all choices of functions $f_1, \dots, f_m \in L^1(\mathbf{R}^{n_j})$ if and only if it holds whenever $f_1, \dots, f_m$ are Gaussian functions centered at the origin, and the optimum constant $\text{BL}(B,p)$ in the inequality is the same in both settings. If, for each $i$, we set $f_i(x) = e^{- \pi x^T X_i x}$, where $X_i$ is a positive definite matrix, then both sides of the inequality become the integrals of Gaussian, which can be easily calculated over all inputs, from which we find that
%
\begin{equation} \label{BrascampLiebSupremum}
    \text{BL}(B,p)^2 = \sup_{X_1, \dots, X_m \succ 0} \frac{\det (\sum_{i = 1}^m p_i B_i^* X_i B_i)}{\prod_{i = 1}^m \det(X_i)^{p_i}}.
\end{equation}
%
Here and in what follows, for a square matrix $A$, we write $A \succ 0$ and $A \succeq 0$ to mean $A$ is positive definite or semidefinite.

The focus of these notes is to introduce analysts to a setting, originally studied by computing scientists to understand problems in combinatorial optimization and quantum information theory, which can provide insight into the computation of the Brascamp-Lieb constant. Utilizing this theory, Garg, Gurvits, and Wigderson \cite{ggow} formulated a polynomial time algorithm for approximating the Brascamp-Lieb constant, a fact that has important theoretical consequences to the general theory of Brascamp-Lieb inequalities independent of practical application to computation of particular Brascamp-Lieb constants. This connection is more fully explored in the following talk. In this summary, we introduce the original setting studied, only noting some similarities to Brascamp Lieb as we proceed.

\section{Operator Scaling}

Let $M(N)$ denote the space of all $N \times N$ complex-valued matrices. The main object of study in these notes are \emph{positive operators} $T: M(N) \to M(N)$, i.e. linear transformations between spaces of matrices such that if $X \succeq 0$, then $T(X) \succeq 0$.  There is a rich theory of these maps, connected to representation theory and free probability theory. For more background, the textbook \cite{bhatia} provides a thorough introduction to this theory. A useful example to keep in mind, given any matrices $B_1,\dots, B_m$ and $p_1,\dots,p_m > 0$, is the operator
%
\begin{equation} \label{completelypositiveoperator}
    T(X) = \sum_{i = 1}^m p_i (B_i^* X B_i).
\end{equation}
%
With any positive operator $T$, we associate a quantity $\text{Cap}(T) \geq 0$, known as the \emph{capacity} of $T$, defined by setting
%
\begin{equation} \label{capacityequation}
    \text{Cap}(T) = \inf_{X \succ 0} \frac{\det(TX)}{\det(X)}.
\end{equation}
%
The connection between capacity and Brascamp-Lieb may be hinted at by comparing \eqref{capacityequation} to \eqref{BrascampLiebSupremum} when $T$ is of the form given in $\eqref{completelypositiveoperator}$. Being defined by a non-convex optimization, it seems difficult to explicitly compute capacity. In this talk, we discuss a result of Gurvits \cite{gurv2004}, who found an efficient algorithm to compute, for each $\varepsilon > 0$, a value $\text{Cap}_{\text{approx}}(T)$ such that
%
\[ \text{Cap}(T) \leq \text{Cap}_{\text{approx}}(T) \leq (1 + \varepsilon) \cdot \text{Cap}(T). \]
%
We obtain this approximation by applying the technique of \emph{operator scaling}, which we describe in this section.

Given any invertible matrices $A,B \in M(N)$, and any positive operator $T: M(N) \to M(N)$, we define a \emph{scaled operator} $T_{A,B}(X) = B T(A X A^*) B^*$. For any $X \succ 0$, if we write $Y = AXA^*$, then
%
\begin{equation} \label{rescaleddetequation}
    \frac{\det(T_{A,B} X)}{\det(X)} = \det(A)^2 \det(B)^2 \cdot \frac{\det(TY)}{\det(Y)}.
\end{equation}
%
Taking infima over both sides of \eqref{rescaleddetequation} for all choices of input $X$ shows that $\text{Cap}(T_{A,B}) = \det(A)^2 \det(B^2) \text{Cap}(T)$. Thus computing $\text{Cap}(T_{A,B})$ immediately gives $\text{Cap}(T)$. The idea of operator is to rescale an operator into something whose capacity we can more easily compute. This motivates the introduction of \emph{doubly stochastic operators}.

A positive operator $T$ is \emph{doubly stochastic} if $T(I) = T^*(I) = I$, where $T^*$ is the adjoint of $T$ with respect to the inner product $(X,Y) \mapsto \text{Tr}(XY^*)$. What interests us about this definition is that $\text{Cap}(T) = 1$ for \emph{any} doubly stochastic operator $T$. The proof of this statement can be found as Theorem 4.32 of \cite{watrous}, with relevant background information about doubly stochastic matrices found in Section 8.5 of \cite{serre}.

Thus, given a positive operator $T$, if we can find $A,B \in M(N)$ such that $T_{A,B}$ is doubly stochastic, then it follows that $\text{Cap}(T) = \det(A)^{-2} \det(B)^{-2}$. This is clearly only possible if $\text{Cap}(T) > 0$, but not \emph{quite} possible for all such $T$. In fact, in \cite{ostaa}, it is shown that this rescaling is only possible if the equation defining capacity has extremizers, i.e. $\text{Cap}(T) = \det(TX)/\det(X)$ for some particular input $X$. Nonetheless, if $\text{Cap}(T) > 0$ then we will find doubly stochastic operators approximating $T$ arbitrarily closely. Since capacity is a continuous function of the input, this suffices to approximate $\text{Cap}(T)$ up to an arbitrary error.

%We remark that, before the theory of capacity was explicitly applied to the theory of Brascamp-Lieb, a variant of operator scaling was already being used. A Brascamp Lieb inequality is called \emph{geometric} if each of the linear maps $\{ B_i \}$ in the inequality are projection maps, and $I = \sum_{j = 1}^m p_j B_j^* B_j$. Ball showed that the Brascamp-Lieb constant of any geometric inequality is one (analogous to the capacity of a doubly stochastic matrix). The Brascamp-Lieb constant behaves well under replacing each of the matrices $B_j$ with $B_j A$ for some invertible matrix $A$, and so it is natural to try and find some matrices which reduce to the study of a geometric Brascamp-Lieb inequality.

Gurvits' approximation algorithm is quite simple, an application of a similar method first utilized by Sinkhorn \cite{sinkhorn}. Given a positive operator $T$, it is easy to scale the operator to an operator $S$ with $S(I) = I$; we simply consider the operator $T_{I, T(I)^{-1/2}}$. Similarily, we can scale $T$ to an operator $S$ with $S^*(I) = I$ by taking the scaling $T_{T*(I)^{-1/2}, I}$. The challenge is to obtain scalings for which both properties are approximately true. Sinkhorn's trick is to iteratively apply each of the rescalings to our operators, obtaining a sequence of matrices $T_0, T_1, T_2, \dots$ with $T_0 = T$, $T_i(I) = I$ for odd $i$, and $T_i^*(I) = I$ for even $i$. If this sequence converges, the limit will be doubly stochastic, as desired.

Convergence to a single stochastic matrix does not occur for all $T$ with $\text{Cap}(T) > 0$, but we will show that if $\text{Cap}(T) > 0$, then the distance between the elements of the sequence to the family of \emph{all} stochastic operators converges to zero, which is sufficient for our purposes. To analyze the convergence, we rely on the capacity as a \emph{potential} for the analysis of the algorithm (this was the main reason Gurvits first defined the capacity in \cite{gurv2004}). To determine how close we are to a doubly stochastic matrix, we use the measure
%
\[ \text{DS}(T) = \| T(I) - I \|^2 + \| T^*(I) - I \|^2. \]
%
The following properties then hold:
%
\begin{enumerate}
        \item If $T(I) = I$ or $T^*(I) = I$, then $A_N \lesssim_N \text{Cap}(T) \leq 1$ for some constant $A_N > 0$ depending on $N$. Moreover, for $\text{Cap}(T) \geq 1/2$,
        %
        \[ \text{DS}(T_n) \leq 6 \log(1/\text{Cap}(T_n)). \]

        \item $\text{Cap}(T_n)$ is increasing in $n > 0$. More precisely,
        %Indeed, $\text{Tr}(T_n(I)) = \text{Tr}(T_n^*(I)) = N$, and so the arithmetic-geometric mean inequality, applied to the sum and product of the eigenvalues of $T_n(I)$ and $T_n^*(I)$, give that $\det(T_n(I)), \det(T_n^*(I)) \leq 1$. Depending on the parity of $n$, either $\text{Cap}(T_{n+1})$ equals $\text{Cap}(T_n) \det(T_n(I))^{-1/2}$ or $\text{Cap}(T_n) \det(T_n^*(I))^{-1/2}$, so we conclude $\text{Cap}(T_{n+1}) \geq \text{Cap}(T_n)$. In fact, we have a deeper bound
        %
        \[ \text{Cap}(T_{n+1}) \geq e^{\min(1,\text{DS}(T_n))/6} \cdot \text{Cap}(T_n). \]

        \item If $T(I) = I$ or $T^*(I) = I$ and $\text{DS}(T) < 1/(N+1)$, then $\text{Cap}(T) > 0$.
\end{enumerate}
%
We claim that from these properties, we can detect in $M_0 = 6(N+1) \log(1/A_N)$ iterations of the algorithm whether $\text{Cap}(T) > 0$. Indeed, if $\text{Cap}(T) > 0$, then Property 1 implies that $\text{Cap}(T_1) \geq A_N$. For any $\delta$ and $M$, repeated applications of Property 3 imply that there either exists $i < M$ such that $\text{DS}(T_i) \leq \delta$, or $\text{Cap}(T_M) \geq e^{\varepsilon M / 6} A_N$. In particular, if $\delta = 1/(N+1)$, and $M = M_0$, then this calculation, combined with Property 1, implies that if $\text{Cap}(T) > 0$, then there must exist $i \leq M_0$ with $\text{DS}(T_i) < 1/(N+1)$. On the other hand, Property 3 implies that if $\text{Cap}(T) = 0$, then $\text{DS}(T_n) \geq 1/(N+1)$ for all $n$. Checking $\text{DS}(T_i)$ for $i \leq M_0$ thus gives a simple way to check whether $\text{Cap}(T) > 0$. Using very similar techniques, we leave it as an exercise to check these properties imply that if $\text{Cap}(T) > 0$, then in $(6/\varepsilon) \log(1/A_N)$ iterations of the algorithm, one can find $i$ such that $\text{DS}(T_i) \leq \varepsilon$. Thus we have a reliable way to approximate the capacity of a matrix, which \cite{ggow} extends to find efficient ways to approximate Brascamp-Lieb constants. More advanced techniques, given in \cite{ostaa}, show that we only actually need to run the algorithm for $\text{Poly}(N,\log(1/\varepsilon))$ iterations to obtain a $\varepsilon$-approximation.

\section{Capacity of Rank-Decreasing Operators}

We conclude these notes by discussing an analogy between Brascamp-Lieb and the study of capacity. Bennett, Carbery, Christ, and Tao \cite{bcct} showed that $\text{BL}(B,p)$ is finite in \eqref{BrascampLiebSupremum} if and only if $\sum_{j = 1}^m p_j n_j = 0$, and
%
\begin{equation} \label{BrascampLiebMappingProperty}
    \dim(V) \leq \sum_{j = 1}^m p_j \dim(B_j V)\quad\quad \text{for all subspaces $V \subset \mathbf{R}^n$}.
\end{equation}
%
Thus the finiteness of \eqref{BrascampLiebSupremum} acts as a guarantee for the mapping properties of the matrices $B_1, \dots, B_m$ given in \eqref{BrascampLiebMappingProperty}, and vice versa. A useful property of a positive operator $T$ is that it is \emph{rank non-decreasing}, i.e. for any $X \succeq 0$,
%
\begin{equation} \label{ranknondecreasing}
    \text{Rank}(TX) \geq \text{Rank}(X).
\end{equation}
%
Condition \eqref{ranknondecreasing} seems somewhat similar to \eqref{BrascampLiebMappingProperty}. And indeed, analogous to the equivalence between \eqref{BrascampLiebMappingProperty} and the finiteness of \eqref{BrascampLiebSupremum}, one has an equivalence between \eqref{ranknondecreasing} and the non-vanishing of \eqref{capacityequation}.

\begin{theorem} \label{rankdecreasingcapacitytheorem}
    $T$ is rank non-decreasing if and only if $\text{Cap}(T) > 0$.
\end{theorem}
\begin{proof}
    A simple family of positive operators are those of the form
    %
    \begin{equation} \label{basicoperator}
        TX = X_{11} A_1 + \dots + X_{NN} A_N,
    \end{equation}
    %
    where $A_1,\dots,A_N \succeq 0$. For such an operator, we can write
    %
    \[ \text{Cap}(T) = \inf_{\gamma_1, \dots, \gamma_N > 0} \frac{\det ( \sum_{j = 1}^N \gamma_j A_j )}{\gamma_1 \cdots \gamma_N}. \]
    %
    Results from a previous paper of Gurvits and Samorodnitsky \cite{gurvits3} imply Theorem \ref{rankdecreasingcapacitytheorem} in the special case of an operator defined by \eqref{basicoperator}. Assuming this result, we indicate how this implies the general case.

    For each orthonormal basis $U = \{ u_1, \dots, u_N \}$, we define the decoherence operator $D_U(X) = \sum \langle Xu_i, u_i \rangle \cdot u_i u_i^*$, and then consider the operator $T_U$ defined such that $T_U(X) = (T \circ D_U)(X) = \sum_{i=1}^N \langle X u_i, u_i \rangle \cdot T(u_i u_i^*)$.
    %
    This operator is, up to a change of basis in $M(N)$, described in the form \eqref{basicoperator}. Thus $T_U$ is rank non-decreasing if and only if $\text{Cap}(T_U) > 0$. The theorem then follows from the following two properties of this construction:
    %
    \begin{enumerate}
        \item $T$ is rank non-decreasing if and only if $T_U$ is as well, for all bases $U$.
        \item $\text{Cap}(T) = \inf_U \text{Cap}(T_U)$.
    \end{enumerate}
    %
    If $\text{Cap}(T) > 0$, then Property 2 implies $\text{Cap}(T_U) > 0$ for all $U$, so $T_U$ is rank non-decreasing for all $U$, and thus Property 1 implies $T$ is rank non-decreasing. The converse is similar.

    The proof of Properties 1 and 2 both rely on a simple trick. We will prove Property 1 here. Given any $X \succeq 0$, we can find an orthonormal basis $U$ diagonalizing $X$, and then for such $U$ we have $T(X) = T_U(X)$. This immediately implies $T$ is rank non-decreasing if $T_U$ is rank non-decreasing for all $U$. The converse follows because the composition of rank non-decreasing operators is rank non-decreasing, and $D_U$ is rank non-decreasing (all doubly stochastic operators are rank non-decreasing).
    %To show that $\text{Cap}(T) = \inf_U \text{Cap}(T_U)$, we note that for any $\gamma_1,\dots, \gamma_N$,
    %
    %\begin{equation} \label{UCapacityUpperBound}
    %    \frac{\det \left( \sum_{j = 1}^N \gamma_j T(u_j u_j^*) \right)}{\gamma_1 \dots \gamma_N} = \frac{\det \left( T \left( \sum_{j = 1}^N \gamma_j \cdot u_j u_j^* \right) \right)}{\det \left( \sum_{j = 1}^m \gamma_j \cdot u_j u_j^* \right)} \geq \text{Cap}(T).
    %\end{equation}
    %
    %Taking infima over all $\gamma_1,\dots,\gamma_N$, and then over all $U$, gives $\inf_U C_U(T) \geq \text{Cap}(T)$. Conversely, given any $X \succeq 0$, pick $U$ with $T(X) = T_U(X)$, and then
    %
    %\begin{equation} \label{UCapacityLowerBound}
    %    \frac{\det(TX)}{\det(X)} = \frac{\det(T_U X)}{\det(X)} \geq C_U(T) \geq \inf_U C_U(T).
    %\end{equation}
    %
    %Taking infima over all $X$ gives $\text{Cap}(T) \geq \inf_U C_U(T)$. Combining these two calculations gives $\text{Cap}(T) = \inf_U C_U(T)$, which proves Property 2.
\end{proof}

\begin{thebibliography}{3}

\bibitem{bcct}
    J.M. Bennett, A. Carbery, M. Christ and T. Tao,
    \emph{The {B}rascamp-{L}ieb Inequalities: Finiteness, Structure and Extremals},
    Geom. Funct. Anal. {\bf 17} (2008), 1343-1415.

\bibitem{bhatia}
    Rajendra Bhatia,
    \emph{Positive Definite Matrices},
    Princeton University Press, 2007.

\bibitem{ostaa}
    Ankit Garg, Leonid Gurvits, Rafael Oliveira, Avi Wigderson,
    \emph{Operator Scaling: Theory and Applications},
    Foundations of Computational Mathematics, {\bf 20} (2020), 223--290.

\bibitem{ggow}
    Ankit Garg, Leonid Gurvits, Avi Wigderson,
    \emph{Algorithmic and Optimization Aspects of {B}rascamp-{L}ieb Inequalities, via Operator Scaling},
    Geom. Funct. Anal, {\bf 28} (2018), 100--145.

\bibitem{gurv2004}
    Leonid Gurvits,
    \emph{Classical Complexity and Quantum Entanglement},
    J. Comput. Syst. Sci. {\bf 69} (2004), 448--484.

\bibitem{gurvits3}
    L. Gurvits, A. Samorodnitsky,
    \emph{a Deterministic Algorithm Approximating the Mixed Discriminant and Mixed Volume, and a Combinatorial Corollary},
    Discrete Comput. Geom. {\bf 27} (2002), 531--550.

\bibitem{lieb1990}
    E.H. Lieb,
    \emph{{G}aussian Kernels have only {G}aussian Maximizers}
    Invent. Math. {\bf 102} (1990), 179--208.

\bibitem{watrous}
    John Watrous,
    \emph{the Theory of Quantum Information}
    Cambridge University Press, 2018.

\bibitem{serre}
    Denis Serre,
    \emph{Matrices},
    Springer, 2010.

\bibitem{sinkhorn}
    Richard Sinkhorn,
    \emph{A Relationship Between Arbitrary Positive Matrices and Doubly Stochastic Matrices},
    Ann. Math. Statist. {\bf 35} (1964), 876--879.

\end{thebibliography}

\noindent \textsc{Jacob Denson, UW Madison}\\
\textit{email:} \texttt{jcdenson@wisc.edu}

\end{document}