\documentclass[12pt]{report}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsopn}
\usepackage{kpfonts}
\usepackage{graphicx}
\usepackage{kbordermatrix}

% Standard tool for drawing diagrams.
\usepackage{tikz}
\usepackage{tkz-berge}

% Draws commutative diagrams in TIKZ.
\usepackage{tikz-cd}

%
\usepackage{multicol}

%
\usepackage{framed}

%
\usepackage{mathtools}

%
\usepackage{float}

%
\usepackage{subfig}

%
\usepackage{wrapfig}

%
\usepackage{mathabx}

% Used for generating `enlightening quotes'
\usepackage{epigraph}

% Forget what this is used for :P
\usepackage[utf8]{inputenc}

% Used for generating quotes.
\usepackage{csquotes}

% Used
\usepackage{diagrams}

% Allows what to generate links inside
% generated pdf files
\usepackage{hyperref}

% Allows one to customize theorem
% environments in mathematical proofs.
\usepackage{thmtools}

% Gives access to a proof
\usepackage{lplfitch}

% I forget what this is for.
\usepackage{accents}

% A package for drawing simple trees,
% as a substitute for unnesacary TIKZ code
\usepackage{qtree}

% Enables sequent calculus proofs
\usepackage{ebproof}






\setlength\epigraphwidth{8cm}

\usetikzlibrary{arrows, petri, topaths}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{exercise}{Exercise}

%\theoremstyle{remark}
\newtheorem*{example}{Example}
\newtheorem*{proof*}{Proof}

\theoremstyle{definition}
\newtheorem*{defi}{Definition}
\newenvironment{definition}
    {\begin{samepage}\begin{framed}\begin{defi}}
    {\end{defi}\end{framed}\end{samepage}}





\usepackage{hyperref} 
\hypersetup{
    colorlinks = true,
    linkcolor = black,
}

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\renewcommand*\contentsname{\hfill Table Of Contents \hfill}

\newcommand{\optionalsection}[1]{\section[* #1]{(Important) #1}}
\newcommand{\deriv}[3]{\left. \frac{\partial #1}{\partial #2} \right|_{#3}} % partial derivative involving numerator and denominator.
\newcommand{\lcm}{\operatorname{lcm}} % Lowest Common Multiple.
\newcommand{\im}{\operatorname{im}} % Image of a function.
\newcommand{\bint}{\mathbf{Z}} % Bold integer Symbol.
\newcommand{\gen}[1]{\langle #1 \rangle} % Generator of a group.
\newcommand{\End}{\operatorname{End}}
\newcommand{\Mor}{\operatorname{Mor}}
\newcommand{\Id}{\operatorname{id}}
\newcommand{\visspace}{\text{\textvisiblespace}}
\newcommand{\Gal}{\text{Gal}}
\newcommand{\coker}{\text{coker}}

\DeclareMathOperator{\Dom}{Dom}

% Sources: Abraham/Marsten Foundations of Mechanics



\title{On Molecules and Natural Numbers:\\
An Introduction to Ergodic Theory}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}

\maketitle

%\tableofcontents

\pagenumbering{arabic}

Hi Everyone! I'm here to talk about Ergodic theory, which is a beautiful branch of mathematics which isn't talk in undergraduate curriculums very often. It's such a shame, because of the beautiful visual results we find in the theory, so in the next 20 minutes hopefully I can show you why you need to go out into the world after this talk and start learning the subject!

Our story begins with a map $T:X \to X$, from a space to itself. Since the map begins and ends at the same space, we can begin iterating it, obtaining a sequence of maps on the space
%
\[ T, T^2, T^2, \dots \]
%
The real fun begins when $T$ is invertible, in which case we can consider an entire group action from $\mathbf{Z}$ into $\text{Inv}(X)$.
%
\[ \dots, T^{-2}, T^{-1}, \mathbf{1}, T, T^2, \dots \]
%
The study of group actions on spaces is known as dynamical systems theory -- the groups `dynamically change the space', if we view the index as a time index.

To introduce the main examples from physics, we need to introduce the concept of a `phase space'. The classic way of viewing a differential equation is to view it algebraically
%
\[ \frac{d^2y}{dt^2} = y \frac{dy}{dt} + C t \]
%
We then apply algebraic techniques to (hopefully) reduce the equation to the form $y = g(x)$, after initial conditions are chosen. It is important to note that, for the initial conditions to solve the equation, we must be able to express the infinitisimal equations exactly, turning the differential equation into a vector field. For instance, the phase space of the equation above is $\mathbf{R}^3$ (a dimension for time, position, and velocity), and the corresponding vector field is
%
\[ v(t,x,\dot{x}) = (1, \dot{x}, x \dot{x} + C t) \]
%
The differential equation initiates


A revolutionary view of differential equations occurs when we see a differential equation as an {\it operator} on the space of initial conditions, which moves the condition to its position in the future.

\begin{example}
    s
\end{example}

The classic examples of dynamical systems theory occur from physics, where we actually obtain operators $f_t:X \to X$, for $t \in \mathbf{R}$. 

Our story begins in the 17th century, as does all of analysis, with the invention of a tool which has been proved somewhat useful in the last 500 years -- Newton's calculus. With it began the `quantitative age' of mathematics, with focus on techniques of integration and of the solution of differential equations arising from physics. But in the late 19th century, the quantitative techniques hit an impasse. It was shown that the differential equation defining the motion of three or more planets could not be solved numerically - a fact analogous to Galois' proof that one could not formulate general solutions to the quintic equation. Such a crisis could only be solved by one of the greats, and it is with a particular great, the Frenchman Henri Poincar\'{e}, that ergodic theory began to take its form.

The classical method of looking at a differential equation is to describe an algebraic equation to be solved. For instance
%
\[ \frac{d^2 y}{dt^2} = t^2 \frac{dy}{dt} - ty \]
%
the game is then to apply certain solution techniques (i.e. separation of variables, etc), to reduce the formula to a simpler statement, with the main goal of obtaining an expression for $y$ as a function of $t$, unique after obtaining certain initial conditions (in this case, the initial position $y(0)$, velocity $y'(0)$, and time $t$).

Poincare's methods are characterized by looking at a solution from a global point of view, rather than a local point of view. Instead of finding a solution for a particular initial condition $x_0$, we try and find the action of the differential equation on the space of all initial conditions. Given a particular initial condition $x_0$, we are able to manipulate the formula to determine the instantaneous rates of change of the equation at a certain position. This gives us a vector field on the `phase space' of instaneous conditions, in this case
%
\[ v(t,x,\dot{x}) = (1,\dot{x},t^2 \dot{x} - tx) \]
%
The theory of uniqueness and existence for differential equations tells us that, for each $(t,x,\dot{x})$, there is a unique function $g(t)$ solving the differential equation in a certain time interval. Such a function gives us a particular curve in phase space, taking the conditions of the function at a particular time point. By uniqueness, we can put all these curves together to obtain an `evolution' function $f$, which takes a certain initial condition $x_0$, and a certain number $t$, and gives us the position $f_t(x_0)$ of the curve in phase space, $t$ seconds after it begins to move. With this switch, it is much easier to describe `qualitative facts about the differential equation', since we have now boxed all solutions in a single object to study.

\begin{example}
    It's best to see this method in action. Consider the spring equation
    %
    \[ y'' = -y \]
    %
    The phase-space for this equation is $\mathbf{R}^2$ (one dimension for the position of the end of the spring, and one dimension for the velocity). Now if we look at the vector field in position-momentum space we obtain the formula
    %
    \[ v(x,\dot{x}) = (\dot{x}, -x) \]
    %
    If you draw out this vector field, it is easy to see that $f_t$ is just a clockwise rotation by an angle $t$. Qualitatively, this tells us why the motion of the spring is periodic, and why the value
    %
    \[ x^2 + \dot{x}^2 \]
    %
    is preserved in a particular trajectory. The beauty of this approach is that we need no formula to see why this is true.
\end{example}

\begin{example}
    If a particle is pushed from left to right on the number line at a uniform velocity, then the differential equation is
    %
    \[ y' = C \]
    %
    with corresponding vector field $v(x) = C$, and propogation operator are translations $f_t(x) = x + Ct$.
\end{example}

This is the origin of the study of `dynamical systems', a switch from studying the evolution of a point over time, to studying the evolution of a system over time. It seems a small change, but it is incredibly useful for analyzing the qualitative structure of systems.

Ergodic theory, in particular, studies the properties of dynamical systems which `mix well' over time. In such a space, one can switch between studying deterministic and indeterministic properties of certain phenomena -- thus ergodic theory can be seen as the foundations of statistical mechanics, through which we understand the deterministic motions of atomic particles through the lenses of probabilistic methods, and vice versa.

An important property of the maps $f_t$, to watch out for, is that
%
\[ f_t \circ f_s = f_{t+s}\ \ \ \ \ \ \ \ f_0 = \text{id}_M \]
%
so that, for those initiated, a differential equation induces a `group action' on phase space. In ergodic theory, we almost always discuss the long term qualitatives of a dynamical system, and thus we often restrict our action to $\mathbf{Z}$. In this case, our action is described by a single, invertible map $T$, where the action is iteration, $\{ T^k : k \in \mathbf{Z} \}$.

To continue the historical development, let us return to Poincare's newly created school of differential equations, and its application in physics. The standard equation of motion in a `Newtonian System' is
%
\[ m \ddot{x} = F(x) \]
%
In 1833, the scientist William Hamilton, discovered that by a change in coordinates one could discover a much more beautiful representation of classical mechanics. If $p = m \dot{x}$ denotes the momentum at a certain time, and we rename the position coordinate $x$ to $q$ (as is the standard for some reason), then we may express the kinetic energy of the situation as
%
\[ \frac{m\dot{q}^2}{2} = \frac{p^2}{2m} \]
%
Often, we find that there is a scalar function $V(q)$ such that $\nabla V = - F$; such a function is known as the potential energy of the system. With these definitions in hand, we define the total energy of the system, known as the Hamiltonian, as
%
\[ H(p,q) = \frac{p^2}{2m} + V(q) \]
%
$H$ is known as the Hamiltonian. Now in this form, Newton's laws take the pleasant form
%
\[ \dot{p} = - \frac{\partial H}{\partial q}\ \ \ \ \ \ \ \ \ \ \dot{q} = \frac{\partial H}{\partial p} \]
%
Note that this {\it isn't} a partial differential equation to be solved, since $H$ is a known quantity, and we are solving for $p$ and $q$. The main reason to apply Hamilton's equations is that the approach generalizes much more simply to arbitrary coordinate systems, and it is often much easier to define $H$, rather than the forces (try defining the forces for a system of balls attached to each other by pulleys, and you'll get what I mean).


Now suppose we only know the intiail position and momentum of an object to a certain precision -- then the conditions of our object in phase space lie in a certain area $U$. If we watch $U$ evolve, we learn that the object must eventually lie in $f_t(U)$, for each $t$. But how does the precision of our measurements change over time?

\begin{lemma}
    If a vector field $v$ has divergence zero,
    %
    \[ \text{div}\ v = \sum \frac{\partial v^i}{\partial x^i} = 0 \]
    %
    Then the operators $f_t$ preserve volume.
\end{lemma}
\begin{proof}
    Let $D$ be a region of space, define $w(0)$ to be the volume of $D$, and more generally, define $w(t)$ to be the volume of $f_t(D)$. Now $f_t$ is a diffeomorphism of $D$, so by the change of variables formula
    %
    \[ w(t) = \int_{f_t(D)} 1 = \int_{D} |\text{det} (Df_t) | \]
    %
    and hence
    %
    \[ w'(t) = \int_D \frac{d |\text{Det}(Df_t)|}{dt} \]
    %
    Now for a small value of $t$,
    %
    \[ f_t(x) = x + v(x)t + O(t^2) \]
    %
    Thus
    %
    \[ \left( \frac{\partial f^i_t}{\partial x^j} \right) = I + (Dv) t + O(t^2) \]
    %
    One has that
    %
    \[ \text{det}(I + t M) = 1 + t\ \text{tr}(M) + O(t^2) \]
    %
    obtained by expanding factors, and ignoring factors including $t^2$. The trace of $Dv$ is exactly the divergence of $v$, so it follows that
    %
    \[ \frac{d|\text{det}(I + t Dv)|}{dt} = \text{div}(v) \]
    %
    proving the claim when $\text{div}(v) = 0$.
\end{proof}

Louiville observed that the vector field governing the motion in Hamiltonian mechanics has zero divergence (mixed partials are equal), which means

\begin{theorem}
    Volume is preserved by motion in momentum-position space, under the action of the Hamiltonian equations.
\end{theorem}

Thus we have a kind of `Heisenberg uncertainty principle' in classical mechanics. We can never infer a more accurate measurement from an initial measurement, without losing accuracy in other measurements in the process.

Now the great part about measure preserving transformations is that they cannot `stretch' or `squish' any part of space, since the volume of that portion of space must stay the same. In spaces with only finite volume, this gives us something to work with. The first theorem taking advantage of this is a continuous form of the pidgeonhole principle.

\begin{theorem}
    Let $T: X \to X$ be a measure preserving map on a space of finite volume. Given a fixed $U \subset X$ of positive volume, almost every point of $U$ returns to $U$ after some time.
\end{theorem}
\begin{proof}
    Let
    %
    \[ A = \{ x \in U : \text{$x$ never returns to $U$} \} \]
    %
    Then $T^{-n}(A)$ is disjoint from $T^{-m}(A)$, for each $n \neq m$, for if $x$ arrives in $A$ in both $n$ iterations and $m$ iterations, for $n < m$, then $T^n x$ arrives in $A$ after $m - n$ iterations, a contradiction to the fact that $T^n x \in A$. Thus $v(A) = 0$, for otherwise
    %
    \[ v(X) \geq v\left( \bigcup_{i = 0}^\infty T^{-i}(U) \right) = \sum_{i = 0}^\infty v(T^{-i}(U)) = \sum_{i = 0}^\infty v(U) > \infty \]
    %
    So almost every point in $U$ returns to $U$ once.
\end{proof}

\begin{example}
    Consider this picture of a cat, and take the map on $\mathbf{R}^2 / \mathbf{Z}^2$ defined by
    %
    \[ T(x,y) = (2x + y, x + y) = \begin{pmatrix} 2 & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} \]
    %
    $T$ is actually an invertible map on the unit square, with inverse
    %
    \[ T^{-1}(x,y) = \begin{pmatrix} 1 & -1 \\ -1 & 2 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} \]
    %
    and is area preserving, since the determinant of the matrix is 1. Thus we can apply Poincare recurrence to see that almost every point returns to a neighbourhood of itself. We can see this in the diagram above.
\end{example}

In the case that $T$ is injective, we can obtain explicit bounds on the time we will have to wait for a point in $U$ to return to $U$. If $U$, $T U, \dots T^k U$ are disjoint, then
%
\[ v(\bigcup_{i = 0}^k T^i U) = n v(U) \]
%
If $T^iU$ and $T^jU$ intersect, $T^{-j}T^iU = T^{i-j}U$ intersects $U$. Thus we find that, to avoid contradiction, a point in $U$ must return to $U$ in $\lceil v(X)/v(U) \rceil$ iterations.

\begin{example}
    Consider opening the lid on a bucket of air molecules in a sealed chamber. Poincare's theorem tells us that, paradoxically, to any desired margin of error, the air molecules will eventually return to their original position inside the bucket. Such a paradox is solved when we consider the curse of dimensionality for the particles. Suppose we only know the initial positions of the particles to within an error of $\pm \varepsilon$. Then the `volume of error' in phase space will be $(2\varepsilon)^{3n}$, and if the sealed chamber is a cube of length $m$, then we will have bounds on the order of
    %
    \[ \left( \frac{m}{2\varepsilon} \right)^{3n} \]
    %
    which grows exponentially as the number of particles increase. Thus to wait for 200,000 particles to return to their original position, we will likely have to wait until the end of the universe.
\end{example}

\begin{example}
    Consider the circle $S^1$, which has a canonical `angular measure' placed on it. Let $T:z \mapsto wz$, for a fixed $w \in S^1$. Then $T$ is a rotation of the circle which preserves measure. If $w^n z = z$, for some $x$, then $w^n = 1$, and is a root of unity. In this case the system of translations $T, T^2, \dots,$ is cyclic, and points return to their initial positions exactly. If $w$ is not a root of unity, then Poincare's recurrence theorem tells us something interesting -- for any $\varepsilon > 0$, there is $n$ such that $|w^n - 1| < \varepsilon$. Take the neighbourhood $U$ around $1$ which sweeps out an angle of $\varepsilon/2$ in each direction. Poincare's theorem tells us that there is $z \in U$ and $n$ for which $w^n z \in U$, so $|w^n z - z| = |w^n - 1| < \varepsilon$.

    Since the group $S^1$ is isomorphic to $\mathbf{R}/\mathbf{Z}$, Poincare's theorem tells us analogously that, if $x$ is an irrational number, then we may always choose $n$ such that $nx$ has decimal values as close to zero as desired. A very elegant argument for what is normally a very messy proof.
\end{example}

Now measure preserving maps cannot `squish and hide' parts of their domain, but they can still keepparts of the domain too still. 

\begin{example}
    Consider the numbers
    %
    \[ 1, 2, 4, 8, 16, 32, \dots, 2^k, \dots \]
    %
    in particular, take the most significant digits of these numbers. Do these digits follow some sort of regular pattern? Perhaps, but it is not an obvious pattern
    %
    \[ 1, 2, 4, 8, 1, 3, 6, 1, 2, 5, \dots \]
    %
    Ergodic theory is not good at discovering `pointwise' patterns, but can help us discover the behaviour of phenomena `on average', or `in the limit'.

    The trick to discovering the properties of the power sequence is to look at the numbers in scientific notation
    %
    \[ 1 \cdot 10^0, 2 \cdot 10^0, 4 \cdot 10^0, 8 \cdot 10^0, 1.6 \cdot 10^1, 3.2 \cdot 10^1, \dots \]
    %
    In general, we may extract the first digit of a number of the form $z = x \cdot 10^y$, where $0 \leq x < 10$ in the following manner. We have
    %
    \[ \log(z) = y + \log(x) \]
    %
    Since $\log$ is an increasing function, we can determine if the first digit of $x$ is $1$, then $1 \leq x < 2$, so $\log(1) \leq x < \log(2)$. Similarily, the first digit of $x$ is $n$, if $\log(n) \leq x < \log(n)$. Now the act of taking a power of $2$ may stretch the area of the shape, but if we consider the action `logarithmically', then multiplying a number by $2$ corresponds exactly to adding $\log(2)$ to the logarithm. Thus if we consider our powers of $2$ as elements of $\mathbf{R}/\mathbf{Z}$, the 
\end{example}

As a final example of our system, consider hitting a ball on a pool table. How will the angle of the shot impact the orbit of the ball?

\section{Continued Fraction Expansions}

A continued fraction is an expression of the form
%
\[ a_0 + \frac{1}{a_1 + \frac{1}{a_2 + \dots}} \]
%
Which is denoted by $[a_0; a_1, a_2, \dots]$. A finite fraction is denoted by $[a_0; a_1, \dots, a_n]$, which is of course of the form
%
\[ a_0 + \frac{1}{a_1 + \frac{1}{a_2 + \dots + \frac{1}{a_n}}} \]
%
We define the value of the formal expression as
%
\[ [a_0; a_1, a_2, \dots] = \lim_{n \to \infty} [a_0; a_1, \dots, a_n] \]
%
In the form we have given, it is not even clear that these expressions converge. However, we shall find that these sequences {\it always} converge, regardless of the values of the $a_i$.

\begin{lemma}
    If each $a_i \in \mathbf{N}$, then, writing
    %
    \[ \frac{p}{q} = [a_0; a_1, \dots, a_n] \]
    %
    written in reduced form, we find
    %
    \[ \begin{pmatrix} p_n & p_{n-1} \\ q_n & q_{n-1} \end{pmatrix} = \prod_{i = 0}^n \begin{pmatrix} a_i & 1 \\ 1 & 0 \end{pmatrix} \]
    %
    where $p_{-1} = 1$, $q_{-1} = 0$, $p_0 = a_0$, and $q_0 = 1$.
\end{lemma}
\begin{proof}
    By definition, the proof holds for $i = 0$. Suppose it holds for all tuples of length $n-1$. Then, in particular,
    %
    \[ \begin{pmatrix} x & x' \\ y & y' \end{pmatrix} = \prod_{i = 1}^n \begin{pmatrix} a_i & 1 \\ 1 & 0 \end{pmatrix} \]
    %
    where $x$ and $y$ are coprime, and
    %
    \[ \frac{x}{y} = [a_1, \dots, a_n] \]
    %
    s
\end{proof}

\end{document}