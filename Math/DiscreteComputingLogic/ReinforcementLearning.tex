\input{../style.tex}

\title{Reinforcement Learning}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}
\maketitle
\tableofcontents
\pagenumbering{arabic}

\newpage

\part{Bandits}

Reinforcement learning attempts to design intelligence that can learn from its own interactions with an environment. It differs from standard machine learning techniques in that it must choose how to interact with the environment in order to obtain training data from which it can learn. In its most general form, an agent must maximize a stochastic reward function in response to changes in its own environment. The main idea of reinforcement learning is the reward hypothesis -- that all goals can be thought of as the maximization of some reward signal corresponding to actions that an agent performs. The main path to intelligence is then to find strategies to maximize this reward signal. There is perhaps some concern with this view, in that `real intelligence' does not have some reward function; endorphins in the brain provide evidence that we may have some estimate. We shall begin by discussing the most mathematically tractable problem, the $N$ armed bandit, and then move on to more challenging situations.

\chapter{Bandits 101}

Imagine you are at a casino, with rows upon rows of slot machines. A strategic game player would quickly determine the slot machine that maximizes your profit, and exploits this machine to obtain the best payoff. From the start, you have no idea of the mechanisms determining how the rewards are given. Thus you must balance the act of determining these mechanisms by trial and error with exploiting the knowledge you have obtained. The problem of maximizing the amount of money you make is known as the {\bf multi armed bandit problem}.

In the general reinforcement learning problem, the exploration exploitation tradeoff mentioned in the last paragraph is just as prevelant. However, the bandit problem is much more mathematically tractable, and is therefore more prone to theoretical analysis. The problem is not just a toy problem though. In medical studies, we want to obtain information about the potency of medicine as early as possible, to prevent possible harm to those in the study -- this was the original context in which the bandit problem was introduced, by William R. Thompson in 1933. In the 1950s, psychologists designed an experiment which placed mice in a T-shaped maze, seeing if they could learn to consistently choose a direction in order to get food. To study a similar problem in humans, the scientists imagined a two armed slot machine, where pulling each arm gives different distributions. In a test where questions are sequentially generated, one wants to ask questions which best evaluate a student's skill. That these Problems can be solved by mathematical means is a windfall to those who need reliable solutions to the related problems.

The domain of a particular instance of the bandit problem is constructed around a finite set of actions $A$ (we shall use the integer $K$ to denote the number of actions). For each $a \in A$, we fix a reward distribution $\rho_a$ over the real numbers. The set of all collections of such distributions will be denoted $\mathcal{E}(A)$, which we call the space of all {\bf environments} over the action set $A$. Given a fixed environment, the goal is to find a sequence of actions to maximize our reward, subject to the fact that we don't know our environment in advance. We must learn which actions are optimal by choosing exploratory moves in such a way that we are minimally penalized.

A policy is the mathematical model of a strategy which, given a memory of past actions, decides (possibly randomly) on the next action to take. Deterministic policies are simplest to define. First, define a {\bf history} of length $k$ to be an element of $(A \times \mathbf{R})^k$ -- a sequence of data obtained from interactions in the environment. We shall denote the set of all histories of length $k$ by $\mathcal{H}_k(A)$. We shall let $\mathcal{H}_0(A)$ consist of the single empty tuple. A {\bf decision function} for a policy is a sequence of functions $\pi_i$, which map histories in $\mathcal{H}_k(A)$ to probability distributions over $A$ which decide based on prior experience what to take as an $i$'th action. The decision function induces a random sequence of actions $A_1, A_2, \dots$, rewards $X_1, X_2, \dots$, and histories $H_k = [(A_1,X_1), \dots, (A_k,X_k)]$, such that
%
\[ A_k = \pi_k(H_{k-1})\ \ \ \ \ \ (X_k | H_{k-1}, A_k) \sim \rho(A_k) \]
%
More generally, a {\bf stochastic policy} maps a history to a probability distribution over $A$. A stochastic policy then induces a sequence $A_1, A_2, \dots$ and rewards $X_1, X_2, \dots$, together with a filtration $\{ \mathcal{F}_1, \mathcal{F}_2, \dots \}$ such that $A_k$ is $\mathcal{F}_k$-measurable, $X_k$ is $\mathcal{F}_{k+1}$-measurable, and
%
\[ (X_k | \mathcal{F}_k) \sim \rho(A_k) \]
%
The reward associated with this choice of actions is
%
\[ Q_n = \sum_{k = 1}^n X_k \]
%
The goal of the bandit problem is to choose a policy which induces the maximum expected reward for some value $n \in \mathbf{N}$ (a `finite horizon' problem) or to control the order of growth of the expected reward as $n \to \infty$ (a `infinite horizon' problem). Sometimes we may not know $n$, and we will develop so-called {\bf anytime algorithms} which give good results without knowledge of the horizon length.

\section{The Regret Function}

The expected regret of a policy depends heavily on the expected regret of the distributions $\rho_a$. For each action $a \in A$, let
%
\[ \mu_a = \mathbf{E}(X_k | A_k = a) = \int_\mathbf{R} x d \rho_a(x) \]
%
We then define $\mu^* = \max \mu_a$, which is the reward taken for choosing an optimal action. If a user knew the distributions of each action, he would obviously try and pick the action corresponding to $\mu^*$ every time. The real problem is determining which $a$ is optimal, while still optimizing the estimated reward. This is the {\it explore-exploit tradeoff}; if we wish to maximize reward, we must attempt to use the arms which have the highest estimated mean reward. But if we only exploit, then we will never learn the optimum reward, because we will not have enough information in order to come to a reliable conclusion about the optimality of the arms. Good general-use policies must balance exploring and exploiting.

Another way to visualize the problem is that rather than maximizing `good' decisions, we minimize the number of `bad' decisions we make. Define the regret function
%
\[ R_n = \mathbf{E} \left[\sum_{k = 1}^n (\mu^* - X_k) \right] \]
%
Maximizing the expected reward in a particular environment of the bandit problem is exactly the same as minimizing the regret. Indeed, we may write
%
\[ R_n = n \mu^* - Q_n  \]
%
and thus the $R_n$ is obtained from $Q_n$ by a sign change and a shift, provided we are only working over a single environment. However, the regret is translation invariant, so it is a much better measure of an algorithm's learning rate if we analyze asymptotics of a policy over many possible environments. One drawback is that the regret is not scale invariant, which means we normally have to normalize our rewards distributions, assuming they are either bounded on a unit interval $[0,1]$, or restricting the spread of the distributions.

To calculate the regret, it is often useful to apply a certain formula measuring the expected regret in terms of the number of bad decisions we make. Define a new random variable
%
\[ T_n(a) = \# \{ 1 \leq k \leq n : A_k = a \} \]
%
which counts the number of times the action $a$ is chosen. We find that
%
\begin{align*}
    \mathbf{E} \left[\sum_{k = 1}^n (\mu^* - X_k) \right] &= \sum_{k = 1}^n \mu^* - \mathbf{E}[ \mathbf{E}[X_k | A_k] ]\\
    &= \sum_{a \in A} (\mu^* - \mu_a) \sum_{k = 1}^n \mathbf{P}(A_k = a)\\
    &= \sum_{a \in A} (\mu^* - \mu_a) \mathbf{E}(T_n(a))\\
    &= \sum_{a \in A} \Delta_a \mathbf{E}(T_n(a))
\end{align*}
%
where $\Delta_a = \mu^* - \mu_a$ is the {\bf action gap} between $a$ and the optimal action.

In applied statistics, it is normal to blindly apply the law of large numbers to conclude that, provided we sample an action infinitely many times, the random variables
%
\[ \widehat{\mu_n}(a) = \frac{1}{T_n(a)} \sum_{k = 1}^n \mathbf{I} \{A_k = a \} X_k \]
%
converge to the sample mean. However, for our purposes, we want to obtain explicit bounds on the quantities obtained, so the convergence rate of the sample means comes into question. In general, the convergence rate is questionable, but reasonable assumptions about random variables will give us quantifiable bounds which we can use to bound the regret we obtain; we will rarely apply the law directly, but use it as a heuristic to tell if a policy is learning correctly.

One problem with calculating the means above is that the naive way needs to store the integer $T_n(a)$, as well as $\mathbf{I}(A_m = a) X_m$ over all choices of $m$. The formula above allows to calculate the sample means in $\Theta(nK)$ time with $\Theta(nK)$ memory usage. However, there is a computational trick for keeping track of these estimates without having to store the values of the indicator function for all $k$. Write
%
\[ \widehat{\mu_{n+1}}(a) = \frac{T_n(a) \widehat{\mu_n}(a) + \mathbf{I} \{ A_{n+1} = a \} X_{n+1}}{T_{n+1}(a)} \]
%
and this gives a recurrence relation meaning we need only keep track of $T_n(a)$ and $\widehat{\mu_n}(a)$, for the rest of the data is given to us sequentially and therefore does not need to be memorized. Thus we can calculate new sample means in $\Theta(K)$ time, and we need only memorize the $T_n(a)$ for the most recent $n$, so that the memory usage is only $\Theta(K)$, a constant amount that does not depend on the horizon.

\section{Naive and Epsilon-Greedy Policies}

A naive policy, the greedy method, always takes the action with the highest sample average. The problem with this method is, if the sample reward for some action becomes distorted from it's actual mean, it may never be taken again, even if the actual expected reward is much higher. This normally occurs if the variance of actions is too high. Given a low-variance reward function, the greedy policy may choose the right action, but it is very difficult to give a mathematical guarantee. One helpful trick may be to set $\widehat{\mu_0}(a)$ optimistically, that is, not the default value of zero but some value higher than all rewards for an action. Then all values are at least tried once before they begin to settle down, so some exploration is done. Again, we cannot prove anything successful about this policy because it's decisions are so volatile. A somewhat smarter policy, the $\varepsilon$-greedy method, attempts to fix this issue by guaranteeing convergence at the cost of the hope of complete optimality.

Fix some real number $\varepsilon$ with $0 \leq \varepsilon \leq 1$. The idea of the $\varepsilon$ greedy method is to act greedily, except for a small portion of time where we act randomly, ensuring we sample enough times for our sample averages to converge. The $\varepsilon$ greedy policy $\pi$ is defined for each action $a$ by the distribution
%
\[ \mathbf{P}(A_{n+1} = a | H_n)  =
        \begin{cases} \varepsilon + \frac{1 - \varepsilon}{K} &: a = \text{argmax}_{b \in A}\ \widehat{\mu_n}(b)\\
        \frac{1 - \varepsilon}{K} &: \text{otherwise}
        \end{cases} \]
%
We can use the regret decomposition formula to conclude that
%
\[ R_n = \sum_a \Delta_a \mathbf{E}(T_n(a)) \leq n \sum_a \Delta_a \left( \varepsilon + \frac{1 - \varepsilon}{K} \right) \]
%
We also have upper bounds
%
\[ R_n \geq \sum_a \Delta_a \mathbf{E}(T_n(a)) \geq \sum_a \Delta_a \frac{1 - \varepsilon}{K} \]
%
This is not the best situation to be in, for the equation show we still have linear regret in $n$, but it is a good start, for at least we have some mathematical bound on the policy, and the slope of the linear regret is bound to be better than a policy which randomly chooses actions.

As a remark, we should expect the second upper bound to be much tighter to the first, because to obtain the first bound we had to. The first upper bounded things from the inequality
%
\[ \mathbf{P}(A_k = a) \leq \varepsilon + \frac{1 - \varepsilon}{K} \]
%
but the inequality should be bounded much closer to $(1 - \varepsilon)/K$ for all actions buts ones which are close to optimal, since these actions just rarely have the best sample mean. However, the greediness implicit in the algorithm makes it hard to bound the probabilities better (or at least, with my current knowledge, I can't find a direct way).

So what are the shortcomings of the $\varepsilon$-greedy policy? The first is that it considers every action by a probability that is lower bounded by a constant greater than zero, so that we have guaranteed linear regret. In order to obtain better results, we must choose actions with probabilities which vary over time -- being large over all actions initially to explore our opportunities, then honing in on potential good choices.

\section{Explore Then Commit}

Another intuitive strategy which seems viable is to explore for a certain amount of time, and once this time is finished, decide on an optimal action and then always perform that action. A mathematical analysis of this strategy requires that the variance of the function by well-defined, so that we assume the reward distribution are always subgaussian (1-subgaussian to make the formulas nice, but we the results can be applied to all subgaussian variables by scaling).

The explore then commit strategy is defined with respect to a parameter $m$, which samples each action $m$ times equally, obtaining estimates $\widehat{\mu}_a$ of the means. The policy then chooses the action $a'$ corresponding to the maximum estimated mean $\widehat{\mu}_a$. The rate of growth of the expected regret then becomes the difference
%
\[ \sum_a \mathbf{P}(a' = a) \Delta_a \]
%
If $a^*$ is an optimal action, then
%
\begin{align*}
    \mathbf{P}(a' = a) &\leq \mathbf{P}(\widehat{\mu_a} - \widehat{\mu_{a^*}} \geq 0)\\
    &= \mathbf{P}((\widehat{\mu_a} - \mu_a) + (\mu^* - \widehat{\mu_{a^*}}) \geq \Delta_a)
\end{align*}
%
so determining the optimality of the algorithm depends on properties of the tail distribution of the random variable $Y = (\widehat{\mu_a} - \mu_a) + (\mu^* - \widehat{\mu_{a^*}})$. Note that if the reward distributions are $1$-subgaussian, then $Y$ is $2/m$ subgaussian, and so by standard properties of subgaussian random variables, we find
%
\[ \mathbf{P}(Y \geq \Delta_a) \leq e^{-m\Delta_a^2/4} \]
%
and so we can bound for the expected regret to be
%
\[ R_n \leq \sum_a \Delta_a \left[ m + (n - |A|m)e^{-m\Delta_a^2/4} \right] \]
%
The expected regret per round can be very large for small values of $m$, but decreases exponentially as $m$ is increased. However, as we increase $m$ the total regret obtained over the initial rounds increases.

For a two-armed bandit, the expected regret after $n$ rounds (for large enough $m$) will be bounded by
%
\[ m \Delta + (n - 2m) \Delta e^{-m \Delta^2/4} \leq m \Delta + n \Delta e^{-m \Delta^2/4} \]
%
We should expect these bounds to be tight, the first because the subgaussian bound is tight, and the second because if $m$ is large, $e^{-m \Delta^2/4}$ will be very small, and the difference between $(n - 2m) e^{-m \Delta^2/4}$ and $ne^{-m \Delta^2/4}$.

If we knew $\Delta$ and $n$ in advance, we could minimize this bound by choosing the optimal $m$. Indeed, we find that
%
\[ \frac{\partial}{\partial m}(m + n e^{-m \Delta^2/4}) = 1 - (\Delta^2 n/4) e^{-m \Delta^2/4} \]
%
and setting this equation equal to zero gives us
%
\[ m = \frac{4}{\Delta^2} \log \left(\frac{n \Delta^2}{4} \right) \]
%
so taking the actual exploration number to be the floor or ceiling of this number should give us optimal estimates, giving us a regret bound approximately equal to
%
\[ R_n \leq \frac{4}{\Delta} (1 + \log(n\Delta^2/4)) \]
%
If $\Delta \to 0$, this bound becomes useless. However, we can always use the trivial bound $R_n \leq n \Delta$, so that
%
\[ R_n \leq \min \left( n \Delta, \frac{4}{\Delta} (1 + \log(n \Delta^2/4)) \right) \]
%
The only problem with this strategy is that we require knowledge of both $n$ and $\Delta$ to obtain logarithmic regret, but this regret depends on both $\Delta$ and $n$, and we rarely know $n$ ahead of time, let alone $\Delta_a$ ahead of time in real applications.

Suppose we know $m$, but not $\Delta$. The mathematical way to measure how good a policy is if it doesn't know some information is to consider the worst case regret with respect to $\Delta$. If we let $\Delta$ take all positive values, then this worst-case would always be infinite, so we normally enforce that $\Delta \in [0,1]$, and so we want to optimize
%
\[ \max_{\Delta \in [0,1]} \min_{1 \leq m \leq n} R_n(m, \Delta) \]
%
where $R_n(m, \Delta)$ is the expected regret over the two-armed bandit environment corresponding to $\Delta$, and $m$ is the exploration parameter to the explore then commit algorithm. We shall prove asymptotically tight bounds on the `correct' choice of $m$ in this situation.

If $\Delta \leq 4n^{-1/3}$, then we trivially have a bound
%
\[ R_n(m, \Delta) \leq n \Delta \leq 4n^{2/3} \]
%
regardless of our choice of $m$. If we assume $\Delta \geq 4n^{-1/3}$, and if we choose $m = \log(n) n^{1/3}/3$, we find
%
\begin{align*}
    R_n(m, \Delta) &\leq \Delta(m + n e^{-m \Delta^2/4})\\
    &= \Delta (\log(n) n^{1/3}/3 + n e^{- \log(n^{1/3})})\\
    &= \Delta(\log(n) n^{1/3}/3 + n^{2/3}) = \Delta O(n^{2/3})
\end{align*}
%
So we have a $\Delta O(n^{2/3})$ bound for the regret. This is effectively the best we can do. For any $n$ and $m$, if we let $\Delta^2 = 4 \log(n^{1/3})/m$, then we find
%
\[ R_n(m, \Delta) \approx \Delta (m + n e^{-m \Delta^2/4}) = \Delta(m + n^{2/3}) \geq \Delta n^{2/3} = \Delta \Omega(n^{2/3}) \]
%
This asymptotic lower bound still occurs if we assume the action gap is bounded. If our choice of $\Delta$ above happens to be $> 1$, then we conclude $\log(n) > 3m/4$, which means we hardly explore at all. We then find
%
\[ \Delta (m + n e^{-m \Delta^2/4}) > \Delta n e^{-\log(n) \Delta^2/3} = \Delta n^{1 - \Delta^2/3} \]
%
Choosing $\Delta = 1$ gives us a $\Delta n^{2/3}$ regret lower bound (since we hardly explore, a large action gap has little effect on the power of the algorithm determining the best action). There are algorithms which have $O(n^{1/2})$ regret, so the explore then commit bandit is still asymptotically suboptimal.

\section{The UCB Algorithm}

A more advanced and recent solution to the problem results from acting optimistically -- we always choose our actions as if the environment was as nice as plausibly possible. In this case, we either choose the best action (our optimism was deserved), or we choose a suboptimal action which we learn is worse than others available. The difference between this algorithm and the greedy algorithm is that confidence intervals are much more tractable to formally analyze, and it's easier to prevent yourself from becoming `trapped' in a suboptimal action. If $X_1, \dots, X_n$ are i.i.d and 1-subgaussian, then the sample means satisfy
%
\[ \mathbf{P}(\widehat{\mu_n} \geq \varepsilon) \leq e^{-n\varepsilon^2/2} \]
%
By a change in variables, we find
%
\[ \mathbf{P}\left(\widehat{\mu_n} \geq \sqrt{\frac{2}{n} \log(1/\delta)} \right) \leq \delta \]
%
So we have constructed confidence intervals with a probability of $\delta$ being correct. If we are optimistic, then it is consistent with our hypothesis that
%
\[ \mu = \widehat{\mu_n} + \sqrt{\frac{2}{n} \log(1/\delta)} \]
%
The upper confidence bound algorithm chooses the action $a$ which maximizes this quantity (after choosing each action once, so that the values are well defined). We see now that our algorithm is more optimistic for small values of $\delta$, and less optimistic for large values. It will be useful to decrease $\delta$ over time, to avoid becoming `trapped' in suboptimal actions, or to become more and more confident in our action over time.

A standard choice is $\delta_n = 1 + n \log^2 n$, which increases slightly faster than $n$. Why is this chosen? If an optimal arm is discredited even once, an algorithm must pay linear regret. If we fix $\delta$, we will choose a suboptimal action $(1 - \delta)$ of the time. Trying to fix this by adjusting the $\delta$ so that this occurs with $1/n$ probability, we find that we need to choose $\delta_n$.

If our algorithm is to achieve good regret bounds, it should only pick an arm many times if it is highly confident of the arm's reward. This leads directly to the notion of confidence intervals, and thus to the upper confidence bound algorithm. The upper confidence bound algorithm is asymptotically much better than the other algorithms we have considered. While all other algorithms have linear regret, this algorithm actually has logarithmic regret. This is proven by analyzing the number of times a suboptimal action $a$ is taken. First, a lemma.

\begin{lemma}
    Let $X_1, X_2, \dots$ be a sequence of $1$-subgaussian random variables, with sample average $\widehat{\mu_n}$, $\varepsilon > 0$, and
    %
    \[ Y = \sum_{k = 1}^n \mathbf{I} \left(\widehat{\mu_k} + \sqrt{\frac{2M}{k}} \geq \varepsilon \right) \]
    %
    Then
    %
    \[ \mathbf{E}[Y] \leq 1 + \frac{2}{\varepsilon^2} \left(M + \sqrt{\pi M} + 1 \right) \]
\end{lemma}
\begin{proof}
    We just use the one inequality we know for subgaussian variables. Let $u = 2M/\varepsilon^2$. Then
    %
    \begin{align*}
        \mathbf{E}[Y] &= \sum_{k = 1}^n \mathbf{P} \left(\widehat{\mu_k} + \sqrt{\frac{2 M}{k}} \geq \varepsilon \right)\\
        &\leq (u+1) + \sum_{k = \lceil u + 1 \rceil}^n e^{-n(\varepsilon - \sqrt{2M/n})^2/2}\\
        &\leq (u+1) + \int_u^\infty e^{-t(\varepsilon - \sqrt{2M/t})^2/2} dt\\
        &= 1 + \frac{2}{\varepsilon^2}(M + \sqrt{\pi M} + 1)
    \end{align*}
    %
    and this gives us the needed bound.
\end{proof}

To analyze the regret of the UCB algorithm, we shall bound $\mathbf{E}(T_n(a))$, for a non-optimal action $a$. Note that for a fixed $\varepsilon$,
%
\begin{align*}
    T_n(a) &= \sum_{k = 1}^n \mathbf{I}(A_k = a)\\
    &\leq \sum_{k = 1}^n \mathbf{I} \left(\widehat{\mu^*}(k-1) + \sqrt{\frac{2 \log 1/\delta_k}{T_a(k-1)}} \leq \mu^* - \varepsilon \right)\\
    &\ \ \ \ \ + \mathbf{I} \left(\widehat{\mu_a}(k-1) + \sqrt{\frac{2 \log 1/\delta_k}{T_a(k-1)}} \geq \mu^* - \varepsilon\ \ \text{and}\ \ A_k = a \right)
\end{align*}
%
Let's bound the first term, using the fact that tails of subgaussian variables are small. If $T_a(k-1) = m$, then $\widehat{\mu_a}$ is $1/m$ subgaussian, and so
%
\begin{align*}
    \mathbf{E}& \left[ \sum_{k = 1}^n \mathbf{I} \left(\widehat{\mu^*}(k-1) + \sqrt{\frac{2 \log 1/\delta_k}{T_a(k-1)}} \leq \mu^* - \varepsilon \right) \right]\\
    &= \sum_{k = 1}^n \mathbf{P}\left(\widehat{\mu^*}(k-1) + \sqrt{\frac{2 \log 1/\delta_k}{T_a(k-1)}} \leq \mu^* - \varepsilon \right)\\
    &\leq \sum_{k = 1}^n \sum_{m = 1}^k \mathbf{P}\left( \left. \widehat{\mu^*} + \sqrt{\frac{2 \log 1/\delta_k}{m}} \leq \mu^* - \varepsilon \right| T_a(k-1) = m \right) \mathbf{P}(T_a(k-1) = m)\\
    &\leq \sum_{k = 1}^n \sum_{m = 1}^k \exp\left(-(m/2) \left(\varepsilon + \sqrt{2 \log(1/\delta_k)/m}\right)^2 \right)\\
    &\leq \sum_{k = 1}^n \frac{1}{\delta_k} \sum_{m = 1}^k e^{-m\varepsilon^2/2}\\
    &\leq \sum_{k = 1}^n \frac{1}{1 + k \log^2 k} \frac{1}{1 - e^{- \varepsilon^2/2}}\\
    &\leq \frac{2}{1 - e^{- \varepsilon^2/2}} \leq \frac{4}{\varepsilon^2}
\end{align*}
%
where we use the fact that $\delta_k = 1 + k \log^2 k$. We apply the previous lemma to the second indicator function to obtain
%
\begin{align*}
    \mathbf{E}& \left( \sum_{k = 1}^n \mathbf{I} \left(\widehat{\mu_a}(k-1) + \sqrt{\frac{2 \log 1/\delta_k}{T_a(k-1)}} \geq \mu^* - \varepsilon\ \ \text{and}\ \ A_k = a \right) \right)\\
    &\leq \mathbf{E} \left[ \sum_{k = 1}^n \mathbf{I}(\widehat{\mu_a}(k-1) + \sqrt{\frac{2 \log 1/\delta_n}{T_a(k-1)}} \geq \mu^* - \varepsilon) \right]\\
    &\leq \mathbf{E} \left[ \sum_{k = 1}^n \mathbf{I}((\widehat{\mu_a}(k-1) - \mu_a) + \sqrt{\frac{2 \log 1/\delta_n}{T_a(k-1)}} \geq \Delta_a - \varepsilon) \right]\\
    &\leq 1 + \frac{2}{(\Delta_a - \varepsilon)^2} \left( \log 1/\delta_k + \sqrt{\pi \log 1/\delta_k} + 1 \right)
\end{align*}
%
Putting the two bounds together, we obtain

\begin{theorem}
    The regret of UCB is bounded by
    %
    \[ R_n \leq \sum_{a \neq a^*} \Delta_a \inf_{\varepsilon \in (0,\Delta_a)} \left(1 + \frac{5}{\varepsilon^2} + \frac{2}{(\Delta_a - \varepsilon)^2} \left(\log 1/\delta_n + \sqrt{\pi \log 1/\delta_n} + 3\right) \right) \]
    %
    If we let $\varepsilon = \Delta_a/2$ in each sum, we find
    %
    \[ R_n \leq \sum_{a \neq a^*} \left( \Delta_a + \frac{1}{\Delta_a} \left( 8 \log 1/\delta_n + 8 \sqrt{\pi \log 1 / \delta_n} + 44 \right) \right) \]
    %
    and there is a universal constant $C > 0$ such that for all $n \geq 2$,
    %
    \[ R_n \leq \sum_{a \neq a^*} \frac{C \log n}{\Delta_a} \]
    %
    If we let $\varepsilon = \log^{1/4}(n)$, and let $n \to \infty$, we find
    %
    \[ \limsup R_n/\log(n) \leq \sum_{a \neq a^*} \frac{2}{\Delta_a} \]
    %
    So $R_n$ has logarithmic regret.
\end{theorem}

We do not need to know the action gaps in order to use UCB, but we require the action gaps in order to obtain the regret bound above. But it is fairly easy to obtain a action-gap free bound. Indeed, if $\Delta = \sqrt{|A| C \log n / n}$, then
%
\begin{align*}
    R_n = \sum \Delta_a \mathbf{E}(T_n(a)) &= \sum_{\Delta_a < \Delta} \Delta_a \mathbf{E}(T_n(a)) + \sum_{\Delta_a \geq \Delta} \Delta_a \mathbf{E}(T_n(a))\\
    &\leq n \Delta + \sum_{\Delta_a \geq \Delta} \frac{C \log n}{\Delta_a}\\
    &\leq n \Delta + |A| \frac{C \log n}{\Delta} = \sqrt{|A| C n \log n}
\end{align*}
%
so that the regret is $O(|A| n \log n)$.

One can improve these bound if we assume the distributions of the arms of the bandit are better than subgaussian. This is mainly done by assuming that the rewards are Bernoulli. Indeed, if we assume $\rho(a)$ is a Bernoulli distribution with mean $\lambda$ and variance $\lambda (1 - \lambda)$. We bound the regret by the two terms bounded above. We keep the same bound for the first one, but for the second bound, we use that ... (I'll finish this later)

\chapter{Lower Bounding Regret}

It turns out that, if we measure a bandit policy by its worst case regret over all the environments in which the policy makes sense, the UCB algorithm has the best asymptotic bound possible over all situations. To show this, we attempt to find the best possible regret we could obtain, assuming our environment is chosen as the worst possible with respect to the current policy. Thus, we are measuring a policy according to the {\bf minimax regret}. We fix some subset $\mathcal{E} \subset \mathcal{E}(A)$, and then take
%
\[ R_n^*(\mathcal{E}) = \inf_\pi \left( \sup_{E \in \mathcal{E}} R_n(\pi, E) \right) \]
%
The value is independant of any policy or environment, and for any policy $\pi$, there is an environment $E \in \mathcal{E}$ such that $R_n(\pi, E) \geq R_n^*(\mathcal{E})$. It essentially tells us how difficult a particular class of environments is for a policy to optimize over. Thus lower bounding $R_n^*$ is exactly what we need to lower bound how well our algorithms will do in the worst case. A {\bf minimax optimal} policy is a policy $\pi$ which minimizes the supremum in the definition of minimax regret. That is, a minimax policy satisfies $R_n^*(\mathcal{E}) \geq R_n(\pi, E)$ for all $E \in \mathcal{E}$. It is almost impossible to find a minimax optimal policy, and it is not even guaranteed that such a policy exists. However, we shall often find {\bf near minimax policies}, which are within a fixed, constant factor of $R_n^*(\mathcal{E})$, for all choices of $n$. If this is too much, we want $\log$ near minimax policies, which are bounding with a logarithmic factor of $R_n^*(\mathcal{E})$. Once we have found the appropriate lower bound for $R_n^*$ over the class $\mathcal{E}_1$ of all $1$-subgaussian bandits, we will have essentially verified that the upper confidence bound algorithm is optimal for the class of $1$-subgaussian environments. The UCB gives us the bound
%
\[ R_n^*(\mathcal{E}_1) \leq C \sqrt{K n \log n} \]
%
We verify that this is with a log factor of the true optimum, so UCB is log near optimum of the set of 1 subgaussian environments. Thus we have set out to prove.

\begin{theorem}
    For any action space $A$, let $\mathcal{E}$ be the space of $1$-subgaussian environments. Then there is a universal constant $C$ such that
    %
    \[ R_n^*(\mathcal{E}) \geq C \sqrt{|A| n} \]
    %
    So that the bandit problem on 1-subgaussian is $\sqrt{n}$ hold.
\end{theorem}

\section{Worst Case Lower Bounds}

How on earth can we lower bound regret? The idea descends from information theory, and essentially the only way we can obtain lower bounds for statistical measurements. Consider the most basic problem, where we observe estimates $X_1, \dots, X_n$ from a gaussian distribution with unknown mean $\mu$ and variance $1/n$. We know either $\mu = 0$, or $\mu = \Delta$ for some $\Delta \in \mathbf{R}$, and we must decide which is true. If $n$ is large compared to $\mu$, then we should expect the problem to be easy, whereas if $n$ is small the problem should be hard. Indeed, the concentration bound gives upper bounds
%
\[ (2 \pi n (\Delta/2)^2)^{-1/2} \exp(-n (\Delta/2)^2/2) \leq \exp(- \frac{n(\Delta/2)^2}{2}) \]
%
so we have exponential decay in $n$. Using integration by parts, we can calculate that if $X \sim N(0,\sigma^2)$, then
%
\[ \mathbf{P}(X > \varepsilon) \geq \left( \frac{\sigma}{\varepsilon} - \frac{\sigma^3}{\varepsilon^3} \right) \frac{\exp(-\varepsilon^2/(2\sigma^2))}{\sqrt{2\pi}} \]
%
This shows that any decision procedure with $n \leq 8c/\Delta^2$ will make a mistake with probability at least $Cc^{-1/2}(1 - 1/c) \exp(-c)$. The high level idea is to construct two environments which are impossible to separate with the number of samples given. We will perform the same technique on bandits, constructing bandits similar enough that cannot be distinguished, but such that a policy doing well on one instance cannot do well on the other instance, so an algorithm is forced to make a decision between the two instances. Thus we must reduce the bandit problem to a hypothesis testing problem. The lower bound has little interest in algorithms ran on a fixed instance, since we can often obtain much better bounds for specific instances (indeed, any environment has a policy that has zero, regret -- the challenge is finding a policy that works uniformly well across all environments).

To prove this, we apply the High probability Pinsker bound of information theory, using the Kullback Leibler distance to measure the ability for a policy to distinguish between two environments. That is,

\begin{theorem}[High Probability Pinsker]
    Let $\mu$ and $\nu$ be probability measures on the same measure space. Then for any event $A$,
    %
    \[ \mu(A) + \nu(A^c) \geq \frac{1}{2} \exp(-D(\mu,\nu)) \]
    %
    Where $D(\mu, \nu)$ is the Kullback Leibler distance betwee $\mu$ and $\nu$.
\end{theorem}

Intuitively, if $\mu$ and $\nu$ are difficult to distinguish between, then we should expect $\mu(A) \approx \nu(A)$, and $\mu(A^c) \approx \nu(A^c)$, so $\mu(A) + \nu(A^c)$ should be large. For instance, for any $\delta$, to guarantee that $\max(\mu(A),\nu(A^c)) \leq 1$, the Pinsker inequality tells us that we should have $D(\mu,\nu) \geq \log(1/4\delta)$. The theorem is useful for proving lower bounds, because it implies that $\mu(A)$ or $\nu(A^c)$ is large. If we make $\mu$ and $\nu$ be such that $A$ is a `bad' event for $\mu$ (a misclassification of a hypothesis), and $A^c$ a `bad' event for $\nu$, then we can guarantee that any decision making procedure working of $\mu$ and $\nu$ could make a mistake with high probability.

So how do we formulate the problem with bandit optimality as a hypothesis testing problem? Given an environment $\mu$, policy $\nu$, and fixed horizon $n$, we let $\mathbf{P}_{\mu, \pi}$ be the distribution over histories of length $n$ induced by the environment $\mu$ and policy $\pi$, and $\mathbf{E}_{\mu, \pi}$ the expectation with respect to this distribution. Our lower bound will depend on fixing $\pi$, and altering $\nu$ slightly so that $\pi$ fails to realize the needed bound. First, note that
%
\[ d\mathbf{P}_{\mu, \pi}(a_1,x_1,\dots,a_n,x_n) = \prod_{m = 1}^n \pi(a_m | a_1, x_1, \dots, a_{m-1}, x_{m-1}) f^\mu_{a_m} (x_m | a_m) d\lambda(x_m) d\nu(a_m) \]
%
where $\lambda$ is a dominating measure for the $\rho_a$, and $d\rho_a = f^\mu_a d\lambda$, and $\nu$ is the counting measure.

\begin{lemma}
    For any policy $\pi$,
    %
    \[ D(\mathbf{P}_{\pi, \mu}, \mathbf{P}_{\pi, \nu}) = \sum_a \mathbf{E}_{\pi, \mu}[T_a(n)] D(\mu(a), \nu(a)) \]
\end{lemma}
\begin{proof}
    First assume $D(\mu_a, \nu_a) < \infty$ for all $a$. Then by the chain rule for Radon Nikodym derivatives, and by using the decomposition formula above, the policy ratios cancel out and we find
    %
    \begin{align*}
        D(\mathbf{P}_{\pi, \mu}, \mathbf{P}_{\pi, \nu}) &= \mathbf{E}_{\pi, \nu} \left[\log \left( \frac{d\mathbf{P}_{\pi, \mu}}{d\mathbf{P}_{\pi, \nu}} \right) \right]\\
        &= \sum_{a_1, \dots, a_n} \int \log \left( \frac{f_{a_1}^\mu(x_1) \dots f_{a_n}^\mu(x_n)}{f_{a_1}^\nu(x_1) \dots f_{a_n}^\nu(x_n)} \right) \\
        &\ \ \ \ \ \ \left( \prod_{m = 1}^n \pi(a_m | a_1, \dots, x_{n-1}) f_{a_m}^\mu(x_m | a_m) \right) d\lambda(x_1) \dots d\lambda(x_n)\\
        &= \sum_{m = 1}^n \mathbf{E}_{\mu,\pi}[D(\mu_{A_m}, \nu_{A_m})]\\
        &= \sum_a \mathbf{E}_{\mu, \pi} \left[ \sum_{m = 1}^n \mathbf{I}(A_m = a) D(\mu_a, \nu_a) \right]\\
        &= \sum_a \mathbf{E}_{\mu, \pi}[T_n(a)] D(\mu_a, \nu_a)
    \end{align*}
    %
    And this was the formula we wanted to prove.
\end{proof}

Denote by $G_\mu \in \mathcal{E}_1$ the bandit environment where all distributions are Gaussian with unit variance and means $\mu \in [0,1]^A$. We let $R_n(\pi,E)$ denote the expected regret for a policy $\pi$ on an environment $E$.

\begin{theorem}
    For any policy $\pi$, there is a mean vector $\mu \in [0,1]^A$ such that
    %
    \[ R_n(\pi, G_\mu) \geq \frac{1}{27} \sqrt{(|A| - 1) n} \]
\end{theorem}
\begin{proof}
    Let $\Delta \in [0,1/2]$ be the action gap, to be chosen later to optimize a bound. Pick some $a^* \in A$, and define
    %
    \[ \mu(a) = \begin{cases} \Delta & a = a^* \\ 0 & \text{otherwise} \end{cases} \]
    %
    Now let $a' = \text{argmin}_{b \neq a^*} \mathbf{E}_{\mu,\pi}[T_b(n)]$. Define a second environment
    %
    \[ \mu'(a) = \begin{cases} \Delta & a = a^* \\ 2 \Delta & a = a' \\ 0 & \text{otherwise} \end{cases} \]
    %
    Note that the optimal action in the first action is $a^*$, whereas in the second algorithm the best action is $a'$. Now if $\pi$ chooses action $a^*$ more often, it will likely succeed on $G_\mu$, but not $G_{\mu'}$. Conversely, if $\pi$ chooses $a'$, then it will succeed on $G_{\mu'}$, but not $G_\mu$. In particular
    %
    \[ R_n(\pi, G_\mu) \geq \mathbf{P}_{\mu, \pi}(T_n(a^*) \leq n/2) \frac{n\Delta}{2}\ \ \ \ \ \ \ \ R_n(\pi, G_{\mu'}) \geq \mathbf{P}_{\mu', \pi}(T_n(a^*) > n/2) \frac{n\Delta}{2} \]
    %
    The Pinsker inequality implies, since $\mathbf{E}_{\mu, \pi}[T_{a'}(n)] \leq n/(|A|-1)$, by the minimality choice,
    %
    \begin{align*}
        R_n(\pi, G_\mu) + R_n(\pi, G_{\mu'}) &\geq \frac{n\Delta}{2} \left( \mathbf{P}_{\mu, \pi}(T_n(a^*) \leq n/2) + \mathbf{P}_{\mu', \pi}(T_n(a^*) > n/2) \right)\\
        &\geq \frac{n \Delta}{4} \exp(- \sum_a \mathbf{E}_{\pi, \mu} [T_n(a)] D(N(\mu_a,1), N(\mu'_a,1))\\
        &= \frac{n \Delta}{4} \exp( - 2 \mathbf{E}_{\pi, \mu}[T_n(a')] \Delta^2 )\\
        &= \frac{n \Delta}{4} \exp( - 2n \Delta^2/(|A| - 1) )
    \end{align*}
    %
    If we tune $\Delta^2 = (|A|-1)/4n$, we obtain the required bound.
\end{proof}

Now we assumed that the class of 1-subgaussian bandits obtained the Gaussian distributions, we only ever used the fact that the class contained probability distributions $\rho^\mu$ with certain means $\mu_a$, such that $D(\rho^\mu_a, \rho^{\mu'}_a) = O((\mu_a - \mu_a')^2)$. This is certainly not true of all families of distributions, but we should expect it to be true for a large class of algorithms, since if we have a parametric family of functions whose densities are twice differentiable, we can approximate these parameters by the second derivative, and this gives us bounds of the form needed above.

Note that UCB is log near optimal, but a slight modification of UCB, known as MOSS, can obtain near optimal estimates by chopping off the log factor. Unfortunately, we do not have time to discuss it here, so we refer to the \href{http://research.microsoft.com/en-us/um/people/sebubeck/COLT09_AB.pdf}{paper} by Bubeck and Audibert.

\section{Instance Dependent Lower Bounds}

Just because the UCB algorithm is log near optimal for the class of 1 subgaussian environments, is not a sufficient argument to justify that the algorithm is necessarily is the correct algorithm to use over all 1-subgaussian environments. Indeed, consider the modification of the UCB algorithm, which takes $0 < D \ll C$, where $C$ is the constant in the upper bound for UCB. The modified version of UCB explores in the first $m = D \sqrt{|A| n}$ rounds, and then switch to the UCB policy. It is not difficult to see that this version of UCB is also log near optimal in the worst case.

So why do we feel like UCB is still a better algorithm? Consider a 1-subgaussian environment $\nu$ where choosing any action suboptimally gives you a reward close to one. Then UCB will stop using suboptimal actions very quickly, so that UCB would achieve small regret. Indeed, the upper bound gives
%
\[ R_n \leq \sum_a \frac{C \log(n)}{\Delta_a} \sim C|A| \log n \]
%
whereas the new exploratory version of UCB we defined obtains $C' \sqrt{|A|n/2}$ guaranteed regret, which for large $n$ will eventually be larger than the other bound. The problem is that even though UCB and exploratory UCB obtain essentially the same regret in the worst case, UCB tends to achieve much better regret bounds on easier instances of the problem.

Thus we desire our algorithms to not only be bad on worst-case instances, but also get better regret bounds on `nice' instances on the problem. We have shown that in certain instances UCB can obtain logarithmic regret. One difficulty of a particular instance of the bandit problem could be measured as
%
\[ \limsup_{n \to \infty} \frac{R_n}{\log n} \]
%
We shall show that in some sense, the logarithmic regret bound for UCB on certain problems is the best possible. No {\it reasonable} strategy can beat UCB on any instance of the bandit problem. This is an important result, separate from the worst case regret bounds, for it shows why UCB can be successfully applied to both reasonable and unreasonable environments, with good asymptotic regrets in both, relative to the environment chosen. However, the logarithmic measure of difficulty is perhaps overly restrictive, so we will only require that the policy chosen has subpolynomial growth.

Let us rigorously define these ideas. A policy $\pi$ over $\mathcal{E}$ is {\bf consistant} if for any environment $\mu \in \mathcal{E}$, and $p > 0$,
%
\[ R_n(\pi, \mu) = O_{p, \mu}(n^p) \]
%
The set of consistant policies is denoted $\Pi_{\text{cons}}(\mathcal{E})$. The UCB policy is consistant, because it gives asymptotically logarithmic regret over individual environments. The main theorem of this section is that UCB is the best consistant policy over Gaussian bandits. Let $\mathcal{E}_N$ be the class of bandits whose arms have unit variance normal distributions.

\begin{theorem}
    For any consistant policy $\pi$ over $\mathcal{E}_N$, and $\nu \in \mathcal{E}_N$,
    %
    \[ \liminf_{n \to \infty} \frac{R_n(\pi \nu)}{\log n} \geq \sum_a \frac{2}{\Delta_a(\nu)} \]
    %
    so that UCB is optimal to a fixed constant factor.
\end{theorem}
\begin{proof}
    We apply the Pinsker inequality over Gaussian environments. Consider a sequence of means $\mu$, pick some action $a'$, and consider a new sequence
    %
    \[ \mu'(a) = \begin{cases} \mu(a) + \lambda & a = a' \\ \mu(a) & \text{otherwise} \end{cases} \]
    %
    Let $A = \{ T_{a'}(n) \geq n/2 \}$, so $A^c = \{ T_{a'}(n) < n/2 \}$. We have
    %
    \[ R_n(\mu, \pi) \geq \frac{n \Delta_{a'}}{2} \mathbf{P}_{\mu, \pi}(A) \ \ \ \ \ R_n(\mu', \pi) \geq \frac{n(\lambda - \Delta_{a'})}{2} \mathbf{P}_{\mu', \pi}(A^c) \]
\end{proof}

\chapter{Adversarial Bandits}

In many situations, we want to treat a problem like a bandit problem, even though there are no guarantees of stationarity in the distribution. For instance, we may be trading stocks, in which case the action of buying a stock is certainly non stationary over time. We also should avoid using the upper confidence bound algorithm, because other traders may catch onto our strategy and exploit our optimism for their own ends. The adversarial environment provides theoretical guarantees on the success of these algorithms, by assuming there's a person on the other end of the algorithm setting the rewards to screw you over. This implies that regardless of the rewards we actually get, we will do better than if there was someone purposefully trying to cause you to get the worst reward.

The reward distribution of the adversarial bandit problem is completely non-stationary. We consider an environment $\nu \in [0,1]^{n \times A}$ (we have to assume the rewards are bounded, or else the adversarial problem cannot really be specified -- someone can always cause you to lose an unbounded amount of money). A policy is specified as in the stochastic bandit problem, giving a probability distribution over histories, and determining a sequence of actions and rewards
%
\[ A_1, X_1, A_2, X_2, \dots \]
%
where $(X_i | A_i = a) \sim \nu(i,a)$ (reward is deterministic, though the choice of action is not). The adversarial regret of a policy $\pi$ with respect to the environment $\nu$ is
%
\[ R_n(\pi,\nu) = \mathbf{E} \left[\left( \max_a \sum_{i=1}^n \nu(i,a) \right) - \sum_{i = 1}^n X_i \right] \]
%
the adversarial regret measures the difference between the optimal `expoit' strategy, and your policy. Given a fixed $\nu$, it is always possible to obtain zero regret, and there is even the possibility of negative regret (how?)!

The difference between the adversarial bandit problem and the stochastic bandit problem is that the goal is not to optimize over a specific environment. Instead, we aim to find a policy which obtains a best worst case regret over all policies. That is, we wish to minimize the minimax regret
%
\[ R_n^*(\pi) = \sup_{\nu \in [0,1]^{n \times A}} R_n(\pi, \nu) \]
%
Game theoretically, we choose a policy, and then our opponent gets to pick the worst environment for our policy. The main question is whether we can find policies for which $R_n^*(\pi)$ is sublinear (as $n \to \infty$).

Unlike in the stochastic problem, deterministic policies are completely unusable in the adversarial setting. If an opponent knows your strategy for a game, then it is very easy to take advantage of your choices. Consider the actions $a_1, a_2, \dots, a_n$ for the deterministic policy, assuming that $\nu(i,a_i) = -1$ for all $i$, and then assign $\nu(j,a_j) = 1$ for all other actions. Then we have
%
\[ R_n(\pi, \nu) = n + \text{max}\left(\sum_{i = 1}^n \nu(i,a) \right) \geq n + n \left( 1-2/|A| \right) \]
%
Which gives us at least linear regret for $R_n^*(\pi)$. This tells us that good algorithms for the adversarial problem should be random, and not just random, but unpredictably random.

Note, however, that an adversarial policy will perform at least as good in the stochastic setting as it does in the adversarial setting, for the stochastic regret will always be pointwise better than the minimax regret. This means we may apply the lower bounds we have obtained before to conclude that $R_n^* \geq C \sqrt{n |A|}$ for some constant $C$. We cannot directly apply the lower bound for bandits, since there we had Gaussian payoffs, but we can provide a very similar argument for Bernoulli bandits, which do apply to this situation.

The algorithm that is near minimax adversarial optimal is the EXP3 algorithm (Exponential weight algorithm for Exploration and Exploitation). The idea is that, since rewards are bounded, there is no way to `sneak' a best policy past an agent. In order for an action to be the `best', it must consistently give good rewards, and we can take advantage of that.

In order to estimate the best action, we keep track of a probability distribution over actions, which we use to sample the next action, and the obtained reward will be used to update the distribution. Initially, the distribution is uniform. Consider the {\bf importance sampling estimators}
%
\[ \widehat{X}_{n,a} = \frac{\mathbf{I}(A_n = a)}{\mathbf{P}(A_n = a | H_{n-1})} X_n \]
%
The estimates are random, and
%
\[ \mathbf{E}[\widehat{X}_{n,a} | H_{n-1}] = \mathbf{E}(X_n | H_{n-1}, A_n = a) = \nu(n,a) \]
%
and they are therefore unbiased estimates. We similarily calculate the second moment
%
\[ \mathbf{E}(\widehat{X}_{n,a}^2 | H_{n-1}) = \frac{\nu(n,a)^2}{\mathbf{P}(A_n = a | H_{n-1})} \]
%
and therefore the variance
%
\[ \mathbf{V}(\widehat{X}_{n,a} | H_{n-1}) = \frac{1 - \mathbf{P}(A_n = a | H_{n-1})}{\mathbf{P}(A_n = a | H_{n-1})} \nu(n,a)^2 \]
%
This variance explodes as the probability of choosing a particular action decreases to zero. A similar estimator is
%
\[ \widehat{X}_{n,a} = 1 - \frac{\mathbf{I}(A_n = a)}{\mathbf{P}(A_n = a | H_{n-1})} (1 - X_n) \]
%
The estimator is unbiased, and if we set $Y_n = 1 - X_n$, and $\widehat{Y}_{n,a} = 1 - \widehat{X}_{n,a}$, then the above formula can be reexpressed as
%
\[  \widehat{Y}_{n,a} = \frac{\mathbf{I}(A_n = a)}{\mathbf{P}(A_n = a | H_{n-1})} Y_n \]
%
So the estimate is essentially the same old importance sampling estimator, but for $Y_n$. We call the $Y_n$ losses, and the estimator above the loss based importance sampling estimator. This is the first time we have met losses, but we could have actually defined the whole bandit regime in terms of losses, where we attempt to minimize loss instead of maximizing reward (except this would only really work on a bandit problem).  The loss based importance sampling estimator will give better results if the losses are on average smaller than the rewards. Note that while the two estimators have the same mean, the first estimator takes values in $[0,\infty)$ and the second estimator takes values in $(-\infty,1]$.

With the estimates of the future actions in hand (where we can, on average, somehow manage to estimate rewards we haven't seen yet), we can define the estimated total reward at the end of round $n$ for action $a$ as
%
\[ \widehat{S}_{n,a} = \sum_{m = 1}^n \widehat{X}_{m,a} \]
%
We then use these estimated totals to perform a weighted average over the next action to select. The standard average to take is an exponential average, taking the probability distribution
%
\[ \mathbf{P}(A_n = a | H_{n-1}) = \frac{e^{\eta \widehat{S}_{n-1, a}}}{\sum e^{\eta \widehat{S}_{n-1, b}}} \]
%
where $\eta > 0$ is a tuning parameter which controls how fast we change probabilities over time. As $\eta \to \infty$ the policy becomes more `greedy'.

As with the mean sampling estimates, it is useful to note that
%
\[ \mathbf{P}(A_{n+1} = a | H_n) = \frac{\mathbf{P}(A_n = a | H_{n-1}) e^{\eta \widehat{X}_{n,a}}}{\sum_b \mathbf{P}(A_n = b | H_{n-1}) e^{\eta \widehat{X}_{n,b}}} \]
%
The algorithm is tricky to implement while also being numerically stable for a large number of rounds. One trick is to calculate $\tilde{S}_{n,a} = \widehat{S}_{n,a} - \min_b \widehat{S}_{n,b}$, and then write
%
\[ \mathbf{P}(A_{n+1} = a | H_n) = \frac{e^{\eta \tilde{S}_{n,a}}}{\sum_j e^{\eta \tilde{S}_{n,b}}} \]
%
This saves us from taking the exponential of too large a value.

Let us prove upper bounds on the minimax regret of this policy.

\begin{theorem}
    For any environment $\nu \in [0,1]^{n \times A}$, the regret of the EXP3 algorithm satisfies
    %
    \[ R_n \leq \sqrt{2 n |A| \log |A|} \]
\end{theorem}
\begin{proof}
    We can reduce the theorem to bounding
    %
    \[ R_{n,a} = \sum_{m = 1}^n \nu(m,a) - \mathbf{E}\left[ \sum_{m = 1}^n X_m \right] \]
    %
    Note that $\mathbf{E}(\widehat{S}_{n,a}) = \sum_m \nu(m,a)$, since the $\widehat{X}_{n,a}$ are unbiased, and
    %
    \[ \mathbf{E}(X_n | H_{n-1}) = \sum_a \mathbf{P}(A_n = a| H_{n-1}) \nu(n,a) = \sum_a \mathbf{P}(A_n = a | H_{n-1}) \mathbf{E}(\widehat{X}_{n,a} | H_{n-1}) \]
    %
    It follows that
    %
    \[ \mathbf{E}\left(\sum_{k = 1}^n X_k\right) = \mathbf{E}\left(\sum_{k = 1}^n \sum_a \mathbf{P}(A_n = a) \widehat{X}_{n,a} \right) \]
    %
    If we define
    %
    \[ \widehat{S}_n = \sum_a \sum_{m=1}^n \mathbf{P}(A_m = a | H_{m-1}) \widehat{X}_{m,a} \]
    %
    It suffices to bound the expectation of $\widehat{S}_{n,a} - \widehat{S}_n$, and we do this by taking exponentials. Denote by $W_n = \sum_a e^{\eta \widehat{S}_{n,a}}$. We let $\widehat{S}_{0,a} = 0$, so that $W_0 = |A|$ and so for any $b$
    %
    \[ e^{\eta \widehat{S}_{n,b}} \leq \sum_a e^{\eta \widehat{S}_{n,a}} = W_n = W_0 \frac{W_1}{W_0} \frac{W_2}{W_1} \dots \frac{W_n}{W_{n-1}} \]
    %
    and
    %
    \[ \frac{W_k}{W_{k-1}} = \sum_a \frac{e^{\eta \widehat{S}_{k-1,a}}}{W_{k-1}} e^{\eta \widehat{X}_{k,a}} = \sum_a \mathbf{P}(A_k = a | H_{k-1}) e^{\eta \widehat{X}_{k,a}} \]
    %
    Since $\widehat{X}_{k,a} \leq 1$ (we're using the loss based estimate), and for $x \leq 0$, $e^x \leq 1 + x + x^2/2$, thus
    %
    \[ e^{\eta \widehat{X}_{k,a}} = e^{\eta} e^{\eta (\widehat{X}_{k,a} - 1)} \leq e^\eta \left[ 1 + \eta (\widehat{X}_{k,a} - 1) + (\eta^2/2) (\widehat{X}_{k,a} - 1)^2 \right] \]
    %
    and so
    %
    \begin{align*}
        \frac{W_k}{W_{k-1}} &\leq \sum_a \mathbf{P}(A_k = a | H_{k-1}) e^{\eta \widehat{X}_{k,a}}\\
        &\leq \sum_a e^\eta \mathbf{P}(A_k = a | H_{k-1}) \left[ 1 + \eta (\widehat{X}_{k,a} - 1) + (\eta^2/2) (\widehat{X}_{k,a} - 1)^2 \right]\\
        &= (1 - \eta + \eta^2/2)e^\eta + \eta e^\eta (1 - \eta) \sum_a \mathbf{P}(A_k = a | H_{k-1}) \widehat{X}_{k,a}\\
        &\ \ \ \ \ + e^\eta (\eta^2/2) \sum_a \mathbf{P}(A_k = a | H_{k-1}) \widehat{X}_{k,a}^2 \\
        &\leq e^{\eta \sum_a \mathbf{P}(A_k = a | H_{k-1}) \widehat{X}_{k,a}} e^{(\eta^2/2) \sum_a \mathbf{P}(A_k = a | H_{k-1}) (\widehat{X}_{k,a} - 1)^2}
    \end{align*}
    %
    Now we put these two inequalities together to find
    %
    \begin{align*}
        e^{\eta \widehat{S}_{n,b}} &\leq |A| e^{\eta \sum_{k=1}^n \sum_a \mathbf{P}(A_k = a | H_{k-1}) \widehat{X}_{k,a}} e^{(\eta^2/2) \sum_{k = 1}^n \sum_a \mathbf{P}(A_k = a | H_{k-1}) (\widehat{X}_{k,a} - 1)^2}
    \end{align*}
    %
    Hence
    %
    \[ \widehat{S}_{n,b} - \widehat{S}_n \leq \frac{\log |A|}{\eta} + (\eta/2) \sum_{k = 1}^n \sum_a \mathbf{P}(A_k = a | H_{k-1}) \widehat{Y}_{k,a}^2 \]
    %
    Now it just remains to bound $\sum_{k = 1}^n \sum_a \mathbf{P}(A_k = a | H_{k-1}) \widehat{Y}_{k,a}^2$, and since $\mathbf{P}(A_k = a | H_{k-1}) \widehat{Y}_{k,a} \leq 1$, we find that the total is bounded by $n |A|$ and so
    %
    \begin{align*}
        \widehat{S}_{n,b} - \widehat{S}_n \leq \frac{\log |A|}{\eta} + (\eta n |A|/2)
    \end{align*}
    %
    And choosing $\eta$ to minimize the inequality gives us the needed result.
\end{proof}

Thus we obtain good expected results. But
%
\[ \widehat{R}_n = \max_a \sum_{m = 1}^n \nu(m,a) - X_m \]
%
is not necessarily small. We cannot guarantee its small, but we should be able to get a high probability bound on such an occurence not happening. Since the rewards in the adversarial case are not independant, the rewards are highly correlated, which means that we cannot get a high probability bound with the vanilla EXP3 algorithm. This occurs because the tail bound is related to the variances of the regret, which normally break up linearly when the rewards are independant.

It shall be convinient now to switch to losses, and write
%
\[ \widehat{L}_n - \widehat{L}_{n,a} \leq \frac{\log |A|}{\eta} + \frac{\eta}{2} \sum_a \widehat{L}_{n,a} \]
%
where
\[ \widehat{L}_n = \sum_{k = 1}^n \sum_a \mathbf{P}(A_k = a | H_{k-1}) \widehat{Y}_{k,a}\ \ \ \ \ \ \ \ \ \ \widehat{L}_{n,a} = \sum_{k = 1}^n \widehat{Y}_{k,a} \]
%
Suppose we also define
%
\[ \tilde{L}_n = \sum_{m = 1}^n Y_t\ \ \ \ \ L_{n,a} = \sum_{m = 1}^n (1 - \nu(m,a)) \]
%
The random regret is the max of $\tilde{L}_n - L_{n,a}$ over all choices of $a$, and thus it just remains to bound this value.

To bounds these values, we need to slightly modify the reward estimates. A simple fix to avoid estimates shooting up is to fix $\gamma > 0$ and define
%
\[ \widehat{X}_{n,a} = 1 - \frac{\mathbf{I}(A_n = a)}{\mathbf{P}(A_n = a | H_{n-1}) + \gamma} (1 - X_t) \]
%
This modified version of the EXP3 algorithm is called the EXP3-IX algorithm, since the algorithm now `xplores implicitly'.

The $\gamma$ certainly biases the estimator, and the bias is upward, in that $\widehat{X}_{n,a}$ is more optimistic.

\section{Contextual Bandits}

We now introduce some state to the bandit problem. In the contextual bandit problem, in each round we recieve a context $c$ from a finite set of contexts $C$, we choose some action $a \in A$ and we recieve a reward sampled from $r(c,a) + \eta$, where $\eta$ is $\sigma^2$-subguassian, given past knowledge. The aim is to minimize the regret
%
\[ R_n = \mathbf{E} \left[ \sum_{m = 1}^n \max_c r(c,a) - \sum_{m = 1}^n X_m \right] \]
%
In the linear contextual bandit problem, we are given a function $\varphi: C \times A \to \mathbf{R}^d$, and we assume there is $\theta \in (\mathbf{R}^n)^*$ such that $r(c,a) = \theta(\varphi(c,a))$. This makes the problem much more feasible.

\section{Nonstationarity}

A final problem we will discuss with bandit problems is the problem of nonstationarity. Regardless of how long a policy interacts with a bandit, the reward distributions $R_a$ remain the same. However, what if these values do change according to some rule, that is, if there is instead a sequence of functions $(R^1_a, R^2_a, \dots)$, which we sample from in each interval. Provided these functions have some relation between them, there is still a chance a policy could find a good strategy for obtaining award by carrying experience between choices. But now regret is not well defined, as $\mu_*$ changes over time. It follows that computing the estimates $\widehat{\mu_n}$ will no longer lead to good performance. Note that the recurrence relation
%
\[ \widehat{\mu_a}(n+1) = \widehat{\mu_n}(a) + \frac{\mathbf{I}(A_{n+1} = a)}{T_a(n+1)} [X_{n+1} - \widehat{\mu_n}(a)] \]
%
It is of the form
%
\[ \text{`new average'} = \text{`old average'} + \text{`step size'}[\text{`new value'} - \text{`old average'}] \]
%
If we adjust the step size from $T_a(n+1)$ to a non-zeroconstant value $\alpha$, our sample average will not settle down to some value, but will still have some convergence to the mean, so that we have a new estimate
%
\begin{align*}
    \widehat{\mu_a}(n+1) &= \widehat{\mu_a}(n) + \alpha \mathbf{I}(A_{n+1} = a) [X_{n+1} - \widehat{\mu_a}(n)]\\
    &= (1 - \alpha)^{T_a(n+1)} \widehat{\mu_a}(a) + \alpha \sum_{k = 1}^n \mathbf{I}(A_k = a) (1 - \alpha)^{T_a(n+1) - T_a(k)} X_k
\end{align*}
%
We call this the exponential recency weighted average, as past results decrease exponentially in importance as new results are obtained. This allows us to get some convergence in the mean, while still not settling down completely. We can even change the step size $\alpha$ for each update to some sequence $\alpha_k$. If the mean of the rewards is fixed, $\sum \alpha_k = \infty$ and $\sum \alpha_k^2 < \infty$, then the law of large numbers continues to hold for our estimates. Alas, then $\alpha_k \to 0$, so we run into the same problem as the standard sample averages.

\chapter{Contextual Bandits}

\chapter{Linear Bandits}

The linear bandit problem is a specialization of the contextual bandit problem, where the state of a problem changes over time, but the states are related to one another in such a way that we can learn about the structure of other states from a particular state which is `similar' to the other states. The structure which allows us to represent similarility is a linear structure to the state space, and a representation of reward as a linear functional. The environment for such a problem is specified by a dimension $N$, and a parameter vector $\theta^* \in \mathbf{R}^N$. In each round, we are given a set of actions $D_n$, represented as a subset of $\mathbf{R}^N$. Choosing an action $A_n \in D_n$ incurs a reward
%
\[ X_n = \langle A_n, \theta^* \rangle + \eta_n \]
%
where the noise vectors $\eta_n$ are 1-subgaussian. The regret is then
%
\[ R_n = \mathbf{E} \left[ \sum_{m = 1}^n \max_{a \in D_m} \langle a, \theta^* \rangle - X_m \right] \]
%
The stochastic $m$-armed bandit problem can be reformulated in this model, by considering $D_n = \{ e_1, \dots, e_m \}$, where $\theta^*_k = \mu_k$. Linear bandits also arise naturally when the action sets are combinatorial, when $D_n \subset \{ 0, 1 \}^N$. Many combinatorial problems can be rephrased by taking action sets of this form (spanning trees, matchings, etc.), and the linear optimization then give us examples of a general algorithm (an artificial intelligence) being able to solve many combinatorial problems at once.

The UCB algorithm for finite armed bandits is a very attractive algorithm to use on the class of 1-subgaussian environments. We would hope that there is a similar confidence interval approach to linear bandits, which enable us to obtain reliable estimates of the reward function. If we construct convex confidence sets which contain $\theta_*$ with high probability, then we can optimize over this confidence set to obtain good regret bounds (assuming $\theta_*$ is such that gives us the best reward possible). That is, if we have a confidence interval $C_n$ on round $n$ for $\theta_*$, then we assign a value to an action $a$ equal to $\max_{\theta \in C_n} \langle a, \theta \rangle$, and choose the action optimistically which maximizes this value. So we are optimizing
%
\[ UCB(a,n) = \max_{a \in D_n} \max_{\theta \in C_n} \langle a, \theta \rangle = \max_{(a, \theta) \in D_n \times C_n} \langle a, \theta \rangle \]
%
A bilinear optimization problem. We can solve this problem optimially, assuming that either
%
\begin{itemize}
    \item The problem $\max_{a \in D_n} \langle a, \theta \rangle$ can be solved efficiently.
    \item $C_n$ is the convex hull of finitely many vertices, in which case the optimum occurs at the vertices, and we need only solve finitely many linear programs. For instance, if $C_n$ is a skewed and shifted ball in the $L_1$ norm, i.e. where there is a nonsingular matrix $A$ for which
    %
    \[ C_n = \{ A\theta + \hat{\theta_n} : \| \theta \|_1 \leq 1 \} \]
    %
    In which case there are $2N$ vertices to check.
    \item If $C_n$ is an ellipsoid, defined by a positive definite matrix $V$ and radius $\beta$ such that
    %
    \[ C_n = \{ \theta \in \mathbf{R}^N : \| \theta - \hat{\theta_n} \|^2_V \leq \beta \} \]
    %
    In this form, it is easy to see that $C_n = \widehat{\theta_n} + \beta^{1/2} V^{-1/2} B_2$. A short calculation then gives the value of a particular action $a$ with respect to $C_n$ as $\langle a, \widehat{\theta_n} \rangle + \beta^{1/2} \| a \|_{V^{-1}}$. Note the similarity of these estimates to the standard UCB estimates. The standard way to choose $\widehat{\theta_n}$ as the least squares estimator of $\theta_*$, and $V$ as the regularized Grammian matrix
    %
    \[ V_n = V_0 + \sum_{m = 1}^n A_m A_m^t \]
    %
    where $V_0 > 0$ is selected arbitrarily, often equal to a scalar $\lambda$ for some $\lambda > 0$.
\end{itemize}

We shall use the ellipsoid method, which together with assumptions about our data gives bounds on the probability of us choosing a poor action. We make the following assumptions about our data.

\begin{itemize}
    \item The rewards are bounded: For any $a \in \bigcup_{n = 1}^\infty D_n$, $|\langle a, \theta_* \rangle| \leq 1$.
    \item For any action $a \in \bigcup_{n = 1}^\infty D_n$, $\| a \|_2 \leq L$.
    \item In all rounds $n$, and actions $a \in D_n$, with probability $1 - \delta$,
    %
    \[ UCB(n,a) - 2 \beta_{n-1}^{1/2} \| a \|_{V_{n-1}^{-1}} \leq \langle a, \theta_* \rangle \leq UCB(n,a) \]
    %
    so the confidence bounds chosen `work' as needed.
\end{itemize}

We will eventually find methods which guarantee these assumptions hold with little work.

\begin{theorem}
    Assume that $\beta_n$ are non-decreasing, and $\beta_n \geq 1$. Then, with probability $1 - \delta$, the pseudo-regret
    %
    \[ \widehat{R_n} = \sum_{m = 1}^n \max_{a \in D_n} \langle a, \theta_* \rangle - \langle A_m, \theta_* \rangle \]
    %
    satisfies
    %
    \[ \widehat{R_n} \leq \sqrt{8n \beta_{n-1} \log \left( \frac{\det V_n}{\det V_0} \right)} \leq \sqrt{8Nn \beta_{n-1} \log \left( \frac{\text{trace}(V_0) + nL^2}{N (\det V_0)^{1/N}} \right)} \]
\end{theorem}
\begin{proof}
    s
\end{proof}

From this result, we see that in order to obtain $O(\sqrt{n})$ regret, we must show $\beta_n$ has polylogarithmic growth. We can also get this bound on the expected regret $R_n$ if $\delta \leq c/\sqrt{n}$ by combining the bound in the theorem with $\hat{R_n} \leq 2n$, which follows from the bounds on the reward.

\section{Constructing Ellipsoid Confidence Intervals for Least Squares Estimates}

\section{Ellipsoidal UCB}

Let us summarize, if we assume that
%
\begin{enumerate}
    \item 1-subgaussian martingale noise, for any $\lambda \in \mathbf{R}$, $\mathbf{E}[e^{\lambda \eta_n} | F_n] \leq e^{\lambda^2/2}$.
    \item $\| \theta_* \| \leq S$.
    \item $\sup_n \sup_{a \in D_n} \| a \|_2 \leq L$.
    \item $|\langle a, \theta_* \rangle| \leq 1$ for all $a \in \bigcup_{n = 1}^\infty D_n$.
\end{enumerate}
%
then we can construct confidence intervals, and prove the high probability bounds
%
\[ \hat{R_n} \leq \sqrt{8 Nn \beta_{n-1} \log \left( \frac{N\lambda + nL^2}{N \lambda} \right)} \]
%
where
%
\begin{align*}
    \beta_{n-1}^{1/2} &= \sqrt{\lambda} S + \sqrt{2 \log(1/\delta) + \log \left( \frac{\det V_{n-1}(\lambda)}{\det(\lambda)} \right)}\\
    &\leq \sqrt{\lambda} S + \sqrt{2 \log(1/\delta) + (N/2) \log(1 + nL^2/N\lambda)}
\end{align*}

Choosing $\delta = 1/n$, and $\lambda$ a constant value through the algorithm, we find $\beta_n^{1/2} = O(N^{1/2} \log^{1/2}(n/N))$ and thus the expected regret of Ellipsoidal-UCB, as a function of $N$ and $n$ satisfies
%
\[ O(\beta_n^{1/2} \sqrt{Nn \log(n/N)}) = O(N\log(n/N) \sqrt{n}) \]
%
Thus the algorithm scales poorly in the dimensional of the problem, but the cardinality of the action set can be arbitrary.

\section{Adversarial Linear Bandits}

Let us now consider adversarial linear bandits, which are analogous to the adversarial setting in the one dimensional case. As with UCB, we will attempt to generalize EXP to the linear bandits setting. We consider loss vectors $y_1, y_2, \dots \in \mathbf{R}^n$, and we assume that each action is identified with a vector $a \in \mathbf{R}^n$. In the one dimensional adversarial problem, we assumed $-1 \leq y_n \leq 1$. In this version of adversarial bandits, we assume $|\langle y_1, a \rangle| \leq 1$. At each point in time, the learner selects an action $A_n \in A$, observes the loss $Y_n = \langle A_n, y_n \rangle$, and obtains the regret
%
\[ R_n = \mathbf{E} \left[ \sum_{m = 1}^n Y_m \right] - \min_{a \in A} \sum_{m = 1}^n \langle a, y_m \rangle \]
%
One can clearly represent the one-dimensional adversarial linear bandit problem in this form, by using the euclidean basis vectors as arms.

\section{Adversarial Bandits on the Unit Ball}

We have shown that for any finite action set, no matter how the adversary selects loss vectors, as long as the action losses $\langle a_n, y_m \rangle$ are in a known interval, the exploration strategy that mixes the distribution of the exponential weights algorithm with a fixed exploration strategy has a regret of $O(\sqrt{dn \log K})$, with a discretization approximating more general results.

We can do even better than this, if we assume our losses lie in the unit ball, obtaining a $O(\sqrt{dn})$ regret, without refrence to the cardinality of the action set. The trick is to use a powerful optimization method called mirror descent. It has an interesting interpretation solely from the viewpoint of online learning.

In online linear optimization, we take a closed convex set $K \subset \mathbf{R}^d$, and the goal is to simultaneously compete with all elements of $K$ for all linear environments $g_1, \dots, g_n \in \mathbf{R}^d$. In round $n$, the learning algorithm choose a point $x_n \in K$ (deterministically), and suffers a loss $\langle g_n, x_n \rangle$. The regret with respect to a fixed competitor $x \in K$, is
%
\[ R_n(x) = \sum_{m = 1}^n \langle g_n, x_n - x \rangle \]
%
Then the actual regret is the maximum of $R_n(x)$ over $K$.

The basic mirror descent method takes two additional parameters: a learning rate $\eta$ and a convex `distance-generating function' $F: D \to \mathbf{R}$. The function $F$ is also called a potential function or a regularizer for reasons that will become clear later.

In the first round, mirror descent predicts $x_1 = \text{argmin}_{x \in K} F(x)$. Then, after we observe $g_n$, the prediction of $x_{n+1}$ is chosen to minimize
%
\[ \eta \langle g_n, x \rangle + D_F(x,x_n) \]
%
over $K \cap D$, where
%
\[ D_F(x,x_n) = F(x) - F(x_n) - \langle \nabla F(x_n), x - x_n \rangle \]
%
A quantity known as the {\bf Bregman divergence}. Note that it is the difference between $F$ and its first order taylor approximation at $x_n$. Since $F$ is convex, the linear approximation is a lower bound and so $D_F$ is a positive function, and choosing $x_{n+1}$ is a convex optimization problem over $K \cap D$. We require that gradients are well defined. It turns out that provided $F$ is a Legendre function, everything works out rather nicely.

The update rule aims to achieve two goals: Making the loss of $x_{n+1}$ under $g_n$ small, while at the same time staying close to the previous estimates, thus the algorithm stays stable. The algorithms generalizes gradient descent, because if $F(x) = \| x \|_2^2/2$, we get $\nabla F(x) = x$, $D_F(x,x_n) = \| x - x_n \|_2^2/2$, and the update rule $x_{n+1} = x_n - \eta g_n$. If $K$ is a compact set, the update rule corresponds to projected gradient descent; after computing the update rule, the resulting vectors just need to be projected back into $K$ by using the Euclidean metric.

\chapter{Partial Monitoring}

The partial monitoring problem models the bandit problem with the additional difficulty that our observation of rewards is obscured. For example, consider $W_1, W_2, \dots \sim \text{Ber}(p)$, where $p \in [0,1]$ is unknown. In each round, we have 3 actions $A$, $B$, $C$, and
%
\[ (X_k | A_k = A) \sim W_k\ \ \ \ \ (X_k | A_k = B) \sim 1 - W_t\ \ \ \ \ (X_k | A_k = C) \sim -1 \]
%
So that in the normal bandit problem, choosing $C$ would always be an accident. But suppose that we only observe $W_t$ when we select action $C$. Then the problem becomes more interesting, for we must choose action $C$ to be able to statistically determine whether $A$ or $B$ is more optimal.

Consider an explore and commit approach, where we attempt to estimate $p$ until we are confident enough that one action is more optimal. Since $W_t$ is one-subgaussian, if we sample $W_t$ $n$ times than
%
\[ \mathbf{P}(|p - \widehat{p}| < x) = 1 - \mathbf{P}(|p - \widehat{p})| \geq x) \geq 1 - 2e^{-(xn)^2} \]
%
So a confidence interval for $1 - \delta$ is $x = \sqrt{\log(2/\delta)}/n$. If we can conclude that $\widehat{p} > 1/2$ with some confidence, it may be wise to play action $B$. If $\widehat{p} < 1/2$, we should play action $A$. The only problem is being able to conclude that $\widehat{p} = 1/2$, which we can only do with some confidence.

We shall consider the adversarial partial monitoring problem. We have finitely many actions $A$, states $E$, and observations $O$ we have a loss function $L(a,e) \in \mathbf{R}$, and an observed function $\Sigma(a,e) \in O$. The regret is then defined as
%
\[ R_n = \sum_{m = 1}^n L(a_m,e_m) - \min_a \sum_{m = 1}^n L(a,e_m) \]
%
The $L$ and $\Sigma$ are given to you to optimize over, but the adversary can choose $e_m$ at whim. The proper way to think about $\Sigma$ is a way to count frequencies of observations.

We can reformulate the problem as being very nearly a linear bandit problem. Given losses $l_a \in [0,1]^E$, and actions $S_a: E \to O$. The states of the problem are chosen by the opponent $e_1, \dots, e_n \in E$. The player then chooses an action $A_k$ on round $k$, observes $S_{A_k}(e_k)$. The reward is
%
\[ R_n = \mathbf{E} \left[ \sum l_{A_k}(e_m) - \min_{a \in A} \sum_{m = 1}^n l_a(e_m) \right] \]
%
Consider the averages
%
\[ q_n = \frac{1}{n} \sum_{m = 1}^n e_m \in \mathbf{R} \langle E \rangle \]
%
which move around the simplex $\Delta_E$ of points $p$ with $\sum_e p(e) = 1$, which we can interpret as the space of probability distributions over possible events. We can see the adversaries problem as manipulating the motion of the sequence $q_1, q_1, \dots, q_n$ in such a way that $l_a(q_n)$ is maximized, in such a way that is confuses the player as much as possible.

We can decompose the simplex $\Delta_E$ into cells which correspond to actions. Indeed, we let $C_a$ be the set of distributions $p$ such that $a$ minimizes $l_a(p)$. These are cells, because they are the intersection of half planes -- the half planes defined by $l_a(p) \leq l_b(p)$, for other actions $b$. We say $a$ and $b$ are neighbors if $C_a \cap C_b \neq \emptyset$. We say $a$ and $b$ are $BC$ if there are $p,q \in \Delta_E$ for which $S_a p = S_a q$, $S_b p = S_b q$, and $\text{sgn}((l_a - l_b)(p)) \neq \text{sgn}((l_a - l_b)(q))$. There is an effective way to determine if two actions are $BC$ -- $l_a - l_b \not \in \text{Im}(S_{ab}^t)$, where $S_{ab}(p) = (S_a(p), S_b(p))$. This is just checked by linear algebra.

Now suppose $p_0$ is an interior point of $\Delta_E$, with $q = p_0 + \varepsilon v \in C_a$, $q' = p - \varepsilon v \in C_b$. We normalize $v$ such that $(l_a - l_b)(q) = \varepsilon$, so $(l_b - l_a)(q') = \varepsilon$. Suppose that the adversary draws events $Y_1, Y_2, \dots, Y_n$ randomly according to the distribution $q$, and $Y_1', \dots, Y_n'$ from $q'$. Then
%
\[ R_n = \mathbf{E} \left[\sum_{m = 1}^n l_{A_k}(Y_k) - \min_a \sum_{m = 1}^n l_a(Y_k) \right] \]
\[ R_n' = \mathbf{E} \left[ \sum_{m = 1}^n l_{A_k}(Y_k') - \min_a \sum_{m = 1}^n l_a(Y_k') \right] \]
%
If we let $T_a$ $T_b$ count the number of times we chose action $b$, we can spit the reward up and apply the Pinsker bound to obtain $\Omega(n^{2/3})$ lower bounds on the regret of any policy in partial monitoring.

The good case of partial monitoring occurs when all actions are distinguishable. We expect good results here, because adversarial bandits lie here (where we see the state as our observation). Suppose there are no dominated actions (for which $C_a = \emptyset$), duplicated actions ($l_a = l_b$), or degenerate actions (there is $b$ with $C_a$ a proper subset of $C_b$). Then the minimax regret satisfies $R_n^*(G) = \inf_\pi \sup_{E \in \mathcal{E}} R_n^\pi(E, G)$. Then either $R_n^*(G) = 0$, if there is one action, $\Theta(n^{1/2})$, if there is more than one action and $G$ is locally observable, or $\Theta(n^{2/3})$ if $G$ is globally observable but not locally observable (locally observable means any two actions can be distinguished by their losses, globally observable means $l_i - l_j \in \oplus_a \text{Im}(S_a^t)$). If $G$ is not globablly observable, the reward is $\Theta(n)$. (The $\Theta$'s may obscure logarithm factors).

To find a good algorithm on a {\bf point-local process}, where the intersection of all $C_a$ is nosn-trivial, is to take estimates $\hat{l}_a$ of the losses, and define $\hat{P}_{n,a} = (1 - \gamma) P_{n,a} + \gamma \frac{1}{|A|}$, where
%
\[ P_{n,a} = \frac{\exp(- \eta \sum_{m = 1}^{n-1} \hat{l}_{m,a})}{\sum_b \exp(- \eta \sum_{m = 1}^{n-1} \hat{l}_{m,b})} \]
%
Where
%
\[ \hat{l}_{n,t} = \left( \frac{A_{n,a}}{\hat{P}_{n,a}} v_{a,A_n'} - v_{A_n,a} \right)^T Y_n \]
%
and $A_n, A_n'$ are two actions drawn from $\hat{P}_n$. Then $\hat{l}_{n,b} - \hat{l}_{n,b}$ is an unbiased estimator of $(l_a - l_b)(y_n)$. The standard proof for exponential weighting owrks.










\part{Markov Decision Processes}

The bandit process correctly describes a situation where one must select actions to obtain data about an environment. The general reinforcement learning problem also adds causality to this challenge, where choices of actions at a particular moment in time affect the choice of actions in the future. Actions thus have further consequences than immediate reward. The problem of associating a sequence of actions with a certain amount of reward is known as the reinforcement learning problem.

The reinforcement learning task consists of an agent/learner/decision-maker interacting with its environment, everything that is outside of the agents direct control. The two interact continually through the environments presentation of rewards to the agents actions in the environment, providing `reinforcement' that allows the agent to improve its behaviour over time, the agents primary motive. Over time, the environment changes, but the environment should give enough information at the current time or via previous interactions to allow an agent to form a state representation of the agents current standing with the markov property - the presense of enough relevant information to make intelligent decisions based on previous information about the same state. We define the reinforcement learning task rigorously through the introduction of a markov decision problem.

The environment in a reinforcement learning task is more complicated to state than for bandits. In addition to actions and rewards, we also have a set $S$ of states, which gives us information about the resultant rewards we choose. The actions $a \in A_s$ are specialized to each state $s$, and the distributions $\rho_a$ are also specialized to states, if the same actions occur in different states. What's more, we also have a function $P$, which takes a state $s$, and an action $a \in A_S$, and gives us a distribution over the states, the possible states we will end up with after applying the action $a$. A policy $\pi$, like in the theory of bandits, maps histories of length $n$ to probabilities over all actions in each state. The aim of a policy is to maximise its return, the expectation of reward the policy may see. Indeed, a policy $\pi$ and an initial state distribution $S_0$ induces a sequence
%
\[ (S_0, A_0, X_0, S_1, A_1, X_1, \dots) \]
%
It is customary to induce a parameter $\gamma \in [0,1]$, such that the infinite sum
%
\[ V = \sum_{n = 0}^\infty \gamma^n \mathbf{E}[X_n] \]
%
is defined for all $n$, and we then try to find $\pi$ which maximizes this quantity. The $\gamma$ factor makes the problem more cupable to analysis, and some psychologists see this as a natural way humans represent our `reward function'; indeed, this makes us value rewards higher if they occur closer in time, which is certainly true in our situation. We will let $\pi^*$ denote a policy which maximizes $Q$ independently of which state we start in -- it is an optimal policy with respect to the measure we have defined.

There is a precise mathematical method to find optimal policies, given a complete description of an MDP environment. We assume our policies operate independently of history, which has no loss of generality because we can always assume our optimum policy operates only on the states it sees at the current point in time. A policy is then a map $\pi$ from states to distributions over actions at that state. To describe the method, we introduce the value functions
%
\[ V^\pi(s) = \mathbf{E}_\pi[ V | S_0 = s]\ \ \ \ \ Q^\pi(s,a) = \mathbf{E}_\pi[ V | S_0 = s, A_0 = a] \]
%
Then by the Markov property, we can write
%
\begin{align*}
    V^\pi(s) &= \sum_{a,s'} \pi(a|s) P(s'|s,a) (\mu_{a,s'} + \gamma V^\pi(s'))\\
    &= \mathbf{E}[X | S = s] + \gamma \sum_{a,s'} \pi(a|s) P(s'|s,a) V^\pi(s')\\
    Q^\pi(s,a) &= \sum_{s', a'} P(s'|s,a) (\mu_{a,s'} + \gamma (\pi(a|s') Q^\pi(s',a')))\\
    &= \mathbf{E}[X | S = s, A = a] + \gamma \sum_{s',a'} P(s'|s,a) \pi(a|s') Q^\pi(s',a')
\end{align*}
%
These are known as the Bellman equations.

The value function of an optimal policy $\pi^*$ is independent of which optimal policy we choose. Indeed, by definition
%
\[ V^{\pi^*}(s) = \max_\pi V^\pi(s) \]

\section{Evolution as a Policy Iteration Method}

Value iteration is a technique for exploting the structure of the space of policies to find optimal policies, but it is not the only technique for doing so. Indeed, evolution does not directly take the structure of our brain's decision making into account when constructing more advanced lifeforms. In a sense, we could say that evolution operates only by looking at the expected reward of an entire policy, rather than the values of each individual state in the markov decision process. This is why we feel that the value iteration policies we consider will be more effective than evolution technique, not in the sense that they will find more optimum policies, but that they will run faster, generating more optimal policies with fewer iterations. Indeed, it took millenia for evolution to produce the lifeforms we find today, whereas policy iteration methods can generate interesting policies for basic games in very few trials; perhaps we are overextrapolating her, because the real world is far more complex than that of simple games, but from attempts at the reinforcement problem, we find that value iteration methods give rise to far more productive algorithms in applications than evolutionary algorithms do.
.

\end{document}