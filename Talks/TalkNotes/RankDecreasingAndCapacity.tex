% LaTeX template for Summer Schools
%
% !!!!!!!!!!!!   INSTRUCTIONS  !!!!!!!!!!!!!!

% 0) Submit your work in tex format (not pdf). It will be pasted into the proceedings file.
%
% 1) please name your file yourlastname.tex, e.g. thiele.tex.
%
% As several files will be concatenated, please 
% use some discipline as to the following:
%
% 2) Whenever you use \label{} to get automatic cross references
% through \ref{} (this is the preferred option for cross references)
% please add your initials to the label such as 
% \label{SLEct} for the Stochastic Loewner equation
% with initials of author Christoph Thiele
% Same with other citations such as in bibitem.
%
% 3) Please STRONGLY avoid using \def or \newcommand unless really necessary.
% We do have macros for black board bold. If you use your favorite
% macros while preparing your summary, please expand them
% (replace by the original definition) everywhere in your file.
% This will save me the work of doing the very same thing.
% Thank you! If you have to use \def, please also add your
% initials to the definition.
%
% 4) if you want to compile the header of the document, 
% uncomment remove the corresponding paragraph signs below
%
% 5) There is a sample lecture below. For the header 
% it is best to keep most of the
% commands and just change the name, title, text. etc
%
% 6) Please follow the conventions below in terms of capitalization of 
% headings etc:
% Only the beginning of a sentence and names are capitalized.


\documentclass[12pt]{article}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{esint}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{comment}
\usepackage{bbm,dsfont}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{proposition}[theorem]{Proposition}

\newcommand{\talktitle}[1]{\section{#1}}
\newcommand{\talkafter}[1]{\textbf{After #1} \addcontentsline{toc}{subsection}{after #1}}
\newcommand{\talkspeaker}[2]{\begin{center}
\textit{A summary written by #1}
\end{center}
\addcontentsline{toc}{subsection}{#1, #2}
}

\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[hidelinks]{hyperref}

\newcommand*{\Z}{\mathbb{Z}}
\newcommand*{\Q}{\mathbb{Q}}
\def\C{\mathbb{C}}
\newcommand*{\N}{\mathbb{N}}
\newcommand*{\R}{\mathbb{R}}

% Absolute values and norms using mathtools.
% \\[lr][vV]ert produces correct spacing, as opposed to | and \|.
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}

\title{Brascamp--Lieb inequalities}

% \author{Summer School, Kopp}
% \thanks{supported by Hausdorff Center for Mathematics, Bonn}

% \date{September 2021}



\begin{document}

% \maketitle

% {
% \center{Organizers:}
% \center{
% Christoph Thiele, Universit\"at Bonn}
% \center{
% Pavel Zorin-Kranich, Universit\"at Bonn}
% \center{$\ $}
% }
% \newpage
% \tableofcontents

% \newpage


\talktitle{Capacity of Rank Decreasing Operators}
\talkafter{Gurvits \cite{gurv2004}}
\talkspeaker{Jacob Denson}{University of Madison, Wisconsin}

\setcounter{equation}{0}
\setcounter{theorem}{0}

\begin{abstract}
{ We describe the theory of rank-decreasing doubly stochastic positive operators, the capacity of such operators, Gurvit's algorithm to rescale an operator so that it is approximated by a doubly stochastic operator, and a brief idea of the connection of these techniques to the theory of Brascamp-Lieb inequalities. }
\end{abstract}

% Operator Scaling Preserves the Rank Non-Decreasing Property
% Polynomial Identity Testing

Recall a theorem of Lieb \cite{lieb1990}, which shows that any inequality of the form
%
\[ \int_{\mathbf{R}^n} \prod_{i = 1}^m |f_i(B_i x)|^{p_i}\; dx \leq \text{BL}(B,p) \cdot \prod_{i = 1}^m \| f_i \|_{L^1(\mathbf{R}^{n_i})}^{p_i}, \]
%
has an optimal Brascamp-Lieb constant $\text{BL}(B,p)$ satisfying
%holds over all choices of functions $f_1, \dots, f_m \in L^1(\mathbf{R}^{n_j})$ if and only if it holds whenever $f_1, \dots, f_m$ are Gaussian functions centered at the origin, and the optimum constant $\text{BL}(B,p)$ in the inequality is the same in both settings. If, for each $i$, we set $f_i(x) = e^{- \pi x^T X_i x}$, where $X_i$ is a positive definite matrix, then both sides of the inequality become the integrals of Gaussian, which can be easily calculated over all inputs, from which we find that
%
\begin{equation} \label{BrascampLiebSupremum}
    \text{BL}(B,p)^2 = \sup_{X_1, \dots, X_m \succ 0} \frac{\det (\sum_{i = 1}^m p_i B_i^* X_i B_i)}{\prod_{i = 1}^m \det(X_i)^{p_i}}.
\end{equation}
%
Here and in what follows, for a square matrix $A$, we write $A \succ 0$ and $A \succeq 0$ to mean $A$ is positive definite or semidefinite. Bennett, Carbery, Christ, and Tao \cite{bcct} found that $\text{BL}(B,p)$ is finite if and only if $\sum_{j = 1}^m p_j n_j = 0$, and
%
\begin{equation} \label{BrascampLiebMappingProperty}
    \dim(V) \leq \sum_{j = 1}^m p_j \dim(B_j V)\quad\quad \text{for all subspaces $V \subset \mathbf{R}^n$}.
\end{equation}
%
Thus the finiteness of \eqref{BrascampLiebSupremum} acts as a guarantee for the mapping properties of the matrices $B_1, \dots, B_m$ given in \eqref{BrascampLiebMappingProperty}, and vice versa.

The focus of these notes is to introduce analysts to a setting, introduced by computing scientists studying combinatorial optimization and quantum information theory, which can provide insight into the computation of the Brascamp-Lieb constant. Using these insights, Garg, Gurvits, and Wigderson \cite{ggow} formulated polynomial time algorithms for approximating the Brascamp-Lieb constant, a fact that has important theoretical consequences to the general theory of Brascamp-Lieb inequalities independent of practical application to computation of particular Brascamp-Lieb constants. This connection is more fully explored in the subsequent presentation. In this summary, we introduce the original setting studied, only noting some similarities to Brascamp Lieb as we proceed.

\section{Operator Scaling}

Let $M(N)$ denote the space of all $N \times N$ complex-valued matrices. The main object of study in the setting we aim to introduce are \emph{positive operators} $T: M(N) \to M(N)$, i.e. linear transformations between spaces of matrices such that if $X \succeq 0$, then $T(X) \succeq 0$.  There is a rich theory of such maps, connected to representation theory and the theory of free probability (see \cite{bhatia} for a more thorough introduction). A useful example to keep in mind, given any matrices $B_1,\dots, B_m$ and $p_1,\dots,p_m > 0$, is the operator
%
\begin{equation} \label{completelypositiveoperator}
    T(X) = \sum_{i = 1}^m p_i B_i^* X B_i
\end{equation}
%
With any positive operator $T$, we associate a quantity $\text{Cap}(T) \geq 0$, known as the \emph{capacity} of $T$, defined by setting
%
\begin{equation} \label{capacityequation}
    \text{Cap}(T) = \inf_{X \succ 0} \frac{\det(TX)}{\det(X)}.
\end{equation}
%
The connection between capacity and Brascamp-Lieb may be hinted at by comparing \eqref{capacityequation} to \eqref{BrascampLiebSupremum} when $T$ is of the form given in $\eqref{completelypositiveoperator}$. Being defined by a non-convex optimization, it is difficult to explicitly compute the capacity of a general positive operator. However, the techniques of \cite{gurv2004} show that for each $\varepsilon > 0$, given a positive operator $T$, represented in $b$ bits on a computer, there is an algorithm to compute a value $\text{Cap}_{\text{approx}}(T)$ in $\lesssim (b \log(1/\varepsilon))^{O(1)})$ steps, such that
%
\[ \text{Cap}(T) \leq \text{Cap}_{\text{approx}}(T) \leq (1 + \varepsilon) \cdot \text{Cap}(T). \]
%
We obtain this approximation by applying the technique of \emph{operator scaling}, which we describe in this section.

Given any nonsingular matrices $A,B \in M(N)$, and any positive operator $T: M(N) \to M(N)$, we define a \emph{scaled operator} $T_{A,B}(X) = B \cdot T(A X A^*) \cdot B^*$. For any $X \succeq 0$, if we write $Y = AXA^*$, then
%
\[ \frac{\det(T_{A,B}X)}{\det(X)} = \frac{\det(B) \cdot T(Y) \cdot \det(B^*)}{\det(X)} = \det(A)^2 \det(B)^2 \frac{T(Y)}{\det(Y)}. \]
%
Taking infima over both sides of the equation thus gives
%
\[ \text{Cap}(T_{A,B}) = \det(A)^2 \det(B^2) \text{Cap}(T). \]
%
Thus computing $\text{Cap}(T_{A,B})$ immediately gives $\text{Cap}(T)$. The idea of operator scaling is to rescale an operator to something whose capacity we can easily compute, which motivates the introduction of \emph{doubly stochastic operators}.

A positive operator $T$ is \emph{doubly stochastic} if $T(I) = T^*(I) = I$, where $T^*$ is the adjoint of $T$ with respect to the inner product $(X,Y) \mapsto \text{Tr}(XY^*)$. What interests us about these operators is that $\text{Cap}(T) = 1$ for any doubly stochastic operator $T$. The proof is somewhat technical from our perspective, reducing the question to the study of doubly stochastic matrices. A proof can be found in Theorem 4.32 of \cite{watrous}, with relevant background about doubly stochastic matrices found in Section 8.5 of \cite{serre}.


\section{Capacity of Rank-Decreasing Operators}

A useful property of a positive operator $T$ is that it is \emph{rank non-decreasing}, i.e. for any $X \succeq 0$,
%
\begin{equation} \label{ranknondecreasing}
    \text{Rank}(TX) \geq \text{Rank}(X).
\end{equation}
%
Condition \eqref{ranknondecreasing} seems somewhat similar to \eqref{BrascampLiebMappingProperty}. And indeed, in \cite{gurv2004} it is shown that one has an equivalence between the non-vanishing of \eqref{capacityequation} and \eqref{ranknondecreasing}, analogous to the equivalence between the finiteness of \eqref{BrascampLiebSupremum} and \eqref{BrascampLiebMappingProperty}.

\begin{theorem} \label{rankdecreasingcapacitytheorem}
    $T$ is rank non-decreasing if and only if $\text{Cap}(T) > 0$.
\end{theorem}
\begin{proof}
    A simple family of positive operators are those of the form
    %
    \begin{equation} \label{basicoperator}
        TX = X_{11} A_1 + \dots + X_{NN} A_N,
    \end{equation}
    %
    where $A_1,\dots,A_N \succeq 0$. For such an operator, we can write
    %
    \[ \text{Cap}(T) = \inf_{\gamma_1, \dots, \gamma_N > 0} \frac{\det ( \sum_{j = 1}^N \gamma_j A_j )}{\gamma_1 \cdots \gamma_N}. \]
    %
    Results from a previous paper of Gurvits and Samorodnitsky \cite{gurvits3} imply Theorem \ref{rankdecreasingcapacitytheorem} in the special case of an operator defined by \eqref{basicoperator}. Assuming this result, we indicate how this implies the general case.

    For each orthonormal basis $U = \{ u_1, \dots, u_N \}$, we define the decoherence operator $D_U(X) = \sum \langle Xu_i, u_i \rangle \cdot u_i u_i^*$, and then consider the operator
    %
    \[ T_U(X) = (T \circ D_U)(X) = \sum_{i=1}^N \langle X u_i, u_i \rangle \cdot T(u_i u_i^*). \]
    %
    This operator is, up to a change of basis in $M(N)$, described in the form \eqref{basicoperator}. Thus $T_U$ is rank non-decreasing if and only if $\text{Cap}(T_U) > 0$. The theorem then follows from the following two properties of this construction:
    %
    \begin{enumerate}
        \item $T$ is rank non-decreasing if and only if $T_U$ is as well, for all bases $U$.
        \item $\text{Cap}(T) = \inf_U \text{Cap}(T_U)$.
    \end{enumerate}
    %
    If $\text{Cap}(T) > 0$, then Property 2 implies $\text{Cap}(T_U) > 0$ for all $U$, so $T_U$ is rank non-decreasing for all $U$, and thus Property 1 implies $T$ is rank non-decreasing. The converse is similar.

    The proof of Properties 1 and 2 both rely on a simple trick. We will prove Property 1 here. Given any $X \succeq 0$, we can find an orthonormal basis $U$ diagonalizing $X$, and then for such $U$ we have $T(X) = T_U(X)$. This immediately implies $T$ is rank non-decreasing if $T_U$ is rank non-decreasing for all $U$. The converse follows because the composition of rank non-decreasing operators is rank non-decreasing, and $D_U$ is rank non-decreasing because it is \emph{doubly stochastic}, a family of operators we will study shortly.
    %To show that $\text{Cap}(T) = \inf_U \text{Cap}(T_U)$, we note that for any $\gamma_1,\dots, \gamma_N$,
    %
    %\begin{equation} \label{UCapacityUpperBound}
    %    \frac{\det \left( \sum_{j = 1}^N \gamma_j T(u_j u_j^*) \right)}{\gamma_1 \dots \gamma_N} = \frac{\det \left( T \left( \sum_{j = 1}^N \gamma_j \cdot u_j u_j^* \right) \right)}{\det \left( \sum_{j = 1}^m \gamma_j \cdot u_j u_j^* \right)} \geq \text{Cap}(T).
    %\end{equation}
    %
    %Taking infima over all $\gamma_1,\dots,\gamma_N$, and then over all $U$, gives $\inf_U C_U(T) \geq \text{Cap}(T)$. Conversely, given any $X \succeq 0$, pick $U$ with $T(X) = T_U(X)$, and then
    %
    %\begin{equation} \label{UCapacityLowerBound}
    %    \frac{\det(TX)}{\det(X)} = \frac{\det(T_U X)}{\det(X)} \geq C_U(T) \geq \inf_U C_U(T).
    %\end{equation}
    %
    %Taking infima over all $X$ gives $\text{Cap}(T) \geq \inf_U C_U(T)$. Combining these two calculations gives $\text{Cap}(T) = \inf_U C_U(T)$, which proves Property 2.
\end{proof}

\section{Operator Scaling}

Being defined by a non-convex optimization procedure, it is difficult to explicitly compute the capacity of a positive operator. However, the techniques described in this section show that for each $\varepsilon > 0$, given a positive operator $T$ (with rational entries described using $b$ bits), there is an algorithm to compute a value $\text{Cap}_{\text{approx}}(T) \in [0,\infty)$ in $\lesssim (b \log(1/\varepsilon))^{O(1)}$ steps, such that
%
\[ \text{Cap}(T) \leq \text{Cap}_{\text{approx}}(T) \leq (1 + \varepsilon) \cdot \text{Cap}(T). \]
%
We obtain this approximation by the technique of \emph{operator scaling}, which we describe in this section.

Given any nonsingular matrices $A,B \in M(N)$, and any positive operator $T$, we define a \emph{scaled operator} $T_{AB}(X) = B \cdot T(AXA^*) \cdot B^*$. For any $X \succeq 0$ with $\det(X) = 1$, if $Y = AXA^*$, then
%
\[ \det(T_{AB}X) = \det(B)^2 \det(T(AXA^*)) = \det(B)^2 \det(T(Y)). \]
%
Since $\det(Y) = \det(A)^2$, taking infima over both sides of the equation gives
%
\[ \text{Cap}(T_{AB}) = \det(A)^2 \det(B)^2 \cdot \text{Cap}(T). \]
%
Thus operator scaling predictably changes the capacity. The idea is now to scale an operator to another operator whose capacity is more simple to compute. Our goal is to rescale our operators to a \emph{doubly stochastic} operator. A positive operator $T$ is called \emph{doubly stochastic} if $T(I) = T^*(I) = I$, where $T^*$ is the adjoint of $T$ with respect to the inner product $(X,Y) \mapsto \text{Tr}(XY^*)$. One property of these operators is that $\text{Tr}(T(X)) = \text{Tr}(X)$ for any input $X$.

\begin{lemma} \label{doublystochasticcapacitytheorem}
    If $T$ is doubly stochastic, then $\text{Cap}(T) = 1$.
\end{lemma}
\begin{proof}
    Since $T(I) = I$, it is clear that $\text{Cap}(T) \leq 1$. The converse is less trivial, reducing the question to the study of doubly stochastic matrices. The proof can be found in Theorem 4.32 of \cite{watrous}, with relevant background about doubly stochastic matrices found in Section 8.5 of \cite{serre}.
%    Indeed, for any such $X \succeq 0$, we can pick orthonormal bases $U = \{ u_1, \dots, u_N \}$ and $V = \{ v_1, \dots, v_N \}$ diagonalizing $X$ and $T(X)$, with corresponding eigenvalues $\lambda = (\lambda_1,\dots,\lambda_N)$ and $\gamma = (\gamma_1, \dots, \gamma_N)$. Then
    %
%    \[ \gamma_i = \sum_{j = 1}^n \lambda_j v_i^* T(u_j u_j^*) v_i, \]
    %
%    so if we define $A_{ij} = v_i^* T(u_j u_j^*) v_i$ then $A\lambda = \gamma$. The fact that $T$ is doubly stochastic implies that the \emph{matrix} $A$ is doubly stochastic, i.e. it is non-negative and the rows and columns of $A$ sum to one. But this means that $\gamma \geq \lambda$, which implies that $\det(T(X)) \geq \det(X)$.
\end{proof}

Thus, given an operator $T$, if we can find nonsingular $A,B \in M(N)$ such that $T_{A,B}$ is doubly stochastic, then it follows from Theorem \ref{doublystochasticcapacitytheorem} that $\text{Cap}(T) = \det(A)^2 \det(B)^2$. This is not quite possible to do for all $T$ with $\text{Cap}(T) > 0$. In fact, it is only possible if the infinum defining $\text{Cap}(T)$ is attained. Nonetheless, if $\text{Cap}(T) > 0$ then we can find doubly stochastic operators approximating $T$ arbitrarily closely. The capacity is a continuous function of the input, so this is good enough to approximate $\text{Cap}(T)$ rather than calculating $\text{Cap}(T)$ exactly.

%We remark that, before the theory of capacity was explicitly applied to the theory of Brascamp-Lieb, a variant of operator scaling was already being used. A Brascamp Lieb inequality is called \emph{geometric} if each of the linear maps $\{ B_i \}$ in the inequality are projection maps, and $I = \sum_{j = 1}^m p_j B_j^* B_j$. Ball showed that the Brascamp-Lieb constant of any geometric inequality is one (analogous to the capacity of a doubly stochastic matrix). The Brascamp-Lieb constant behaves well under replacing each of the matrices $B_j$ with $B_j A$ for some invertible matrix $A$, and so it is natural to try and find some matrices which reduce to the study of a geometric Brascamp-Lieb inequality.

Our approximation algorithm is quite simple, an application of a similar method first used by Sinkhorn to reduce a matrix to something resembling a doubly stochastic process. Given a positive operator $T$ it is easy to scale the operator to an operator $S$ with $S(I) = I$. We simply consider $T_{I, T(I)^{-1/2}}$. Similarily, we can scale $T$ to an operator $S$ with $S^*(I) = I$ by taking the scaling $T_{T*(I)^{-1/2}, I}$. The challenge is to obtain scalings for which both properties are true. Sinkhorn's trick is to iteratively rescale our operators , obtaining a sequence of matrices $T_0, T_1, T_2, \dots$ with $T_i(I) = I$ for odd $i$, and $T_i^*(I) = I$ for even $i$. If this sequence converges, continuity of the equations imply the limit will be a doubly stochastic operator as desired.

Unfortunately, this sequence does not necessarily converge for any positive operator $T$ (it converges if and only if the infinum defining $\text{Cap}(T)$ is attained by a particular input). But provided that $\text{Cap}(T) > 0$, the distance between the elements of the sequence to the set of all stochastic operators \emph{will} converge to zero, which is sufficient for our purposes. To analyze the convergence, we rely on the capacity as a \emph{potential} for the analysis of the algorithm (this was the main reason Gurvit's introduced the capacity in \cite{gurv2004}). To determine how close we are to a doubly stochastic matrix, we use the measure $\text{DS}(T) = \| T(I) - I \|^2 + \| T^*(I) - I \|^2$. The following properties then hold:
%
\begin{enumerate}
        \item If $T(I) = I$ or $T^*(I) = I$, then $1 \lesssim_N \text{Cap}(T) \leq 1$. Moreover, for $\text{Cap}(T) \geq 1/2$,
        %
        \[ \text{DS}(T_n) \leq 6 \log(1/\text{Cap}(T_n)), \]
        %
        so the closer the capacity is to one, the closer a matrix is to being doubly stochastic.

        \item $\text{Cap}(T_n)$ is increasing in $n > 0$. Indeed, $\text{Tr}(T_n(I)) = \text{Tr}(T_n^*(I)) = N$, and the arithmetic-geometric mean inequality (applied to the sum and product of the eigenvalues of a matrix) imply that
        %
        \[ \text{Det}(T_n(I)), \text{Det}(T_n^*(I)) \leq 1, \]
        %
        which implies the capacity is increasing. In fact, we have a more precise bound, namely
        %
        \[ \text{Cap}(T_{n+1}) \geq e^{\min(1,\text{DS}(T_n))/6} \cdot \text{Cap}(T_n). \]

        \item If $T(I) = I$ or $T^*(I) = I$ and $\text{DS}(T) < 1/(N+1)$, then $\text{Cap}(T) > 0$.
\end{enumerate}
%
Property 2 implies that the capacities of the operators in the sequence increase faster the further away the operators are from being stochastic. Property 1 shows that the distance from being stochastic grows smaller the closer $\text{Cap}(T_n)$ is from being equal to one. Thus regardless of our input operator $T = T_0$, provided that $\text{Cap}(T) > 0$, the algorithm will eventually scale $T$ to be as close as possible to a doubly stochastic operator, providing us with a way to approximate $\text{Cap}(T)$. On the other hand, if $\text{Cap}(T) = 0$, then Property 3 implies that we will be able to check this by observing whether $\text{DS}(T_n) > 1/(N+1)$ after a certain number of iterations of the algorithm. Thus we have a reliable way to approximate the capacity of a matrix, which can be extended to give us an efficient way to approximate Brascamp-Lieb constants.

\begin{thebibliography}{3}

\bibitem{bcct}
    J.M. Bennett, A. Carbery, M. Christ and T. Tao,
    \emph{The {B}rascamp-{L}ieb Inequalities: Finiteness, Structure and Extremals},
    Geom. Funct. Anal. {\bf 17} (2008), 1343-1415.

\bibitem{bhatia}
    Rajendra Bhatia,
    \emph{Positive Definite Matrices},
    Princeton University Press, 2007.

\bibitem{ostaa}
    Ankit Garg, Leonid Gurvits, Rafael Oliveira, Avi Wigderson,
    \emph{Operator Scaling: Theory and Applications},
    Foundations of Computational Mathematics, {\bf 20} (2020), 223--290.

\bibitem{ggow}
    Ankit Garg, Leonid Gurvits, Avi Wigderson,
    \emph{Algorithmic and Optimization Aspects of {B}rascamp-{L}ieb Inequalities, via Operator Scaling},
    Geom. Funct. Anal, {\bf 28} (2018), 100--145.

\bibitem{gurv2004}
    Leonid Gurvits,
    \emph{Classical Complexity and Quantum Entanglement},
    J. Comput. Syst. Sci. {\bf 69} (2004), 448--484.

\bibitem{gurvits3}
    L. Gurvits, A. Samorodnitsky,
    \emph{a Deterministic Algorithm Approximating the Mixed Discriminant and Mixed Volume, and a Combinatorial Corollary},
    Discrete Comput. Geom. {\bf 27} (2002), 531--550.

\bibitem{lieb1990}
    E.H. Lieb,
    \emph{{G}aussian Kernels have only {G}aussian Maximizers}
    Invent. Math. {\bf 102} (1990), 179--208.

\bibitem{watrous}
    John Watrous,
    \emph{the Theory of Quantum Information}
    Cambridge University Press, 2018.

\bibitem{serre}
    Denis Serre,
    \emph{Matrices},
    Springer, 2010.

\end{thebibliography}

\noindent \textsc{Jacob Denson, UW Madison}\\
\textit{email:} \texttt{jcdenson@wisc.edu}

\end{document}