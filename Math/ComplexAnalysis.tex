\input{../style.tex}

\title{Complex Analysis}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}
\maketitle
\tableofcontents
\pagenumbering{arabic}

\chapter{Introduction}

The study of complex numbers is the analysis of a strange mathematical creation. Without any knowledge of these numbers, we may think of this `imaginary' creation as being a mathematical abstraction for mathematicians to entertain themselves with. After some introduction, we may think that the complex plane simply restates results about the 2D plane $\mathbf{R}^2$ with new notation. However, a more subtle analysis reveals that complex differentiation, defined in terms of the new operations the complex numbers brings to the table, brings drastic differences to the study of the complex plane rather than the real plane, and offers results many describe as `miraculous', or `magic'. What's more, the theory of complex analysis introduces many concepts which have formed the building blocks of the mathematical theories of the 19th and 20th centuries. In this first section, we introduce the complex numbers formally, as well as examining the basic tools we will use in the field of complex analysis.

\section{The Complex Numbers}

As a numerical calculation system, the real numbers have many advantages: they are complete, enabling us to use Newton's calculus with precisious, as well as forming an ordered system of numbers, giving us a standard tool for comparing magnitudes which occur throughout the sciences. Nevertheless, the number system has one deficiency: there are some mathematical equations we are unable to solve. Most famously, since the square of every real number is positive, we cannot find a number $x$ which satisfies the equation $x^2 = -1$. The complex number system $\mathbf{C}$ is obtained by `imagining' a new number $i$, which does satisfy the equation $i^2 = -1$, and building a new number system containing $i$, and all real numbers, in such a way that most of the useful mathematical laws which hold for the real numbers continue to hold for all numbers in $\mathbf{C}$.

To be precise, we introduce the language of modern algebra. The algebraic rules in $\mathbf{R}$ we tend to use the most often are those which make $\mathbf{R}$ into a {\it field}. We define $\mathbf{C}$ to be the smallest set of numbers containing $\mathbf{R}$, as well as containing a new $i$ satisfying the equation $i^2 = -1$, while also forming a field. Since we must be able to add and multiply arbitrary numbers in $\mathbf{C}$, we must have numbers of the form $a + ib$, for each $a,b \in \mathbf{R}$, because we must be able to multiply each real number by $i$, and to add the result to a real number. Using the commutative and distributive law, we find that
%
\[ (a + ib) + (x + iy) = (a + x) + i(b + y) \]
\[ (a + ib)(x + iy) = (ax - by) + i(bx + ay) \]
%
so the operations of addition and multiplication are closed under these operations. What's more, every nonzero number of the form $z = x + iy$ also has an inverse of this form, because if we define the {\bf complex conjugate} $\overline{z} = x - iy$, we find that $z\overline{z} = x^2 + y^2$, which is a nonzero real number if either $x$ or $y$ is nonzero, and then
%
\[ z \frac{\overline{z}}{x^2 + y^2} = z \left[ \frac{x}{x^2 + y^2} + i \frac{-y}{x^2 + y^2} \right] = 1 \]
%
so the set of numbers expandable in the form $a + ib$ is exactly the {\it smallest} field containing $\mathbf{R}$ and $i$. Thus every complex number is formally expandable in this form. This expansion must be unique, because if $x + iy = a + ib$, then $(x - a) + i(y-b) = 0$, and if $y \neq b$, we conclude that $i = (a-x)/(y-b)$ is a real number, which is impossible, so we must have $y = b$, and then $x = a$ follows automatically. Thus a complex number $z = a + ib$ is uniquely defined by its real and complex parts $\Re(z) = a$ and $\Im(z) = b$. Summing up these results using the language of field theory, we say that $\mathbf{C}$ is the field extension of degree two obtained from $\mathbf{R}$ by solving the polynomial equation $x^2 = -1$. We also note that since the standard laws of arithmetic apply to the complex numbers, we find that the equations
%
\[ (a + ib) + (x + iy) = (a + x) + i(b + y) \]
\[ (a + ib)(x + iy) = ax + (ay + bx)i + (by)i^2 = (ax - by) + (ay + bx)i \]
%
can be used to formally define the complex numbers; We don't need to `assume' the existence of $\mathbf{C}$ as an axiom. We can instead construct it by placing certain arithmetical operations on $\mathbf{R}^2$, using the definitions above.

\section{Geometry of the Complex Plane}

Geometrically, the complex plane can be identified with $\mathbf{R}^2$. This leads to useful insight into the theorems of complex analysis. First off, it gives a topological structure to the class of complex numbers, which when viewed geometrically is called the complex {\it plane}. The $x$ and $y$ axis of the plane are called the {\bf real axis} and {\bf imaginary axis}. A novel result of the introductions of arithmetical operations to $\mathbf{R}^2$ is that the norm has a pleasant form. If we denote the norm of a complex number $z$ by $|z|$, then the value satisfies $|z|^2 = z\overline{z}$, and we call $|z| > 0$ the {\bf modulus} of the complex number $z$. Since
%
\[ |zw|^2 = zw \overline{zw} = z\overline{z} w \overline{w} = |z|^2|w|^2 \]
%
we find that $|zw| = |z||w|$ and $|\overline{z}| = |z|$. The modulus gives a complete metric space structure to the complex plane. We note the triangle inequality $|z + w| \leq |z| + |w|$, as well as the other useful inequalities $|\Re(z)|, |\Im(z)| \leq |z|$, and $||z| - |w|| \leq |z - w|$. The first follows from the definition, and the second from the triangle inequality, since $|z| \leq |w| + |z - w|$ and $|w| \leq |z| + |w - z|$.

Geometrically, the arithmetical operations of the complex numbers become incredibly beautiful when we look at the numbers in their {\bf polar form}. Recall that any point on the plane $\mathbf{R}^2$ can be expressed in terms of an angle $\theta$ and a radius $r$. Looking ahead, we note that the complex exponential $e^{it}$ satisfies Euler's formula $e^{it} = \cos t + i \sin t$ (we will prove this rigorously later), so that is polar coordinates, the number with coordinates $r$ and $\theta$ is exactly $re^{i\theta}$. Because we have the addition formula $e^{i(x + y)} = e^{ix}e^{iy}$, we find that $re^{it}r'e^{it'} = (rr')e^{i(t + t')}$, so that multiplying too complex numbers dilates their radius multiplicatively, and rotates their angular coordinate additively. Multiplying a fixed complex number about the whole plane is geometrically just a rotation and a dilation in two dimensions. This shows that the arithmetic of complex numbers is not just an abstractly defined construction to satisfy a single algebraic equation; it has a purely geometric definition, which explains why complex analysis leads to such majestic geometrical results.

\section{Complex Functions and Holomorphicity}

The really interesting part of complex analysis begin when we start looking at complex-valued functions defined on subsets of the complex plane. Classically, we write such a function in the form $w = f(z)$, referring to the domain as the `$z$-plane', and the codomain as the `$w$-plane'. We will mostly be analyzing maps defined on {\bf domains} of the plane, that is, open, connected subsets.

%How do we visualize such a function? Is it fairly simple to visualize a real-valued function, defined on a subset of the real numbers, since the graph of such a function lies in two-dimensional space. For complex numbers, it is not so easy to see that the graph approach works, since the graph lies in four dimensional space. We can still analyze pieces of the functions which lie in the real axis. For instance, we can analyze the modulus, argument, real, and imaginary parts of a complex function separately.

%A more advanced and very modern way of analyzing a complex function is through {\bf domain colouring}. Via the colour wheel, we may identify every point on the complex plane by a certain colour and intensity. This is the trick which enables us to draw a four-dimensional complex function. At each point $z$, we draw the colour representing $f(z)$. You might not be able to draw this by hand, but if you are able to use a computer, it's a great visual tool.

%\begin{example}
%    Consider the function $w = z^2$. If we analyze the graph of the modulus $w = |z^2| = |z|^2$, we see that it is a paraboloid of revolution. The level curves are circles around the origin.
    %
%    \begin{center}
%    \includegraphics[scale = 0.3]{complexz2abssurface}
%    \includegraphics[scale = 0.3]{complexz2resurface}
%    \includegraphics[scale = 0.3]{complexz2imsurface}

%    \includegraphics[scale = 0.5]{colorz2}
%    \end{center}
    %
%    The graph of the real part $w = \Re(z^2)$ can be written as $w = \Re(z)^2 - \Im(z)^2$. This is a hyperbololic paraboloid, as is the imaginary part can be written $w = 2\Re(z)\Im(z)$, with level curves
    %
%    \[ \Im(z) = \frac{w}{2 \Re(z)} \]
    %
%    Graphs of the three component functions are shown above.
%\end{example}

The standard definition of differentiability of real-valued functions generalizes easily to complex-valued functions, but the definition hides the fact that complex differentiability is a much stronger property than standard differentiability. A complex-valued function $f: \mathbf{C} \to \mathbf{C}$ is {\bf complex differentiable
}, {\bf holomorphic}, or {\bf regular} at $z$ if the limit
%
\[ \lim_{w \to 0} \frac{f(z + w) - f(z)}{w} \]
%
exists, which is now a value in the complex plane, denoted $f'(z)$. Unlike differentiability on the real plane, we note that complex differentiability is a much stronger condition on the local behaviour of $f$ around $z$. Whereas the real-valued limit can only by approached by two directions, complex differentiability is a limit approached from any approach trajectory. The set of all holomorphic functions defined on a set $D \subset \mathbf{C}$ is denoted $C^\omega(D)$.

If $f$ is complex differentiable at $z$, then the limit implies exactly that we can write
%
\[ f(z + w) = f(z) + w f'(z) + o(w) \]
%
In terms of complex multiplication, that means that $f$ is locally just a translation, combined with a rotation and a dilation. If we have a function $g$ which is also differentiable, then
%
\[ f(z + w) + g(z + w) = [f(z) + g(z)] + w[f'(z) + g'(z)] + o(w) \]
\begin{align*}
    f(z + w)g(z + w) &= [f(z) + wf'(z) + o(w)][g(z) + wg'(z) + o(w)]\\
    &= f(z)g(z) + w[f'(z)g(z) + f(z)g'(z)] + o(w)
\end{align*}
\begin{align*}
    f(g(z + w)) &= f(g(z) + wg'(z) + o(w))\\
    &= f(g(z)) + [wg'(z) + o(w)]f'(g(z)) + o(wg'(z) + o(w))\\
    &= f(g(z)) + wg'(z)f'(g(z)) + o(w)
\end{align*}
%
so we have the addition, product, and chain rules $(f + g)' = f' + g'$ $(fg)' = fg' + f'g$, and $(f \circ g)' = g'(f' \circ g)$. The chain rule, combined with the product rule, gives the quotient rule
%
\[ (f/g)' = f'/g + f(1/g)' = f'/g - (fg')/(g^2) = \frac{f'g - fg'}{g^2} \]
%
These properties are very useful for calculating the derivatives of the elementary complex functions.

\begin{example}
    Every polynomial $f(z) = a_0 + a_1z + \dots + a_nz^n$ is differentiable at any point in the complex plane, with derivative $f'(z) = a_1 + 2a_2z + \dots + na_nz^{n-1}$. It is easy to see that the derivative of $w = z$ is $w' = 1$, and then the addition and product rule verifies the rule for all polynomials. A function is {\bf entire} if it is differentiable on the whole complex plane.
\end{example}

\begin{example}
    The function $w = 1/z$ is differentiable on $\mathbf{C} - \{ 0 \}$, because
    %
    \[ \lim_{w \to 0} \frac{(z + w)^{-1} - z^{-1}}{w} = \lim_{w \to 0} -\frac{1}{z(z+w)} = -\frac{1}{z^2} \]
\end{example}

\begin{example}
    The function $f(z) = \overline{z}$, though just a simple reflection in the complex plane, is not {\it complex} differentiable anywhere. This is because
    %
    \[ \lim_{w \to 0} \frac{\overline{z + w} - \overline{z}}{w} = \lim_{w \to 0} \frac{\overline{w}}{w} \]
    %
    And this limit does not exist, because if we approach the limit from the imaginary axis, with $w = ti$, then $\overline{w}/w = -1$, and if we approach the limit from the real axis, with $w = t$, then $\overline{w}/w = 1$. This expresses the fact that reflection is not locally a rotation and a dilation everywhere.
\end{example}

Now the function $f(z) = \overline{z}$, viewed as a function from $\mathbf{R}^2$ to $\mathbf{R}^2$, is certainly differentiable, and in fact, $C^\infty$. We find that we can write this function as $f(a,b) = (a,-b)$, which therefore has Jacobian
%
\[ Df(a,b) = \begin{pmatrix} +1 & 0 \\ 0 & -1 \end{pmatrix} \]
%
Thus the existence of a real derivative does not guarantee the existence of a complex derivative. This expresses the fact that not all linear endomorphisms of the plane are rotations and dilations. However, there are relations which guarantee a linear map to be a rotation and a dilation. We note that if the rotation and dilation is given by a complex number $w = a + ib$, then it maps the unit vector $e_1 = 1$ to $a + ib$, and the unit vector $e_2 = i$ to $-b + ai$. Thus the linear transformation is represented by the matrix
%
\[ \begin{pmatrix} a & -b \\ b & a \end{pmatrix} \]
%
This shows that every matrix $\left( \begin{smallmatrix} x_{11} & x_{12} \\ x_{21} & x_{22} \end{smallmatrix} \right)$ with $x_{11} = x_{22}$ and $x_{12} = -x_{21}$ is a rotation and a dilation, and conversely, every rotation and dilation satisfies this equation. In terms of differentiation, we know that if $f$ is differentiable at a point in $\mathbf{R}^2$, then $f = u + iv$ is also partial differentiable at the point, and the Jacobian matrix is
%
\[ \begin{pmatrix} \frac{\partial u}{\partial x} & \frac{\partial u}{\partial y} \\ \frac{\partial v}{\partial x} & \frac{\partial v}{\partial y} \end{pmatrix} \]
%
It follows that a function $f: \mathbf{C} \to \mathbf{C}$ is complex differentiable at a point if and only if it is real differentiable viewed as a map from $\mathbf{R}^2$ to $\mathbf{R}^2$, while also satisfying the {\bf Cauchy Riemann equations}
%
\[ \frac{\partial u}{\partial x} = \frac{\partial v}{\partial y}\ \ \ \ \ \frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x} \]
%
As an alternate viewpoint, we can also approach this purely from the complex perspective (something we should push for more and more often as we pursue further into the field). If $f$ is complex differentiable at $z$, then
%
\[ f(z + w) = f(z) + f'(z)w + o(w) \]
%
so
%
\[ \frac{\partial f}{\partial x} = \lim_{t \to 0} \frac{f(z + t) - f(z)}{t} = \lim_{t \to 0} \frac{f'(z)t + o(t)}{t} = f'(z) \]
\[ \frac{\partial f}{\partial y} = \lim_{t \to 0} \frac{f(z + it) - f(z)}{t} = \lim_{t \to 0} \frac{itf'(z) + o(it)}{t} = if'(z) \]
%
Therefore if $f$ is complex differentiable, then
%
\[ \frac{\partial f}{\partial y} = i\frac{\partial f}{\partial x} \]
%
If we write $\frac{\partial f}{\partial y} = \frac{\partial u}{\partial y} + i \frac{\partial v}{\partial y}$, and $\frac{\partial f}{\partial x} = \frac{\partial u}{\partial x} + i \frac{\partial v}{\partial x}$, and separate the equation into its real and imaginary parts, we obtain exactly the Cauchy Riemann equations.

\section{Integration Along Curves}

One distinguishing feature separating the study of complex analysis from Newton's one dimensional calculus is the reliance on line integration, which you may have encountered in multivariate calculus. Firstly, we will find ourselves not integrating over the whole complex plane, but instead integrating over curves lying in the complex plane.

Using the language of differential geometry, a {\bf smooth curve} is an immersion of a closed interval into $\mathbf{C}$ (viewing the interval as an oriented one dimensional manifold with boundary, and $\mathbf{C}$ as a two dimensional manifold). More simply, we define a {\bf smooth parameterization} $z: [a,b] \to \mathbf{C}$ to be a differentiable map whose derivative is continuous, and $z'(t) \neq 0$ for all $t \in [a,b]$, where we interpret $z'(a)$ and $z'(b)$ as the left and right hand limits
%
\[ \lim_{h \to 0^+} \frac{z(a + h) - z(a)}{h}\ \ \ \ \ \lim_{h \to 0^-} \frac{z(b + h) - z(b)}{h} \]
%
Two smooth curves $z_0: [a_0,b_0] \to \mathbf{C}$ and $z_1: [a_1,b_1] \to \mathbf{C}$ are {\bf equivalent} if there is a continuously differentiable function $s: [a_0,b_0] \to [a_1,b_1]$ with $s'(t) > 0$ for all $t$, such that $z_0(t) = z_1(s(t))$ for all $t \in [a_0,b_0]$. We then define a smooth curve to be an equivalence class of smooth parameterizations. More generally, a {\bf piecewise smooth curve} is an equivalence class of continuous parameterizations $z: [a,b] \to \mathbf{C}$, with values $a = a_0 < a_1 < \dots < a_n = b$ such that $z$ is a smooth curve on $[a_i,a_{i+1}]$, where $z_0: [a_0,b_0] \to \mathbf{C}$ and $z_1:[c_0,d_0] \to \mathbf{C}$ are equivalent if there are $a = a_0 < \dots < a_n = b$ and $c = c_0 < \dots < b_n = d$ such that $z_0$ and $z_1$ are equivalent on each of the intervals $[a_i,a_{i+1}]$ and $[c_i,c_{i+1}]$. This is a general enough class of curves for our purposes in complex analysis.

\begin{example}
    The standard example of a curve is the circle $C_r(z_0)$, of radius $r$ centered at $z_0$. The {\it positive orientation} of this curve is given by the parameterization
    %
    \[ z(t) = z_0 + re^{it} \]
    %
    On $[0,2\pi]$, and the {\it negative orientation}
    %
    \[ z(t) = z_0 + re^{-it} \]
    %
    also defined on $[0,2\pi]$. In general, we will interpret $C_r(z_0)$ as having the positive orientation. In particular, we denote $C_1(0)$ by $S^1$, the unit circle.
\end{example}

We will also need to introduce a couple more tool to use to manipulate curves. The {\bf inverse orientation} of a curve with parameterization $z: [a,b] \to \mathbf{C}$, which is the curve with parameterization $z^-: [a,b] \to \mathbf{C}$ with $z^-(t) = z(a+b-t)$. A curve $z: [a,b] \to \mathbf{C}$ is {\bf closed} if $z(a) = z(b)$, which means its image forms a `closed loop' in the plane.

Next, note that if $f: X \to \mathbf{C}$ is a complex-valued function, then thanks to the our discussion of the expansion properties of complex numbers, we can write $f = u + iv$, where $u,v: X \to \mathbf{R}$ are real value functions known as the real and complex parts of $f$, denoted $\Re(f)$ and $\Im(f)$. If $u$ and $v$ are both integrable functions (by whatever definition you're comfortable with), then we define the integral of $f$ to be
%
\[ \int f = \int u + i \int v \]
%
Thus integrals of complex valued functions are just componentwise integrals of real valued functions: a two dimensional average is just the average of two components. Now given a smooth curve $C$ with parameterization $z: [a,b] \to D$, where $D$ is some open subset of the complex plane, and a continuous function $f: D \to \mathbf{C}$, we define the {\bf line integral} of $f$ on $C$ as
%
\[ \int_C f(z) dz = \int_a^b f(z(t)) z'(t) dt \]
%
which we can remember by the mneumonic
%
\[ f(z) dz = f(z) \left( \frac{dz}{dt} \right) dt \]
%
so we can obtain the formula by `multiplying by a $dt/dt$ factor'. If $z'$ is only piecewise smooth, with partition $a_0 < \dots < a_n$, then we define
%
\[ \int_C f(z) dz = \sum \int_{a_i}^{a_{i+1}} f(z(t)) z'(t) dt \]
%
The change of variables formula for one dimensional integrals verifies that these definitions are independent of parameterization.

\begin{theorem}
    The line integral satisfies the following properties
    %
    \begin{itemize}
        \item For a given curve $C$ on some domain $D$, for any two $f,g \in C(D)$, $\alpha, \beta \in \mathbf{C}$,
        %
        \[ \int_C (\alpha f + \beta g)(z) dz = \alpha \int_C f(z) dz + \beta \int_C g(z) dz \]

        \item If $C^-$ is the reverse orientation of $C$, then
        %
        \[ \int_{C^-} f(z) dz = - \int_C f(z) dz \]

        \item One has
        %
        \[ \left| \int_C f(z) dz \right| \leq l(C) \| f \|_\infty \]
        %
        where $\| f \|_\infty$ is the supremum of $f$ over image of the curve $C$, and $l(C)$ is the {\bf length} of the curve $C$, defined to be the integral $\int_a^b |z'(t)| dt$.
    \end{itemize}
\end{theorem}
\begin{proof}
    The first theorem follows by the linearity of one dimensional integration by expanding out formulas, and the second theorem follows because $(z^-)'(t) = -z'(a+b-t)$. Finally, the third theorem follows because if $z: [a,b] \to \mathbf{C}$ is smooth, then
    %
    \[ \left| \int_C f(z)dz \right| = \left| \int_a^b f(z(t)) z'(t) dt \right| \leq \int_a^b |f(z(t))||z'(t)| dt \leq \| f \|_\infty \int_a^b |z'(t)| dt \]
    %
    and the result for piecewise smooth curves follows by linearity.
\end{proof}

Finally in our review of line integration, we recall a very useful result. We say a function $f: D \to \mathbf{C}$ on some domain $D$ has a {\bf primitive} if there is a function $F: D \to \mathbf{C}$ holomorphic on $D$ such that $F'(z) = f(z)$.

\begin{theorem}
    If $f$ is continuous, and has a primitive $F$ on $D$, then a curve $C$ begining at $z$ and ending at $w$ has
    %
    \[ \int_C f(z)dz = F(w) - F(z) \]
    %
    In particular, if $C$ is a closed curve in $D$, then $\int_C f(z)dz = 0$.
\end{theorem}
\begin{proof}
    If $C$ is smooth, then
    %
    \[ \int_C f(z)dz = \int_a^b f(z(t)) z'(t) dt \]
    %
    Now if $F'(z) = f(z)$ for each $z \in D$, then the chain rule (which is easily proved for compositions of functions from intervals to $\mathbf{C}$ and from holomorphic functions on $\mathbf{C}$) implies that $(F \circ z)'(t) = F'(z(t)) z'(t)$, and therefore the fundamental theorem of calculus (applied componentwise on the integral of the complex valued function $f(z(t)) z'(t)$), we conclude that
    %
    \[ \int_a^b f(z(t)) z'(t) dt = F(z(b)) - F(z(a)) = F(w) - F(z) \]
    %
    The theorem is then proved for piecewise smooth curves by a telescoping summation.
\end{proof}

\begin{corollary}
    If $f$ is a holomorphic function on a domain $D$ with $f' = 0$, then $f$ is constant on $D$.
\end{corollary}
\begin{proof}
    If $z$ and $w$ are two points on $D$, let $C$ be a smooth curve beginning at $z$ and ending at $w$. Then since $f' = 0$,
    %
    \[ \int_C f'(z) dz = 0  \]
    %
    But also since $f'$ has a primitive $f$, we find that
    %
    \[ \int_C f'(z) dz = f(w) - f(z) \]
    %
    and hence $f(w) = f(z)$.
\end{proof}

\begin{example}
    The function $f(z) = 1/z$ does not have a primitive in $\mathbf{C} - \{ 0 \}$, where it is defined, because
    %
    \[ \int_{S^1} \frac{dz}{z} = \int_0^{2\pi} \frac{ie^{it}}{e^{it}} = \int_0^{2\pi} i = 2 \pi i \]
    %
    Whereas if $1/z$ did have a primitive, we would conclude this integral is zero.
\end{example}














\section{Optional: Complex Manifold Theory}

The study of differentiable manifolds generalizes the calculus of real variables in Euclidean space to spaces which locally behave like Euclidean space. In this section we discuss the theory which generalizes the theory of holomorphic functions on domains in the complex plane to spaces which locally behave like the complex plane. These `one-dimensional complex manifolds' are known as {\bf Riemann surfaces}.

Topological intuition suggests that complex manifolds should be 2-dimensional real manifolds, with additional structure which enables us to identify a class of holomorphic functions. The standard way to identify differentiable functions on a real manifold is to introduce a $C^\infty$ structure, considering an atlas of local embeddings $x: U \to \mathbf{R}$, such that if $y: V \to \mathbf{R}$ is any other embedding, the transition map $x \circ y^{-1}$ is $C^\infty$ where defined. Similarily, we introduce a {\bf $C^\omega$ atlas} to be a cover of a two dimensional manifold by charts such that any two chars $(x,U)$ and $(y,V)$ have a holomorphic transition map $x \circ y^{-1}$ where defined. A two dimensional manifold $M$ with a fixed $C^\omega$ atlas is known as a Riemann surface.

Every Rieman surface $M$ is certainly a two dimensional manifold, so we can consider the tangent space $T_p M$. What's more, we can give these two dimensional tangent spaces a one dimensional complex streucture. Recall that if $V$ is an $\mathbf{R}$ vector space, and $J: V \to V$ is a linear map such that $J^2 = -1$, then we may make $V$ into a complex vector space by defining $(a + bi)v = a + bJv$, so that a complex vector space is just a real vector space with a fixed `90 degree rotation'. We define a family of linear maps $J_p: T_p M \to T_p M$ at each point by use of the holomorphic coordinates. If $(x,U)$ is a chart at $p$, then we may write each tangent vector $v$ at $p$ in the coordinates of $x$ with a unique $z \in \mathbf{C}$, by the correspondence $x_*$. We define $Jv$ in the coordinates of $x$ to be $iz$. If $y$ is another chart at $p$, then $v$ has coordinates $(y \circ x^{-1})'(x(p)) \cdot z$, and $Jv$ has coordinates $(y \circ x^{-1})'(x(p))(iz) = i(y \circ x^{-1})'(x(p)) z$. Precisely because the transition map is holomorphic, the multiplication by $i$ is invariant of being taken inside brackets or outside brackets. This means $J$ is independant of the coordinate system, and so the tangent spaces are one-dimensional complex vector spaces.

A map $f: M \to N$ between Riemann surfaces is holomorphic if the map is holomorphic in every coordinate system. Since we have the tangent bundle, there is a more natural coordinate-invariant definition. Provided $f$ is differentiable (which it must be if it is holomorphic), $f$ induces a map $f_*: TM \to TN$. $f$ is holomorphic at a point if and only if $f_*$ is complex-linear at the point. This makes sense, because Riemann surfaces should model the local properties of the complex plane, and holomorphic functions on the complex plane are exactly those functions which are locally complex linear maps.

\begin{example}
    The fundamental example of a Riemann surface is the Riemann sphere $\mathbf{C}^\infty = \mathbf{C} \cup \{ \infty \}$, whose topology is defined to be the compactification of $\mathbf{C}$. The subsets $\mathbf{C}$ and $\mathbf{C}^\infty - \{ 0 \}$ both have homeomorphisms onto $\mathbf{C}$, the first by the identity map, and the second by the map $z \mapsto 1/z$. The maps are compatible since $1/z$ is holomorphic for $z \neq 0$ on $\mathbf{C}$. A function $f: \mathbf{C}^\infty \to \mathbf{C}$ is holomorphic if and only if $f(z)$, and $f(1/z)$ are holomorphic. Life is more fun when we can consider holomorphic functions from $\mathbf{C}$ to $\mathbf{C}^\infty$, which are meromorphic functions, and holomorphic functions from $\mathbf{C}^\infty$ to $\mathbf{C}^\infty$, which are meromorphic functions meromorphic at $\infty$.
\end{example}

Fundamentally, the `$J$' map is what defines the holomorphic properties of a $2$-manifold. If a given bundle map $J:TM \to TM$ is given such that $J^2 = -1$, then this map defines a complex linear structure on the tangent spaces, and we define $f: M \to N$ to be holomorphic if the corresponding map $f_*$ is complex linear at each point. A holomorphic atlas on $M$ may then be define to be the set of all `holomorphic charts', for if $(x,U)$ and $(y,V)$ are holomorphic charts, then $x \circ y^{-1}$ is holomorphic, for it is certainly differentiable, and the derivative is complex linear. Thus as long as $J$ is `consistant', in the sense that there exist enough holomorphic charts to cover the manifold, then $J$ is a perfectly reasonable way to define a complex structure on a $2$-manifold. In fact, $J$ is what we call a {\bf almost complex structure} on the manifold, for it is almost all we need to define holomorphic functions.

Even if we forget the tangent space $T_p M$, we may still reconstruct it from a holomorphic atlas by considering derivations. Recall that a $p$-derivation is a complex-valued function $\Lambda$ such that for any two functions $f,g \in C^\infty(M,\mathbf{C})$,
%
\[ \Lambda (fg) = f(p) \Lambda g + g(p) \Lambda f \]
%
By induction, provided $f_i(p) \neq 0$ for any $p$,
%
\[ \Lambda \left(\prod_{i = 1}^n f_i \right) = \prod_{i = 1}^n f_i(p) \left[\sum_{i = 1}^n \frac{\Lambda f_i}{f_i(p)}\right] \]
%
If we define the partial differential operators
%
\[ \deriv{f}{x}{p} = \left.\frac{\partial \Re f}{\partial x}\right|_p + i \left.\frac{\partial \Im f}{\partial x}\right|_p\ \ \ \ \ \left.\frac{\partial f}{\partial y}\right|_p = \left.\frac{\partial \Re f}{\partial y}\right|_p + \left.\frac{\partial \Im f}{\partial y}\right|_p \]
%
Then these operators are easily shown to be contained in $T_p \mathbf{C}$. As in the real case, we shall show these operators span over the complex linear space $T_p \mathbf{C}$, that is, every linear operator in $T_p \mathbf{C}$ can be written as a complex combination of the two operators. The proof is not too different from the real case. In the sequel, we shall let $\Lambda$ stand for an arbitrary $p$-derivation.

\begin{lemma}
    If $c$ is a constant function $c: w \to c$, for $c \in \mathbf{C}$, then $\Lambda c = 0$.
\end{lemma}
\begin{proof}
    Consider the constant $1$ function. Then $1 = 1 \cdotp 1$, so
    %
    \[ \Lambda 1 = \Lambda (1 \cdotp 1) = \Lambda 1 + \Lambda 1 \]
    %
    Hence $\Lambda 1 = 0$. Now $\Lambda c = c \Lambda 1 = c \cdotp 0 = 0$.
\end{proof}

\begin{lemma}
    If $f$ and $g$ are equal in a neighbourhood of $p$, then $\Lambda f = \Lambda g$.
\end{lemma}
\begin{proof}
    If $f$ and $g$ are as above, then $f - g$ is constantly zero in a neighbourhood $U$ of $p$ (which we may assume to be bounded). Consider a function $\Psi$ which has value 1 on a closed neighbourhood $\overline{V} \subset U$, and which vanishes outside of $U$. Then $(f - g) \Psi = 0$, so
    %
    \[ 0 = \Lambda 0 = \Lambda (f - g) \Psi = (f - g)(p) \Lambda \Psi + \Psi(p) \Lambda (f - g) = \Lambda f - \Lambda g \]
    %
    And we have shown $\Lambda f = \Lambda g$.
\end{proof}

It is for this reason that Grothendieck thought of the tangent space as defined on the {\it sheaf} generated by the $C^\infty$ functions, that is, the equivalence classes of functions locally equal around $p$.

\begin{corollary}
    $T_p U$ is isomorphic to $T_p \mathbf{C}$, for any open subset $U$ of $\mathbf{C}$.
\end{corollary}

The following is a trivial consequence of the property of a derivation.

\begin{lemma}
    If $f(p) = g(p) = 0$, then $\Lambda (fg) = 0$.
\end{lemma}

Now for the big theorem.

\begin{theorem}
    Any linear derivation $\Lambda$ can be written as the complex combination of the partial differential operators defined above.
\end{theorem}
\begin{proof}
    Consider any $f \in C^\infty \mathbf{C}$. By appealing to Taylor's theorem in the real case, one may write
    %
    \begin{align*} f(z) &= f(p) + \left.\frac{\partial f}{\partial x}\right|_p (\Re z - \Re p) + i \left. \frac{\partial f}{\partial y} \right|_p (\Im z - \Im p) + \text{Second Order Terms}...
    \end{align*}
    %
    Then
    %
    \begin{align*}
        \Lambda f &= \Lambda f(p) + \left.\frac{\partial f}{\partial x}\right|_p \Lambda(\Re z - \Re p) + i \left. \frac{\partial f}{\partial y} \right|_p \Lambda(\Im z - \Im p) + \Lambda(\text{second order terms})\\
        &= 0 + \left.\frac{\partial f}{\partial x}\right|_p \Lambda (\Re z) + i \left. \frac{\partial f}{\partial y} \right|_p \Lambda(\Im z) + 0
    \end{align*}
    %
    Considering all $f$, we conclude
    %
    \[ \Lambda = \left.\frac{\partial}{\partial x}\right|_p \Lambda (\Re z) + i \left. \frac{\partial}{\partial y} \right|_p \Lambda(\Im z) \]
    %
    And our proof is complete, any such complex combination of the form
    %
    \[ z \left.\frac{\partial}{\partial x}\right|_p + w \left. \frac{\partial}{\partial y} \right|_p \]
    %
    Is a $p$-derivation, and any $p$-derivation can be written uniquely in this form.
\end{proof}

As with a tangent space, we can consider the cotangent space $T_p^* \mathbf{C}$, the dual space of $T_p \mathbf{C}$. Given any smooth $f: \mathbf{C} \to \mathbf{C}$, we may define $df_p \in T_p^*$ by the equation
%
\[ df_p (v) = v(f) \]
%
We may put these maps together to form a map $df$ on all of $T^* \mathbf{C}$. Appealing to the theorem above, we may write any $\omega \in T_p^* \mathbf{C}$ as $z\ dx_p + w\ dy_p$. 
%
We have
%
\[ df = \frac{\partial f}{\partial x} dx + \frac{\partial f}{\partial y} dy \]

The most intriguing part of this construction occurs when we examine analytic functions. In classic notation, define $z$ to be the identity map on $\mathbf{C}$. Then $z$ is trivially smooth, and so we may consider $dz = dx + i dy$. Define $\overline{z} : w \to \overline{w}$. Then $d\overline{z} = dx - i dy$. These equations are linearly independant, and thus span the cotangent space at each point. Thus any $w \in T_p^* \mathbf{C}$ may be written
%
\[ \alpha dz + \beta d\overline{z} \]
%
In particular, this holds for any $df$. We define the coefficients
%
\[ \left.\frac{\partial f}{\partial z}\right|_p \ \ \ \ \ \left.\frac{\partial f}{\partial \overline{z}}\right|_p \]
%
by the equation
%
\[ df = \left.\frac{\partial f}{\partial z}\right|_p dz + \left.\frac{\partial f}{\partial \overline{z}}\right|_p d \overline{z} \]
%
Since
%
\[ \alpha dz + \beta d\overline{z} = (\alpha + \beta) dx + i(\alpha - \beta) dy \]
%
We obtain the equalities
%
\[ \frac{\partial f}{\partial z} + \frac{\partial f}{\partial \overline{z}} = \frac{\partial f}{\partial x}\ \ \ \ \ \frac{\partial f}{\partial z} - \frac{\partial f}{\partial \overline{z}} = -i \frac{\partial f}{\partial y} \]
%
Reordering these equations, we determine that
%
\[ \frac{\partial f}{\partial z} = \frac{1}{2} \left( \frac{\partial f}{\partial x} - i \frac{\partial f}{\partial y} \right)\ \ \ \ \ \frac{\partial f}{\partial \overline{z}} = \frac{1}{2} \left( \frac{\partial f}{\partial x} + i \frac{\partial f}{\partial y} \right) \]
%
The conjugate map $T: z \mapsto \overline{z}$ is a real linear map which is not complex linear. If $U$ is the identity map, then every real linear map $S: \mathbf{C} \to \mathbf{C}$ may be uniquely written as $zT + wI$, with $z,w \in \mathbf{C}$. To see this, we calculate $T$, $iT$, $I$, and $iI$ in matrix notation
%
\[ T = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix} \ \ \ \ \ \ \ iT = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix} \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix} = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} \]
%
\[ I = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}\ \ \ \ \ \ \ \ iI = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix} \]
%
and then find the four matrices are linearly independant over $\mathbf{R}$, hence a spanning set. Thus our discussion is a simple generalization of this fact to arbitrary differentiable maps, since $dz$ is equal to the identity matrix $I$, and $d \overline{z}$ is equal to $T$, on each fibre $T_p \mathbf{C}$.




\section{The Cauchy-Riemann Equations}

We emphasize that to be complex differentiable, a function must be approximated locally by a {\it complex linear function}. In two dimensions, the only functions of this type are of the form $\Lambda z = wz$. Let us calculate what the matrix representation of $\Lambda$ is with respect to the basis $\mathcal{B} = (1, i)$ in $\mathbf{C}$. If $w = a + bi$, then
%
\[ [\Lambda]_{\mathcal{B}} = \begin{pmatrix} a & b \\ -b & a \end{pmatrix} \]
%
Geometrically, $\Lambda$ is just a homothety, a rotation and a dilation (stretching). Since $\Lambda$ is well defined at $z_0$, we may take partial derivatives, and conclude
%
\[ \deriv{\Re f}{x}{z_0} = a = \deriv{\Im f}{y}{z_0}\ \ \ \ \ \deriv{\Im f}{x}{z_0} = b = -\deriv{\Re f}{y}{z_0} \]
%
Putting these equations together, we may write
%
\[ \deriv{f}{x}{z_0} = -i \deriv{f}{y}{z_0} \]
%
Where we define the complex partial derivative linear on each factor. Conversely, if this equation holds, and $f$ is continuously differentiable, then a complex linear $\Lambda$ exists at $z_0$. We have something of practical importance for determining if a function is holomorphic.

\begin{theorem}
    A function $f$ satisfies the {\bf Cauchy-Riemann equations} at a point $z_0$ if
    %
    \[ \deriv{f}{x}{z_0} = -i \deriv{f}{y}{z_0} \]
    %
    If $f$ is complex differentiable at $z_0$, then $f$ satisfies the Cauchy-Riemann equations. Conversely, if $f$ is $C^1$ and satisfies the Cauchy-Riemann equations at $z_0$, then $f$ is complex differentiable at $z_0$.
\end{theorem}

In the language of the complex tangent space, the Cauchy-Riemann equation states exactly that
%
\[ \frac{\partial f}{\partial \overline{z}} = 0 \]
%
Now as with $C^\infty \mathbf{C}$, we may consider all derivations on $C^\omega \mathbf{C}$, to determine the analytic tangent space $T^\omega \mathbf{C}$. Surely, we may restrict the functions in $C^\infty \mathbf{C}$ to $C^\omega \mathbf{C}$, but this does not imply the sets are the same, for two functions which differ originally may be equal when restricted to a smaller set. Since the inclusion $i:C^\omega \mathbf{C} \to C^\infty \mathbf{C}$ is injective, We obtain a surjective bundle map $i^*: T \mathbf{C} \to T^\omega \mathbf{C}$. The kernel of this map (when restricted to the tangent space) is precisely the linear operators of the form
%
\[ \beta \frac{\partial}{\partial \overline{z}} \]
%
Therefore all derivations on $C^\omega \mathbf{C}$ can be written as
%
\[ w \frac{\partial}{\partial z} \]
%
And so this space is one dimensional.

When analyzing the analytic case, we often write the operator as
%
\[ \frac{df}{dz} \]
%
To mimic the 1-dimensional real case. For any analytic function $f$, we have
%
\[ df = \frac{df}{dz} dz \]
%
Most of the real theorems on the tangent space apply to the complex case due to the extension of definitions by linearity. In particular, Stoke's theorem also holds, so if $\Delta$ is an open set in $\mathbf{C}$ whose boundary is a one-manifold, then
%
\[ \int_{\partial \Delta} f\ dz = \int_\Delta \frac{df}{dz}\ dz \wedge dz = \int_\Delta 0 = 0 \]
%
We may consider complex differentiable forms, for instance, and if $\Delta$ is a contractible, open set of $\mathbf{C}$, on which $d \omega = 0$, then $\omega = df$ for some analytic function $f$.



\section{Power Series}

Polynomials are the simplest examples of holomorphic functions. Our next family of holomorphic functions will be obtained by taking limits of polynomials. In fact, we will soon show that this family contains all holomorphic functions.

\begin{definition}
    A {\bf formal power series} around a point $z_0$ is an `infinite dimensional polynomial', a sequence written in the form
    %
    \[ \sum_{k = 0}^\infty c_k (z - z_0)^k \]
    %
    The $c_k$ are known as the coefficients of the series. We may evaluate a power series to obtain a function $f$, defined as
    %
    \[ f(z) = \sum_{k = 0}^\infty a_k (z - z_0)^k \]
    %
    whose domain is the set where the series converges. A function $f$ can be {\bf expanded in a power series} at a point $z_0$ if we may write $f$ as a power series locally around $z_0$. An {\bf analytic} function is a function which can be locally expanded as a power series at every point in its domain.
\end{definition}

Standard tricks from real power series come into play here, and the proofs go unedited. The most useful is the root test, or Cauchy-Hadamard theorem, which states that, if we define
%
\[ 1/R = \limsup_{n \to \infty} |c_n|^{1/n} \]
%
Then the power series $f$ converges for $|z - z_0| < R$, and diverges for $|z - z_0| > R$. It may sometimes be easier to apply the ratio test. If the limit
%
\[ L^{-1} = \lim_{n \to \infty} \left| \frac{c_{n+1}}{c_n} \right| \]
%
exists, then $f$ converges for $|z - z_0| < L$, and diverges for $|z - z_0| > L$. These statements do not say anything about what happens on the boundary, when $|z - z_0|$ is equal to $R$ or $L$. If $g$ 

If $g$ can also be expanded as a power series at $z_0$,
%
\[ g(z) = \sum_{k = 0}^\infty a_k (z - z_0)^k \]
%
Then $(fg)$ can also be expanded at the point, and we have the product formula
%
\[ (fg)(z) = \sum_{m = 0}^\infty \left( \sum_{k = 0}^m a^kc^{m-k} \right) (z - z_0)^k \]
%
The quotient of two power series is also a power series, but we are not able to discuss this with our current knowledge. Thus the set of all analytic functions has a nice algebraic structure.

\section{The Exponential, and Series Differentiation}

The most important power series, and perhaps the most important function in mathematics, is the exponential function, defined at all points in the complex plane by the formula
%
\[ e^z = \exp(z) = \sum_{k = 0}^\infty \frac{z^k}{k!} \]
%
The product formula above shows that
%
\begin{align*}
    \exp(z + w) &= \sum_{k = 0}^\infty \frac{(z + w)^k}{k!} = \sum_{k = 0}^\infty \sum_{j = 0}^k \binom{k}{j} \frac{z^j w^{k-j}}{k!}\\
    &= \sum_{k = 0}^\infty \sum_{j = 0}^k \frac{z^j}{j!} \frac{w^{k-j}}{(k - j)!} = \exp(z) \exp(w)
\end{align*}
%
Which gives us the multiplicative property of the function.

Calculus tells us that the derivative of the exponential function is the exponential function itself. If there is any justice in the world, this idea should extend to the complex case. We can calculate
%
\[ \exp'(z) = \sum_{k = 0}^\infty \left( \frac{z^k}{k!} \right)' = \sum_{k = 1}^\infty \frac{z^{k-1}}{(k-1)!} = \sum_{k = 0}^\infty \frac{z^k}{k!} = \exp(z) \]
%
But how do we know that the derivative of the power series can be obtained by differentiating each term in the series?

\begin{theorem}
    Let $f(z) = \sum_{k = 0}^\infty c_k z^k$ be a power series, convergent inside a disk $D$. Then $f$ is holomorphic in $D$, and $f'(z) = \sum_{k = 1}^\infty k c_k z^{k-1}$.
\end{theorem}
\begin{proof}
    The fact that the power series defining $f'$ converges in $D$ follows from the Hadamard formula. Now let us verify that this series, which defines a function $g$, approaches the derivative of $f$. Define $S_n$ to be the finite polynomial of degree $n$ defined by $f$, $S_n(z) = \sum_{k = 0}^n c_k z^k$. Let $E_n$ denote the error term, $E_n(z) = \sum_{k = n+1}^\infty c_k z^k$. Then $S_n'(z) = \sum_{k = 1}^n k c_k z^{k-1}$, and
    %
    \[ \frac{f(z + w) - f(z)}{w} = \left[ \frac{S_n(z + w) - S_n(z)}{w} - S_n'(z) \right] + \left[ \frac{E_n(z + w) - E_n(z)}{w} \right] + S_n'(z) \]
    %
    The first term converges to zero for small enough $w$. The third term converges to $g(z)$ as $n \to \infty$. The only tricky component is the second term. Since $a^n - b^n = (a - b)(a^{n-1} + a^{n-2}b + \dots + ab^{n-2} + b^{n-1})$. If we choose $|z|, |z + w| < r - \varepsilon$, then
    %
    \begin{align*}
        \left| \frac{E_n(z + w) - E_n(z)}{w} \right| &= \left| \sum_{k = n + 1}^\infty c_k \frac{(z + w)^k - z^k}{w} \right| \leq \sum_{k = n + 1}^\infty k |c_k| (r - \varepsilon)^{n-1}
    \end{align*}
    %
    The right side is a convergent power series, and hence as $n \to \infty$, the term converges to zero. By picking $n$ large enough, and $|z - w|$ small enough, the value can be made as close to $g$ as possible.
\end{proof}

\begin{corollary}
    A function $f(z) = \sum_{i = 0}^\infty c_i z^i$ expandable in a power series about a point has holomorphic derivatives $f', f'', f^{(3)}, \dots$ around the point, which are also expandable in a power series
    %
    \[ f^{(n)} = \sum_{k = n}^\infty \frac{k!}{(k - n)!} c_k z^{k-n} \]
\end{corollary}

\begin{corollary}
    Every analytic function is holomorphic
\end{corollary}

We shall later prove that every holomorphic function is analytic. First, let us examine some useful properties of the exponential. Most sensationally, we shall verify Euler's formula
%
\[ e^{i \pi} = 1 \]
%
This fact depends on the fact that the complex exponential function is much more robust than 



\chapter{Geometric Function Theory}

An intuitive way to visualize analytic functions is how they act on the geometry of the complex plane. The Riemann school believes that this should be the primary way of understanding these maps, giving us interesting geometric results. Here we introduce conformal maps, and pave the way to Riemann's classical mapping theorem.

The study of geometry was fundamentally changed by Felix Klein's Erlangen program. To rationalize the emergence of the paradoxical non-euclidean geometries, The fundamental thesis of the program is that a study of the invariants of space under actions from a certain group. Klein's Euclidean geometry studies invariants of the plane invariant under similarity transformations, which preserve angles and proportions, collected into the Euclidean group $E(2)$. Projective geometry studies invariants of the sphere under transformations which preserve the incidence of straight lines, collected into the projective group $P(2)$. Of course, one may perturb these groups to obtain more interesting results. For instance, to study properties of space which remain after oriented similarity, we weaken our actions to the special euclidean group $SE(2)$, a subgroup of $E(2)$.

Geometrically, the set of all {\bf biholomorphic functions} (holomorphic maps with a holomorphic inverse) forms a group.

After studying all these analytic functions, it is of course, of interest to determine the properties of the plane which remain invariant under the actions of bijective analytic functions, which form a group.

Let us pretend we have never seen the definition of an analytic function. In fact, forget we have ever invented complex numbers at all (we consider $\mathbf{C}$ as another name for the real plane $\mathbf{R}^2$). $\mathbf{C}$ is a two-dimensional surface, and therefore posseses a tangent space $T\mathbf{C}$, which naturally can be considered as $\mathbf{C}^2$ -- tangent vectors in this space will be denoted as $v_p$, the vector $v$ at the point $p$. $T\mathbf{C}$ is also a Riemannian manifold, since we have a natural inner product on the tangent vectors at each point on the plane, defined by
%
\[ \langle v_p, w_p \rangle = v_1 w_1 + v_2 w_2 = \Re[v\overline{w}] \]
%
One obtains `infinitisimal' equivalents to the structures on an inner product space. For instance, one may talk about the `infinitisimal' length of a tangent vector, defined by
%
\[ \| v_p \| = \langle v_p, v_p \rangle \]
%
More interesting to us, we are able to talk about `infinitisimal angles' at points. That is, the angle between two tangent vectors $v_p$ and and $w_p$ is the angle $\theta$ such that
%
\[ \cos(\theta) = \frac{\langle v_p, w_p \rangle}{\|v\| \|w\|} \]
%
We are now ready to discuss conformal maps, the geometric method of introducing analytic functions.

\begin{definition}
    A complex-valued differentiable map $f$ defined on an open subset of the plane is {\bf conformal} if it is oriented, and preserves infinitisimal angles. Pointwise conformality is defined similarily
\end{definition}

\begin{lemma}
    A function analytic at a point with non-zero derivative is conformal there.
\end{lemma}
\begin{proof}
    Let $f$ be analytic at $p$, with $f'(p) = z \neq 0$. The matrix representation of $f'(p)$ is
    %
    \[ \begin{pmatrix} \Re[z] & \Im[z] \\ -\Im[z] & \Re[z] \end{pmatrix} \]
    %
    Whose determinant is $\Re[z]^2 + \Im[z]^2 > 0$, hence the map is oriented at $p$. A calculation verifies that
    %
    \[ \langle f^*(v_p), f^*(w_p) \rangle = \langle zv, zw \rangle = |z|^2 \langle v, w \rangle  \]
    %
    So that, since $|zv| = |z||v|$, and $|zw| = |z||w|$,
    %
    \[ \frac{\langle zv, zw \rangle}{|zv||zw|} = \frac{|z|^2 \langle v, w \rangle }{|z|^2|v||w|} = \frac{\langle v, w \rangle}{|v||w|} \]
    %
    Thus the map is conformal.
\end{proof}

We shall show that, the differential map at any point in the plane is just a rotation and scaling. For each unit tangent vector $e^{i\theta}$ at a point $p$, consider the differential map $f_*((e^{i\theta})_p)$.

Let us now use our knowledge of the complex plane to reintroduce an old friend. We may express the differential of a conformal map $f = u + iv$ as
%
\[ f^*(v_p) = \]

Consider the action of a conformal map $f$ on the unit tangent vectors $(e_1)_p$ and $(e_2)_p$
%
\[ f^*((e_1)_p) = Df(p)(e_1) = \deriv{f^1}{x}{p} \deriv{}{x}{f(p)} + \deriv{f^2}{x}{p} \deriv{}{y}{f(p)} \]
%
\[ f^*((e_2)_p) = Df(p)(e_2) = \deriv{f^1}{y}{p} \deriv{}{x}{f(p)} + \deriv{f^2}{y}{p} \deriv{}{y}{f(p)} \]
%
Since $f$ is conformal, these vectors must be at right angles, and $(f^*(1_p), f^*(i_p))$ must be an oriented basis. In two dimensions, the only vectors orthogonal to a vector $v = (a,b)$ are scalar multiples of the vector $w = (-b,a)$. Since $(v, \lambda w)$ must be oriented, we must have $\lambda (a^2 + b^2) > 0$, hence $\lambda > 0$. Taken in the case of our conformally mapped tangent vectors, we must have some $\lambda > 0$ for which,
%
\[ f^*((e_2)_p) = \lambda \left( -\deriv{f^2}{x}{p} \deriv{}{x}{f(p)} + \deriv{f^1}{x}{p} \deriv{}{y}{f(p)} \right) \]
%
Equating our two values of $f^*((e_2)_p)$, we see that
%
\[ \deriv{f^1}{y}{p} = - \lambda \deriv{f^2}{x}{p}\ \ \ \ \ \deriv{f^2}{y}{p} = \lambda \deriv{f^1}{x}{p} \]

\begin{thebibliography}{10}
    \bibitem{intro} Michael Spivak,
    \emph{A Concise Introduction to Differential Geometry: Vol. One}

    \bibitem{halm} Paul Halmos,
    \emph{Naive Set Theory}

    \bibitem{wiki} Wikipedia,
    \emph{Lie Groups}
\end{thebibliography}

\end{document}