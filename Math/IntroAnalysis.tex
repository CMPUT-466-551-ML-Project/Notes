\input{../style.tex}

\title{Introductory Analysis}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}
\maketitle
\tableofcontents
\pagenumbering{arabic}

\chapter{Inequalities}

Principles of inequalities:

\begin{itemize}
    \item To understand a quantitative result, does the result imply a qualitative result which can be proved without using the quantitative result.
\end{itemize}

Methods to prove inequalities:

\begin{itemize}
    \item If a theorem depends on an integer $N$, is it possible to prove the theorem by induction, or at least to take a simpler form of the theorem for a specific parameter of $N$ to see the theorem more easily?

    \item If you find an inequality with a one-sided symmetry, can you optimize over that symmetry to improve the inequality?
\end{itemize}

Inequalities every mathematician should know:

\begin{itemize}
    \item The Cauchy-Schwarz inequality: For any set of real numbers $a_1, \dots, a_N$ and $b_1, \dots, b_N$,
    %
    \[ a_1b_1 + \dots + a_Nb_N \leq \sqrt{a_1^2 + \dots + a_N^2} \sqrt{b_1^2 + \dots + b_N^2} \]
    %
    which can be interpreted as a notationally simpler way to apply Pythagoras' theorem in higher dimensions. More generally, in any inner product space, $|\langle v, w \rangle| \leq |v| |w|$.
\end{itemize}

\section{The Cauchy Schwarz Inequality}

The Cauchy-Schwarz inequality says that for a set of real numbers $a_1, \dots, a_N$ and $b_1, \dots, b_N$, then
%
\[ a_1b_1 + \dots + a_Nb_N = \sqrt{a_1^2 + \dots + a_N^2} \sqrt{b_1^2 + \dots + b_N^2} \]
%
It is without a doubt one of the most useful inequalities in analysis. The inequality and it's generalizations occur again and again throughout any area of the field. You should master it in order to succeed in mathematical analysis. It provides a way to `unblend' a sequence of numbers which have been merged together, to estimate their size.

\section{Cauchy Schwarz by Induction}

\section{Cauchy Schwarz by Trigonometry}

The Cauchy Schwarz inequality in two dimensions is just Pythagoras' theorem in disguise. Consider the two dimensional inequality. TODO: Expand

If we define
%
\[ u = v - \frac{\langle v, w \rangle}{\langle w, w \rangle} w \]
%
then $u$ is orthogonal to $w$, and so using Pythagoras' theorem, we conclude
%
\[ |v|^2 = \left| u + \frac{\langle v, w \rangle}{\langle w,w \rangle} w \right|^2 = |u|^2 + \left| \frac{\langle v,w \rangle}{\langle w,w \rangle} \right|^2 |w|^2 = |u|^2 + |\langle v, w \rangle|^2 \]
%
\[ |u + w|^2 = |u|^2 + |w|^2 = \left| v - \frac{\langle v, w \rangle}{\langle w, w \rangle} w \right|^2 + |w|^2 \]
%
Thus the degree to which the Cauchy Schwarz inequality is tight is measured precisely by the quantity $|u|^2$. The inequality is precisely tight when $u = 0$, so $v$ is a scalar multiple of $w$. But $u$ is just the projection of $u$ onto the perpendicular hyperplane to the line spanned by $w$, so we can safely apply the Cauchy Schwarz inequality if $v$ is close to orthogonal.

\section{Cauchy Schwarz by Symmetry}

If we were trying to convert a multiplicative inequality into an additive inequality, the most basic equation we might expect to hold is that $xy \leq A(x^2 + y^2)$, for some number $A$. Using the fact that $(x - y)^2 = x^2 - 2xy + y^2$, we find that this inequality works for $A = 1/2$. Thus $2xy \leq x^2 + y^2$, and adding up, we find
%
\[ a_1b_1 + \dots + a_Nb_N = \frac{a_1^2 + \dots + a_N^2}{2} + \frac{b_1^2 + \dots + b_N^2}{2} \]
%
One feature of this inequality is that the equation is not symmetric under scaling. If we choose any scalar's $\lambda, \gamma$, and we swap $a_n$ with $\lambda a_n$, and $b_n$ with $\gamma b_n$, then we find
%
\[ \lambda \gamma ( a_1b_1 + \dots + a_Nb_N ) \leq (\lambda^2/2) (a_1^2 + \dots + a_N^2) + (\gamma^2/2) (b_1^2 + \dots + b_N^2) \]
%
Thus we clearly have
%
\[ a_1b_1 + \dots + a_Nb_N \leq \min_{\lambda,\gamma} (\lambda/2\gamma) (a_1^2 + \dots + a_N^2) + (\gamma/2\lambda) (b_1^2 + \dots + b_N^2) \]
%
Using basic calculus, if we set $\lambda/\gamma = \sqrt{B/A}$, then we find
%
\[ a_1b_1 + \dots + a_Nb_N \leq \sqrt{AB} \]
%
and this is just the Cauchy-Schwarz inequality, which is now an inequality symmetric under scaling.

\section{Applications of Cauchy Schwarz}

\begin{theorem}[The One Trick]
    For any $a_1, \dots, a_N$,
    %
    \[ a_1 + \dots + a_N \leq \sqrt{N} \sqrt{a_1^2 + \dots + a_N^2} \]
\end{theorem}
\begin{proof}
    We just note that $a_n = 1 \cdot a_n$, and so we may apply Cauchy Schwarz, since $\sqrt{1^2 + 1^2 + \dots + 1^2} = \sqrt{N}$. This inequality is only tight if the $a_1, \dots, a_N$ are perpendicular to $1,\dots,1$
\end{proof}

\begin{theorem}[The Splitting Trick]
    For any $a_1, \dots, a_N$, and $p,q \geq 0$ with $p + q = 2$,
    %
    \[ a_1 + \dots + a_N \leq \left( \sum |a_n|^p \right)^{1/2} \left( \sum |a_n|^q \right)^{1/2} \]
\end{theorem} 
\begin{proof}
    We can write $a_n = a_n^{p/2} a_n^{q/2}$, and so by applying Cauchy-Schwarz, we find
    %
    \[ a_1 + \dots + a_N \leq \sqrt{\sum (|a_n|^{p/2})^2} \sqrt{\sum (|a_n|^{q/2})^2} \]
    %
    and this gives the inequality.
\end{proof}

\begin{theorem}
    If $p_n \geq 0$ and $p_1 + \dots + p_N = 1$, and we have the termwise inequality $a_nb_n \geq 1$ for a family of positive numbers $a_1, \dots, a_n$ and $b_1, \dots, b_N$, then
    %
    \[ (p_1a_1 + \dots + p_Na_N)(p_1b_1 + \dots + p_Nb_N) \geq 1 \]
\end{theorem}
\begin{proof}
    This can be proven by writing
    %
    \begin{align*}
        \left( p_1a_1 + \dots + p_N a_N \right) & \left( p_1b_1 + \dots + p_N b_N \right)\\
        &\geq \left( p_1a_1 + \dots + p_Na_N \right) \left( p_1/a_1 + \dots + p_N/a_N \right)
    \end{align*}
    %
    and now we can apply Jensen's inequality, which says $\mathbf{E}[1/X] \geq 1/\mathbf{E}[X]$. Alternatively, we can prove the inequality for induction. For $N = 2$, the theorem says that
    %
    \[ (pa_1 + (1 - p)a_2)(pb_1 + (1 - p)b_2) \geq p^2 + (1 - p)^2 + p(1-p)[a_1/a_2 + a_2/a_1] \]
    %
    and $a_1/a_2 + a_2/a_1$ is minimized for $a_1 = a_2$, with value $2$, giving the required bound. For a general induction, if we let $A = p_1a_1 + \dots + p_Na_N$ and $B = p_1b_1 + \dots + p_Nb_N$, then by induction we know $AB \geq (1 - p_{N+1})^2$, and so
    %
    \begin{align*}
    \left( A + p_{N+1}a_{N+1} \right) & \left( B + p_{N+1}b_{N+1} \right)\\
    &= AB + p_{N+1}^2a_{N+1}b_{N+1} + p_{N+1}(Ab_{N+1} + Ba_{N+1})\\
    &\geq (1 - p_{N+1})^2 + p_{N+1}^2 + p_{N+1}[b_{N+1}A + a_{N+1}B]
    \end{align*}
    %
    and
    %
    \[ b_{N+1}A + a_{N+1}B \geq \sum_{n = 1}^N p_n(a_n/a_{N+1} + a_{N+1}/a_n) \geq 2 \sum_{n=1}^N p_n = 2(1 - p_{N+1}) \]
    %
    Hence
    %
    \begin{align*}
        (1 - p_{N+1})^2 +& p_{N+1}^2 + p_{N+1}[b_{N+1}A + a_{N+1}B]\\
        &\geq (1 - 2p_{N+1} + p_{N+1}^2) + p_{N+1}^2 + p_{N+1}[2(1 - p_{N+1})] \geq 1
    \end{align*}
\end{proof}

By spltting up a three-way sum using the Cauchy-Schwarz inequality, writing $a_nb_nc_n = a_n(b_nc_n)$, we obtain a bound for the three-way sum of elements.

\begin{theorem}
    For any $a_n, b_n, c_n$,
    %
    \[ \left( \sum_{n = 1}^N a_nb_nc_n \right)^4 \leq \left( \sum_{n = 1}^N a_n^2 \right)^2 \left( \sum_{n = 1}^N b_n^4 \right) \left( \sum_{n = 1}^N c_n^4 \right) \]
\end{theorem}
\begin{proof}
    Applying the Cauchy-Schwarz inequality twice, we find
    %
    \[ \sum_{n = 1}^N a_nb_nc_n \leq \left( \sum a_n^2 \right)^{1/2} \left( \sum b_n^2 c_n^2 \right)^{1/2} \leq \left( \sum a_n^4 \right)^{1/4} \left( \sum a_n^4 \right)^{1/4} \left( \sum b_n^4 \right)^{1/4} \]
    %
    and then we take everything to the power of four.
\end{proof}

Alternatively, writing $a_nb_nc_n = (a_n^{1/2}b_n)(a_n^{1/2}c_n)$,

\begin{theorem}
    For any $a_n,b_n,c_n$,
    %
    \[ \left( \sum_{n = 1}^N a_nb_nc_n \right)^2 \leq \sum_{n = 1}^N a_n^2 \sum_{n = 1}^N b_n^2 \sum_{n = 1}^N c_n^2 \]
\end{theorem}
\begin{proof}
    If we decompose the three-way product as we wanted, we find
    %
    \begin{align*}
        \sum_{n = 1}^N a_nb_nc_n &\leq \sqrt{\sum a_nb_n^2} \sqrt{\sum a_nc_n^2}\\
        &\leq \sqrt[4]{\sum a_n^2} \sqrt[4]{\sum b_n^4} \sqrt[4]{\sum a_n^2} \sqrt[4]{\sum c_n^4}\\
        &= \sqrt{\sum a_n^2} \sqrt[4]{\sum b_n^4} \sqrt[4]{\sum c_n^4}
    \end{align*}
\end{proof}






\chapter{The Riemann Integral}

In calculus, you encountered the Riemann integral
%
\[ \int_a^b f(x)\; dx \]
%
which measures the uniform `infinitisimal sum' of the values of $f$ on the interval $[a,b]$. Here we define precisely what this integral is, and it's existence for a large family of funtions $f$. The idea, as with the Riemann integral, is to estimate the sum above and below by partitions of an interval. So let $[a,b]$ be an integral. A {\it partition} of $[a,b]$ is a sequence $P = \{ a = x_0 < x_1 < \dots < x_n = b \}$. If $f$ is a bounded, real-valued function on $[a,b]$, then associated with each partition $P$ we have upper and lower bounds $m_i = \inf \{ f(x): x_i \leq x \leq x_{i+1} \}$, and $M_i = \sup \{ f(x): x_i \leq x \leq x_{i+1} \}$. Using our intuitive idea about the Riemann integral, if we define
%
\[ L(f,P) = \sum m_i (x_{i+1} - x_i)\ \ \ \ \ U(f,P) = \sum M_i (x_{i+1} - x_i) \]
%
Then surely we have a bound $L(f,P) \leq \int_a^b f(x)\; dx \leq U(f,P)$. Thus, if we have
%
\[ \sup_P L(f,P) = \inf_P U(f,P) \]
%
Then we would have $\sup_P L(f,P) = \int_a^b f(x)\; dx = \inf_P U(f,P)$, and we can take this as our {\it definition} of the Riemann integral. Thus we say $f$ is Riemann integrable if the suprema of the lower bounds is equal to the infima of the upper bounds, and we define the common value as the integral of $f$.

More generally, in various we now consider an extension of the Riemann integral, which enables us to consider an `infinitisimal' weighted sum of values, rather than a uniform sum, which has many uses in analysis. Given a monotically increasing function $\alpha$ on an interval $[a,b]$, and another bounded function $f$, we consider the lower and upper bounds
%
\[ L(f,\alpha,P) = \sum m_i (\alpha(x_{i+1}) - \alpha(x_i))\ \ \ \ \ U(f,\alpha,P) = \sum M_i(\alpha(x_{i+1}) - \alpha(x_i)) \]
%
If $\sup L(f,\alpha,P) = \inf U(f,\alpha,P)$, then we say that $f$ is {\it Riemann-Stieltjes} integral with respect to $\alpha$, and the common value is the integral, denoted $\int f(x)\; d\alpha(x)$.

\begin{example}
    If $\alpha(x) = x$, then $L(f,\alpha,P) = L(f,\alpha)$ and $U(f,\alpha,P) = U(f,\alpha)$, and so $f$ is Riemann-Stieltjes integrable with respect to $\alpha$ if and only if it is Riemann integrable, and
    %
    \[ \int_a^b f(x)\; d\alpha(x) = \int_a^b f(x)\; dx \]
\end{example}

\chapter{Differentiation under Integrals}

Under what conditions can we prove that
%
\[ \frac{d}{dt} \int_a^b \varphi(x,t) dx = \int_a^b \frac{\partial \varphi}{\partial t}(x,t)\ dx \]
%
Surely, for a fixed value $s \neq t$,
%
\[ \frac{\int_a^b \varphi(x,t)\ dx - \varphi(x,s)\ dx}{t - s}\ dx = \int_a^b \frac{\varphi(x,t) - \varphi(x,s)}{t - s}\ dx \]
%
Let us fix $t$, and let $s$ vary closer and closer to $t$. Then for each $s$ and $x$, there is a number $u(s,x)$ between $t$ and $s$ such that
%
\[ \frac{\varphi(x,t) - \varphi(x,s)}{t - s} = \partial_t \varphi(x,u(s,x)) \]
%
If we assume that $\partial_t \varphi(x,s) \to \partial_t \varphi(x,t)$ uniformly over $a \leq x \leq b$ as $s \to t$, then we see that
%
\[ \lim_{s \to 0} \int_a^b \frac{\varphi(x,t) - \varphi(x,s)}{t - s}\ dx = \lim_{t \to 0} \int_a^b \partial_t \varphi(x,u(s,x))\ dx = \int_a^b \partial_t \varphi(x,t)\ dx- \]
%
In particular, this occurs if $\partial_t \varphi$ is continuous

\chapter{Differential Equations}

Here we describe some general techniques for solving differential equations. The goal is simple -- we are given an arbitrary vector field $v: \mathbf{R}^n \to \mathbf{R}^n$, and we must find the family of all curves $c$ for which $v \circ c = c'$ holds on the domain of the curve. More generally, we are given a function $F: \mathbf{R}^{n+1} \to \mathbf{R}^m$ (which can be seen as a system of $m$ equations), and we wish to find real-valued differentiable functions $f$ such that
%
\[ F(f(t), \dots, f^{(n)}(t), t) = 0 \]
%
Geometrically, we must find a curve $c: \mathbf{R} \to \mathbf{R}^{n+1}$ such that
%
\[ c_1'(t) = c_2\ \ \ c_2'(t) = c_3(t)\ \ \ \dots\ \ \ c_{n-1}'(t) = c_{n-1}(t) \]
\[ F(c_1(t), c_2(t), \dots, c_{n-1}(t), c_n'(t), c_{n+1}(t)) = 0 \]
\[ c_{n+1}'(t) = 1 \]
%
Thus we think of $c_{n+1}$ as the time variable, and the other variables as derivatives of another function. Normally we try and find such a solution such that $c_1(0) = x_0, c_2(0) = x'_0, \dots, c_n(0) = x^{(n)}_0, c_{n+1}(0) = t_0$. If $F$ is $C^1$ and everywhere non-singular, we can use the inverse function theorem to locally write
%
\[ c_{n+1}'(t) = f(c_1(t), \dots, c_n(t)) \]
%
around $(x_0, \dots, x^{(n)}_0, t_0)$, which gives us a local vector field around $(c_1, \dots, c_n)$ defining the same set of solutions as $F$, so if we assume $F$ is non-singular we have no real loss of generality in proofs of the local properties of solutions in assuming a vector field defines the differential equation. We will also use Leibnitz notation in certain cases, for instance writing
%
\[ f(x,y) dx + g(x,y) dy = 0 \]
%
For the simple differential equation corresponding to
%
\[ F(t,x,x') = f(t,x) + g(t,x) x' \]
%
which is non-singular provided that for no $(t,x,x')$ we have
%
\[ \partial_t f(t,x,x') = \partial_t g(t,x) x' = \partial_x f(t,x) = \partial_x g(t,x) x' = g(t,x) = 0 \]
%
simultanously hold.

The most basic type of differential equation is of the form $f'(x) = g(x)$, where $g$ is continuous. The solution family is then found by applying the fundamental theorem of calculus, writing
%
\[ f(x) = f(0) + \int_0^x g(t) \]
%
and if we let $f(0)$ over all real numbers, we obtain all solutions to the equation. In general, if we find a single function with $f'(x) = g(x)$, then the set of all solutions to the equation is of the form $f(x) + A$.

More advanced techniques are required for more general formulas. A differential equation is separable if it can be rearranged to the form
%
\[ h(y) dy + f(x) dx = 0 \]
%
Any solution to this equation is also a solution to the equation
%
\[ h(f(x)) f'(x) = g(x) \]
%
Applying the chain rule, in reverse, we find
%
\[ h(f(0)) + \int_{f(0)}^{f(x)} h(t) = \int_0^x g(t) \]
%
Which hopefully simplifies the equation to a more simpler form. If we write the equation in Leibnitz notation
%
\[ \frac{dy}{dx} = g(x)/f(y) \]
%
Then the strategy of separability can be obtained by (formally) writing
%
\[ f(y) dy = g(x) dx \]
%
And then integrating
%
\[ \int f(y) dy = \int g(x) dx \]

\begin{example}
    Consider the equation
    %
    \[ e^{f(x)} f'(x) = x (x-2) \]
    %
    Integrating, we find
    %
    \[ e^{f(x)} - e^{f(0)} = \int_{f(0)}^{f(x)} e^t = \int_0^x t(t-2) = x^3/3 - x^2 \]
    %
    Hence
    %
    \[ f(x) = \log(e^{f(0)} + x^3/3 - x^2) \]
\end{example}

An equation $f: \mathbf{R}^n \to \mathbf{R}$ is {\bf homogenous of degree $n$} if $f(tv) = t^n f(v)$. A differential equation
%
\[ M(x,y) dy + N(x,y) dx = 0 \]
%
is homogenous if $M$ and $N$ are homogenous of the same order $n$. If we let $y = xv$, then $dy = vdx + xdv$, and so
%
\[ x M(x,xv) dv + [v M(x,xv) + N(x,y)] dx = 0 \]
%
Now
%
\[ x M(x,xv) =  x^{n+1} M(1,v) \]
\[ v M(x,xv) + N(x,xv) = x^n[vM(1,v) + N(1,v)] \]
%
which separates the variables of $N$, so we can write
%
\[ \frac{M(1,v)}{vM(1,v) + N(1,v)} dv + \frac{dx}{x} \]
%
which is a separable equation.

\begin{example}
    \[ (x^2 + y^2) dx - 2xy dy = 0 \]
    %
    Both coefficients are homogenous of degree 2, so if we let $y = xv$, we find
    %
    \[ x^2[1 - v^2] dx - 2x^3v dv \]
    %
    Which can be rewritten
    %
    \[ 2 \frac{v dv}{1-v^2} = \frac{dx}{x} \]
    %
    Hence
    %
    \[ \log\left( \frac{1-v_0^2}{1 - v^2} \right) = \log(x/x_0) \]
    %
    \[ v = \pm \sqrt{ 1 - \frac{x_0 (1 - v_0^2)}{x} } \]
    \[ y = \pm x \sqrt{ 1 - \frac{1 - y_0^2}{x x_0} } \]
\end{example}

The differential of a function $f: \mathbf{R}^2 \to \mathbf{R}$ is
%
\[ df = \frac{\partial f}{\partial x} dx + \frac{\partial f}{\partial y} dy \]
%
Any differential $a dx + b dy$ can be written in this form, as long as the domain is simply connected and $\partial_x b = \partial_y a$. Then if $f(x,c(x)) = A$ for all $a$, we find
%
\[ a + b c' = \partial_x f + \partial_y f c'(x) = 0 \]
%
So $c$ satisfies the differential equation.

\begin{example}
    The differential equation
    %
    \[ (1 - 2xy) dx + (4y^3 - x^2) dy = 0 \]
    %
    can be expressed $df = 0$, where
    %
    \[ f(x,y) = x - x^2y + y^4 \]
    %
    So the solutions the equation satisfy the algebraic equation
    %
    \[ x - x^2 c(x) + c^4(x) = A \]
\end{example}

If a differential equation
%
\[ a dx + b dy \]
%
is not exact, we may be able to find $f$ such that $f a dx + f b dy$ is exact. $f$ is known as an integrating factor for the equation.

\section{Linear Differential Equations}

Consider the differential equations
%
\[ f^{(n)} + a_{n-1}f^{(n-1)} + \dots + a_1f' + a_0f = 0 \]
%
where $a_0, \dots, a_{n-1} \in \mathbf{C}$ are constant coefficients. The nice fact about this equation is that the equation is linear. If we let $D$ denote the linear operator bringing $f$ to it's derivative, then the equation reads
%
\[ (D^n + a_{n-1}D^{n-1} + \dots + a_1D + a_0)(f) = 0 \]
%
so we are trying to find the nullspace of a {\it linear differential operator} on the space of differentiable functions. More specifically, we are trying to find the nullspace of a nonzero polynomial applied to a differential operator $D$ with constant coefficients. We can then apply the general theory of linear algebra to the problem.

Because the operator above is linear, the solution space forms a {\it subspace} of the set of all differentiable functions. The theorem of uniqueness and existence for ordinary differentiable equations tells us that for any $(x_0,y_0, \dots, y_{n-1}) \in \mathbf{R}^n$, there is a unique function $f$ solving the differential equation above locally with
%
\[ f(x_0) = y_0\ \ \ f'(x_0) = y_1\ \ \ \dots\ \ \ f^{(n-1)}(x_0) = y_{n-1} \]
%
This can be expressed as the fact that the linear restriction map $f \mapsto (f(x_0), f'(x_0), \dots, f^{(n-1)}(x_0))$ is an isomorphism between the solution space of the differential equation and $\mathbf{C}^n$. The solution space is therefore an {\it $n$-dimensional} subspace of the set of all differentiable functions.

The standard way to understand a linear operator is to understand it's eigenspaces. We begin by understanding the differentiation operator $D$. An eigenvector for $D$ with eigenvalue $\alpha$ is a differentiable function $f$ such that $f' = \alpha f$. If $f$ is such a function, then
%
\[ (fe^{-\alpha x})' = f'e^{-\alpha x} - \alpha f e^{-\alpha x} = 0 \]
%
so $f = Ce^{\alpha f}$ for some constant $C$; we have proved the eigenspace is one dimensional. The delicate question of how well these eigenvectors decompose the space of differentiable functions is part of the subject of Fourier analysis. However, we do not need such heavy functional analysis to solve linear ordinary differential equations.

Now consider the polynomial $P(X) = X^n + a_{n-1}X^{n-1} + \dots + a_1X + a_0$ formed from the coefficients of the differential equation. For any $\lambda \in \mathbf{C}$, we find
%
\[ P(D)(e^{\lambda x}) = \lambda^n e^{\lambda x} + a_{n-1} \lambda^{n-1} e^{\lambda x} + \dots + a_0 e^{\lambda x} = P(\lambda) e^{\lambda x} \]
%
It follows that $e^{\lambda x}$ is a solution to the differential equation if and only if $P(\lambda) = 0$. Since $D(x^m e^{\lambda x}) = (\lambda x^m + mx^{m-1})e^{\lambda x} = (\lambda + D)(x^m) e^{\lambda x}$, we find
%
\[ P(D)(x^m e^{\lambda x}) = P(\lambda + D)(x^m) e^{\lambda x} \]
%
and since
%
\[ (\lambda + D)^k(x^m) = \sum_n {k \choose n} \lambda^{k-n} \frac{m!}{(m-n)!} x^{m-n} = \sum_n {m \choose n} \frac{k!}{(k-n)!} \lambda^{k-n} x^{m-n} \]
%
we obtain that $P(\lambda + D)(x^m) = \sum {m \choose n} P^{(n)}(\lambda) x^{m-n}$, so
%
\[ P(D)(x^m e^{\lambda x}) = \sum {m \choose n} P^{(n)}(\lambda) x^{m-n} e^{\lambda x} \]
%
So if $\lambda$ is a root of $P$ of multiplicity of $m$, then $e^{\lambda x}, xe^{\lambda x}, \dots, x^{m-1}e^{\lambda x}$ are all solutions to the differential equation. Now the family $\{ x^ne^{\lambda x} : n \in \mathbf{N}, \lambda \in \mathbf{C} \}$ is linearly independant, because we first have a direct sum decomposition of the span of $x^ne^{\lambda x}$ into generalized eigenspaces for $D$, which reduces the problem to verifying that the $e^{\lambda x}, xe^{\lambda x}, x^2e^{\lambda x}, \dots$ are linearly independant for a fixed $\lambda$. But this follows because if $\sum_n a_nx^ne^{\lambda x} = 0$, then $P(x)e^{\lambda x} = 0$ for $P(x) = \sum_n a_nx^n$, and since $P(x)$ and $e^{\lambda x}$ are both analytic functions, and $e^{\lambda x}$ is a nonzero function, then we conclude $P(x) = 0$ for all $x$.

Given the linear differential operator
%
\[ D^n + a_{n-1}D^{n-1} + \dots + a_1D + a_0 \]
%
We consider the polynomial $P(\lambda) = \lambda^n + a_{n-1}\lambda^{n-1} + \dots + a_1\lambda + a_0$, and decompose it into linear factors with multiplicities, so
%
\[ P(\lambda) = (\lambda - \lambda_1)^{n_1} \dots (\lambda - \lambda_m)^{n_m} \]
%
where $\sum n_i = \deg(P) = n$. It follows that if each $i$, $e^{\lambda_i x}$, $xe^{\lambda_i x}$, $x^{n_i - 1}e^{\lambda_i x}$ are all solutions to the differential equation, giving us $n$ different linearly independant solutions. It follows from this that these solutions span the entire space of solutions, and therefore every solution can be expressed as a linear combination of solutions. This completes the analysis.

\section{Uniqueness of Differential Equations}

\begin{lemma}
    If $v$ and $w$ are one dimensional vector fields with $v < w$ everywhere, and we have two integral curves $x$ and $y$ with $x_0 = y_0$ and $\dot{x} = v(x)$, $\dot{y} = w(y)$, then $x_t \leq y_t$ for $t \geq 0$, and $x_t \geq y_t$ for $t \leq 0$.
\end{lemma}
\begin{proof}
    We prove the theorem for $t \geq 0$, and leave the case $t \leq 0$ to the reader. Let $T$ be the supremum of all $t$ such that $x \leq y$ on $[0,t]$. If $T < \infty$, then by continuity $x \leq y$ on $[0,T]$, but there are arbitrarily small $\varepsilon > 0$ such that $x(T+\varepsilon) > y(T+\varepsilon)$ and $x_T = y_T$. This implies that
    %
    \[ v(x_T) - w(y_T) = \dot{x}_T - \dot{y}_T = \lim_{\varepsilon \to 0^+} \frac{x(T+\varepsilon) - y(T+\varepsilon)}{\varepsilon} \geq 0 \]
    %
    which is impossible by assumption.
\end{proof}

We would like to strengthen this theorem such that if $v \leq w$, then $x_t \leq y_t$ for $t \geq 0$ and $x_t \geq y_t$ for $t \leq 0$, but this cannot be true without additional regularity assumptions on $v$ and $w$, because if $v = w$, the theorem would easily imply the uniqueness of solutions to differential equations.

\begin{theorem}
    If $v$ is a differentiable vector field with $v(0) = 0$, then no curve can approach $0$ in finite time.
\end{theorem}
\begin{proof}
    Let $c$ be a curve, and assume without loss of generality that $c_0 > 0$. We show that there is a value $A$ such that $c_t \geq c_0e^{-A|t|}$, which shows in particular that $c_t \neq 0$ for all $t$. To obtain the value $A$, find a value $A$ such that $|v(x)| < A|x|$ for $x \neq 0$ in $[-1,1]$. This is possible because $v$ is Differentiable at $0$, hence locally Lipschitz there. This implies that for $x > 0$, $-Ax < v(x) < Ax$. We then obtain the inequality by applying the last lemma to the two integral curves $c_0e^{At}$ and $c_0e^{-At}$ for the vector fields $Ax$ and $-Ax$.-
\end{proof}

Though we don't really need this theorem, we can apply what we know to prove the following:

\begin{theorem}
    Let $v \leq w$ be differentiable functions of a single variable. If $\dot{x} = v(x)$ and $\dot{y} = w(y)$, and $x_0 = y_0$, then $x_t \leq y_t$ for all $t \geq 0$, and $x_t \geq y_t$ for all $t \leq 0$.
\end{theorem}
\begin{proof}
    Assume that $v$ and $w$ don't vanish anywhere. Note that $dx = vdx$ and $dy = vdy$. Let $x_0 = y_0 = a$, and suppose that $x_{t_0} = y_{t_1} = b$. If $b \geq a$, then
    %
    \[ t_0 = \int_a^b \frac{dx}{v(x)} \geq \int_a^b \frac{dy}{w(y)} = t_1 \]
    %
    we can only have an equality here if $v = w$ almost everywhere on $[a,b]$, otherwise $t_0 > t_1$ and we conclude that it takes $x$ longer to reach $b$ than it takes $y$. If $b \leq a$, then we conclude $t_0 < t_1$, unless $v = w$ on $[b,a]$. If $v$ and $w$ don't vanish anywhere, this proves the theorem in general. But then if $b \neq a$, then $v$ and $w$ cannot vanish between $a$ and $b$, because it takes infinite time to pass this point.
\end{proof}

Note that in general, this lemma cannot be used to prove the uniqueness of integral curves in any dimension, because in this case it is possible for us to `take a shortcut' in two or more dimensions. We leave this to the reader to formulate. However, we can use the problem to obtain one dimensional time dependent uniqueness, since the shortcut forming technique does not work if we are forced to proceed along a dimension at a uniform rate.

\begin{lemma}
    If $v(x,t)$ and $w(x,t)$ are one dimensional vector fields with $v < w$ everywhere, and we have two integral curves $x$ and $y$ with $x_0 = y_0$ and $\dot{x} = v(x,t)$, $\dot{y} = w(y,t)$, then $x_t \leq y_t$ for $t \geq 0$, and $x_t \geq y_t$ for $t \leq 0$.
\end{lemma}

\begin{theorem}
    The differential equation $\dot{x} = v(x,t)$ has unique solutions.
\end{theorem}
\begin{proof}
    Without loss of generality, consider initial conditions $x_0 = t_0 = 0$. Let $c$ be a curve with $\dot{c} = v(c,t)$, and consider the function $y(x,t) = x - c(t)$. Then $(y,t)$ form a coordinate system with $dy = dx - c'(t)dt$. In particular, the tangents to the curve $c$ satisfy $dx = v(x,t)dt$ and so $dy = [v(x,t) - c'(t)]dt = w(y,t)dt$. We have thus reduced our proof of uniqueness to a function $dy = w(y,t)dt$ with $w(0,t) = 0$ for all $t$.

    Using Taylor's theorem, if $\partial_y w(0,0) = A$, and $w$ is $C^2$, we can write
    %
    \[ w(y,t) = Ay + t \int_0^1 (\partial_t w)(uy,ut)\ du + y \int_0^1 (\partial_y w)(uy,ut)\ du = Ay + tf(y,t) + yg(y,t) \]
    %
    Since $\partial_y w$ is continuous, it is bounded by a constant if we assume the values of $y$ and $t$ are bounded, and so on this interval $|g(y,t)| = O(1)$. Next, the function $F(y,t) = f(y,t)/y$ can be extended to a continuous function on the $x$ axis, because, since $w$ is $C_2$, we can conclude
    %
    \[ \lim_{(y,t) \to (0,t_0)} \frac{f(y,t)}{y} = \frac{\partial f}{\partial y}(0,t) = \int_0^1 \partial_{yt} w(0,ut_0)\ du \]
    %
    It follows that the function $F(y) = f(y,t)/y$ can be extended to a continuous function on the $y$ axis, and in particular in every bounded neighbourhood of the origin $|f(y,t)| = O(|y|)$. We conclude that for $t,y \neq 0$, there are constants such that $|w(y,t)| < A|y| + B|ty|$. Solving this equation, it follows that for any curve $y$ with initial conditions $(y_1,t_1)$, where $y_1, t_1 > 0$,
    %
    \[ -Ay - Byt < w(y,t) < Ay + Byt \]
    %
    and the resulting integral curves of the upper and lower bounds give a bound of the form
    %
    \[ y_t > y_1e^{A|t - t_1| -(B/2)|t-t_1|^2} = \Omega(e^{-(B/2)|t-t_1|^2}) \]
    %
    and so we cannot have $y_0 = 0$, though we can now approach slightly faster than inverse exponentially.
\end{proof}

\chapter{The Exponential Function}

We now provide a brief interlude on the subject on the powers of a number. For any complex number $x$, we may consider its powers $x^n$, for $n > 0$. Two incredibly useful formulas hold
%
\[ x^{n+m} = x^n x^m\ \ \ \ \ (x^n)^m = x^{nm} \]
%
which can be used to reduce arbitrary calculations of powers to questions about the addition and multiplication of integers. In generalizing the power function to more general numeric systems, we would like these same formulas hold. First, we would like to define $x^0$. Since we would then have $x^n = x^{n + 0} = x^n x^0$, we are forced to define $x^0 = 1$, for all $x$. Since we would like to have
%
\[ x^{-n} x^n = x^{n-n} = x^0 = 1 \]
%
we are then forced to define $x^{-n}$ to be the unique multiplicative inverse of $x^n$. These notions hold in any field, and do not require any analysis to define. A more careful analysis is required for generalization to rational numbers. Since
%
\[ (x^n)^q = x^{nq} = x^p \]
%
we must define $x^n$ to be a $q$'th root of $x^p$. The problem here is that $q$'th roots are not unique, and for general complex numbers there is no particular way to make a standard choice. For positive reals, however, we can define $x^n$ to be the unique {\it positive} $q$'th root of $x^p$. One verifies that this choice is well defined, for if $a/b = p/q$, then $aq = bp$, so that if $y^b = x^a$, then $y^{aq} = y^{bp} = x^{ap}$. By taking unique positive roots, we conclude $y^q = x^p$. If we want the powers of a real number to be real, we are effectively forced into this path, because then $x^{p/q} = (x^{p/2q})^2 \geq 0$.

Really, we have not applied any real analysis yet, other than the continuity and monotonicity of $n$'th roots. Without more analysis, our attempt to generalize the power function stops. Stated more elegantly, we desire, for each $x > 0$, to find a real-valued continuous function $f$, for which
%
\[ f(a + b) = f(a)f(b)\ \ \ \ \ f(1) = x \]
%
$f$ must necessarily be positive, for $f(x) = f(x/2)^2 \geq 0$. This shows why our programme fails for non-positive numbers. If $g$ also satisfies these formulae, then $g(1) = f(a)$, then $g(m/n) = f(a)^{m/n} = f(am/n)$, and by continuity, letting $m/n \to b$ for any $b$, we find $g(b) = f(ab)$, so that the product formula automatically holds.

One automatically verifies that $f$ must necessarily be unique in the formula, for if $f(1) = g(1)$, and $f$ and $g$ satisfy the formulae, then one verifies that $f$ and $g$ agree on all rational values, implying by continuity that $f = g$ everywhere. Thus we need only find one formula that works for each positive $x$, and we shall define $x^t = f(t)$.

For any complex $z$, we define the exponential function
%
\[ \exp(z) = \sum_{k = 0}^\infty \frac{z^k}{k!} \]
%
Since
%
\[ \sqrt[n]{\frac{|z|^n}{n!}} = \frac{|z|}{\sqrt[n]{n!}} \leq \frac{|z|}{\sqrt{n}} \to 0 \]
%
the series converges absolutely for all $z$, and therefore converges uniformly on any bounded subset. This implies the exponential function is continuous. Since
%
\[ \frac{d}{dz} \left( \sum_{k = 0}^N \frac{z^k}{k!} \right) = \sum_{k = 0}^{N-1} \frac{z^k}{k!} \]
%
which converges absolutely to the exponential function, we have the incredibly useful formula $\exp'(z) = \exp(z)$. What's more, the absolute convergence of the series implies the manipulation
%
\[ \sum_{k = 0}^\infty \sum_{j = 0}^\infty \frac{z^k w^j}{k! j!} = \sum_{n = 0}^\infty \sum_{k = 0}^n \frac{z^k w^{n-k}}{k! (n-k)!} = \sum_{n = 0}^\infty \frac{(z + w)^n}{n!} \]
%
so $\exp(z+w) = \exp(z) \exp(w)$. Thus we have found a function $f$, for $f(1) = \exp(0)$, which we define to be the number $e$.

\begin{theorem}
    \ 
    \begin{enumerate}
        \item[(a)] $\exp(z) \neq z$ for any complex $z$.
        \item[(b)] $\exp(t) > 0$ for all $t \in \mathbf{R}$.
        \item[(c)] $\exp$ is monotonically increasing on the real axis.
        \item[(d)] $\exp(t) \to \infty$ as $t \to \infty$, and $\exp(t) \to 0$ for $t \to -\infty$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    (a) is proved by noticing
    %
    \[ \exp(z) \exp(-z) = \exp(0) = 1 \]
    %
    What's more, $\exp(\mathbf{R})$ is a connected subset of $\mathbf{R}$ containing $1$ but not $0$, so $\exp(\mathbf{R}) \subset (0,\infty)$, proving (b). Then (c) is shown, since $\exp' = \exp > 0$. We have the very weak estimate $\exp(t) > 1 + t$ for $t > 0$, so that $\exp(t) \to \infty$ as $t \to \infty$ and therefore (since $\exp(-t) = \exp(t)^{-1}$) that $\exp(t) \to 0$ as $t \to - \infty$. We conclude that the exponential function provides a homeomorphism between $\mathbf{R}$ and $(0,\infty)$.
\end{proof}

This finalizes are discussion of powers, for if $x$ is any positive number, then $x = e^y$ for some unique $y \in \mathbf{R}$ (which we define to be $\log x$, called the logarithm of $x$), and the function $f(a) = e^{ay}$ satisfies the properties which define an extension of the power function. For negative $x$, there is no natural choice of $y$ for which $e^y = x$, so that general powers cannot be defined. Nonetheless, we will define $e^z = \exp(z)$, for any complex number $z$, and this is a continuous function satisfying the desired properties.

Let us switch to studying the miraculous properties of the function $e^{it}$, for $t \in \mathbf{R}$.

\begin{theorem}\ 
    \begin{enumerate}
        \item[(a)] $|e^{it}| = 1$ for all $t$.
        \item[(b)] There exists a number $\pi$ such that $e^{i(\pi/2)} = i$, and any number satisfying $e^{x} = 1$ is an integer multiple of $2 \pi i$.
        \item[(c)] The map $t \mapsto e^{it}$ is surjective onto the set $\mathbf{T}$ of complex numbers with absolute value 1, and the exponential map is surjective onto $\mathbf{C} - \{ 0 \}$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    Note that
    %
    \[ \overline{e^{it}} = \sum_{k = 0}^\infty \frac{(-it)^k}{k!} = e^{-it} \]
    %
    so that
    %
    \[ |e^{it}|^2 = e^{it} \overline{e^{it}} = e^{it} e^{-it} = 1 \]
    %
    which is enough to prove (a).

    Finding $\pi$ requires a careful exploration of the power series of $e^{it}$. Notice it is alternating in each axis of the plane, which enables us to easily get precise estimates on the values of the function. Take the projections of the power series of the exponential onto the real and imaginary axis, obtaining the expansions
%
\[ \Re(e^{ix}) = \sum_{k = 0}^\infty (-1)^k \frac{x^{2k}}{(2k)!}\ \ \ \ \ \Im(e^{ix}) = x \sum_{k = 0}^\infty (-1)^k \frac{x^{2k}}{(2k+1)!} \]
%
For $x \in (0,2]$ the absolute values of the coeffients begin to decrease for $k \geq 1$, so we obtain the bounds
%
\[ \Re(e^{ix}) \leq 1 - x^2/2 + x^4/24\ \ \ \ \ \ \ \ x(1 - x^2/6) \leq \Im(e^{ix}) \]
%
This implies that $0 < \Im(e^{ix})$, and $\Re(e^{2i}) \leq 1 - 2 + 2/3 = -1/3$, so that $e^{iy} = i$ for some $y \in (0,2)$. There is only one such solution in this region, because here
%
\[ \frac{d}{dx} \Re(e^{ix}) = - \Im(e^{ix}) < 0 \]
%
We define $\pi$ to be the smallest positive number for which $e^{i \pi} = -1$. It is easy to see $\pi = 2y$, because if $e^{iz} = -1$, then $e^{i(z/2)} = \pm i$, and if $0 \leq z/2 \leq y$, we must in fact have $e^{i(z/2)} = i$, hence $z/2 = y$. It is also simple to show that $e^{ix} = 1$ holds if and only if $x$ is a multiple of $2 \pi i$. First, the set of all such $x$ must lie in $i \mathbf{R}$, because
%
\[ |e^{a + ib}| = e^a |e^{ib}| = e^a \]
%
and so $|e^{a+ib}| = 1$ if and only if $e^a = 1$, so $a = 0$. Secondly, note that the set of all $x$ for which $e^{ix} = 1$ must form a discrete subgroup $G$ of $\mathbf{R}$, because there is a neighbourhood of the origin such that $e^{ix} \neq 1$ unless $x = 0$. This implies that $G = y \mathbf{Z}$ for the smallest positive $y \mathbf{R}$ satisfying $e^{iy} = 1$. We must have $e^{i(y/2)} = -1$, hence $y/2 \geq \pi$, and so $y \geq 2 \pi$, and we must in fact have equality since $e^{2 \pi i} = 1$.

Now we prove that the map $e^{ix}$ is surjective onto $S^1$. First, consider $w = x + iy$, with $x,y \geq 0$. Then $x \leq 1$, and because $\Re(e^0) = 1$, $\Re(e^{i\pi/2}) = 0$, there is some $t \in (0,\pi/2)$ for which $\Re(e^{it}) = x$. Since $\Im(e^{it}) \geq 0$, we have
%
\[ \Im(e^{it}) = \sqrt{1 - \Re(e^{it})^2} = \sqrt{1 - x^2} = y \]
%
so $e^{it} = w$. If $w = x + iy$ with $x \geq 0$, $y < 0$, then $iw = ix - y$, and $x, -y \geq 0$, so there is $t$ for which $e^{it} = iw$, and then $e^{i(t-\pi/2)} = w$. The other cases can be shown by employing the same technique. Since any non-zero complex number $z$ can be written $aw$, where $a > 0$ and $|w| = 1$, there are $t,t'$ such that $a = e^t$, $w = e^{it'}$, and then $z = e^{t + it'}$.
\end{proof}

Thus $\exp$ is a surjective homomorphism from the additive group of complex numbers to the multiplicative group, with kernel $2\pi i \mathbf{Z}$. If we look at the topological structure of $\mathbf{C}/2 \pi i \mathbf{Z}$, we see the shape forms an infinitely long cylinder, which is exactly the way we think of complex multiplication -- the imaginary part of a number forms a loop around the origin, and the real part scales the number radially outward from the origin, like a cylinder.

\chapter{Convex Functions}

Convex functions occur abound in mathematics, and are normaly the `feasible' objects of study in hard analysis -- where we want to obtain explicit inequalities rather than `limit' arguments that give no guarantee on convergence rates. Geometrically, a function is convex if the graph of the function lies below the straight line of any two points on the graph. Analytically, we can express a convex function $f$ as one for which
%
\[ f((1-t)a + tb) < (1-t)f(a) + tf(b) \]
%
for $0 < t < 1$.

Convex functions are fairly pleasant objects to study, for they may be upper bounds on an interval $(a,b)$ once we know the definition of the function at $a$ and $b$. The contrapositive also says that if we can lower bound a convex function at a point $x$ between $a$ and $b$, then we can obtain lower bounds at $b$ once we know the value of the function at $a$. Here's an example application.

\begin{theorem}
    A convex function defined on an open interval is continuous.
\end{theorem}
\begin{proof}
    Let $f:(a,b) \to \mathbf{R}$ be convex, and fix $a < c < b$. Since $f$ is upper bound by a line passing through $c$, $f$ is upper semicontinuous. But for sufficiently small $t$, we find
    %
    \[ f(c) \leq \frac{f(c + t) + f(c-t)}{2} \]
    %
    If $f(c+t) \leq f(c) - \varepsilon$, then $f(c-t) \geq f(c) + \varepsilon$, or else
    %
    \[ f(c) < \frac{f(c) - \varepsilon + f(c) + \varepsilon}{2} = f(c) \]
    %
    Since we can choose $t$ small enough that $f(c-t) < f(c) + \varepsilon$ for any $\varepsilon$, we must eventually have $f(c+t) > f(c) - \varepsilon$, so that $f$ is also lower semicontinuous.
\end{proof}

The definition of convexity can be expressed as the fact that for $a < b$,
%
\[ f(x) < \frac{f(b) - f(a)}{b - a} (x - a) + f(a) \]
%
The two definitions are realized equivalent after a coordinate change, for if $x = (1-t)a + tb$, then $t = (x - a)/(b-a)$. If we rearrange this equation, we find
%
\[ \frac{f(x) - f(a)}{x - a} < \frac{f(b) - f(a)}{b - a} \]
%
So that the approximation of the tangent line at $a$ is a decreasing function of the approximation point. In this form, the next theorem is essentially trivial.

\begin{theorem}
    If $f:(a,b) \to \mathbf{R}$ is convex, then $f'$ is increasing where defined.
\end{theorem}
\begin{proof}
    Let $x < y < z$, and suppose $f$ is differentiable at $x$ and $z$. Applying the last definition of inequality twice, we find that
    %
    \[ f'(x) < \frac{f(y) - f(x)}{y - x} < \frac{f(z) - f(x)}{z - x} < \frac{f(z) - f(y)}{z - y} < f'(z) \]
    %
    so $f'$ is increasing.
\end{proof}

Thus a convex differentiable function is either increasing, decreasing, or decreasing to the left of some point and increasing to the right.

It follows that if $f$ is twice differentiable and convex, then $f'' > 0$. The converse is a fact implicitly used in first year calculus courses, but intuitively obvious. It isn't too hard to prove either, with the power of analysis at our hand. Consider the next partial result, which will give us the full result when we apply a Rolle's theorem like argument (this kind of method can always be applied when we are discussing inequalities involving tangent lines, since these inequalities are easy to rotate so that the tangent lines are straight).

\begin{theorem}
    Suppose $f$ is differentiable, and $f'$ is increasing. If $a < b$, and $f(a) = f(b)$, then $f(x) < f(a) = f(b)$ for $a < x < b$.
\end{theorem}
\begin{proof}
    We may apply the mean value theorem to find $a < y < b$ for which $f'(y) = 0$. Since $f'$ is increasing, $f' < 0$ on $(a,y)$, and $f' > 0$ on $(y,b)$. But this essentially proves the inequality, because it implies that $f$ decreases to a minimum point, and then increases. The maximum point of $f$ therefore occurs on the boundary.
\end{proof}

\begin{corollary}
    If $f'$ is increasing, then $f$ is convex.
\end{corollary}
\begin{proof}
    Fix $a < x < b$, and consider the function
    %
    \[ g(x) = f(x) - \frac{f(b) - f(a)}{b - a} (x - a) \]
    %
    Then $g'$ is just $f'$ plus a constant, so $g'$ is also increasing. Since $g(a) = f(a) = g(b)$, the last theorem implies that
    %
    \[ f(x) - \frac{f(b) - f(a)}{b - a}(x - a) = g(x) < f(a) \]
    %
    which we can rewrite as
    %
    \[ \frac{f(x) - f(a)}{x - a} < \frac{f(b) - f(a)}{b-a} \]
    %
    so we have convexity!
\end{proof}

This makes it easy to find a plethora of convex functions. Since
%
\[ \frac{d^2x^n}{dx^2} = n(n-1)x^{n-2} \]
%
we find the map $f(x) = x^n$ is convex on $[0,\infty)$, and on $(-\infty,\infty)$ if $n$ is positive and even. The map $f(x) = e^x$ is positive, and since $f''(x) = f(x) > 0$, the exponential function is convex. If we look at the graphs of these functions, we see that the tangent line lower bounds the functions on the whole domain. This is not hard to prove in general.

\begin{theorem}
    If $f$ is convex on $(a,b)$ and differentiable at $y$, then for $x \neq y$
    %
    \[ f(x) > f(y) + f'(y)(x - y) \]
\end{theorem}
\begin{proof}
    For $x < y < z$,
    %
    \[ \frac{f(x) - f(y)}{x - y} < \frac{f(z) - f(y)}{z - y} \]
    %
    Letting $x \to y$, we find
    %
    \[ f'(y) < \frac{f(z) - f(y)}{z - y} \]
    %
    so $f(y) + f'(y)(z - y) < f(z)$, and by letting $z \to y$, we find
    %
    \[ \frac{f(x) - f(y)}{x - y} < f'(y) \]
    %
    so
    %
    \[ f(x) > f(y) + f'(y)(x - y) \]
    %
    and we have obtained lower bounds both to the left and right of $y$.
\end{proof}

The converse is also true

\begin{theorem}
    If the graph of a differentiable function lies above the tangent line at a point, then the function is convex.
\end{theorem}
\begin{proof}
    s
\end{proof}

We can also consider {\bf concave functions}, which are lower bounded by the lines between points. Any concave function can be obtained from a convex function by taking the negative, and conversely any convex function can be obtained from a concave function, so they are essentially the same class of functions, and by negating $f$ in the theorems above we obtain a family of results for concave functions. For instance, the graph of a concave function lies below any tangent line, twice differentiable concave functions are precisely those for which $f'' < 0$, and $f'$ is decreasing where defined even when the function is not twice differentiable.

If $f$ is a twice continuously differentiable function, then at almost every point on the domain it is either locally convex or locally concave. If $f''(x) > 0$, then $f'' > 0$ in a neighbourhood around $x$, so $f$ is locally convex around $x$, and if $f''(x) < 0$, then $f'' < 0$ in a neighbourhood around $x$, so $f$ is locally concave. The only problems are inflection points, when $f''(x) = 0$, but these can normally be locally analyzed (they are normally where the graph twists from convex to concave, or vice versa). For instance, $x^3$ is convex on $(0,\infty)$, concave on $(-\infty,0)$, and has an inflection point at $0$.

\begin{theorem}
    If the tangent lines of a differentiable function $f:(a,b) \to \mathbf{R}$ intersect the graph of $f$ only once, then $f$ is either convex or concave.
\end{theorem}
\begin{proof}
    We first prove that no straight line can intersect $f$ more than three times. Indeed, if a straight line intersection $a < b < c$, then we would find
    %
    \[ \frac{f(b) - f(a)}{b - a} = \frac{f(c) - f(a)}{c - a} \]
    %
    If we consider
    %
    \[ g(x) = \frac{f(x) - f(a)}{x - a} \]
    %
    Then $g(b) = g(c)$, so there is $b < y < c$ with $g'(y) = 0$, so
    %
    \[ f'(y)(y - a) = [f(y) - f(a)] \]
    %
    and this can be rephrased as $f(y) + f'(y)(a - y) = f(a)$, so that the tangent line at $y$ intersect the graph of $f$ at more than one place.

    Now we consider the function
    %
    \[ g(x,y,t) = \frac{f((1-t)x + ty) - f(x)}{ty - tx} - \frac{f(y) - f(x)}{y - x} \]
    %
    defined for $x < y$, $0 < t < 1$. By the argument above, $g(x,y,t) \neq 0$ on the domain, so by continuity $g(x,y,t) > 0$ for all values, or $g(x,y,t) < 0$, so $f$ is either convex or concave.
\end{proof}

Convexity behaves slightly well under composition, but not as well as we'd like it too.

\begin{theorem}
    If $f$ is increasing and convex, and $g$ is also convex, then $f \circ g$ is convex.
\end{theorem}
\begin{proof}
    We prove by calculation, finding that $g(tx + (1-t)y) \leq tg(x) + (1-t)g(y)$, so, since $f$ is increasing,
    %
    \[ (f \circ g)(tx + (1 - t)y) \leq f(tg(x) + (1 - t)g(y)) \leq t(f \circ g)(x) + (1-t)(f \circ g)(y) \]
    %
    If $f$ is not increasing, then we cannot guarantee $f \circ g$ is convex. Indeed, if $f = e^{-x}$, then $f'' = e^{-x} > 0$, but $g = f \circ f = e^{e^{-x}}$, so $g' = -e^{-x}e^{e^{-x}}$, and
    %
    \[ g'' = e^{-x}e^{e^{-x}} + e^{-2x} e^{e^{-x}} = e^{-x}e^{e^{-x}}(1 + e^{-x}) \]
    %
    which is not necessarily positive for $x$ small enough.
\end{proof}

The main inequality for convex functions is very important in almost any part of hard analysis. First, consider $a_1, \dots, a_n > 0$ with $\sum a_i = 1$. Given $x_1 \leq x_2 \leq \dots \leq x_n$, which aren't all equal, we have the trivial interpolation inequality $x_1 < \sum_{i = 1}^n a_i x_i < x_n$. If $t = \sum_{i = 1}^{n-1} a_i = 1 - a_n$, then we also have
%
\[ t x_1 < \sum_{i = 1}^{n-1} a_i x_i < t x_n \]
%
obtained by subtracting $a_n x_n$ from the inequality above. With these trivial observations, we can prove a nontrivial inequality.

\begin{theorem}
    If $f$ is convex, and defined on an interval containing all $x_i$, then $\sum a_i = 1$ and $a_i > 0$, then $f(\sum a_i x_i) < \sum a_i f(x_i)$
\end{theorem}
\begin{proof}
    Using the basic definition of convexity, we can complete the proof by induction. We find
    %
    \[ f \left(\sum_{i = 1}^n a_i x_i \right) = f \left( (1 - a_n) \sum_{i = 1}^{n-1} \frac{a_i}{1 - a_n} x_i + a_n x_n \right) < (1 - a_n) f \left( \sum_{i = 1}^{n-1} \frac{a_i}{1 - a_n} x_i \right) + a_n f(x_n) \]
    %
    By induction, we find
    %
    \[ f \left( \sum_{i = 1}^{n-1} \frac{a_i}{1 - a_n} x_i \right) < \sum \frac{a_i}{1 - a_n} f(x_i) \]
    %
    and this completes the proof by substitution. The previous arguments just show that the manipulations are justified since the values considered lie in the domain of $f$.
\end{proof}

As an example of this theorem, the exponential is a convex function, so that for any $x_1, \dots, x_n$,
%
\[ \sqrt[n]{e^{x_1} e^{x_2} \dots e^{x_n}} = e^{\sum x_i/n} < \frac{1}{n} \sum e^{x_i} \]
%
If $y_i = e^{x_i}$, we discover the arithmetic geometric mean inequality
%
\[ \sqrt[n]{y_1 \dots y_n} < \frac{y_1 + \dots + y_n}{n} \]
%
One of the classical inequalities in analysis.

This theorem can be generalized to a very important theorem in probability theory -- if $X$ is a random variable, and $f$ is convex, then $f(\mathbf{E}X) \leq \mathbf{E}[f(X)]$. To see this, note that if $X$ is a simple random variable of the form $\sum a_i \mathbf{I}(X \in E_i)$, then applying the discrete Jensen's inequality we find
%
\[ f(\mathbf{E}X) = f \left( \sum \mathbf{P}(X \in E_i) a_i \right) \leq \sum \mathbf{P}(X \in E_i) f(a_i) = \mathbf{E}[f(X)] \]
%
If $X$ is a general random variable, then $X$ can be expressed as the monotone limit of simple random variables $X_1 \leq X_2 \leq \dots \to X$, and then the monotone convergence theorem guarantees that $\mathbf{E}X_i \to \mathbf{E}X$, so that by continuity
%
\[ f(\mathbf{E}X) = \lim_{n \to \infty} f(\mathbf{E}X_n) \leq \liminf_{n \to \infty} \mathbf{E}[f(X_n)] \]
%
The proof would be complete, but it's hard to limit the right hand side, so we appeal to a more abstract proof.

\begin{theorem}
    If $f$ is convex on $-\infty \leq a < b \leq \infty$, $f(\mathbf{E}X) \leq \mathbf{E}[f(X)]$.
\end{theorem}
\begin{proof}
    Let $y = \mathbf{E}X$, then $a \leq y \leq b$. Let
    %
    \[ \alpha = \sup_{a \leq x \leq y \leq b} \frac{f(y) - f(x)}{y - x} \]
    %
    Then $\alpha < \infty$ by convexity, and for any $a \leq x \leq y$, $f(x) \geq f(y) + \alpha(x - y)$. But then by integrating boths dies we find
    %
    \[ \mathbf{E}[f(X)] \geq f(y) + \alpha(\mathbf{E}[X] - y) = f(y) = f(\mathbf{E}X) \]
\end{proof}

\chapter{Vector Calculus}

\section{The Laplacian}

Almost definitely the most important operator is the {\bf Laplacian}
%
\[ \Delta = \sum \frac{\partial^2}{\partial x_i^2} = \nabla \cdot \nabla \]
%
It occurs in almost all physical contexts with spatial invariance. One way to interpret the value $\Delta f(x)$ is as the average differences $f(x+y) - f(x)$ in an infinitisimal neighbourhood of $x$.

\begin{theorem}
    If $f$ is twice differentiable at $x$, then
    %
    \[ \fint_{B_r} [f(x+y) - f(x)]\ dy = \frac{\Delta f(x)}{2(d+2)} r^2 + o(r^2) \]
\end{theorem}
\begin{proof}
    Suppose $f$ is twice differentiable at $x$. Then Taylor's theorem implies
    %
    \[ f(x + y) = f(x) + \nabla f(x) \cdot y + \frac{1}{2} Hf(x)(y) + \sum o(y_iy_j) \]
    %
    where $Hf(x)(y) = \sum f_{ij}(x) y_iy_j$. Now we calculate
    %
    \begin{align*}
        \fint_{B_r} &[f(x + y) - f(x)]\ dy\\
        &= \fint_{B_r} \nabla f(x) \cdot y\ dy + \sum \frac{f_{ij}(x)}{2} \fint_{B_r} y_iy_j\ dy + \sum \fint_{B_r} o(y_iy_j)\ dy
    \end{align*}
    %
    The first integral vanishes because the function is odd in every direction. The second integrals vanish for $i \neq j$ because the functions are odd in directions $i$ and $j$. Next,
    %
    \[ \fint_{B_r} o(y_iy_j)\ dy = \frac{1}{r^d |B_1|} \int_{B_r} o(y_iy_j) dy = \fint_{B_1} o(r^2 y_iy_j)\ dy = o(r^2) \]
    %
    This means the integral is equal to
    %
    \[ \frac{\Delta f(x)}{2} \fint_{B_r} y_i^2\ dy + o(r^2) \]
    %
    Now
    %
    \[ \fint_{B_r} y_1^2\ dy = \frac{1}{r^d |B_1|} \int_{B_r} y_1^2\ dy = \frac{r^2}{|B_1|} \int_{B_1} y_1^2\ dy = r^2 \frac{|B_1^{d-1}|}{|B_1^d|} \int_{-1}^1 s^2 (1 - s^2)^{\frac{d-1}{2}}\ ds \]
    %
    and
    %
    \[ |B_1^d| = \int_{B_1^d} dx = |B_1^{d-1}| \int_{-1}^1 (1 - s^2)^{\frac{d-1}{2}} \]
    %
    and so
    %
    \begin{align*}
        \fint_{B_r} y_1^2\ dy &= r^2 \left( \int_{-1}^1 (1 - s^2)^{\frac{d-1}{2}}\ ds \right)^{-1} \left( \int_{-1}^1 s^2 (1 - s^2)^{\frac{d-1}{2}}\ ds \right)\\
        &= r^2 \left( \frac{\Gamma( \frac{d+1}{2} )}{\Gamma ( \frac{d + 2}{2} )} \sqrt{\pi} \right)^{-1} \left( \frac{\Gamma( \frac{d+1}{2} )}{\Gamma( \frac{d + 4}{2} )} \frac{\sqrt{\pi}}{2} \right) = \frac{r^2}{2} \frac{\Gamma( \frac{d+2}{2})}{\Gamma( \frac{d+4}{2} )} = \frac{r^2}{d + 2}
    \end{align*}
    %
    and so we conclude that
    %
    \[ \fint_{B_r} [f(x+y) - f(x)] = \frac{\Delta f(x)}{2(d+2)} r^2 + o(r^2) \]
    %
    so the Laplacian measures the second order difference of the average of the function on balls around a point.
\end{proof}

\begin{theorem}
    If $f(x) = g(r)$ is radially invariant, then
    %
    \[ \Delta f(x) = \frac{d - 1}{r} g'(r) + g''(r) \]
    %
    Which we can write as
    %
    \[ \Delta = \frac{d-1}{r} \partial_r + \partial_{rr} \]
\end{theorem}
\begin{proof}
Suppose $f(x) = g(r)$ is radially invariant, and twice differentiable at the origin. We calculate that since $r^2 = \sum x_i^2$, so
%
\[ \frac{\partial r}{\partial x_i} = \frac{x_i}{r} \]
%
so $f_i(x) = g'(r) (x_i/r)$, and so
%
\[ f_{ii}(x) = \left( \frac{1}{r} - \frac{x_i^2}{r^3} \right) g'(r) + \frac{x_i^2}{r^2} g''(r) \]
%
The Laplacian formula is then obvious.
\end{proof}

\section{Integration by Parts for Divergence}

Let $f$ be a vector-valued function, and $\psi$ scalar-valued. We then find that we have a product rule
%
\[ \nabla \cdot (f \psi) = \sum \frac{\partial f_i \psi}{\partial x_i} = \sum \frac{\partial f_i}{\partial x_i} \psi + f_i \frac{\partial \psi}{\partial x_i} = (\nabla \cdot f) \psi + f \cdot \nabla \psi \]
%
This implies an `integration by parts' type result for the divergence, using the divergence theorem, because now
%
\[ \int_\Omega \psi (\nabla \cdot f) = \int_\Omega \nabla \cdot (f \psi) - \int_\Omega f \cdot \nabla \psi = \int_{\partial \Omega} \psi(f \cdot \nu) d\sigma - \int_\Omega f \cdot \nabla \psi \]

\end{document}