\input{../style.tex}

\title{The Representation Theory of Lie Algebras}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}

\maketitle

\tableofcontents

\pagenumbering{arabic}

\chapter{Basic Definitions}

If $K$ is a field, then a Lie algebra over $K$ is a $K$ vector-space $\mathfrak{g}$ equipped with an alternating, bilinear form $[\cdot, \cdot]$, known as the {\bf Lie bracket}, satisfying the Jacobi identity
%
\[ [X,[Y,Z]] + [Y,[Z,X]] + [Z,[X,Y]] = 0 \]
%
for any $X,Y,Z \in \mathfrak{g}$. The majority of Lie algebras emerge in geometry and some parts of number theory (normally where $K = \mathbf{R}$ or $K = \mathbf{C}$), where a Lie group $G$ gives rise to a natural Lie algebra structure on the finite dimensional space $\mathfrak{g}$ of tangent vectors at the origin (There is a natural theory of `Lie groups' for fields other than the real and complex numbers, though we won't discuss them here). We obtain the Lie algebra by differentiating the operation on the group, so we can think of a Lie algebra as an infinitesimal linear approximation to the group operations. Indeed, most algebraic concepts about Lie groups have `infinitesimal formulations' on Lie algebras. Because of the correspondence between $G$ and $\mathfrak{g}$, we normally denote a Lie algebra corresponding to a Lie group by the `frakturized' name of the group.

\begin{example}
    If we differentiate the Lie group $GL_n(K)$ of invertible matrices with coefficients in $K$, we obtain our first example of a Lie algebra. The tangent space at the origin can be identified with the space of all matrices, since $GL_n(K)$ is an open submanifold of $M_n(K)$, and with some work we show that the Lie bracket takes the form $[X,Y] = XY - YX$. In general, the set of invertible endomorphisms $GL(V)$ on a finite dimensional vector space forms a Lie group, the Lie algebra can be identified with the set of all endomorphisms $T: V \to V$, and the Lie bracket takes the form $[T,S] = T \circ S - S \circ T$.
\end{example}

\begin{example}
    In general, if $A$ is any associative algebra over $K$, then the commutator $[X,Y] = XY - YX$ gives a Lie algebra structure on $A$, denoted $\mathfrak{a}$. From the perspective of differential geometry, if $A$ is an arbitrary finite-dimensional algebra, then $A$ has the structure of a differential manifold since it is a vector space, and $U(A)$ is an open neighbourhood of the origin, hence a Lie group. The induced Lie algebra structure on $U(A)$ can then be identified with the Lie bracket on $A$ defined by $[X,Y] = XY - YX$. Note, however, that this is no more general than our discussion of the general linear group, because any finite dimensional algebra $A$ can be faithfully represented as endomorphisms $\text{End}(A)$ over the vector space structure of $A$ by the map $\rho: A \to \text{End}(A)$, $\rho(X)(Y) = XY$, and this essentially identifies $U(A)$ as a subalgebra of $\mathfrak{gl}(V)$.
\end{example}

\begin{example}
    If $\mathfrak{g}$ is any vector space, then the trivial bracket $[X,Y] = 0$ gives $\mathfrak{g}$ a Lie algebra structure. $\mathfrak{g}$ is known as a commutative Lie algebra, because then $[X,Y] = [Y,X]$, and for Lie algebras over fields of characteristic $\neq 2$, this is the only circumstance in which this can occur. If a Lie group $G$ is abelian, than $\mathfrak{g}$ is abelian, and the converse holds provided $G$ is connected.
\end{example}

\begin{example}
    On any field $K$, the space of linear endomorphisms on $K[X]$ forms an associative algebra over $K$ under composition. Of particular interest are the operators
    %
    \[ f(X) \mapsto g(X) f(X) \]
    %
    for $g \in K[X]$, which we shall identify with the polynomial $g$, and the differentiation operators
    %
    \[ f(X) \mapsto f'(X) \]
    %
    which we denote $\partial_X$. If we consider the space of `differential operators with polynomial coefficients', operators which can be written in the form
    %
    \[ \sum_{k = 0}^N g_k(X) \partial_X^k \]
    %
    with $g_k(X) \in K[X]$, for some $N$, then we find these form a subalgebra of the space of endomorphisms, since we have the identity
    %
    \[ (\partial_X X)(f) = X \partial_X f + f \partial_X X = X \partial_X f + f \]
    %
    so that $\partial_X X = X \partial_X + 1$, and we may use this identity to rearrange the product of any two differential operators to coincide with an operator of the form above. As an algebra, the space of differential operators has essentially no more relations. If
    %
    \[ \sum_{k = 0}^N g_k(X) \partial_X^k = 0 \]
    %
    Then successively applying the operator to the monomials $X^m$ gives
    %
    \[ g_0(X) = g_1(X) = \dots = g_N(X) = 0 \]
    %
    provided we are working over a field of characteristic 0, in which case we have an isomorphism of the ring of operators with $K \langle X, Y \rangle / (YX - XY - 1)$. In this form, the algebra is useful in quantum mechanics, where the operators represent certain physical measurements, which are non-commutative (their outcome depends on the order in which they are measured). If a field has finite characteristic $p$,  this method only allows us to determine that
    %
    \[ g_0(X) = g_1(X) = \dots = g_{p-1}(X) = 0 \]
    %
    but we find $\partial_X^p = 0$, so if we add this additional relation $Y^p = 0$, we obtain another isomorphism with a quotient of $K \langle X, Y \rangle$. In general, the ring of differential operators in $n$ variables is called the $n$'th Weyl algebra. It is obtained from $K\langle X_1, \dots, X_n, Y_1, \dots, Y_n \rangle$ modulo the relations $Y_i X_j - X_j Y_i = \delta_{ij}$ and $Y_i^p = 0$, which makes sense if we view $Y_i$ as the operator which is partial differentiation in the $i$'th variable. The commutator on these algebras gives a particularly interesting Lie algebra structure.
\end{example}

\begin{example}
    If $A$ is an associative algebra over $K$, then $M_n(A)$ is an algebra over $K$, and therefore a Lie algebra, denoted $\mathfrak{gl}_n(A)$. A particularly interesting example occurs if $A = \mathbf{C}[X,X^{-1}]$, in which case we call $\mathfrak{gl}_n(A)$ a Loop Lie algebra. These algebras have applications in various fields of theoretical physics, including String theory.
\end{example}

\begin{example}
    Given a (possibly non associative) algebra $A$, a derivation on $A$ is a linear map $d: A \to A$ satisfying $d(xy) = xd(y) + d(x)y$. Given two derivations $d$ and $d'$, $d \circ d'$ may not be a derivation, but the commutator
    %
    \[ [d_1, d_2] = d_1 \circ d_2 - d_2 \circ d_1 \]
    %
    is always a derivation, because
    %
    \begin{align*}
        (d_1 \circ d_2 - &d_2 \circ d_1)(fg) = d_1(f d_2(g) + d_2(f) g) - d_2(d_1(f) g + f d_1(g))\\
        &= [d_1(f) d_2(g) + f (d_1 \circ d_2)(g) + d_2(f) d_1(g) + (d_1 \circ d_2)(f) g]\\
        &\ - [(d_2 \circ d_1)(f) g + d_1(f) d_2(g) + d_2(f) d_1(g) + f (d_2 \circ d_1)(g)]\\
        &= f(d_1 \circ d_2 - d_2 \circ d_1)(g) - (d_1 \circ d_2 - d_2 \circ d_1)(f) g
    \end{align*}
    %
    Thus the set of derivations on $A$, denoted $\text{der}(A)$, forms a Lie algebra. We should expect the space of derivations to play a fundamental role in the study of Lie algebras, because if $X,Y,Z$ are elements of any Lie algebra, then the Jacobi identity tells us that
    %
    \[ [X,[Y,Z]] = - [Y,[Z,X]] - [Z,[X,Y]] = [Y,[X,Z]] + [[X,Y],Z] \]
    %
    Introducing the {\bf adjoint map} $\text{adj}_X: \mathfrak{g} \to \mathfrak{g}$ defined by $\text{adj}_X(Y) = [X,Y]$, we can restate the equation above as
    %
    \[ \text{adj}_X[Y,Z] = [Y, \text{adj}_X Z] + [\text{adj}_XY, Z] \]
    %
    so that $\text{adj}_X$ is actually a derivation on $\mathfrak{g}$. The map $X \mapsto \text{adj}_X$ is actually a representation of $\mathfrak{g}$ on $\text{der}(\mathfrak{g})$, known as the {\bf adjoint representation} because
    %
    \begin{align*}
        [\text{adj}_X, \text{adj}_Y](Z) &= (\text{adj}_X \text{adj}_Y - \text{adj}_Y \text{adj}_X)(Z)\\
        &= [X,[Y,Z]] - [Y,[X,Z]]\\
        &= [X,[Y,Z]] + [[X,Z],Y]\\
        &= [[X,Y],Z]\\
        &= \text{adj}_{[X,Y]}(Z)
    \end{align*}
    %
    The kernel of the representation being the centre of $\mathfrak{g}$, $Z(\mathfrak{g})$. In general, a bilinear skew-symmetric map is a Lie bracket if and only if the corresponding adjoint maps are all derivations; thus the Jacobi identity is exactly the derivation equation in disguise, harkening back to the definition of vector fields as derivations on the space of $C^\infty$ functions on the Lie group.
\end{example}

\begin{example}
    The formula
    %
    \[ \det(I + tM) = 1 + t\ \text{tr}(M) + o(t^2) \]
    %
    which can be proved by using the Leibnitz formula to expand the determinant out algebraically, and then only taking into the account the first-order terms. The tangent space of $SL_n(K)$ can be identified as a subspace of the tangent bundle on $GL_n(K)$, and since elements of $SL_n(K)$ are defined by the equation $\det(X) = 1$, the set of tangent vectors at the origin are exactly those which annihilate the determinant -- i.e., those matrices $M$ such that
    %
    \[ \left. \frac{\det(I + tM)}{dt} \right|_{t = 0} = 0 \]
    %
    This implies that the Lie algebra $\mathfrak{sl}_n(K)$ consists of the matrices $M \in \mathfrak{gl}_n(K)$ with trace zero. Algebraically, we can verify that $\mathfrak{sl}_n(K)$ is a Lie subalgebra of $\mathfrak{gl}_n(K)$, because it is closed under the Lie bracket. Noticing the trace identity $\text{tr}(XY) = \text{tr}(YX)$, we find
    %
    \[ \text{tr}(XY - YX) = \text{tr}(XY) - \text{tr}(YX) = 0 \]
    %
    Thus $\mathfrak{sl}_n(K)$ is formally a Lie subalgebra of $\mathfrak{gl}_n(K)$.
\end{example}

\begin{example}
    The orthogonal group $O_n(K)$ consists of matrices in $GL_n(K)$ satisfying $X^tX = I$. Since
    %
    \[ (I + hX)^t(I + hX) = I + h(X^t + X) + h^2X^tX \]
    %
    $X$ is a tangent vector at the origin for $O_n(K)$ if and only if $X^t = -X$, which causes the first order term of the expansion of $(I + hX)$ to be zero. If $\text{char}\ K \neq 2$, then $X^t = -X$ implies that $X$ vanishes along the diagonal, hence $\mathfrak{o}_n(K) \subset \mathfrak{sl}_n(K)$. This essentially follows because the special orthogonal Lie group $SO_n(K)$ of orthogonal matrices of determinant one forms a connected component of $O_n(K)$, hence $\mathfrak{so}_n(K) = \mathfrak{o}_n(K)$. Algebraically, $\mathfrak{o}_n(K)$ is a subalgebra of $\mathfrak{gl}_n(K)$ because if $X,Y \in \mathfrak{o}_n(K)$, then
    %
    \[ (XY - YX)^t = Y^tX^t - X^tY^t = YX - XY = -(XY - YX) \]
    %
    so the Lie bracket preserves orthogonality.
\end{example}

\begin{example}
    If $n$ is even, $n = 2m$, then matrices $M \in M_n(K)$ can represent operators from $K^n \times K^n$ to $K^n \times K^n$. Consider the canonical symplectic form $\omega$ on $K^n \times K^n$ (a non-degenerate, antisymmetric bilinear form), defined by
    %
    \[ \omega(x_0 + y_0, x_1 + y_1) = \omega(x_0,y_1) + \omega(y_0,x_1) = \sum (x_0^i y_1^i - y_0^i x_0^i) \]
    %
    We let the symplectic group $SP_n(K)$ be the set of operators preserving this symplectic form. In matrix form, if we define the matrix $J \in M_n(K)$ by
    %
    \[ J = \begin{pmatrix} 0 & I_m \\ -I_m & 0 \end{pmatrix} \]
    %
    then $SP_n(K)$ consists exactly of those matrices $M$ such that $M^tJM = J$, or $M^tJMJ^t = I$. If we consider a first order approximation at the origin
    %
    \[ (I + hM)^tJ(I + hM)J^t = I + h(M^t + JMJ^t) + h^2M^tJMJ^t \]
    %
    we see the tangent vectors consist of matrices $M$ such that $M^t = -JMJ^t$, or $M^tJ + JM = 0$. Together, this subalgebra forms the symplectic Lie algebra $\mathfrak{sp}_n(K)$. If we write
    %
    \[ X = \begin{pmatrix} A & B \\ C & D \end{pmatrix} \]
    %
    Then
    %
    \[ X^tJ = \begin{pmatrix} -C^t & A^t \\ -D^t & B^t \end{pmatrix}\ \ \ \ \ JX = \begin{pmatrix} C & D \\ -A & -B \end{pmatrix} \]
    %
    Implying that $M$ is in $\mathfrak{sp}_n(K)$ if and only if $C^t = C$, $A^t= -D$, and $B^t = B$.
\end{example}

\begin{example}
    The Heisenberg group $H_n(K)$ is the Lie group of matrices of the form
    %
    \[ \begin{pmatrix} 1 & a & c \\ 0 & I_n & b \\ 0 & 0 & 1 \end{pmatrix} \]
    %
    where $a \in K^n$ is a row vector, $c \in K$, and $b \in K^n$ is a column vector. It's corresponding Lie algebra $\mathfrak{h}_n(K)$ consists of matrices of the form
    %
    \[ \begin{pmatrix} 0 & a & c \\ 0 & 0 & b \\ 0 & 0 & 0 \end{pmatrix} \]
    %
    which is flat, since the Heisenberg group is essentially flat.
\end{example}

One of the main purposes of the abstract study of Lie algebras is that a Lie group's Lie algebra gives most of the required information about the structure of the Lie group. If we can characterize the structure of all Lie algebras, we are well on our way to understanding the structure of Lie groups. The standard way of breaking apart some algebraic structure is, like in most algebraic categories, by considering subalgebras and quotients. A homomorphism $f$ of Lie algebras is, of course, a linear map preserving the Lie bracket operation. As with many other algebraic objects, a basic way to understand a Lie algebra is to factor it into two simpler algebras $\mathfrak{h}$ and $\mathfrak{k}$ by considering a short exact sequence
%
\[ 0 \to \mathfrak{h} \to \mathfrak{g} \to \mathfrak{k} \to 0 \]
%
Then we can identity $\mathfrak{g}$ with the vector space $\mathfrak{h} + \mathfrak{k}$ equipped with a Lie bracket formed from a `twisted product'
%
\[ [(H_0,K_0), (H_1,K_1)] = \left([H_0,H_1], [K_0,K_1] + A(H_0,K_1) - A(H_1,K_0) + B(H_0,H_1) \right) \]
%
for some bilinear $A: \mathfrak{h} \times \mathfrak{k} \to \mathfrak{k}$ and $B: \mathfrak{h}^2 \to \mathfrak{k}$. If we are lucky, then we will find $A = B = 0$, in which case we call $\mathfrak{g}$ the direct sum of $X$ and $Y$, and denote it by $\mathfrak{h} \oplus \mathfrak{k}$. We have then have literally decomposed $\mathfrak{g}$ into simpler Lie algebras.

The standard way to form a short exact sequence for a Lie algebra $\mathfrak{g}$ is to find a (two-sided) ideal $\mathfrak{a}$, which is a subspace of $\mathfrak{g}$ such that $[X,Y]$ and $[Y,X]$ are both in $\mathfrak{a}$ for any $X \in \mathfrak{a}$, $Y \in \mathfrak{g}$. We can then consider the quotient Lie algebra $\mathfrak{g}/\mathfrak{a}$, and we have an exact sequence
%
\[ 0 \to \mathfrak{a} \to \mathfrak{g} \to \mathfrak{g}/\mathfrak{a} \to 0 \]
%
The other way to form an exact sequence is to consider a surjective homomorphism $f: \mathfrak{g} \to \mathfrak{h}$, in which case if we let $\mathfrak{k} = \ker f$, then
%
\[ 0 \to \mathfrak{k} \to \mathfrak{g} \to \mathfrak{h} \to 0 \]
%
is exact. As in most algebraic categories, these two processes are essentially the same by the first isomorphism theorem. In fact, we have a version of all standard isomorphism theorems in the category of Lie algebras.

\begin{theorem}[The First Isomorphism Theorem]
    The kernel of a Lie algebra homomorphism $f: \mathfrak{g} \to \mathfrak{h}$ is an ideal, and the kernel $\mathfrak{a}$ forms an ideal of $\mathfrak{g}$ inducing an injective map $\tilde{f}: \mathfrak{g}/\mathfrak{a} \to \mathfrak{h}$, such that
    %
    \begin{center}
    \begin{tikzcd}
        \mathfrak{g} \arrow{r}{f} \arrow{d} & \mathfrak{h}\\
        \mathfrak{g}/\mathfrak{h} \arrow{ru}[below]{\tilde{f}}
    \end{tikzcd}
    \end{center}
    %
    commutes.
\end{theorem}

\begin{theorem}[The Second Isomorphism Theorem]
    The set of subalgebras of $\mathfrak{h}$ of $\mathfrak{g}/\mathfrak{a}$ is one to one with the class of subalgebras of $\mathfrak{g}$ containing $\mathfrak{a}$, and the correspondence maps ideals to ideals, where the subalgebra corresponding to $\mathfrak{a} \subset \mathfrak{h}$ is denoted $\mathfrak{h}/\mathfrak{a}$. If $\mathfrak{h}$ is an ideal, then $(\mathfrak{g}/\mathfrak{a})/(\mathfrak{h}/\mathfrak{a})$ is isomorphic to $\mathfrak{g}/\mathfrak{h}$.
\end{theorem}

\begin{theorem}[The Third Isomorphism Theorem]
    If $\mathfrak{a} \subset \mathfrak{b}$ are ideals of $\mathfrak{g}$, then $\mathfrak{a} + \mathfrak{b}$ and $\mathfrak{a} \cap \mathfrak{b}$ are ideals of $\mathfrak{g}$, and $(\mathfrak{a} + \mathfrak{b})/\mathfrak{a}$ is isomorphic to $\mathfrak{b}/(\mathfrak{a} \cap \mathfrak{b})$.
\end{theorem}

\begin{example}
    Consider the homomorphism $f: \mathfrak{gl}_n(K) \to K$ obtained by taking the trace of the matrix, which is a homomorphism because
    %
    \[ \text{tr}\ [x,y] = \text{tr}(xy - yx) = 0 = [\text{tr}(x), \text{tr}(y)] \]
    %
    The kernel is the special linear group, which is therefore an ideal of the general linear group. Thus we have an exact sequence
    %
    \[ 0 \to \mathfrak{sl}_n(K) \to \mathfrak{gl}_n(K) \to K \to 0 \]
    %
    We conclude that $\mathfrak{gl}_n(K)$ is obtained from a twisted product of $\mathfrak{sl}_n(K)$ and $K$. But since $K$ is a subset of $Z(\mathfrak{gl}_n(K))$, we find that $\mathfrak{gl}_n(K)$ is just the direct sum of $\mathfrak{sl}_n(K)$ and $K$.
\end{example}





\section{Solvable Lie Algebras}

On Lie groups $G$, we can consider brackets of normal subgroups $H$,
%
\[ [H_0,H_1] = \langle h_0h_1h_0^{-1}h_1^{-1} : h_0 \in H_0, h_1 \in H_1 \rangle \]
%
and this subgroup of $G$ will also be normal. Of particular interest in the derived subgroup $[G,G]$, and when we take the quotient we have an abelian group $G_{\text{ab}} = G/[G,G]$. If $H$ is a normal Lie subgroup in $G$, then the Lie algebra $\mathfrak{h}$ of $H$ can be identified as a subalgebra of $\mathfrak{g}$, and we find that, provided $G$ is connected, then $\mathfrak{h}$ is an ideal in $\mathfrak{g}$. Conversely, if $H$ is connected then the fact that $\mathfrak{h}$ is an ideal implies that $H$ is a normal subgroup of $G$. The corresponding operator for the brackets of normal subgroups are the brackets of ideals
%
\[ [\mathfrak{a}, \mathfrak{b}] = \text{span} \{ [X,Y] : X \in \mathfrak{a}, \mathfrak{b} \} \]
%
Analogous to the commutator subgroups of a group is the derived subalgebra $\mathfrak{g}' = D\mathfrak{g} = [\mathfrak{g}, \mathfrak{g}]$, which is the smallest ideal such that $\mathfrak{g}/\mathfrak{g}'$ is abelian, and we call this the {\bf abelianization} of $\mathfrak{g}$, denoted $\mathfrak{g}_{\text{ab}}$). Since we have the exact diagram
%
\[ 0 \to \mathfrak{g}' \to \mathfrak{g} \to \mathfrak{g}_{\text{ab}} \to 0 \]
%
we can write $\mathfrak{g} = \mathfrak{g}_{\text{ab}} + \mathfrak{g}'$, where
%
\[ [(X_0,Y_0),(X_1,Y_1)] = (0, A(X_0,Y_1) - A(X_1,Y_0) + B(X_0,X_1)) \]
%
for some Bilinear $A$ and $B$. Thus we can think of the elements of $\mathfrak{g}'$ as infinitesimals, since they have no impact on the Lie bracket structure of $\mathfrak{g}_{\text{ab}}$, existing somewhat `beneath the surface' of the calculations. If we consider $\mathfrak{g}'' = (\mathfrak{g}')'$, then we can compute the abelian approximation `to a second order', writing $\mathfrak{g} = \mathfrak{g}_{\text{ab}} + \mathfrak{g}'_{\text{ab}} + \mathfrak{g}''$. Continuing this process, we hope to write $\mathfrak{g}$ as the successive product of abelian infinitesimals, $\mathfrak{g} = \mathfrak{g}^{(1)}_{\text{ab}} + \mathfrak{g}^{(2)}_{\text{ab}} + \dots + \mathfrak{g}^{(n)}_{\text{ab}}$, where $\mathfrak{g}^{(n)}$ is the $n$'th element of the {\bf derived series}. For this to work, we require that $\mathfrak{g}^{(n+1)} = 0$ for some $n$, in which case we say $\mathfrak{g}$ is a {\bf solvable} Lie algebra. This is equivalent to the existence of an abelian tower, and by refinement, a cyclic tower as well.

Solvability on Lie algebras is a tower property, in the sense that if any two of $\mathfrak{g}/\mathfrak{h}, \mathfrak{h}$, and $\mathfrak{g}$ are solvable, then the other element of the triple is solvable. It is also natural to consider the {\bf lower central series}
%
\[ \mathfrak{g}_1 = \mathfrak{g}\ \ \  \mathfrak{g}_2 = [\mathfrak{g}, \mathfrak{g}]\ \ \ \mathfrak{g}_3 = [\mathfrak{g}, [\mathfrak{g}, \mathfrak{g}]]\ \ \  \dots \]
%
where the derived series is slightly finer, $\mathfrak{g}^{(n)} \subset \mathfrak{g}_n$, yet the infinitesimals have the additional property that if $X \in \mathfrak{g}_n$ and $Y \in \mathfrak{g}_m$, then $[X,Y] \in \mathfrak{g}_{n+m}$ (for the derived series, we can only guarantee that $[X,Y] \in \mathfrak{g}^{(m+1)}$ if $X \in \mathfrak{g}^{(m+1)}$ and $Y \in \mathfrak{g}^{(m)}$, and that $X \in \mathfrak{g}^{(m)}$ if $X \in \mathfrak{g}$). If the lower central series eventually terminates, we call the algebra {\bf nilpotent}. We then have a decomposition
%
\[ \mathfrak{g} = (\mathfrak{g}_1/\mathfrak{g}_2) \times (\mathfrak{g}_2/\mathfrak{g}_3) \times \dots \times (\mathfrak{g}_n/\mathfrak{g}_{n+1}) \]
%
Of course, this is a very strong condition for a Lie algebra to have.

\begin{example}
    The Lie group $UT_n(K)$ of unitriangular matrices (upper triangular matrices with ones on the diagonal) is a Lie group, with corresponding Lie algebra $\mathfrak{ut}_m(K)$ consisting of strictly upper triangular matrices. This algebra is nilpotent, for we find that $(\mathfrak{ut}_m(K))_k$ consists of the strictly upper triangular matrices which vanish  above the $k$'th diagonal.
\end{example}

If $\mathfrak{a}$ and $\mathfrak{b}$ are two solvable ideals in a Lie algebra $\mathfrak{g}$, then $\mathfrak{a} + \mathfrak{b}$ is a solvable ideal, since $(\mathfrak{a} + \mathfrak{b})/\mathfrak{b}$ is isomorphic to $\mathfrak{a}/(\mathfrak{a} \cap \mathfrak{b})$, which is solvable as a quotient of a solvable ideal, and $\mathfrak{b}$ is solvable. As a corollary to this, we see that any Lie algebra $\mathfrak{g}$ has a maximum solvable ideal. If $\mathfrak{a}$ is a {\it maximal} solvable ideal, and $\mathfrak{b}$ is any other solvable ideal, then $\mathfrak{a} + \mathfrak{b}$ is solvable, hence $\mathfrak{a} + \mathfrak{b} = \mathfrak{a}$, and so $\mathfrak{b} \subset \mathfrak{a}$. We shall denote the maximum solvable ideal of a Lie algebra by $\text{rad}(\mathfrak{g})$, and call it the {\bf radical} of the algebra. The radical essentially separates the approximately commutative section of the algebra from the non-commutative section. The most non-commutative Lie algebras are the {\bf semi-simple} ones, such that $\text{rad}(\mathfrak{g}) = (0)$. For any algebra $\mathfrak{g}$, $\mathfrak{g}/\text{rad}(\mathfrak{g})$ is semisimple, so the radical efficiently extracts the commutative section of the algebra. It is an essential tool in the description of the finite dimensional Lie algebras, because it means we can write any Lie algebra as the product of a semisimple algebra and a solvable one.

The {\bf Levi decomposition} says that this product is actually a direct sum, so that {\it any} Lie algebra is the direct sum of a solvable algebra and a semisimple algebra. We will find that this implies solvability is much more useful than nilpotency, because it has the tower property, and is therefore amenable to algebraic manipulation. If $\mathfrak{a}$ and $\mathfrak{b}$ are nilpotent ideals, then $\mathfrak{a} + \mathfrak{b}$ is nilpotent, so we can consider the maximal nilpotent ideal, called the nilradical and denoted $\text{nil}(\mathfrak{g})$, and we can consider the short exact sequence to the quotient
%
\[ 0 \to \text{nil}(\mathfrak{g}) \to \mathfrak{g} \to \mathfrak{g}_{\text{red}} \]
%
where $\mathfrak{g}_{\text{red}}$ is known as the {\bf reduced algebra} of $\mathfrak{g}$. Unfortunately, we do not have a direct sum decomposition, though the sequence is useful at times.

A {\bf simple} Lie algebra is a non-abelian Lie algebra $\mathfrak{g}$ having no ideals other than the trivial ideals $(0)$ and $\mathfrak{g}$. We shall soon prove that semi-simple Lie algebras break down into the direct sum of simple Lie algebras, so that we need only study the simple Lie algebras to understand the semisimple ones. As we've stated before, the classification theorem for simple Lie algebras over the complex numbers gives a very simple set of families to understand, $\mathfrak{sl}_n$, $\mathfrak{so}_n$, $\mathfrak{sp}_n$, and some `eccentric' algebras $\mathfrak{e}_6$, $\mathfrak{e}_7$, $\mathfrak{e}_8$, $\mathfrak{f}_4$, and $\mathfrak{g}_2$. In our journey into finding this classification, we will require the sophisticated theorems of representation theory.






\chapter{Matrix Lie Algebras}

An important class of Lie algebras are those which occur as subalgebras of $\mathfrak{gl}(V)$, for some finite dimensional vector space $V$. This is not only an important class of examples of Lie algebras, but provides a suitable training ground for the understanding of the representation theory of Lie algebras -- the idea being that we can completely characterize a Lie algebra by it's actions on vector spaces. For now, we assume our Lie algebras occur as matrix subalgebras of $\mathfrak{gl}(V)$, but note that this theory will soon be used to talk about general actions of Lie algebras on vector spaces.

Eigenvectors and eigenvalues are incredibly important to the classification of linear operators over a finite dimensional vector space. They also play an important part in understanding the action of a Lie algebra on a vector space. Given a Lie algebra $\mathfrak{g}$ acting on a vector space $V$, we define an eigenvector of $\mathfrak{g}$ to be a vector $v \in V$ such that $Xv$ is a scalar multiple of $v$ for each $X \in \mathfrak{g}$. The scalar multiple may differ depending on the $X$ we choose. For instance, the eigenvectors for the subalgebra of diagonal matrices in $\mathfrak{gl}_n(K)$ consist of exactly the basis vectors $e_i$. The eigenvectors essentially allow us to simultaneously diagonalize all the operators in a subalgebra at once. Dual to the concept of eigenvectors are a sort of `generalized eigenvalue'. If $v$ is an eigenvector for $\mathfrak{g}$, then we may define a function $\lambda: \mathfrak{g} \to K$ by $Xv = \lambda(X) v$. $\lambda$ is a linear map, because
%
\[ \lambda(X + Y)v = (X + Y)v = Xv + Yv = [\lambda(X) + \lambda(Y)]v \]
%
We call such a map a {\bf weight} for $\mathfrak{g}$. In general, for a linear functional $\lambda \in \mathfrak{g}^*$, we define $V_\lambda$ to be the set of vectors $v$ such that $Xv = \lambda(X)v$ holds for all $X \in \mathfrak{g}$. A weight is precisely a linear functional for which $V_\lambda$ is non-trivial.

\begin{example}
    As we have shown, if $\mathfrak{g}$ is the subalgebra of diagonal matrices in $\mathfrak{gl}_n(K)$, then the eigenvectors consist of the basis vectors $e_i$, and the corresponding weights are precisely the weights $\varepsilon_i(X) = X_{ii}$, because $Xe_i = X_{ii} e_i$. If the eigenvectors of a matrix subalgebra $\mathfrak{g}$ over $V$ form a basis of $V$, then $\mathfrak{g}$ is isomorphic to some family of diagonal matrices over $V$.
\end{example}

The weight spaces have nice invariance properties which, like for the Jordan normal form, allow us to decompose the action of Lie algebras into certain subfamilies.

\begin{lemma}
    If $\mathfrak{a}$ is an ideal of a subalgebra $\mathfrak{g}$ on $\mathfrak{gl}(V)$, the space of vectors $v \in V$ such that $Xv = 0$ for all $X \in \mathfrak{a}$ is $\mathfrak{g}$-invariant.
\end{lemma}
\begin{proof}
    If $Xv = 0$ for all $X \in \mathfrak{a}$, then for any $Y \in \mathfrak{g}$, $[X,Y] \in \mathfrak{a}$, so $[X,Y]v = 0$, hence
    %
    \[ XYv = YXv = Y0 = 0 \]
    %
    thus we have verified $\mathfrak{g}$-invariance.
\end{proof}

Thus we have verified that the set of eigenvectors for $\mathfrak{a}$ with weight 0 form a $\mathfrak{g}$ invariant subspace. We can generalize this result to the case where we have a family of eigenvectors of the same weight, where the weight might not necessarily be equal to zero. This will be very important to the decomposition results we will prove for Lie algebras.

\begin{theorem}
    Over a field of characteristic zero, if $\mathfrak{g}$ is a subalgebra of $\mathfrak{gl}(V)$ with ideal $\mathfrak{a}$, then the weight space of any $\lambda: \mathfrak{a} \to K$ is $\mathfrak{g}$ invariant.
\end{theorem}
\begin{proof}
    If $v \in V$ satisfies $Xv = \lambda(X)v$ for all $X \in \mathfrak{a}$, then we must show $XYv = \lambda(X)Yv$ for any $Y \in \mathfrak{g}$. Note that
    %
    \[ XYv = YXv + [X,Y]v = \lambda(X) Yv + \lambda[X,Y] v \]
    %
    Thus it suffices to verify that $\lambda[X,Y] = 0$ for any $X \in \mathfrak{a}$, and $Y \in \mathfrak{g}$. Consider the span of $\{ v, Yv, \dots, Y^nv \}$. We claim that for any $Z \in \mathfrak{a}$,
    %
    \[ ZY^n v = \lambda(Z) (Y^n v) + \text{span}(v,Yv, \dots, Y^{n-1}v) \]
    %
    For $n = 0$, the claim is assumed. In general, since $[Z,Y] \in \mathfrak{a}$, we have
    %
    \begin{align*}
        ZY^nv &= YZY^{n-1}v + [Z,Y]Y^{n-1}v\\
        &= \lambda(X) Y^n v + [Z,Y]Y^{n-1}v + \text{span}(v,Yv, \dots, Y^{n-2}v)\\
        &= \lambda(X) Y^n v + \text{span}(v,Yv, \dots, Y^{n-1}v)
    \end{align*}
    %
    The matrix representation of $\text{adj}_Z$ with respect to this basis has trace $n\lambda(Z)$. If we let $Z = [X,Y]$, then we find $n\lambda[X,Y] = 0$, hence $\lambda[X,Y] = 0$.
\end{proof}

Here's a simple consequence giving a useful result about complex matrix algebras.

\begin{theorem}
    If $V$ is a complex vector space, and $X, Y \in \mathfrak{gl}(V)$ commute with $[X,Y]$, then $[X,Y]$ is nilpotent.
\end{theorem}
\begin{proof}
    Since we are working over the complex numbers, it suffices to show that the only eigenvalue of $[X,Y]$ is 0. Let $\lambda$ be an eigenvalue of $[X,Y]$, and let $\mathfrak{g}$ be the subalgebra of $\mathfrak{gl}(V)$ generated by $X$, $Y$, and $[X,Y]$. Then the span of $[X,Y]$ is a Lie algebra ideal of $\mathfrak{g}$, hence the space $V_\lambda$ of vectors $v$ satisfying $[X,Y]v = \lambda v$ is $\mathfrak{g}$ invariant. This implies that $X$ and $Y$ restrict to operators on $V_\lambda$, and the restriction of $[X,Y]$ to $V_\lambda$ is the same as $XY - YX$, hence $[X,Y]$ has trace zero. But $[X,Y]$ is diagonalizable on $V_\lambda$, with trace $n \lambda$ if $V_\lambda$ is $n$-dimensional, hence $\lambda = 0$.
\end{proof}

\section{Engel's Theorem}

In linear algebra, we form classification theorems for linear operators. The theorems show the existence of certain basis elements on the vector space, such that the corresponding matrix representation of the operators have nice structure. But given a family of linear operators, it is a much more difficult problem to find a basis such that {\it all} the matrix representations of the family of linear operators all have nice structure. Any nilpotent operator on a vector space has a basis with respect to which the matrix representation is upper triangular. We would like to know which families of nilpotent linear operators can be simultaneously `strictly upper triangularized'. Also natural is to know which families of linear operators can be simultaneously `upper triangularized'. In the case that these families form a Lie subalgebra of operators, then these results essentially constitute Engel's theorem and Lie's theorem.

\begin{lemma}
    If $\mathfrak{g}$ is a Lie subalgebra of nilpotent transformations on a finite dimensional vector space $V$, and $V \neq 0$, then there is $v \neq 0$ for which $Xv = 0$ for all $X \in \mathfrak{g}$.
\end{lemma}
\begin{proof}
    We proceed by induction on the dimension of $\mathfrak{g}$. If $\mathfrak{g}$ is one-dimensional, spanned by some $X \neq 0$, then there is $n$ such that $X^n \neq 0$, $X^{n+1} = 0$, and if $X^n v \neq 0$, then $X(X^n v) = 0$, so $v$ satisfies the theorem.

    In general, let $\mathfrak{a}$ be a maximal proper Lie subalgebra of $\mathfrak{g}$. For any $X \in \mathfrak{a}$, the adjoint map $\text{adj}_X: \mathfrak{g} \to \mathfrak{g}$ descends to a map $\text{adj}_X: \mathfrak{g}/\mathfrak{a} \to \mathfrak{g}/\mathfrak{a}$, because if $Y - Z \in \mathfrak{a}$, then $[X,Y] - [X,Z] = [X,Y-Z] \in \mathfrak{a}$. Note that if $X$ is nilpotent, then $\text{adj}_X: \mathfrak{g} \to \mathfrak{g}$ is nilpotent, since
    %
    \[ \text{adj}^m_X(Y) = \sum_{k = 0}^m (-1)^k {m \choose k} (X^kYX^{m-k}) \]
    %
    and if $X^n = 0$, then $\text{adj}^m_X(Y) = 0$ when $m \geq 2n$. Since the family $\text{adj}_X$, viewed as endomorphisms of $\mathfrak{g}/\mathfrak{a}$ form a subalgebra of nilpotent transformations on $\mathfrak{gl}(\mathfrak{g}/\mathfrak{a})$ with dimension less than $\mathfrak{g}$, we conclude that there is $Y + \mathfrak{a} \in \mathfrak{g}/\mathfrak{a}$ with $Y \not \in \mathfrak{a}$, such that $[X,Y] \in \mathfrak{a}$ for all $X \in \mathfrak{a}$. This implies that $\mathfrak{a} + KY$ is a subalgebra of $\mathfrak{g}$, and since $\mathfrak{a}$ is a maximal subalgebra, $\mathfrak{a} + KY = \mathfrak{g}$, so $\mathfrak{a}$ has codimension 1 in $\mathfrak{g}$. Now we apply induction again to conclude that the space $W$ of $v \in V$ with $Xv = 0$ for all $X \in \mathfrak{a}$ is non-trivial. We have proved that $W$ is invariant under multiplication by $\mathfrak{g}$, so in particular $YW \subset W$. Since $Y$ is nilpotent, we can find a non-zero $w \in W$ with $Yw = 0$, and then $(\mathfrak{a} + KY)w = 0$.
\end{proof}

\begin{theorem}[Engel]
    For any Lie algebra $\mathfrak{g}$ of nilpotent linear transformations on a vector space $V$, there is a basis which simultaneously upper triangularizes all elements of the algebra.
\end{theorem}
\begin{proof}
    We adapt the proof strategy in the case of a single transformation. First, we find a vector $v$ such that $Xv = 0$ for all $X \in \mathfrak{g}$. If $W = K v$, then each $X$ descends to a linear endomorphism on $V/W$. The image of $\mathfrak{g}$ under this descending process satisfies the hypothesis of Engel's theorem, and therefore by induction there is a basis $v_1 + W, \dots, v_m + W$ of $V/W$ in which all $X$ are upper triangular. Then $v, v_1, \dots, v_m$ is a basis for $V$, and
    %
    \[ Xv_i \in \text{span}(v_1, \dots, v_{i-1}) + \text{span}(v) \]
    %
    and so the $X$ are upper triangular in this new space.
\end{proof}

\begin{corollary}
    $\mathfrak{g}$ is nilpotent if and only if $\text{adj}_X$ is nilpotent for all $X \in \mathfrak{g}$.
\end{corollary}
\begin{proof}
    A Lie algebra $\mathfrak{g}$ is nilpotent if and only if there is a value $n$ such that
    %
    \[ \text{adj}_{X_1} \circ \text{adj}_{X_2} \dots \circ \text{adj}_{X_n} = 0 \]
    %
    for any $X_i \in \mathfrak{g}$. In particular, $\text{adj}_X^n = 0$ for all $X$. Conversely, if the $\text{adj}_X$ are nilpotent, then Engel's theorem implies the existence of a basis of $X_i$ such that all $\text{adj}$ are strictly upper triangular. But if the Lie algebra is $m$ dimensional, this implies that the nilpotency condition above holds for $n = m$.
\end{proof}

Engel's theorem holds for Lie algebras over arbitrary fields. The analogous theorem for solvable Lie algebras is not so easy to generalize.

\begin{lemma}
    If $\mathfrak{g} \subset \mathfrak{gl}(V)$ is a solvable Lie algebra over the complex numbers, and $V \neq 0$, there there is $v \neq 0$ which is an eigenvector for every element of $\mathfrak{g}$.
\end{lemma}
\begin{proof}
    We prove this theorem by induction on the dimension of $\mathfrak{g}$. If $\mathfrak{g}$ has dimension 1, it suffices to show that every matrix has an eigenvector, and this follows because $\mathbf{C}$ is a complete field. For an arbitrary $\mathfrak{g}$, we note that $\mathfrak{g}'$ is a proper subalgebra of $\mathfrak{g}$, hence we may enlarge it to a subalgebra $\mathfrak{a}$ of $\mathfrak{g}$ of codimension 1, choosing $X$ such that $\mathfrak{g} = \mathfrak{a} + \mathbf{C} X$ and by induction $\mathfrak{a}$ has an eigenvector $v$ with corresponding weight $\lambda: \mathfrak{a} \to \mathbf{C}$. We know that $V_\lambda$ is $\mathfrak{g}$ invariant, so that there is a vector $w \in V_\lambda$ which is an eigenvector of $X$ with eigenvalue $\gamma$. We claim that this implies that $w$ is an eigenvector for all elements of $\mathfrak{g}$, because if we write $Z = \alpha X + \beta Y$, where $Y \in \mathfrak{a}$, then $Zw = (\alpha \gamma + \beta \lambda(Y))w$.
\end{proof}

Lie's theorem is proved using essentially the same techniques as Engel's theorem.

\begin{theorem}
    If $\mathfrak{g} \subset \mathfrak{gl}(V)$ is a solvable finite dimensional Lie algebra, then there is a basis for $V$ such that the matrix representations of $\mathfrak{g}$ are upper triangular.
\end{theorem}
\begin{proof}
    We choose an eigenvector $v$ for $\mathfrak{g}$, and then consider $V/W$ for $W = \mathbf{C}v$. Every element of $\mathfrak{g}$ descends to an operator on $V/W$, and by induction we can write each element as an upper triangular matrix with a certain basis $v_1 + W, \dots, v_n + W$. Passing back up, we find that each element of $\mathfrak{g}$ is upper triangular with respect to the basis $v, v_1, \dots, v_n$.
\end{proof}

If $\mathfrak{g}$ is a complex solvable Lie algebra, which is a subset of some $\mathfrak{gl}(V)$, then Lie tells us there is a basis of $V$ in which every element of $\mathfrak{g}$ is upper triangular. If $A$ and $B$ are upper triangular matrices, then
%
\[ (AB - BA)_{ii} = \sum_{j = 1}^n A_{ij}B_{ji} - B_{ij}A_{ji} = A_{ii}B_{ii} - B_{ii}A_{ii} = 0 \]
%
Hence the matrix representations of $\mathfrak{g}'$ are strictly upper triangular, and therefore nilpotent! Conversely, if $\mathfrak{g}'$ is nilpotent, then $\mathfrak{g}'$ is solvable, and $\mathfrak{g}/\mathfrak{g}'$ is abelian, hence $\mathfrak{g}$ is solvable. Thus for Lie algebras which are concretely represented as subalgebras of $\mathfrak{gl}(V)$, $\mathfrak{g}$ is solvable if and only if $\mathfrak{g}'$ is nilpotent.


\chapter{Representation Theory}

Using representation theory, we can view abstract Lie algebras as subalgebras of endomorphisms over a finite dimensional vector space. Just as groups can be represented as automorphisms over some category, reflecting their construction as the mathematical representation of symmetries, Lie algebras act on vector spaces, reflecting their construction as left-invariant vector fields on a Lie group. We define a {\bf representation} of a Lie algebra $\mathfrak{g}$ to be a homomorphism $\rho: \mathfrak{g} \to \mathfrak{gl}(V)$. The homomorphism induces an {\bf action} of $\mathfrak{g}$ on $V$, defined by $Xv = \rho(X)(v)$. Conversely, if we have an action $\mathfrak{g} \times V \to V$ of $\mathfrak{g}$ on $V$, satisfying
%
\[ (\lambda X + \gamma Y)v = \lambda Xv + \gamma Yv \]
\[ X(\mu v + \nu w) = \mu Xv + \nu Xw \]
\[ [X,Y] v = X(Yv) - Y(Xv) \]
%
Then this induces a representation of $\mathfrak{g}$ on $\mathfrak{gl}(V)$. A representation is {\bf faithful} if it is injective, and {\bf transitive} if we can map any $v \in V$ to any $w \in V$ by some $X \in \mathfrak{g}$.

\begin{example}
    We have already seen that the adjoint map $\text{ad}: \mathfrak{g} \to \mathbf{gl}(\mathfrak{g})$ is a representation of $\mathfrak{g}$. It occurs very often in the theory. The kernel of the homomorphism is the center $Z(\mathfrak{g})$.
\end{example}

A {\bf submodule} of a $\mathfrak{g}$ module $V$ is a subspace which is closed under multiplication by elements of $\mathfrak{g}$. As examples, the submodules of $\mathfrak{g}$ with the adjoint representation are exactly the ideals. Lie's theorem says that every module over a complex solvable Lie algebra has a one dimensional submodule. Given a submodule $W$, we can form the factor module $V/W$, and consider the exact sequence
%
\[ 0 \to W \to V \to V/W \to 0 \]
%
which allows us to write $V$ as a twisted product of $W$ and $V/W$. The other isomorphism theorems hold here as well.

We wish to break modules down into their simpler representations, so that the action of $\mathfrak{g}$ on the modul becomes clear. A module $V$ is called {\bf irreducible}, or {\bf simple}, if it has no submodules other than $(0)$ and $V$ itself. If $V$ is not irreducible, we can find a submodule $W$ of minimal dimension. Then $V/W$ will have an irreducible submodule, and we may proceed inductively to write any module as the twisted product of irreducible modules. The irreducible submodules essentially form building blocks for all modules. Another reason why irreducible modules are useful is that they are precisely the modules for which the action of the Lie algebra is transitive.

\begin{example}
    The only Lie algebras which are irreducible with respect to their adjoint representation are the simple Lie algebras, since the submodules of a Lie algebra correspond exactly to the ideals.
\end{example}

\begin{example}
    The only irreducible modules over the complex numbers are one dimensional, because the only vector space containing only trivial subspaces is one dimensional.
\end{example}

If we can write a module as the direct sum of other submodules, then we have said to decompose the module. A module is {\bf indecomposable} if it cannot be broken down into a direct sum. A module is {\bf completely reducible} if it can be written as the direct sum of irreducible submodules. This is the ideal situation to be in for understanding the structure of the action.

As can be expected, a module homomorphism between two $\mathfrak{g}$ modules is exactly one preserving the action of the algebra. The three isomorphism theorems continue to hold here. Homomorphisms provide the best way at determining the structure of modules, and it is natural to begin looking at homomorphisms of irreducible modules. Since the image of a module homomorphism is always a submodule of that module, these homomorphisms must either be trivial, or isomorphisms.

\begin{lemma}[Schur]
    If $\mathfrak{g}$ is a complex Lie algebra, and $V$ is a finite dimensional irreducible $\mathfrak{g}$ module, then the only module endomorphisms on $V$ are scalar multiples of the identity.
\end{lemma}
\begin{proof}
    Let $T: V \to V$ be a Lie algebra homomorphism. If $T \neq 0$, then $T$ has an eigenvector $v$, satisfying $Tv = \lambda v$. Then $T - \lambda$ is a module homomorphism mapping $v$ to zero, hence $T - \lambda = 0$, so $T = \lambda$.
\end{proof}

It follows that if $X \in Z(\mathfrak{g})$, and $V$ is an irreducible $\mathfrak{g}$ algebra, then there exists $\lambda$ such that $Xv = \lambda v$ for all $v \in V$, because $X$ acts as a module endomorphism.




\section{Representations of $\mathfrak{sl}_2(\mathbf{C})$}

Many of the ideas which occur in the general representation theory of complex Lie algebras occurs in the theory of representations of $\mathfrak{sl}_2(\mathbf{C})$. What's more, we will find that the representations of this Lie algebra control a large part of the representation theory. Recall the basis $e = E_{12}$, $f = E_{21}$, and $h = E_{11} - E_{22}$ spans the algebra. We begin by constructing a family of irreducible representations.

For each $n$, define $V_n$ to be the subspace of $\mathbf{C}[X,Y]$ generated by the homogenous polynomials with degree $n$
%
\[ X^n, X^{n-1}Y, \dots, XY^{n-1}, Y^n \]
%
$V_n$ is therefore a space of dimension $n+1$. We represent $\mathfrak{sl}_2(\mathbf{C})$ on $\mathfrak{gl}(V_n)$ with a homomorphism $\rho: \mathfrak{sl}_2(\mathbf{C}) \to \mathfrak{gl}(V_n)$ by defining
%
\[ \rho(e) = X \frac{\partial}{\partial Y} \ \ \ \ \ \ \rho(f) = Y \frac{\partial}{\partial X}\ \ \ \ \ \rho(h) = X \frac{\partial}{\partial X} - Y \frac{\partial}{\partial Y} \]
%
These are well defined operators since they carry degree $n$ monomials into degree $n$ monomials. Note that now $\rho(h)(X^aY^b) = (a - b) X^aY^b$, so that each monomial is an eigenvector of $\rho(h)$. By construction, $\rho$ is linear. To verify that brackets are preserved, we need only verify this for the basis. We calculate $[e, f] = h$, and using the relations we calculated for the Weyl algebra, we find
%
\begin{align*}
    [\rho(e),\rho(f)] &= \left[ X \frac{\partial}{\partial Y}, Y \frac{\partial}{\partial X} \right] = X \frac{\partial}{\partial Y} Y \frac{\partial}{\partial X} - Y \frac{\partial}{\partial X} X \frac{\partial}{\partial Y}\\
    &= \left(1 + Y \frac{\partial}{\partial Y} \right) \left( X \frac{\partial}{\partial X} \right) - \left( 1 + X \frac{\partial}{\partial X} \right) \left( Y \frac{\partial}{\partial Y} \right)\\
    &= X \frac{\partial}{\partial X} - Y \frac{\partial}{\partial Y} = \rho(h)
\end{align*}
%
Next, we calculate $[e, h] = -2 e$, and find
%
\begin{align*}
    [\rho(e), \rho(h)] &= \left[ X \frac{\partial}{\partial Y}, X \frac{\partial}{\partial X} - Y \frac{\partial}{\partial Y} \right]\\
    &= X^2 \frac{\partial}{\partial X} \frac{\partial}{\partial Y} - X \left(1 + X \frac{\partial}{\partial X} \right) \frac{\partial}{\partial Y}\\
    &- X \left( 1 + Y \frac{\partial}{\partial Y} \right) \frac{\partial}{\partial Y} + X Y \frac{\partial^2}{\partial Y^2}\\
    &= -2 X \frac{\partial}{\partial Y} = -2 \rho(e)
\end{align*}
%
Finally, $[f, h] = 2f$, and we can reduce the calculation of the product to the calculation above by exchanging variables, so
%
\begin{align*}
    [\rho(f), \rho(h)] &= \left[ Y \frac{\partial}{\partial X}, X \frac{\partial}{\partial X} - Y \frac{\partial}{\partial Y} \right] = - \left[ Y \frac{\partial}{\partial X}, Y \frac{\partial}{\partial Y} - X \frac{\partial}{\partial X} \right]\\
    &= 2 Y \frac{\partial}{\partial X} = 2 \rho(f)
\end{align*}
%
In the basis $X^n,X^{n-1}Y, \dots, Y^n$, we have matrix representations
%
\[ \rho(e) = \begin{pmatrix} 0 & 1 & 0 & \dots & 0 \\ 0 & 0 & 2 & \dots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \dots & n \\ 0 & 0 & 0 & \dots & 0 \end{pmatrix}\ \ \ \ \ \rho(f) = \begin{pmatrix} 0 & 0 & \dots & 0 & 0 \\ n & 0 & \dots & 0 & 0 \\ 0 & n-1 & \dots & 0 & 0 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & \dots & 1 & 0 \end{pmatrix} \]
\[ \rho(h) = \begin{pmatrix} n & 0 & \dots & 0 & 0 \\ 0 & n-2 & \dots & 0 & 0 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & \dots & 2 - n & 0 \\ 0 & 0 & \dots & 0 & -n \end{pmatrix} \]
%
It is easy to see that any submodule containing a monomial contains the entire space, since we can use the representations of the basis elements to permute the monomials around, so the submodule contains all monomials, and hence the entire space. In fact, each $V_n$ is irreducible.

\begin{theorem}
    $V_n$ is an irreducible $\mathfrak{sl}_n(\mathbf{C})$ module.
\end{theorem}
\begin{proof}
    Let $W$ be a non-zero submodule of $V_n$. An eigenvector of $\rho(h)$ lies in $W$, because $\rho(h)$ restricts to an operation on this space. But this implies that $W$ contains a monomial, since the eigenvalues of $\rho(h)$ are all distinct, and since $\rho(e)$ and $\rho(f)$ permute the monomials to the left and the right (disgarding scalars), we find $W = V_n$.
\end{proof}

It turns out that the class of $V_n$ represent all irreducible modules of $\mathfrak{sl}_2(\mathbf{C})$. The trick, given a general module $V$, is to look at the eigenvectors of $h$.

\begin{lemma}
    If $v \in V$ is an eigenvector of $h$ with eigenvalue $\lambda$, then
    %
    \begin{enumerate}
        \item[(i)] $ev = 0$, or $ev$ is an eigenvector of $h$ with eigenvalue $\lambda + 2$.
        \item[(ii)] $fv = 0$, or $fv$ is an eigenvector of $h$ with eigenvalue $\lambda - 2$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    (i) is proved by calculation,
    %
    \begin{align*}
        h(ev) &= e(\lambda v) + [h, e] v\\
        &= \lambda ev + 2ev = (\lambda + 2) E_{12} v
    \end{align*}
    %
    and essentially the same calculation shows (ii) holds as well.
\end{proof}

\begin{lemma}
    If $V$ is a finite dimensional $\mathfrak{sl}_2(\mathbf{C})$ module, then $V$ contains an eigenvector $v$ for $h$ such that $ev = 0$.
\end{lemma}
\begin{proof}
    Using the last lemma, we find that if $v$ is an eigenvector with eigenvalue $\lambda$, then either $ev = 0$, or $ev$ is an eigenvector with eigenvalue $\lambda + 2$, and since the eigenvalue is different, $ev$ is independent of $v$. Continuing this process, we either find that $e^m v$ is an eigenvector for $v$ with eigenvalue $\lambda + 2m$ for all $m$, or there is an eigenvector $w$ satisfying $ew = 0$. But if each $e^m v$ is an eigenvector, then they form an infinite family of independent vectors, contradicting finite dimensionality.
\end{proof}

\begin{theorem}
    If $V$ is a finite dimensional irreducible $\mathfrak{sl}_2(\mathbf{C})$ module, then $V$ is isomorphic to some $V_n$.
\end{theorem}
\begin{proof}
    Find an eigenvector $v$ for $h$ such that $ev = 0$, with eigenvalue $\lambda$. Consider the sequence $v, fv, f^2 v, \dots$. The last lemma essentially implies that $f^{m+1} v = 0$, $f^m v \neq 0$. We claim that $v, fv, \dots, f^m v$ form a basis for a submodule of $V$. They are certainly linearly independant, since they are eigenvectors of $h$ with different eigenvalues. It remains to show that $e(f^k v) \in \text{span}(f^l v)$, for $l \leq k$. For $k = 0$, we know $ev = 0$. For the induction, we find
    %
    \begin{align*}
        e(f^k v) &= (f e + h) (f^{k-1} v)\\
        &= f(e(f^{k-1} v)) + (\lambda + 2(k-1)) f^{k-1} v
    \end{align*}
    %
    and by induction, $e(f^{k-1} v) \in \text{span}(f^l v)$, for $l < k$, and therefore $e(f^{k-1} v) \in \text{span}(f^l v)$, for $l \leq k$.

    By irreducibility, $V$ is the span of the $f^k v$. The matrix of $h$ with respect to the basis $f^k v$ is diagonal, with trace
    %
    \[ \lambda + (\lambda - 2) + \dots + (\lambda - 2m) = (m+1) \lambda - m(m+1) \]
    %
    hence $\lambda = m$, since the image of $f$ is in the derived subgroup, and therefore has trace zero.

    We now have enough information to provide an explicit homomorphism with $V_m$. Note that $V_m$ is spanned by $X^m$, $fX^m, \dots, f^m X^m$. If we set $\psi(f^k v) = f^k X^m$ this defines a vector space isomorphism which commutes with the action of $h$ and $f$. It remains to show that $\psi$ commutes with $e$, we use induction. For $k = 0$,
    %
    \[ eX^m = 0 = \psi(ev) \]
    %
    Now by induction,
    %
    \begin{align*}
        ef^k X^m &= f e f^{k-1} X^m + h f^{k-1} X^m\\
        &= \psi(f e f^{k-1} v) + (\lambda - 2(k-1)) f^{k-1} X^m \\
        &= \psi(ef^k v) - \psi(hf^{k-1} v) + (\lambda - 2(k-1)) f^{k-1} X^m\\
        &= \psi(ef^k v) + (\lambda - 2(k-1))(f^{k-1} X^m - \psi(f^{k-1} v))\\
        &= \psi(ef^k v)
    \end{align*}
    %
    and this completes the correspondence.
\end{proof}

\begin{corollary}
    If $V$ is a finite dimensional $\mathfrak{sl}_2(\mathbf{C})$ and $v \in V$ is an eigenvector of $h$ such that $ev = 0$, then $fv = mv$ for some integer $m$, and the submodule of $V$ generated by $v$ is isomorphic to $V_m$.
\end{corollary}
\begin{proof}
    We have argued that $v, fv, \dots, f^m v$ span the submodule generated by $v$. Now we just apply irreducibility to conclude that the module generated is isomorphic to $V_m$.
\end{proof}

The vector $v$ we constructed with the largest eigenvalue is known as the {\bf heighest weight vector}. The associated eigenvalue $m$ is known as the {\bf highest weight}.

\section{Weyl's Theorem}

\begin{theorem}[Weyl]
    If $\mathfrak{g}$ is a complex, semisimple Lie algebra, then every finite dimensional module over $\mathfrak{g}$ is completely reducible.
\end{theorem}

\section{Cartan's Criterion}

From the definition, it is very difficult to verify that an algebra is semisimple. In this chapter, we develop simple methods to decide if an algebra is semisimple, or, on the other extreme, solvable. Recall that every linear operator $T$ on a finite dimensional complex can be uniquely written $T = D + N$, where $D$ is a diagonalizable operator, $N$ is a nilpotent operator, and $D$ and $N$ commute. $N = 0$ precisely when $T$ is diagonalizable, in which case we say $T$ is {\bf semisimple}, and $D = 0$ precisely when $T$ is nilpotent. The only semisimple nilpotent operator is the zero operator.

If $\mathfrak{g}$ is a solvable Lie algebra, which is a subalgebra of $\mathfrak{gl}(V)$, then we know there is a basis in which all elements of $\mathfrak{g}$ have upper triangular matrix representations, and the representations of $\mathfrak{g}'$ are strictly upper triangular. It follows that $\text{tr}(XY) = 0$ for all $X \in \mathfrak{g}$ and $Y \in \mathfrak{g}'$, because
%
\[ \sum_{i,j} X_{ij}Y_{ji} = \sum_{\substack{i \leq j\\j < i}} X_{ij} Y_{ji} = 0 \]
%
Thus we have a necessary condition for a concrete subalgebra to be solvable. Cartan found that this is essentially a sufficient condition.

\begin{theorem}
    If $\mathfrak{g}$ is a complex Lie subalgebra of $\mathfrak{gl}(V)$ such that $\text{tr}(XY) = 0$ for $X,Y \in \mathfrak{g}$ then $\mathfrak{g}$ is solvable.
\end{theorem}
\begin{proof}
    We prove that every linear operator in $\mathfrak{g}'$ is nilpotent. Engel's theorem then says that $\mathfrak{g}'$ is a nilpotent Lie algebra, and therefore $\mathfrak{g}$ is solvable. Using the Jordan decomposition, write $X \in \mathfrak{g}'$ as a sum $D + N$ of a diagonal operator and a nilpotent operator. Fix a basis where $D$ is diagonal, and $N$ is strictly upper triangular. Let $D$ have entries $\lambda_1, \dots, \lambda_n$. It will suffice to show that $\sum |\lambda_i|^2 = 0$, so that $D = 0$, and so $X$ is nilpotent. If we consider the diagonal matrix $\overline{D}$ obtained by taking the complex conjugate on the diagonal, then $\sum |\lambda_i|^2$ is just the trace of $\overline{D}X$. Write
    %
    \[ X = \gamma_1 [Y_1, Z_1] + \dots + \gamma_m [Y_m, Z_m] \]
    %
    It suffices to show that the trace of $\overline{D}[Y_i,Z_i]$ is zero. But since
    %
    \[ \text{tr}(\overline{D}(Y_iZ_i - Z_iY_i)) = \text{tr}([\overline{D}, Y_i]Z_i) \]
    %
    provided we can show that $[\overline{D},Y_i] \in \mathfrak{g}$, we can apply our hypothesis to conclude the trace is zero. Note that the Jordan decomposition of $\text{adj}_X$ is $\text{adj}_D + \text{adj}_N$, and therefore there is a polynomial $f \in \mathbf{C}[X]$ with $f(\text{adj}_X) = \overline{adj_D} = \text{adj}_{\overline{D}}$. Since $\text{adj}_X$ maps $\mathfrak{g}$ into itself, so too does $\text{adj}_{\overline{D}}$. Thus the proof is completed.
\end{proof}

Since $\mathfrak{g}$ is solvable if and only if it's image in the adjoint representation is solvable, we conclude that $\mathfrak{g}$ is solvable if and only if $\text{tr}(\text{adj}_X \circ \text{adj}_Y) = 0$ for all $X \in \mathfrak{g}$, $Y \in \mathfrak{g}'$, for this guarantees that $\mathfrak{g}'$ is solvable, and therefore $\mathfrak{g}$ is solvable as well. Because of this discussion, it appears that the symmetric, bilinear form on $\mathfrak{g}$ defined by
%
\[ \kappa(X,Y) = \text{tr}(\text{adj}_X \circ \text{adj}_Y) \]
%
is of interest. It is known as the {\bf Killing form}. Another very nice property of the Killing form is that it is in fact associative,
%
\[ \kappa([X,Y],Z) = \kappa(X,[Y,Z]) \]
%
which follows from the trace identity. The theorem above can be stated that $\mathfrak{g}$ is solvable if and only if $\kappa(X,Y) = 0$ for all $X \in \mathfrak{g}$ and $Y \in \mathfrak{g}'$. This is known as Cartan's first criterion.

\begin{example}
    Consider the two dimensional non-abelian Lie algebra with basis $X,Y$ such that $[X,Y] = X$. Then the adjoints have matrix representations
    %
    \[ \text{adj}_X = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}\ \ \ \ \ \text{adj}_Y = \begin{pmatrix} -1 & 0 \\ 0 & 0 \end{pmatrix} \]
    %
    and by looking at these representations we see $\kappa = Y^* \otimes Y^*$. The Killing form vanishes on the span of $X$, and therefore our algebra is solvable. Indeed, the derived subalgebra is abelian.
\end{example}

\begin{lemma}
    If $\mathfrak{a}$ is an ideal of $\mathfrak{g}$, then the Killing form on $\mathfrak{a}$ is the restriction of the Killing form on $\mathfrak{g}$.
\end{lemma}
\begin{proof}
    Let $\kappa$ denote the Killing from on $\mathfrak{g}$, and $\kappa_\mathfrak{a}$ the Killing form on $\mathfrak{a}$. If $X \in \mathfrak{a}$, then $\text{adj}_X: \mathfrak{g} \to \mathfrak{g}$ maps $\mathfrak{g}$ into $\mathfrak{a}$, and therefore if we take a basis $v_1, \dots, v_n$ for $\mathfrak{a}$ and extend it to a basis on $\mathfrak{g}$, the matrix representation of $\text{adj}_X$ will be
    %
    \[ \begin{pmatrix} A_X & B_X \\ 0 & 0 \end{pmatrix} \]
    %
    If we then consider $\text{adj}_Y: \mathfrak{g} \to \mathfrak{g}$, for $Y \in \mathfrak{a}$, then the matrix representation will be
    %
    \[ \begin{pmatrix} A_Y & B_Y \\ 0 & 0 \end{pmatrix} \]
    %
    and so
    %
    \[ \text{adj}_X \circ \text{adj}_Y = \begin{pmatrix} A_XA_Y & A_XB_y \\ 0 & 0 \end{pmatrix} \]
    %
    and so the trace of the restriction will agree with the trace of the adjoint on all of $\mathfrak{g}$.
\end{proof}

The Killing form not only gives us a criterion for solvability, but also a criterion for semisimplicity. As should be expected, it is essentially the opposite of saying the Killing form vanishes. We say $\kappa$ is {\bf non-degenerate} if there is no $X \in \mathfrak{g}$ such that $\kappa(X,Y) = 0$ for all $Y$.

\begin{lemma}
    A Lie algebra is semisimple if and only if it has no non-zero abelian ideals.
\end{lemma}
\begin{proof}
    If an algebra $\mathfrak{g}$ has a non-zero abelian ideal, the ideal is solvable, and thus $\text{rad}(\mathfrak{g}) \neq 0$. Note that $Z(\mathfrak{g})$ is always an ideal of $\mathfrak{g}$, because if $[X,Y] = 0$ for all $Y$, then $[X,Y] = 0 \in Z(\mathfrak{g})$. Thus in a semisimple algebra, $Z(\mathfrak{g}) = 0$. If $\mathfrak{a}$ is an ideal of $\mathfrak{g}$, then $Z(\mathfrak{a})$ is an ideal of $\mathfrak{g}$, because if $[X,Z] = 0$ for all $Z \in \mathfrak{a}$, then for any $Y$, $[Y,Z] \in \mathfrak{a}$, and hence
    %
    \[ [[X,Y],Z] = [X,[Y,Z]] + [Y,[Z,X]] = 0 + [Y,0] = 0 \]
    %
    In any solvable Lie algebra $\mathfrak{h}$, $\mathfrak{h}'$ is nilpotent, hence $\mathfrak{h}'_{k+1} = [\mathfrak{h}, \mathfrak{h}_k] = 0$ for some $k$, implying that $Z(\mathfrak{h}')$ is a non-trivial abelian subalgebra of $\mathfrak{h}$, and we have also show it is in fact an ideal of $\mathfrak{h}$.
\end{proof}

\begin{theorem}[Cartan's Second Criterion]
    $\mathfrak{g}$ is semisimple if and only if the Killing form is nondegenerate.
\end{theorem}
\begin{proof}
    The set of $X \in \mathfrak{g}$ such that $\kappa(X,Y) = 0$ for all $Y$ forms an ideal of $\mathfrak{g}$ because the Killing form is associative. Since $\kappa_\alpha = 0$, we conclude that this ideal is solvable. Thus if $\mathfrak{g}$ is semisimple, $\alpha = 0$, and so $\kappa$ is nondegenerate. Conversely, if $\mathfrak{g}$ has a non-trivial abelian ideal $\mathfrak{a}$, and if $Y \in \mathfrak{a}$ is non-zero, then $\text{adj}_Y \circ \text{adj}_X \circ \text{adj}_Y = 0$ for any $X \in \mathfrak{g}$, hence $(\text{adj}_Y \circ \text{adj}_X)^2 = 0$. Nilpotent maps have trace zero, so $\kappa(Y,X) = 0$ for all $X$, and so $\kappa$ is degenerate.
\end{proof}

These criterions are incredibly powerful to deriving structural results for Lie algebras. We will start by showing that every semisimple Lie algebra is the direct sum of simple Lie algebras (so the algebras really determine the name semisimple). Define the perpendicular $\mathfrak{h}^\perp$ of a subalgebra $\mathfrak{h}$ of a $\mathfrak{g}$ to be the perpendicular with respect to the Killing form
%
\[ \mathfrak{h}^\perp = \{ x \in \mathfrak{g} : (\forall y \in \mathfrak{h}: \kappa(y,x) = 0) \} \]
%
The killing form is non-degenerate on $\mathfrak{g}$ if $\mathfrak{g}^\perp = (0)$.

\begin{lemma}
    If $\mathfrak{g}$ is a complex semisimple Lie algebra, and $\mathfrak{a}$ is an ideal, then $\mathfrak{g} = \mathfrak{a} \oplus \mathfrak{a}^\perp$. $\mathfrak{a}$ is also a semisimple Lie algebra.
\end{lemma}
\begin{proof}
    Because $\kappa: \mathfrak{a} \cap \mathfrak{a}^\perp \to \mathfrak{a} \cap \mathfrak{a}^\perp$ is equal to zero, $\mathfrak{a} \cap \mathfrak{a}^\perp$ is a solvable subalgebra of a semisimple algebra, hence $\mathfrak{a} \cap \mathfrak{a}^\perp = (0)$, and by dimension counting we certainly have the vector space structure of $\mathfrak{g}$ as a direct sum of $\mathfrak{a}$ and $\mathfrak{a}^\perp$. If $X \in \mathfrak{a}$, and $Y \in \mathfrak{a}^\perp$, then $[X,Y] \in \mathfrak{a} \cap \mathfrak{a}^\perp$, because $\mathfrak{a}$ and $\mathfrak{a}^\perp$ are both ideals, hence $[X,Y] = 0$, and so $\mathfrak{g}$ is the direct sum of $\mathfrak{a}$ and $\mathfrak{a}^\perp$. Finally, if $\mathfrak{a}$ was not semisimple, then $\kappa_\mathfrak{a}$ would be degenerate, but this would imply the degeneracy of $\kappa$.
\end{proof}

We verify that $\mathfrak{a}^\perp$ is also an ideal of $\mathfrak{g}$, so by recursively performing this calculation, we can break any semisimple Lie algebra down into the direct sum of simple Lie algebras. Conversely, if we consider a lie algebra $\mathfrak{g} = \mathfrak{g}_1 \oplus \mathfrak{g}_2 \oplus \dots \oplus \mathfrak{g}_m$, where each $\mathfrak{g}_m$ is simple, and $\mathfrak{a} \subset \mathfrak{g}$ is a solvable ideal of $\mathfrak{g}$, then $[\mathfrak{a}, \mathfrak{g}_i] \subset \mathfrak{a} \cap \mathfrak{g}_i$ is a solvable ideal of $\mathfrak{g}_i$. But this implies $[\mathfrak{a}, \mathfrak{g}_i] = 0$, hence
%
\[ [\mathfrak{a}, \mathfrak{g}] = [\mathfrak{a}, \mathfrak{g}_1 + \mathfrak{g}_2 + \dots + \mathfrak{g}_m] = [\mathfrak{a}, \mathfrak{g}_1] + \dots + [\mathfrak{a}, \mathfrak{g}_m] = 0 \]
%
so $\mathfrak{a} \subset Z(\mathfrak{g})$. But $Z(\mathfrak{g}) = \bigoplus Z(\mathfrak{g}_i) = \bigoplus 0 = 0$, hence $\mathfrak{a} = 0$. Using very similar ideals, we can prove that the quotient of any semisimple Lie algebra is semisimple, because if $\mathfrak{a}$ is any ideal of $\mathfrak{g}$, then $\mathfrak{g} = \mathfrak{a} \oplus \mathfrak{a}^\perp$, and $\mathfrak{g}/\mathfrak{a}$ is isomorphic to $\mathfrak{a}^\perp$, which is semisimple.

\begin{theorem}
    If $\mathfrak{g}$ is a complex semisimple Lie algebra, then the adjoint representation of $\mathfrak{g}$ is an isomorphism with the space of all derivations on $\mathfrak{g}$.
\end{theorem}
\begin{proof}
    Denote the image of $\mathfrak{g}$ by $\text{adj}\ \mathfrak{g}$. $\text{adj}\ \mathfrak{g}$ is an ideal of $\text{Der} \mathfrak{g}$, for if $d$ is any derivation, then
    %
    \[ [\text{adj}_X, d](Y) = [X,dY] - d[X,Y] = -[dX,Y] = \text{adj}_{-dX}(Y) \]
    %
    Thus $\text{adj}\ \mathfrak{g}$ is an ideal on $\text{Der}\ \mathfrak{g}$. Note that this fact is true for any Lie algebra. For a complex semisimple Lie algebra, the adjoint representation is injective. The Killing form on $\text{adj}\ \mathfrak{g}$ is the restriction of the killing form on $\text{Der}\ \mathfrak{g}$. Since $\text{adj}\ \mathfrak{g}$ is semisimple, $(\text{adj}\ \mathfrak{g}) \cap (\text{adj}\ \mathfrak{g})^\perp = 0$. Thus $\text{Der}\ \mathfrak{g} = (\text{adj}\ \mathfrak{g}) \oplus (\text{adj}\ \mathfrak{g})^\perp$. This implies that if $d \in (\text{adj}\ \mathfrak{g})^\perp$, then $[d,\text{adj}_X] = \text{adj}_{dX} = 0$ for all $X$, hence $dX = 0$ for all $X$, because the center of $\mathfrak{g}$ is trivial, so $d = 0$.
\end{proof}

We can also use the Killing form and its structure of semisimple Lie algebras to classify the representations of the algebra as operators. An important property of the semisimple Lie algebras is that the `Jordan decomposition' is invariant of the representation. Note that in general, the Jordan decomposition of a representation can be fairly arbitrary. For instance, the representations of $\mathbf{C}$ over some vector space $V$ are obtained my mapping $1$ to an arbitrary element of $V$, so that the Jordan decomposition for elements of $\mathbf{C}$ take the form of arbitrary matrices.

\begin{lemma}
    If $\mathfrak{g}$ is a complex Lie algebra, and $D \in \text{Der}(\mathfrak{g})$ has Jordan decomposition $D = A + N$, then $A$ and $N$ are both derivations on $\mathfrak{g}$.
\end{lemma}
\begin{proof}
    For each $\lambda \in \mathbf{C}$, let $\mathfrak{g}_\lambda$ be the set of $X$ such that $(D - \lambda)^mX = 0$ for some $m$. Then the vector space structure of $\mathfrak{g}$ decomposes into the sum of the subspaces $\mathfrak{g}_\lambda$. We have $[\mathfrak{g}_\lambda, \mathfrak{g}_\gamma] \subset \mathfrak{g}_{\lambda + \gamma}$, because
    %
    \[ (D - (\lambda + \gamma))^m(XY - YX) = \sum {m \choose k} [(D - \lambda)^k X, (D - \gamma)^{m-k} Y] \]
    %
    Since $A$ is diagonalizable, the $\lambda$ eigenspace for $A$ is $\mathfrak{g}_\lambda$. If $X \in \mathfrak{g}_\lambda$ and $Y \in \mathfrak{g}_\gamma$, then $[X,Y] \in \mathfrak{g}_{\lambda + \gamma}$, hence
    %
    \[ A[X,Y] = (\lambda + \gamma)[X,Y] = [AX, Y] + [X,AY] \]
    %
    hence $A$ is a derivation. $N$ is then a derivation, because it is the difference of two derivations.
\end{proof}

\begin{theorem}
    If $\mathfrak{g}$ is semisimple, then any $X \in \mathfrak{g}$ can be written uniquely as $D + N$, where $\text{adj}_D$ is diagonalizable, $\text{adj}_N$ is nilpotent, and $[D,N] = 0$. If $Y$ commutes with $X$, then $[D,Y] = [N,Y] = 0$.
\end{theorem}
\begin{proof}
    We apply the last theorem to $\mathfrak{g}$, viewed as $\text{adj}\ \mathfrak{g}$ by an isomorphism. Given $X$, we have the Jordan decomposition $\text{adj}_X = \text{adj}_D + \text{adj}_N$ for some $D,N \in \mathfrak{g}$, and hence $X = D + N$. Since $\text{adj}_D$ and $\text{adj}_N$ commute, $[D,N] = 0$. If $Y$ commutes with $X$, then $\text{adj}_X(Y) = 0$, then because $\text{adj}_D$ and $\text{adj}_N$ are polynomials in $\text{adj}_X$, we see
    %
    \[ \text{adj}_N = \sum c_k \text{adj}^k_X \]
    %
    hence $\text{adj}_N(Y) = c_0$. But since $\text{adj}_N$ is nilpotent, $c_0 = 0$.
\end{proof}

The unique decomposition $X = D + N$ into semisimple and nilpotent elements is known as the {\bf abstract Jordan decomposition} of $X$.

\begin{theorem}
    If $\rho: \mathfrak{g} \to \mathfrak{gl}(V)$ is a representation of a complex semisimple Lie algebra, and $X \in \mathfrak{g}$ has a Jordan decomposition $D + N$, then the (concrete) Jordan decomposition of $\rho(X)$ is $\rho(D) + \rho(N)$.
\end{theorem}
\begin{proof}
    The image of any representation of a complex semisimple Lie algebra is semisimple, since if $\mathfrak{h}$ is the kernel of $\rho$, then $\mathfrak{g}/\mathfrak{h}$ is isomorphic to $\rho(\mathfrak{g})$. We can therefore talk about the abstract Jordan decomposition of elements of $\rho(\mathfrak{g})$. The concrete decomposition of $\rho(X)$ agrees with the asbtract decomposition, because if $Y$ is nilpotent/diagonalizable it implies $\text{adj}_Y$ is as well. If $\text{adj}_D$ is diagonalizable, then it is diagonalizable on $\mathfrak{h}$ by some basis $X_1, \dots, X_n$, and if we extend this basis of eigenvectors to the whole space, $X_1, \dots, X_n, Y_1, \dots, Y_m$, then, viewed as a map on the quotient $\mathfrak{g}/\mathfrak{h}$, we have $[D, Y_i + \mathfrak{h}] = \lambda_i Y_i + \mathfrak{h}$. If $\text{adj}_N$ is nilpotent, then $\text{adj}_N$ is nilpotent on the quotient as well. This verifies that $\rho(X)$ has abstract Jordan decomposition $\rho(D) + \rho(N)$, because the decomposition is unique.
\end{proof}








\chapter{Root Systems}

Recall the proof that $\mathfrak{sl}_2(\mathbf{C})$ is the only simple 3 dimensional Lie algebra.
%
\begin{enumerate}
    \item We found $X \in \mathfrak{sl}_2(\mathbf{C})$ such that $\text{adj}_X$ is diagonalizable.
    \item We took a basis of $\mathfrak{sl}_2(\mathbf{C})$ consisting of eigenvectors for $\text{adj}_X$, and showed that the corresponding structural constants
\end{enumerate}
%
To classify the semisimple Lie algebras, we will utilize a very important generalization of this technique. In order to apply this technique to higher dimensional Lie algebras, we will need to generalize the first step to finding a sufficiently large simultaneously diagonalizable subalgebra of the Lie algebra. This subalgebra is known as the Cartan subalgebra.

Let us try and generalize the technique to $\mathfrak{sl}_n(\mathbf{C})$. Consider the subalgebra $\mathfrak{h}$ of $\mathfrak{sl}_n(\mathbf{C})$ consisting of the diagonal matrices. Since
%
\[ [E_{ij}, E_{kl}] = \delta_j^k E_{il} - \delta_l^i E_{kj} \]
%
We find that for any diagonal matrix $X = (\sum a_i E_{ii})$, if $m \neq n$, then
%
\[ [X,E_{mn}] = \sum a_i [E_{ii}, E_{mn}] = (a_m - a_n) E_{mn} \]
%
Since the diagonal matrices form an abelian subalgebra of $\mathfrak{sl}_n(\mathbf{C})$, we can extend the $E_{ij}$ to a full basis diagonalizing all diagonal matrices. In particular, we see the weights for this action are precisely the maps $\varepsilon_{ij}(M) = M_{ii} - M_{jj}$. The weight space for $\varepsilon_{ij}$ for $i \neq j$ corresponds precisely to the span of $e_{ij}$, so the space is 1 dimensional, and for $i = j$, the weight space is precisely the space of diagonal matrices, which is $n - 1$ dimensional. Since the weight spaces form $\mathfrak{h}$-invariant subspaces, we have a module decomposition
%
\[ \mathfrak{sl}_n(\mathbf{C}) = \mathfrak{h} \oplus \bigoplus_{i \neq j} \mathbf{C} e_{ij} \]
%
which effectively characterizes the action of the adjoint maps from $\mathfrak{h}$.

It seems that the adjoint representation abelian subalgebras are very useful for decomposing Lie algebras. Indeed, let $\mathfrak{h}$ be an arbitrary abelian subalgebra of $\mathfrak{g}$. If $X, Y \in \mathfrak{h}$, then $\text{adj}_X$ and $\text{adj}_Y$ are commutative, because
%
\[ [X,[Y,Z]] = [[X,Y],Z] + [Y,[X,Z]] = [Y,[X,Z]] \]
%
hence $\text{adj}_X \circ \text{adj}_Y = \text{adj}_Y \circ \text{adj}_X$. Since each $\text{adj}_X$ is diagonalizable, the elements of $\mathfrak{h}$ are {\it simultaneously diagonalizable} -- there is a basis of $\mathfrak{g}$ consisting of eigenvectors. We thus have a decomposition of $\mathfrak{g}$ as
%
\[ \mathfrak{g} = \mathfrak{h} \oplus \bigoplus_{\lambda \neq 0} \mathfrak{g}_\lambda \]
%
where $\lambda$ ranges over the set of weights for $\mathfrak{h}$, and the $\mathfrak{g}_\lambda$ are $\mathfrak{h}$ invariant under the Lie bracket.

\begin{theorem}
    If $X \in \mathfrak{g}_\lambda$ and $Y \in \mathfrak{g}_\gamma$, then $[X,Y] \in \mathfrak{g}_{\lambda + \gamma}$.
\end{theorem}
\begin{proof}
    Using the Jacobi identity, we find that for any $Z \in \mathfrak{h}$,
    %
    \begin{align*}
        [Z,[X,Y]] &= [[Z,X],Y] + [X,[Z,Y]]\\
        &= \lambda(Z)[X,Y] - \gamma(Z)[Y,X]\\
        &= (\lambda(Z) + \gamma(Z))[X,Y]
    \end{align*}
    %
    In particular, if $\lambda + \gamma$ is not a root for $\mathfrak{h}$, then $X$ and $Y$ commute.
\end{proof}

\begin{theorem}
    If $\lambda + \gamma \neq 0$, and $X \in \mathfrak{g}_\lambda$, $Y \in \mathfrak{g}_\gamma$, then $\kappa(X,Y) = 0$.
\end{theorem}
\begin{proof}
    If $Z \in \mathfrak{h}$, then using the Jacobi identity, we find using linearity and commutativity of $\kappa$ that
    %
    \[ \lambda(Z) \kappa(X,Y) = \kappa([Z,X],Y) = -\kappa(X,[Z,Y]) = - \gamma(Z) \kappa(X,Y) \]
    %
    subtracting one side of this equation from the other, we find that $(\lambda + \gamma)(Z) \kappa(X,Y) = 0$. If $(\lambda + \gamma)(Z) \neq 0$, then $\kappa(X,Y) = 0$, and we can always choose $Z$ such that $(\lambda + \gamma)(Z) \neq 0$.
\end{proof}

This shows that $\kappa$ is non-degenerate on $\mathfrak{h}$, because if there was $X \in \mathfrak{g}_0$ such that $\kappa(X,Y) = 0$ for all $Y \in \mathfrak{g}_0$, then for any $Z \in \mathfrak{g}$, we may write $Z = \sum Z_\lambda$, and then $\kappa(X,Z) = \kappa(X,Z_0) = 0$, so that $X$ annihilates all elements of $\mathfrak{g}$, and thus $X = 0$. It follows that $\mathfrak{h}$ is a semisimple Lie algebra.

In general, our aim will be to identify an abelian subalgebra of semisimple elements of every semisimple Lie algebra, in which case we can find a structural decomposition of the algebra. If the abelian subalgebra is too small, the weight decomposition is likely to be too coarse, and the results about orthogonality on $\kappa$ and how the bracket operates on weights. It turns out that every semisimple Lie algebra has a maximal abelian subalgebra containing semisimple elements, called a {\bf Cartan subalgebra}. This algebra has the property that the Lie algebra elements of weight zero correspond precisely to elements of the Cartan subalgebra, so that the weight decomposition is very tight.

\begin{example}
    If $\mathfrak{h}$ is the Lie subalgebra of $\mathfrak{sl}_n(\mathbf{C})$ spanned by $h$, then $\mathfrak{g}_0$ consists precisely of the matrices $X$ with $X_{1n} = X_{n1} = 0$ for $n \neq 1$, and $X_{2n} = X_{2n} = 0$ for $n \neq 2$, so $\mathfrak{g}_0 = \mathfrak{h}$ precisely when $n = 2$. The $\mathfrak{g}_0$ has dimension $n^2 - 1 - 2(n-1) - 2(n-2) = n^2 - 4n + 5$, so the decomposition is very coarse for $n > 2$.
\end{example}

\begin{theorem}
    Cartan subalgebras exist on semisimple complex Lie algebras.
\end{theorem}
\begin{proof}
    First, we note that semisimple elements must exist on a semisimple Lie algebra $\mathfrak{g}$. We can write any $X \in \mathfrak{g}$ uniquely as the sum of a semisimple element and a nilpotent element, so if a semisimple Lie algebra contains no semisimple elements, every element of the algebra is nilpotent, and therefore $\mathfrak{g}$ is nilpotent. It follow that $\mathfrak{g}$ is solvable, yet no semisimple Lie algebras are solvable. If $X$ is a semisimple element, then the span of $X$ is abelian and semisimple, and by finite dimensionality we may enlarge this algebra to be maximum abelian and semisimple, in which case the algebra obtained is a Cartan subalgebra.
\end{proof}

If $S \subset \mathfrak{g}$, then we define the {\bf centralizer} of $S$ with respect to $\mathfrak{g}$, denoted $C_\mathfrak{g}(S)$, to be the set of all $X \in \mathfrak{g}$ such that $[X,Y] = 0$ for all $Y \in S$. The centralizer of Cartan subalgebra combined with the maximality of the Cartan algebra will turn out to be very useful. The centralizer is always a subalgebra of $\mathfrak{g}$, because if $X,Y \in C_\mathfrak{g}(S)$, and $Z \in S$, then $[[X,Y],Z] = [X,[Y,Z]] + [[X,Z],Y] = 0 + 0 = 0$. Using our previous notation, we have $Z(\mathfrak{g}) = C_\mathfrak{g}(\mathfrak{g})$.

\begin{lemma}
    If $\mathfrak{h}$ is a Cartan subalgebra of $\mathfrak{g}$. If $X \in \mathfrak{h}$ is chosen such that $C_\mathfrak{g}(X)$ is minimized, then $\mathfrak{h} \subset Z(C_\mathfrak{g}(X))$, so $C_\mathfrak{g}(X) = C_\mathfrak{g}(\mathfrak{h})$.
\end{lemma}
\begin{proof}
    We claim that if $Y \in C_\mathfrak{g}(X) \cap \mathfrak{h}$ is not in $Z(C_\mathfrak{g}(X))$, then there is some element of $\mathfrak{h}$ of the form $aX + bY$ whose centralizer has smaller dimension than $X$. First, consider some basis $\{ Z_1, \dots, Z_k \}$ on $C_\mathfrak{g}(X) \cap C_\mathfrak{g}(Y)$. Extend this basis with vectors $\{ Z_1', \dots, Z_n' \}$ on $C_\mathfrak{g}(X)$ to diagonalize $\text{adj}_Y$, and consider an alternate extension of $\{ Z_1'', \dots, Z_m'' \}$ which diagonalize $\text{adj}_X$ on $C_\mathfrak{g}(Y)$. Then the triple of basis are linearly independant, for if
    %
    \[ \sum a_i Z_i + \sum b_i Z_i' + \sum c_i Z_i'' = 0 \]
    %
    Then for some $\lambda_i$
    %
    \[ \left[ X, \sum a_i Z_i + \sum b_i Z_i' + \sum c_i Z_i'' \right] = \sum c_i \lambda_i Z_i'' \]
    %
    Hence $c_i \lambda_i = 0$, and by assumption $Z_i'' \not \in C_\mathfrak{g}(X)$, hence $\lambda_i \neq 0$, hence $c_i = 0$ for all $i$. Simlarily, considering the bracket with $Y$, we find $b_i = 0$, and then $a_i = 0$ as well. Thus the triple is a basis for $C_\mathfrak{g}(X) + C_\mathfrak{g}(Y)$. Finally, since $X$ and $Y$ commute, we can simultaneously diagonalize $X$ and $Y$, so we consider a final set of independent elements $\{ Z_1''', \dots, Z_l''' \}$ such that the quadraple forms a basis for $\mathfrak{h}$, and simultaneously diagonalizes $X$ and $Y$.

    If $[Y,Z_i'] = \lambda_i$, $[X,Z_j''] = \gamma_j$, $[X,Z_k'''] = \sigma_k$, and $[Y,Z_k'''] = \nu_k$, then all $\lambda_i$ $\gamma_j$, $\sigma_k$ and $\nu_k$ are non-zero, and if we choose a non-zero $\mu$ such that $\mu \neq - \nu_i/\sigma_i$ for any $i$, we find
    %
    \begin{align*}
        &\left[ Y + \mu X, \sum a_i Z_i + \sum b_i Z_i' + \sum c_i Z_i'' + \sum d_i Z_i''' \right]\\
        &\ \ \ \ \ = \sum b_i \lambda_i Z_i' + \mu \sum c_i \gamma_i Z_i''' + \sum d_i \left( \nu_i + \mu \sigma_i \right) Z_i'''
    \end{align*}
    %
    Hence $C_\mathfrak{g}(Y + \mu X) = C_\mathfrak{g}(X) \cap C_\mathfrak{g}(Y)$, a strictly smaller set than $C_\mathfrak{g}(X)$ because $Y \in C_\mathfrak{g}(Y)$, $Y \not \in C_\mathfrak{g}(X)$.
\end{proof}

\begin{theorem}
    If $\mathfrak{h}$ is a Cartan subalgebra, and $X \in \mathfrak{h}$ satisfies $C_\mathfrak{g}(X) = C_\mathfrak{g}(\mathfrak{h})$, then $C_\mathfrak{g}(X) = \mathfrak{h}$. Thus $\mathfrak{h}$ is self-centralizing.
\end{theorem}
\begin{proof}
    Certainly $\mathfrak{h} \subset C_\mathfrak{g}(X)$. If $Y \in C_\mathfrak{g}(X)$, consider the Jordan decomposition $Y = S + N$. Since $X$ commutes with $Y$, $S$ and $N$ both commute with $X$. Thus it suffices to show that $N = 0$, in which case $Y$ is semisimple, and therefore lies in $\mathfrak{h}$. We already know $S \in \mathfrak{h}$ by maximality.

    We claim the only nilpotent element in $C_\mathfrak{g}(\mathfrak{h})$ is zero. First, we claim that $C_\mathfrak{g}(X)$ is nilpotent. If we take any $Y \in C_\mathfrak{g}(X)$, and write $Y = S + N$, then $\text{adj}_Y$ is equal to $\text{adj}_N$ on $C_\mathfrak{g}(X)$, because $S \in \mathfrak{h} = C_\mathfrak{g}(X)$. This implies $\text{adj}_Y$ is nilpotent, hence $Y$ is nilpotent, and since $Y$ was arbitrary we use Engel's theorem to conclude that $C_\mathfrak{g}(\mathfrak{h})$ is nilpotent.

    Next, we claim that every element of $C_\mathfrak{g}(\mathfrak{h})$ is semisimple. If $Y \in C_\mathfrak{g}(\mathfrak{h})$, write $Y = S + N$. $C_\mathfrak{g}(\mathfrak{h})$ is nilpotent, hence solvable, so by Lie's theorem there is a basis for $\mathfrak{g}$ in which case $\text{adj}_Y$ is represented by an upper triangular matrix. Since $\text{adj}_N$ is nilpotent, the matrix is strictly upper triangular, hence if $Z \in C_\mathfrak{g}(X)$,
    %
    \[ \kappa(N,Z) = \text{tr}(\text{adj}_N \circ \text{adj}_Z) = 0 \]
    %
    But we know that $\kappa$ is non-degenerate on $C_\mathfrak{g}(\mathfrak{h})$, because it contains $\mathfrak{h}$, so $N = 0$.
\end{proof}

Thus a general semisimple Lie algebra $\mathfrak{g}$ has a Cartan subalgebra $\mathfrak{h}$, and since the eigenvector of $\mathfrak{h}$ of weight zero are precisely $C_\mathfrak{g}(\mathfrak{h})$, and we have shown $\mathfrak{h} = C_\mathfrak{g}(\mathfrak{h})$, we may write $\mathfrak{g}$ via the {\bf root space decomposition}
%
\[ \mathfrak{g} = \mathfrak{h} \oplus \bigoplus_{\alpha \in \Phi} \mathfrak{g}_\alpha \]
%
where $\Phi$ is the set of roots of $\mathfrak{h}$, the set of non-zero weights. The roots and weights depend on the Cartan subalgebra $\mathfrak{h}$ we choose, but for the classical algebras the Cartan subalgebra is canonical -- it is simply the diagonal matrices in the algebra. We shall assume that a particular Cartan subalgebra $\mathfrak{h}$ has been fixed over the space, so we can discuss the roots of $\mathfrak{g}$ without ambiguity.

\section{Subalgebras isomorphic to $\mathfrak{sl}_2(\mathbf{C})$}

It turns out that semisimple Lie algebras contain an abundance of subalgebras isomorphic to $\mathfrak{sl}_2(\mathbf{C})$, which enables us to bring the representation theory we have developed for $\mathfrak{sl}_2(\mathbf{C})$ to obtain structural results for all semisimple Lie algebras.

\begin{lemma}
    If $\alpha$ is a non-zero root, then $-\alpha$ is a non-zero root, and for every non-zero $X \in \mathfrak{g}_\alpha$ there is $Y \in \mathfrak{g}_{-\alpha}$ such that $\text{span}(X,Y,[X,Y])$ is a Lie subalgebra of $\mathfrak{g}$ isomorphic to $\mathfrak{sl}_2(\mathbf{C})$.
\end{lemma}
\begin{proof}
    Fixing some $X \in \mathfrak{g}_\alpha$, the non-degeneracy of $\kappa$ implies that there is some $Z \in \mathfrak{g}$ with $\kappa(X,Z) \neq 0$. If we let $Z = \sum Z_\alpha$, then this implies that $Z_{-\alpha} \neq 0$, hence $-\alpha$ is a root of $\mathfrak{h}$. Let $Y = Z_{-\alpha}$. Then $[X,Y]$ is a weight zero element, hence $[X,Y] \in \mathfrak{h}$. Since $\alpha \neq 0$, find $K \in \mathfrak{h}$ with $\alpha(K) \neq 0$, and then
    %
    \[ \kappa(K,[X,Y]) = \kappa([K,Y],X) = \alpha(K) \kappa(Y,X) \neq 0 \]
    %
    hence $[X,Y] \neq 0$. The span of $X, Y$, and $[X,Y]$ is therefore a 3-dimensional subalgebra $\mathfrak{k}$ of $\mathfrak{g}$. We claim that $\alpha [X,Y] \neq 0$, which would imply $\mathfrak{k}' = \mathfrak{k}$, and therefore that $\mathfrak{k}$ is isomorphic to $\mathfrak{sl}_2(\mathbf{C})$. If $\alpha [X,Y] = 0$. then $[X,[X,Y]] = [Y,[X,Y]] = 0$. Then $X$ and $Y$ commute with $[X,Y]$, so $[X,Y]$ is nilpotent. But then $[X,Y]$ is both nilpotent and semisimple, which can only occur if $[X,Y] = 0$, which we know not to be the case.
\end{proof}

We let $\mathfrak{sl}(\alpha)$ denote the subalgebra obtained in the lemma. As we now know, $\mathfrak{sl}(\alpha)$ might not be unique, but we will soon show it is, because each weight space is one dimensional. We let $e_\alpha \in \mathfrak{g}_\alpha$, $f_\alpha \in \mathfrak{g}_{-\alpha}$, and $h_\alpha \in \mathfrak{g}$ be elements of $\mathfrak{sl}(\alpha)$ which map to $e,f$, and $h$ in the isomorphism with $\mathfrak{sl}_2(\mathbf{C})$.

The Killing form is non-degenerate, and it therefore gives a canonical isomorphism of $\mathfrak{h}$ with $\mathfrak{h}^*$, mapping $X$ to $\langle X | \in \mathfrak{h}^*$ defined by $\langle X| (Y) = k(X,Y)$. In particular, for each $\alpha$ there is $t_\alpha \in \mathfrak{h}$ with $\kappa(t_\alpha,X) = \alpha(X)$ for all $X \in \mathfrak{h}$.

\begin{lemma}
    If $\alpha$ is a root, let $X \in \mathfrak{g}_\alpha$ and $Y \in \mathfrak{g}_{-\alpha}$. Then $[X,Y] = \kappa(X,Y) t_\alpha$. In particular, $h_\alpha = [e_\alpha, f_\alpha] \in \text{span}(t_\alpha)$.
\end{lemma}
\begin{proof}
    If $Z \in \mathfrak{h}$, then
    %
    \[ \kappa(Z,[X,Y]) = \kappa([Z,X],Y) = \alpha(Z) \kappa(X,Y) = \kappa(Z, \kappa(X,Y) t_\alpha) \]
    %
    Since $\kappa$ is non-degenerate on $\mathfrak{h}$, $[X,Y] = \kappa(X,Y) t_\alpha$.
\end{proof}

Here is where all our work comes to fruition. Since any semisimple Lie algebra $\mathfrak{g}$ contains some $\mathfrak{sl}(\alpha)$, we can view $\mathfrak{g}$ as a $\mathfrak{sl}_2(\mathbf{C})$ module by first considering the isomorphism of $\mathfrak{sl}_2(\mathbf{C})$ with $\mathfrak{sl}(\alpha)$, and then taking the adjoint representation. The submodules of $\mathfrak{g}$ with respect to this representation are precisely the vector subspaces $V$ with $[X,Y] \in V$ for all $X \in \mathfrak{sl}_2(\mathbf{C})$ and $Y \in V$.

\begin{lemma}
    If $V$ is a $\mathfrak{sl}(\alpha)$ submodule of $\mathfrak{g}$, then the eigenvalues of $h_\alpha$ acting on $V$ are integers.
\end{lemma}
\begin{proof}
    This follows from the classification of modules over $\mathfrak{sl}_2(\mathbf{C})$.
\end{proof}

\begin{example}
    The Cartan subalgebra of $\mathbf{sl}_3(\mathbf{C})$ consists of all diagonal matrices with trace zero. The weights are $\varepsilon_{ij}(X) = X_{ii} - X_{jj}$, for $i \neq j$. Let $\alpha = \varepsilon_{12}$. An eigenvector for $\alpha$ DO LATER.
\end{example}

\begin{example}
    Let $\mathfrak{u} = \mathfrak{h} + \mathfrak{sl}(\alpha)$. Let $\mathfrak{k} = \ker \alpha \subset \mathfrak{h}$. Then $\mathfrak{k}$ has codimension 1 in $\mathfrak{h}$. As $\mathfrak{h}$ is abelian, $[h_\alpha,X] = 0$ for all $X \in \mathfrak{k}$, and
    %
    \[ [e_\alpha, X] = - [X, e_\alpha] = -\alpha(X) e_\alpha = 0 \]
    %
    and similarily $[f_\alpha, X] = 0$. Thus every element of $\mathfrak{sl}(\alpha)$ acts trivially on $\mathfrak{k}$, and we can decompose $\mathfrak{u}$ into the direct sum of $\mathfrak{k}$ and $\mathfrak{sl}(\alpha)$. $\mathfrak{sl}(\alpha)$ is isomorphic as an $\mathfrak{sl}(\alpha)$ module to $V_2$, so $\mathfrak{u}$ is isomorphic to the direct sum of $\dim \mathfrak{h} - 1$ copies of the trivial representation $V_0$, and one copy of $V_2$.
\end{example}

\begin{example}
    If $\beta \in \Phi$, or $\beta = 0$, let
    %
    \[ V = \bigoplus_{z \in \mathbf{C}} \mathfrak{g}_{z\alpha + \beta} \]
    %
    Then $V$ is a $\mathfrak{sl}(\alpha)$ submodule of $\mathfrak{g}$, known as the {\bf $\alpha$-root string through $\beta$}, because if $X \in \mathfrak{g}_{z\alpha + \beta}$, then $[e_\alpha, X] \in \mathfrak{g}_{(z + 1)\alpha + \beta}$, $[f_\alpha, X] \in \mathfrak{g}_{(z-1)\alpha + \beta}$, and $[h_\alpha,X] = (z\alpha + \beta)(h_\alpha) X \in \mathfrak{g}_{z\alpha + \beta}$.
\end{example}

\begin{theorem}
    The root spaces of any non-zero weight $\alpha$ on a semisimple Lie algebra are one-dimensional, and the only multiples of $\alpha$ which lie in $\Phi$ are $\pm \alpha$.
\end{theorem}
\begin{proof}
    If $z\alpha$ is a root, then $h_\alpha$ has $z\alpha(h_\alpha) = 2z$ as an eigenvalue. As the eigenvalues of $h_\alpha$ are integral, $2z \in \mathbf{Z}$. Consider the root string module
    %
    \[ V = \mathfrak{h} \oplus \bigoplus_{n \neq 0} \mathfrak{g}_{(n/2)\alpha} \]
    %
    Let $\mathfrak{k} = \ker \alpha \subset \mathfrak{h}$. Then $\mathfrak{k} \oplus \mathfrak{sl}(\alpha)$ is a $\mathfrak{sl}(\alpha)$ submodule of $V$ containing $\mathfrak{h}$. Since modules over $\mathfrak{sl}(\alpha)$ are completely reducible, we can find a complementary submodule $W$ such that $V = \mathfrak{k} \oplus \mathfrak{sl}(\alpha) \oplus W$. If the conclusion of this theorem was false, then we would find $W \neq 0$, hence it suffices to prove that $W = 0$. Note that the eigenvectors of $h_\alpha$ on $V$ of eigenvalue zero are exactly the elements of $\mathfrak{k}$, so $W$ cannot contain any eigenvectors of eigenvalue zero, thus $W$ cannot contain any irreducible submodule isomorphism to $V_n$, where $n$ is even. This already gives us an interesting consequence. If $2\alpha \in \Phi$, then $h_\alpha$ has $(2\alpha)(h_\alpha) = 4$, hence $V$ has an eigenvector of eigenvalue 4. But every element of $\mathfrak{k} \oplus \mathfrak{sl}(\alpha)$ has eigenvalue $0$ and $\pm 2$, which would imply $W$ contains a submodule isomorphic to some $V_n$ with $n$ even. Thus if $\alpha$ is a root, $2 \alpha$ is never a root. Finally, assume $W$ has a submodule isomorphic to $V_n$ with $n$ odd. Then $V$ contains a $h_\alpha$ eigenvector $X$ with eigenvalue 1, which must be contained in $W$. Thus there is a root $\beta$ with $\beta(h_\alpha) = 1$, but then $2\beta = \alpha$, since $2\beta$ and $\alpha$ agree at $h_\alpha$, and we have just verified this we cannot obtain a root by doubling another root. Thus $W$ has no irreducible submodules, and as such $W = 0$.
\end{proof}

\begin{theorem}
    Let $\alpha, \beta \in \Phi$, with $\beta \neq \pm \alpha$.
    %
    \begin{enumerate}
        \item[(i)] $\beta(h_\alpha) \in \mathbf{Z}$.
        \item[(ii)] There are positive integers $a,b \geq 0$ such that $\beta + k\alpha \in \Phi$ if and only if $k \in \mathbf{Z}$ and $-a \leq k \leq b$, and $a - b = \beta(h_\alpha)$.
        \item[(iii)] If $\alpha + \beta \in \Phi$, then $[e_\alpha, e_\beta]$ is a non-zero scalar multiple of $e_{\alpha + \beta}$.
        \item[(iv)] $\beta - \beta(h_\alpha)\alpha \in \Phi$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    Note that $\beta(h_\alpha)$ is the eigenvalue of $h_\alpha$ on some element of $\mathfrak{g}_\beta$, hence it must be an integer by the classification of $\mathfrak{sl}_2(\mathbf{C})$ representations. If we consider the root string $V$, which is the direct sum of $\mathfrak{g}_{\beta + k \alpha}$ where $k$ is chosen such that $\beta + k \alpha$ is a root, then we have $(\beta + k \alpha)(h_\alpha) = \beta(h_\alpha) + 2k$, thus the eigenvalues of $h_\alpha$ on the root string are either all even or all odd, and we therefore find that the root string is an irreducible $\mathfrak{sl}(\alpha)$ module, hence $V$ is isomorphic to some $V_n$. On $V_n$, $h_\alpha$ acts diagonally with eigenvalues $n,n-2\dots,-(n-2),-n$, and this must be paired up with $\beta(h_\alpha) + 2k$. Thus we may let $-n = \beta(h_\alpha) - 2a$, $n = \beta(h_\alpha) + 2b$, in which case $a - b = \beta(h_\alpha)$. This proves (ii). (iv) essentially follows from (ii) in the same manner. If $X \in \mathfrak{g}_\beta$, then $X$ belongs to the $h_\alpha$ eigenspace of eigenvalue $\beta(h_\alpha)$, and if $[e_\alpha, e_\beta] = 0$, then $e_\beta$ is the highest weight vector in the irreducible representation $V$ isomorphic to $V_n$. If $\alpha + \beta$ is a root, then $h_\alpha$ has eigenvalue $(\alpha + \beta)(h_\alpha) = 2 + \beta(h_\alpha)$, hence $e_\beta$ is not the highest weight vector, hence $[e_\alpha, e_\beta] \neq 0$, and this proves (iii).
\end{proof}

Thus we find that the roots of a Lie algebra essentially give us all the needed structural constants between brackets of the weight space decomposition. It determines the brackets $[e_\alpha, e_\beta]$ for up to a scalar constant, and tells us that $[e_\alpha, e_{-\alpha}]$ is in the span of $h_\alpha$.




\section{Cartan Subalgebras as Inner Product Spaces}

\begin{lemma}
    If $X \in \mathfrak{h}$ is non-zero, then there is a root $\alpha$ with $\alpha(X) \neq 0$, and therefore the roots span $\mathfrak{h}^*$.
\end{lemma}
\begin{proof}
    If $\alpha(X) = 0$ for all roots $\alpha$, then for any $Y \in \mathfrak{g}$, we can write $Y = Y_0 + \sum_{\alpha \in \Phi} Y_\alpha$, then $[X,Y] = \sum \alpha(X) Y = 0$, hence $X \in Z(\mathfrak{g}) = 0$, hence $X = 0$ by semisimplicity. If $V$ is the span of the roots on $\mathfrak{h}^*$, and if $V \neq \mathfrak{h}^*$, then the annihilator $W^\circ = \{ X \in \mathfrak{h} : (\forall \lambda \in V: \lambda(X) = 0) \}$ has non-zero dimension, which we have proved is impossible.
\end{proof}

\begin{lemma}
    For any $\alpha \in \Phi$,
    %
    \[ t_\alpha = \frac{h_\alpha}{\kappa(e_\alpha, f_\alpha)}\ \ \ \ \ \ h_\alpha = \frac{2t_\alpha}{\kappa(t_\alpha, t_\alpha)}\ \ \ \ \ \ \kappa(t_\alpha, t_\alpha) \kappa(h_\alpha, h_\alpha) = 4 \]
\end{lemma}
\begin{proof}
    We obtain the formula for $t_\alpha$ by multiplying by $\kappa(e_\alpha, f_\alpha)$ on both sides of the equation
    %
    \[ h_\alpha = [e_\alpha, f_\alpha] = \kappa(e_\alpha, f_\alpha) t_\alpha \]
    %
    which we have already proved. Now $\alpha(h_\alpha) = 2$, hence
    %
    \[ 2 = \kappa(t_\alpha,h_\alpha) = \kappa(t_\alpha, \kappa(e_\alpha,f_\alpha)t_\alpha) \]
    %
    The second formula then follows by substitution. Using this formula, we find
    %
    \[ \kappa (h_\alpha, h_\alpha) = \kappa \left( \frac{2t_\alpha}{\kappa(t_\alpha,t_\alpha)}, \frac{2t_\alpha}{\kappa(t_\alpha,t_\alpha)} \right) = \frac{4}{\kappa(t_\alpha, t_\alpha)} \]
\end{proof}

As a corollary, we find that $\kappa(h_\alpha, h_\beta) \in \mathbf{Z}$ and $\kappa(t_\alpha, t_\beta) \in \mathbf{Q}$ for all $\alpha, \beta$, because by the root space decomposition
%
\[ \kappa(h_\alpha, h_\beta) = \text{tr}(\text{adj}_{h_\alpha} \circ \text{adj}_{h_\beta}) = \sum_{\gamma \in \Phi} \gamma(h_\alpha) \gamma(h_\beta) \]
%
and the previous lemmas imply
%
\[ \kappa(t_\alpha, t_\beta) = \frac{\kappa(t_\alpha, t_\alpha) \kappa(t_\beta, t_\beta)}{4} \kappa(h_\alpha, h_\beta) \in \mathbf{Q} \]
%
Hence the Killing form doesn't take many complex values on the roots.

The Killing form on $\mathfrak{h}$ translates to a non-degenerate symmetric bilinear form on $\mathfrak{h}^*$, denoted $(\cdot, \cdot)$. We can define the form as
%
\[ (\lambda,\gamma) = \kappa(t_\lambda, t_\gamma) \]
%
Where $t_\lambda$ and $t_\gamma$ are the elements such that $\kappa(t_\lambda,X) = \lambda(X)$ and $\kappa(t_\gamma,X) = \gamma(X)$. We have verified that $(\alpha, \beta) \in \mathbf{Q}$ if $\alpha, \beta$ are roots.

Since the roots of $\mathfrak{h}^*$ span $\mathfrak{h}$, $\mathfrak{h}^*$ has a basis of roots $\{ \alpha_1, \dots, \alpha_n \}$. Something stronger can be said on this front.

\begin{lemma}
    If $\beta$ is a root, then $\beta$ is a linear combination of the $\alpha_i$ with rational coefficients.
\end{lemma}
\begin{proof}
    Certainly we may write $\beta = \sum \lambda_i \alpha_i$, with $\lambda_i \in \mathbf{C}$. We have
    %
    \[ (\beta, \alpha_j) = \sum \lambda_i (\alpha_i, \alpha_j) \]
    %
    This is a system of linear equations with rational coefficients in the values $\lambda_i$, and since each $(\alpha_i, \alpha_j)$ is rational, and $(\beta, \alpha_j)$ is rational, we conclude the $\lambda_i$ are rational.
\end{proof}

Thus the {\it real} subspace generated by the $\alpha_i$ contains all the roots of $\Phi$, and doesn't depend on the particular choice of basis roots $\alpha_i$. Let $E$ denote this subspace.

\begin{theorem}
    $(\cdot, \cdot)$ is a real valued inner product on $E$.
\end{theorem}
\begin{proof}
    Since $(\cdot, \cdot)$ takes the value of rational numbers on the roots , we know that the bilinear map is real-valued on $E$, and if $\lambda \in E$ is a given element, then for any $X \in \mathfrak{g}_\alpha$, $\text{adj}_{t_\lambda}(X) = \alpha(t_\lambda)$, and if $X \in \mathfrak{h}$, $\text{adj}_{t_\lambda}(X) = 0$, ehcne
    %
    \[ (\lambda, \lambda) = \kappa(t_\lambda, t_\lambda) = \text{tr}(\text{adj}_{t_\lambda^2}) = \sum_\alpha \left[\alpha(t_\lambda)\right]^2 \]
    %
    hence if $(\lambda, \lambda) = 0$, $\alpha(t_\lambda) = 0$ for all $\alpha$, hence $t_\lambda = 0$, so $\lambda = 0$.
\end{proof}






\section{Root systems}

Let $V$ be a real vector space with some inner product $(\cdot,\cdot)$. Given a vector $v \in V$, let $s_v$ be the reflection in the hyperplane orthogonal to $v$. Thus $s_v(v) = -v$, and $s_v(w) = w$ if $v \perp w$, and $s_v$ is uniquely defined by these properties, hence we have the formula
%
\[ s_v(w) = w - 2 \frac{(w,v)}{(v,v)} v \]
%
and $s_v$ preserves the inner product. We write $\langle w, v \rangle = 2(w,v)/(v,v)$ for the inner product which normalizes $v$. A {\bf root system} on $V$ is a finite subset $R$ of $V$ which spans $V$, does not contain zero, contains only the scalar multiples $\alpha$ and $-\alpha$ for each $\alpha \in R$, each $s_\alpha$ permutes the elements of $R$, and $\langle \beta, \alpha \rangle \in \mathbf{Z}$ for each $\alpha, \beta \in R$. The elements of $R$ are called {\bf roots}.

\begin{example}
    If $\mathfrak{g}$ is a complex semisimple Lie algebra, then the set of roots form a root system over the real span of the roots, where the Killing form induces the inner product. By definition $0 \not \in \Phi$, the set of roots is finite, and only the scalar multiples of $\alpha$ and $-\alpha$ are roots. Using the fact that
    %
    \[ \langle \beta, \alpha \rangle = 2 \frac{\kappa(t_\beta, t_\alpha)}{\kappa(t_\alpha, t_\alpha)} = 2 \frac{\kappa(t_\beta, h_\alpha)}{\kappa(t_\alpha, h_\alpha)} = 2 \frac{\beta(h_\alpha)}{\alpha(h_\alpha)} = \beta(h_\alpha) \]
    %
    Hence $\langle \beta, \alpha \rangle$ is an integer. If $\alpha$ and $\beta$ are roots, then
    %
    \[ s_\alpha(\beta) = \beta - \langle \beta, \alpha \rangle \alpha = \beta - \beta(h_\alpha) \alpha \]
    %
    and we have verified this is already a root.
\end{example}

\begin{lemma}
    If $R$ is a root system, for two roots $\alpha$ and $\beta$, with $\beta \neq \pm \alpha$,
    %
    \[ \langle \alpha, \beta \rangle \langle \beta, \alpha \rangle \in \{ 0, 1, 2, 3 \} \]
\end{lemma}
\begin{proof}
    For any $v,w \in E$, the angle $\theta$ between $v$ and $w$ is
    %
    \[ \langle \alpha, \beta \rangle \langle \beta, \alpha \rangle = \frac{4 (\alpha, \beta)^2}{(\alpha, \alpha)(\beta, \beta)} = 4\cos^2 \theta \leq 4 \]
    %
    Thus $\langle \alpha, \beta \rangle \langle \beta, \alpha \rangle$ is an integer between 0 and 4, and if $\langle \alpha, \beta \rangle \langle \beta, \alpha \rangle = 4$, then $\cos^2 \theta = 1$, hence $\alpha$ and $\beta$ are linearly dependent, which is impossible.
\end{proof}

We therefore can determine the roots

\begin{center}
\begin{tabular}{|c | c | c | c |}
    \hline
    $\langle \alpha, \beta \rangle$ & $\langle \beta, \alpha \rangle$ & $\theta$ & $\frac{(\beta, \beta)}{(\alpha, \alpha)}$\\
    \hline
    0 & 0 & $\pi/2$ & underdetermined\\
    1 & 1 & $\pi/3$ & 1\\
    -1 & -1 & $2\pi/3$ & 1\\
    1 & 2 & $\pi/4$ & 2\\
    -1 & -2 & $3\pi/4$ & 2\\
    1 & 3 & $\pi/6$ & 3\\
    -1 & -3 & $5\pi/6$ & 3\\
    \hline
\end{tabular}
\end{center}

\begin{theorem}
    If $\alpha$ and $\beta$ are roots, and the angle between $\alpha$ and $\beta$ is strictly obtuse, then $\alpha + \beta$ is a root. If the angle is strictly acute, and $(\beta, \beta) \geq (\alpha, \alpha)$, then $\alpha - \beta$ is a root.
\end{theorem}
\begin{proof}
    In either case, we may assume $(\beta, \beta) \geq (\alpha, \alpha)$. It therefore follows that $s_\beta(\alpha) = \alpha - \langle \alpha, \beta \rangle \beta$ is an element of the root system. If the angle is strictly obtuse, then $\langle \alpha, \beta \rangle = -1$, and if the angle is strictly acute, then $\langle \alpha, \beta \rangle = 1$.
\end{proof}

\begin{example}
    We already have enough information to classify all the two dimensional root systems with respect to the standard inner product. The inner must contain at least two independent roots $\alpha$ and $\beta$. We may assume that the angle of $\alpha$ and $\beta$ is the largest possible in the algebra, and by considering a rotation and a scale, that $\alpha$ is the unit vector in the $x$ axis.
    %
    \begin{itemize}
        \item If $\beta$ lies at an angle $\pi/2$, then
        %
        \[ \{ \alpha, \beta, -\alpha, -\beta \} \]
        %
        forms a root system, because $s_\alpha(\beta) = -\beta$, $s_\beta(\alpha) = -\alpha$. This is the root system $A_1 \times A_1$.

        \item If $\beta$ lies at an angle $2\pi/3$ from $\alpha$, then we find $\alpha + \beta$ is also a root, as is $-(\alpha + \beta)$. What's more, the set of roots
        %
        \[ \{ \alpha, -\alpha, \beta, -\beta, \alpha + \beta, -(\alpha + \beta) \} \]
        %
        form a root system. We call this system $A_2$.

        \item If $\beta$ lies at an angle $3\pi/4$ from $\alpha$, then $\alpha + \beta$ and $-(\alpha + \beta)$ are roots. $2\alpha + \beta$ is then obtained from reflective $\beta$ across the line $\alpha + \beta$, so the root system contains
        %
        \[ \{ \alpha, -\alpha, \beta, -\beta, \alpha + \beta, -(\alpha + \beta), (2\alpha + \beta), -(2\alpha + \beta) \} \]
        %
        and any other root would make an angle smaller than $\pi/4$, contradicting that $\beta$ is the largest angle. This is the $B_2$ root system.

        \item If $\beta$ lies at an angle $5\pi/6$ from $\alpha$, then the root system $12$ vectors
    \end{itemize}
\end{example}





\section{Root Spaces for the Classical Lie Algebras}

It will be important for us to consider the Cartan subalgebras and root spaces of the classical Lie algebras, that occur as spaces of matrices.

We have already seen the Cartan subalgebra $\mathfrak{h}$ of $\mathfrak{sl}_n(\mathbf{C})$, which consists of the diagonal matrices. Indeed, if $XY = YX$ for all diagonal matrices $Y \in \mathfrak{sl}_n(\mathbf{C})$, then in particular
%
\[ 0 = [X,E_{ii} - E_{(i+1)(i+1)}] = \sum_j X_{ji} E_{ji} - X_{ij} E_{ij} - X_{j(i+1)} E_{j(i+1)} + X_{(i+1)j} E_{(i+1)j} \]
%
which implies $X_{ij} = 0$ for $i \neq j$, hence $\mathfrak{h}$ is maximal. For $i \neq j$,
%
\[ \left[\sum a_k E_{kk} ,E_{ij} \right] = (a_i - a_j) E_{ij} \]
%
so the weights consist of the maps $\varepsilon_{ij}(X) = X_{ii} - X_{jj}$, for $i \ne j$.



\end{document}











\section{Low Dimensional Classifications}

There is a useful criterion for verifying some abstractly defined bracket is actually a Lie bracket, which will enable us to classify the low dimensional Lie algebras. Given a vector space $\mathfrak{g}$, after fixing a basis $E_1, \dots, E_n$, we find an arbitrary bracket (not necessarily a Lie bracket) $[\cdot, \cdot]: \mathfrak{g} \to \mathfrak{g}$ can be written as
%
\[ [X,Y] = \sum f_k(X,Y) E_k \]
%
where each $f_i$ is a bilinear form, and so we may find constants $a_{ij}^k \in K$ such that
%
\[ f_k = \sum a^k_{ij} ( E_i^* \otimes E_j^* ) \]
%
which is the same as saying $[E_i, E_j] = \sum a_{ij}^k E_k$. If $[\cdot, \cdot]$ was actually a Lie bracket, then the alternating property of the Lie algebras implies that each $f_k$ is a skew-symmetric Bilinear form, hence $a_{ij}^k = -a_{ji}^k$. The Jacobi identity takes the form
%
\[ \sum_l a_{jk}^l a_{il}^m + a_{ki}^l a_{jl}^m + a_{ij}^l a_{kl}^m = 0 \]
%
for every fixed $i,j,k,m$. Conversely, given a set of constants $a^k_{ij}$ over some vector space satisfying these equations, the corresponding bilinear map is a Lie bracket, because if $X = \sum a^i e_i$, $Y = \sum b^i e_i$, and $Z = \sum c^i e_i$, then
%
\begin{align*}
    [X,[Y,Z]] + &[Y,[Z,X]] + [Z,[X,Y]]\\
    &= \sum_{i,j,l} a^i b^j c^l \left( [e_i,[e_j,e_l]] + [e_j,[e_l,e_i]] + [e_l,[e_i,e_j]] \right)\\
    &= 0
\end{align*}
%
This coordinatization is particular applicable in low dimensions, where the calculations are relatively trivial, and allows us to classify the algebras up to isomorphism, because two algebras $\mathfrak{g}$ and $\mathfrak{h}$ are isomorphic if and only if we can specify a basis in such a way that the structure constants obtained are the same. In the one dimensional case, the only skew symmetric matrix is trivial, so that every one dimensional Lie algebra is abelian. The calculation of the two and three dimensional cases gets progressively more and more involved.

Before we do this though, we should mention a little trick to help verify the Jacobi identity for certain abstract Lie brackets. Given a bilinear map $\omega: \mathfrak{g}^2 \to \mathfrak{g}$, we consider the trilinear map $T\omega: \mathfrak{g}^3 \to \mathfrak{g}$ defined by
%
\[ (T \omega)(X,Y,Z) = \omega(X, \omega(Y,Z)) + \omega(Y,\omega(Z,X)) + \omega(Z,\omega(X,Y)) \]
%
Then $T \omega = 0$ if and only if $\omega$ satisfies the Jacobi identity. By linearity, each coefficient of $\omega(X,\omega(Y,Z))$ can be written as a polynomial in the variables $X^i$, $Y^j$, and $Z^k$,
%
\[ \omega(X, \omega(Y,Z)) = \sum b_{ijk}^l X^i Y^j Z^k E_l \]
%
and
%
\[ T\omega = \sum (b_{ijk}^l + b_{jki}^l + b_{kij}^l) X^i Y^j Z^k E_l \]
%
Applying the theory of multivariate polynomials, $T \omega = 0$ if and only if $b_{ijk}^l + b_{jki}^l + b_{kij}^l = 0$ for each $l, i, j, k$. In most cases, we will find that only two of the three coefficients will be non-zero, and they will be the negation of one another. Thus we can verify the Jacobi identity by `pairing cycles' in the coefficients of $\omega(X,\omega(Y,Z))$.

\begin{theorem}
    There is a single nonabelian two dimensional Lie algebra.
\end{theorem}
\begin{proof}
    If $\mathfrak{g}$ is a two dimensional Lie algebra, and if $(X,Y)$ is a basis, then $[X,Y]$ spans the derived subalgebra, hence the derived subalgebra is one dimensional if the space is nonabelian. Fix a non-zero $X$ in the derived subalgebra, and extend $X$ to a basis $(X,Y)$. Then $[X,Y] \neq 0$, for otherwise the bracket is abelian. Write $[X,Y] = \lambda X$. By scaling $Y$, we may actually assume $[X,Y] = X$. This shows that there can be at most one non-abelian Lie algebra, because we have found a basis with a particular set of structural constants, and provided these constants specify a Lie algebra, we have determined that there is a single two dimensional Lie algebra. The bracket $[X,Y] = X$ does actually give a Lie algebra structure, since
    %
    \[ [a_1X + a_2Y, [b_1X + b_2Y, c_1X + c_2Y]] = (a_2b_2c_1 - a_2b_1c_2) X \]
    %
    And $a_2b_2c_1$ is obtained from $a_2b_1c_2$ by a cycle permutation in the variables $a,b$, and $c$, hence the Jacobi holds.
\end{proof}

The three dimensional case is more involved. We split our discussion into three cases, where the derived subalgebra has dimension 0,1,2, and 3. The zero dimensional case is obviously the abelian Lie algebra, and needs no further discussion.

\begin{theorem}
    The Heisenberg Lie algebra is the unique three dimensional Lie algebra whose derived subalgebra is one dimensional, and contained within the center of the algebra.
\end{theorem}
\begin{proof}
    Given such a Lie algebra $\mathfrak{g}$, take $X$ and $Y$ such that $[X,Y] = Z \neq 0$. Then $X$, $Y$, and $Z$ are linearly independant, for if $aX + bY + cZ = 0$, then
    %
    \[ [aX + bY + cZ, X] = b[Y,X] = -bZ = 0\ \ \ \ \ [aX + bY + cZ, Y] = aZ = 0 \]
    %
    implying $a = b = 0$, hence $c = 0$. Thus $X,Y$, and $Z$ are a basis of the Lie algebra. The structural constants of this Lie algebra are identical to the structural constants of the Heisenberg Lie algebra, where $X = E_{12}$, $Y = E_{23}$, and $Z = E_{33}$.
\end{proof}

The remaining dimension 1 case occurs when the derived subalgebra is not contained within the center. If we consider the direct product the nonabelian two-dimensional Lie algebra $\mathfrak{g}$ with the field $K$, denoted $\mathfrak{g} \oplus K$, then the derived subalgebra of this Lie algebra is not contained within the centre, because
%
\[ (\mathfrak{g} \oplus K)' = KX \oplus (0) \]
\[ Z(\mathfrak{g} \oplus K) = (0) \oplus K \]
%
We shall now prove that this is the defining three dimensional Lie algebra whose derived subalgebra is one-dimensional, and is not contained within the centre.

\begin{theorem}
    There is a unique three dimensional Lie algebra with one dimensional derived subalgebra not contained within the center of the algebra.
\end{theorem}
\begin{proof}
    Let $\mathfrak{h}$ be such an algebra. Pick $X \neq 0$ spanning the derived $\mathfrak{h}'$, then pick $Y$ with $[X,Y] = X$. Then $X$ and $Y$ are linearly independant, and we may consider some basis $\{ X, Y, Z \}$. There must $a$ and $b$ such that $[X,Z] = aX$, $[Y,Z] = bX$. If we consider the equations
    %
    \begin{align*}
        (\lambda X + \gamma Y + \eta Z, X) &= -(\gamma + \eta a) X\\
        (\lambda X + \gamma Y + \eta Z, Y) &= (\lambda - \eta b) X\\
        (\lambda X + \gamma Y + \eta Z, Z) &= (a \lambda + b \gamma) X
    \end{align*}
    %
    And if we consider the matrix
    %
    \[ \begin{pmatrix} 0 & -1 & -a \\ 1 & 0 & -b \\ a & b & 0 \end{pmatrix} \]
    %
    we notice the determinant is $0$, hence the matrix is non-invertible, and the nullspace must contain some non-zero $(\lambda, \gamma, \eta)$, i.e. $Z(\mathfrak{g})$ is non trivial, and contains some non-zero $\lambda X + \gamma Y + \eta Z$. If $\eta = 0$, then the equations above would imply $\gamma = \lambda = 0$, which is impossible, hence $\eta \neq 0$, and we have found a nonzero $w \in Z(\mathfrak{h})$ not contained in the span of $x$ or $y$, hence we have the decomposition
    %
    \[ \mathfrak{h} = \langle x, y \rangle \oplus Kw \cong \mathfrak{g} \oplus K \]
    %
    Hence $\mathfrak{g} \oplus K$ is the unique such Lie algebra.
\end{proof}

There is actually an infinite family of non-isomorphic Lie algebras whose derived algebra is two-dimensional in the case where $K$ is the complex numbers. but we have a nice classification of this family. We take a basis $(Y,Z)$ of the derived subalgebra, and complete it to a basis $(X,Y,Z)$ of the entire algebra. We claim that the derived subalgebra is always abelian. To prove this, it suffices to show $[Y,Z] = 0$. Consider the adjoint map $\text{adj}_Y$, which can be written in matrix form with respect to the basis given as
%
\[ \begin{pmatrix} 0 & 0 & 0 \\ * & 0 & \alpha \\ * & 0 & \beta \end{pmatrix} \]
%
for some constants $\alpha$ and $\beta$. We claim that the trace of any adjoint operator from a derived element is zero, so that $\beta = 0$. By consider the adjoint map $\text{adj}_Z$, we conclude that $\alpha = 0$. Thus the derived subalgebra is abelian.

\begin{lemma}
    For any Lie algebra $\mathfrak{g}$, and $X \in \mathfrak{g}'$, $\text{tr}(\text{adj}_X) = 0$.
\end{lemma}
\begin{proof}
    If $X = [Y,Z]$, then
    %
    \[ \text{adj}_{[Y,Z]} = [\text{adj}_Y, \text{adj}_Z] \in \mathfrak{gl}(\mathfrak{g})' = \mathfrak{sl}(\mathfrak{g}) \]
    %
    Hence the trace the adjoint is zero.
\end{proof}

In fact, $\text{adj}_X$ is an isomorphism of $\mathfrak{g}'$, because $[X,Y]$ and $[X,Z]$ span the derived subalgebra, hence the map is surjective, and therefore an isomorphism. Over $\mathbf{C}$, we may apply the theory of the Jordan normal form, and break our derivation down into two cases.

\begin{enumerate}
    \item If we can choose $Y$ and $Z$ such that $[X,Y] = \lambda Y$, $[X,Z] = \gamma Z$, then by scaling, we may assume $\lambda = 1$. For a fixed $\gamma$, this does indeed define a Lie algebra structure on $\mathbf{C}^3$, denoted $\mathfrak{g}_\gamma$, because
    %
    \begin{align*}
        [a_1X + a_2Y& + a_3Z, [b_1X + b_2Y + b_3Z, c_1X + c_2Y + c_3Z]]\\
        &= [a_1X + a_2Y + a_3Z, (b_1c_2 - b_2c_1) Y + \gamma (b_1 c_3 - b_3c_1) Z]\\
        &= (a_1b_1c_2 - a_1 b_2c_1) Y + \gamma^2 (a_1 b_1c_3 - a_1 b_3 c_1) Z
    \end{align*}
    %
    and $a_1b_1c_2$ is obtained from $a_1b_2c_1$, and $a_1b_1c_3$ is obtained from $a_1b_3c_1$ by a cycle permutation, hence the Jacobi identity holds. These form a class of distinct Lie algebras, for $\mathfrak{g}_\lambda$ is isomorphic to $\mathfrak{g}_\gamma$ if and only if $\lambda = \gamma$ or $\lambda = 1/\gamma$.

    \item Suppose our operator is not diagonalizable. Since we are working over $\mathbf{C}$, we have a Jordan normal form. That is, there exists an eigenvalue $\lambda$, and a basis $Y,Z$ such that $[X,Y] = \lambda Y + Z$, $[X,Z] = \lambda Z$. By scaling $X$, we may assume $\lambda = 1$, but then this defines a unique Lie algebra structure. Indeed, then
    %
    \begin{align*}
        &[a_1X + a_2Y + a_3Z, [b_1X + b_2Y + b_3Z, c_1X + c_2Y + c_3Z]]\\
        &= [a_1X + a_2Y + a_3Z, (b_1c_2 - b_2c_1) Y + (b_1c_3 - b_3c_1 + b_1c_2 - b_2c_1) Z]\\
        &= (a_1b_1c_2 - a_1 b_2c_1) (Y + Z) + (a_1 b_1c_3 - a_1 b_3 c_1 + a_1b_1c_2 - a_1b_2c_1) Z\\
        &= (a_1b_1c_2 - a_1b_2c_1) Y + (2a_1b_1c_2 - 2a_1b_2c_1 + a_1b_1c_3 - a_1b_3c_1) Z
    \end{align*}
    %
    and we have cycle pairs, hence the Jacobi identity holds.
\end{enumerate}

Finally, suppose that $\mathfrak{g}$ is a Lie algebra such that $\mathfrak{g}' = \mathfrak{g}$. We already know such an algebra exists, $\mathfrak{sl}_2(\mathbf{C})$, because if we consider the basis of the algebra $X = \left( \begin{smallmatrix} 0 & 1 \\ 0 & 0 \end{smallmatrix} \right)$, $Y = \left( \begin{smallmatrix} 0 & 0 \\ 1 & 0 \end{smallmatrix} \right)$, and $Z = \left( \begin{smallmatrix} 1 & 0 \\ 0 & -1 \end{smallmatrix} \right)$, then
%
\[ [X,Y] = Z\ \ \ \ \ [X,Z] = -2X\ \ \ \ \ [Y,Z] = 2Y \]
%
which span $\mathfrak{sl}_2(\mathbf{C}$. We shall find this is the only such algebra up to isomorphism.

\begin{theorem}
    If a three dimensional Lie algebra $\mathfrak{g}$ satisfies $\mathfrak{g}' = \mathfrak{g}$, then $\mathfrak{g}$ is isomorphic to $\mathfrak{sl}_2(\mathbf{C})$.
\end{theorem}
\begin{proof}
Given such an algebra, if $X \in \mathfrak{g}$ is non-zero, then $\text{adj}_X$ has rank 2, because $\mathfrak{g}'$ is spanned by $[X,Y]$, $[X,Z]$ and $[Y,Z]$, which must be linearly independant, and $[X,Y]$ and $[X,Z]$ span the range of $\text{adj}_X$. This also implies that $[X,M] = 0$, then $M$ is in the span of $X$. We claim that we can choose $X$ such that $\text{adj}_X$ has a non-zero eigenvalue. If all the eigenvalues of $X$ are non-zero, applying the Jordan canonical form theorem we conclude that there is a basis $X_0, Y_0$, and $Z_0$ such that $[X,X_0] = 0$, $[X,Y_0] = X_0$, and $[X,Z_0] = Y_0$. By rescaling $X$, we may actually assume $X = X_0$, in which case $[Y_0,X_0] = -[X_0,Y_0] = -X_0$, so that $\text{adj}_{Y_0}$ has a non-zero eigenvalue.

If $\text{adj}_X$ has one non-zero eigenvalue, it must actually have two non-zero eigenvalues which are the negations of one another (because the trace of the adjoint is zero), and therefore $\text{adj}_X$ is diagonalizable. Thus we extend $X$ to a basis $X,Y,Z$ with $[X,Y] = \lambda Y$, $[X,Z] = -\lambda Z$, and $X,Y$, and $Z$ are a basis of the space. To fully describe the structure of the algebra, we need to determine $[Y,Z]$. Note that
%
\[ [X,[Y,Z]] = [[Z,X],Y] + [[X,Y],Z] = \lambda [Z,Y] + \lambda [Y,Z] = 0 \]
%
Which implies $[Y,Z]$ is a non-zero scalar multiple of $X$. We may assume that $X = [Y,Z]$ by rescaling $Y$. Finally, by rescaling $X$, we can change the value of $\lambda$ to an arbitrary value, and this shows that the Lie algebra structure is unique, because we have specified all structural constants exactly. For $\lambda = 2$, we obtain the special linear group constants we found above.
\end{proof}







































A useful (and standard) basis for $\mathfrak{gl}_n(K)$ are the matrices $E_{ij}$, which are only non-zero on row $i$ and column $j$, where the matrix coefficient has value 1. We note that
%
\[ [E_{ij}, E_{kl}] = \delta_j^k E_{il} + \delta_i^l E_{kj} \]
%
If we define $H_k = E_{kk} - E_{k+1\ k+1}$, then the $H_k$, together with the $E_{ij}$ for $i \neq j$, span $\mathfrak{sl}_n$, and
%
\[ [H_k, E_{ij}] = [E_{kk}, E_{ij}] - [E_{k+1\ k+1}, E_{ij}] = (\delta_k^i + \delta_k^j - \delta_{k+1}^i - \delta_{k+1}^j) E_{ij} \]
%
\[ [H_i, H_j] = [H_i, E_{jj}] - [H_i, E_{j+1\ j+1}] = 2(\delta_i^{j+1} - \delta_{i+1}^j) E_{jj} \]
%
Thus $E_{ij}$ is an eigenvector for the {\bf adjoint} map $\text{adj}(X)$

So $E_{ij}$ is an eigenvector for the {\bf adjoint} map $\text{adj}(X)$, defined by $\text{adj}(X)(Y) = [X,Y]$, where $X = H_k$.