\input{../style.tex}

\title{Metamathematics}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}
\maketitle
\tableofcontents

\part{Mathematical Logic}

\chapter{What's Logic All About?}

\pagenumbering{arabic}

Metamathematics is the formal analysis of mathematical deduction and proof. Its aim is to understand the procedure and limitations of proof. Though studied since Plato's time, `hard' metamathematics arose in response to paradoxes in early twentieth century mathematics. Each individual step of the paradoxes is intuitive, yet the conclusion ludicrous.

\begin{example}[Russell]
    Consider the following set.
    %
    \[ X = \{ x: x \not \in x \} \]
    %
    Russell's paradox rests on the following question: Is $X$ in itself? If $X$ was in $X$, then by definition, we would find that $X$ cannot be contained in itself. So we are lead to believe that $X \not \in X$. But then, by construction of $X$, we conclude $X$ is in $X$ after all! More colloquially, consider a town with a single barber, shaving everyone who does not shave themselves. Does the barber shave himself?
\end{example}

\begin{example}[Cantor]
    For any set $X$, Cantor proved that there is no injective function from $\mathcal{P}(X)$ to $X$. When we take $X$ to be the set of all things, then $\mathcal{P}(X) \subset X$. There is an obvious injection of $\mathcal{P}(X)$ in $X$.
\end{example}

\begin{example}[Richard]
    There are countably many english expressions, yet uncountable amounts of real numbers. Some real numbers cannot be described in english. Consider a particular enumeration containing all describable real numbers. Then, take the expression
    %
    \begin{displayquote}
        ``The number constructed from Cantor's diagonal argument on the enumeration considered.''
    \end{displayquote}
    %
    Such a number cannot be described in english, yet is describable in english.
\end{example}

\begin{example}[L\"{o}b]
    Consider the Preposition $B$, defined to be true when $B \implies A$ is true. If $B$ is true, then $B \implies A$ is true, so $A$ is true. But then $B \implies A$ is true, so $B$ is true, and we conclude $A$ is true. But $A$ was arbitrary, so we conclude that every logical statement is true!
\end{example}

\begin{example}[Curry]
     Consider
    %
    \[ C = \{ x : (x \in x) \implies P(x) \} \]
    %
    Then $(C \in C)$ holds if and only if $(C \in C) \implies P(C)$ holds. We must have $C \in C$, for if $C \not \in C$, then $(C \in C) \implies P(C)$ holds vacantly. But this implies $(C \in C) \implies P(C)$, so $P(C)$ is true. We conclude $P(C)$ is true, irrespective of the content of the statement $P(C)$.
\end{example}

Resolving these paradoxes requires an acute understanding of proof, which only metamathematics can be provide. Which logical procedures summarize the construction of paradoxes? What standards do we use to define a mathematical statement? Our only hope to fix these issues is to apply the precise weapon of mathematical rigor. In the face of adversity, we do what mathematicians do best -- define and conquer.

Defining fundamental principles of a field is rarely possible, and mathematical logic is no exception. Intuition is the only being who can completely grasp these principles; our concious minds will never obtain final definitions. Nonetheless, some properties are commonly accepted; it is these we use to understand the principle. A physicists's models are ideals, carved from reality in all senses but experimental parameters. No model describes a systems evolution exactly, and it is myopic to suggest a model is perfect. In spite of this, physics still does a bloody good job! In metamathematics, we discuss a `model' of mathematical principles. Some principles are pinned down, others lost. We hope this model has enough vitality to provide key insights into the proofs found in real life. Whether the method is successful it up to interpretation.

A source of confusion in physics is the stylistic treatment of assumptions as absolute facts. A physicist describes ``a planet moving according to the equation $\ddot{x} = -m/x^2$'' even if he is actually talking about the dynamical system whose evolution is described by the differential equation $\ddot{x} = -m/x^2$, which {\it models} the motion of a planet. Such use is unavoidable, since the former description is viceral, the latter dry to the bone. Keep this principle in mind as we begin to study logic.

The whole idea of metamathematics may seem circular. How do we understand mathematics through mathematic's eyes? This is the reason for formality. By placing mathematical understanding in a model, formal mathematics becomes a second order object. Proofs about this system rely on intuition, whereas proofs in the system have static, precise formulations. In the conversion mathematics becomes stale, abstract, and unintuitive. Such is the price to pay for precise theorems about mathematics itself.



\chapter{Prepositional Logic}

We shall begin with propositional logic, the simplest model of truth. To understand propositional logic, we construct a mathematical model, known as a {\bf formal language}, which represents the language in which mathematics is performed. The formal language is then analyzed by common mathematics. The standard formal language for logic is an analysis of strings, sequences of abstract symbols from a given {\bf alphabet}. Strings represent mathematical statements; manipulating these strings models how a mathematician infers some mathematical statement from another. It is best to see the tool in action to understand its utility, so we proceed swiftly.

\section{Syntax}

Each abstract symbol in an alphabet represents a precise form in colloquial speech. We begin with prepositional logic, which analyzes basic notions of truth and falsity. Some statements are {\bf atomic}, because they cannot be divided into more base statements. ``Socrates is a man'' is an atomic statement, as is ``every woman is human''. ``Socrates is a man and every woman is a human'' is not atomic, for the statement consists of two separate statements, composed by the connective ``and''. In English, ``every woman is a human'' can be broken into statements like ``Julie is a human'', ``Laura is a human'', yet propositional logic still considers this statement as atomic; the model does not have the capability to model these complex statements, which are the realm of predicate logic, which we shall analyze in the next chapter.

\begin{definition}
    Let $\Lambda$ be a set. The set $\Lambda^*$ of {\bf strings over $\Lambda$} is all finite, possibly empty, sequences with elements in $\Lambda$.
\end{definition}

The empty string is denoted $\varepsilon$. A string $(v_1, \dots, v_n)$ is often denoted $v_1 v_2 \dots v_n$. We shall identify an element in $\Lambda$ with the corresponding one letter string in $\Lambda^*$. The concatenation of two strings $s = s_1 \dots s_n$ and $w = w_1 \dots w_m$, denoted $sw$, is the string $s_1 \dots s_n w_1 \dots w_m$. A {\bf substring} of a string $s$ is a string $u$ such that $s = wuv$, for some strings $w$ and $v$. Substrings are consequtive subsequences of characters in $s$. If we view concatenation as an associative algebraic operation, then $\Lambda^*$ is the smallest {\it monoid} containing $\Lambda$, relative to the operation of concatenation.

\begin{definition}
    Let $\Lambda$ be a set disjoint from $\{ (, ), \wedge, \vee, \neg, \Rightarrow, \Leftrightarrow \}$. The {\bf propositional language with atoms in $\Lambda$}, denoted $\text{SL}(\Lambda)$, is the smallest subset of $(\Lambda \cup \{ (, ), \wedge, \vee, \neg, \Rightarrow, \Leftrightarrow \})^*$ such that
    %
    \begin{enumerate}
        \item $\Lambda \subset \text{SL}(\Lambda)$.
        \item If $s, w \in \text{SL}(\Lambda)$, then $(\neg s), (s \wedge w), (s \vee w), (s \Rightarrow w), (s \Leftrightarrow w) \in \text{SL}(\Lambda)$.
    \end{enumerate}
    %
    An element of $\text{SL}(\Lambda)$ is called a {\bf formula} or {\bf statement}.
\end{definition}

Each {\bf connective} represents a certain linguistical form.
%
\begin{center}
\begin{tabular}{| c | c | c |}
    \hline Connective & Name of Connective & Meaning of statement \\
    \hline $\neg s$ & Negation & ``$s$ is {\it not} true''\\
    $s \wedge w$ & Conjuction & ``$s$ {\it and} $w$ is true''\\
    $s \vee w$ & Disjunction & ``$s$ {\it or} $w$ is true''\\
    $s \Rightarrow w$ & Implication & ``{\it If} $s$ is true, {\it then} $w$ is true''\\
    $s \Leftrightarrow w$ & Bicondition & ``$s$ is true, {\it if, and only if,} $w$ is true''\\
    \hline
\end{tabular}
\end{center}
%
Later on, the connection of symbols to meaning will become clear. For now, they are abstract symbols without intrinsic meaning.

Take care to notice that $\text{SL}(\Lambda)$ is the {\it smallest} set constructed, in the same way that most `smallest objects' exist in mathematics, because the intersection of sets satisfying the set of statements defining the set also satisfy the statements. This property leads to the most useful proof method in logic.

\begin{theorem}[Structural Induction]
    Consider a proposition that can be applied to $\text{SL}(\Lambda)$. Suppose the proposition is true of all elements of $\Lambda$, and that if the proposition is true of $s$ and $w$, then the proposition is also true of $(\neg s), (s \wedge w), (s \vee w), (s \Rightarrow w)$, and $(s \Leftrightarrow w)$. Then the proposition is true for all of $\text{SL}(\Lambda)$.
\end{theorem}
\begin{proof}
    Let $P$ be some property to consider. Note the set
    %
    \[ K = \{ s \in (\Lambda \cup \{ (, ), \wedge, \vee, \neg, \Rightarrow, \Leftrightarrow \})^* : P(s)\ \text{is true} \} \]
    %
    satisfies axioms (1) and (2) which define $\text{SL}(\Lambda)$, so $\text{SL}(\Lambda) \subset K$.
\end{proof}

Consider the following application of the proof method.

\begin{theorem}
    Any sentence in $\text{SL}(\Lambda)$ contains as many left as right brackets.
\end{theorem}
\begin{proof}
    Any atom in $\Lambda$ contains no left brackets, and no right brackets, and thus the same number of each. Let $s$ have $n$ pairs of brackets, and let $w$ have $m$. Then $(\neg s)$ contains $n + 1$ pairs of brackets, and $(s \circ w)$, where $\circ \in \{ \wedge, \vee, \neg, \Rightarrow, \Leftrightarrow \}$, contains $n + m + 1$ brackets. By structural induction, we have proved our claim.
\end{proof}

We need a more in depth theorem, to correctly parse statements. What is the value of $2 + 7 - 5 - 4$? Is it, by collecting factors in fairs, $9 - 1 = 8$, or $9 - 9 = 0$. This is the reason for parenthesis. One splits mathematical logic between into syntax, studying strings, and semantics, interpreting strings. We are studying syntax to begin with, so that we may begin semantics. Equations must be understood before we calculate with them.

\begin{theorem}
    If $wu = s \in \text{SL}(\Lambda)$, where $w \neq \varepsilon$, then $w$ has at least as many left brackets as right brackets, and $w = s$ if and only if $w$ has the same number of left and right brackets.
\end{theorem}
\begin{proof}
    If $s \in \Lambda$, then $s$ is only one letter long, so $s = w$, and has no brackets. Now let $s$ and $s'$ satisfy the theorem. We split our proof into two cases.
    %
    \begin{enumerate}
        \item $wu = (\neg s)$: March through all cases. Suppose $w$ has the same number of left brackets than right. $w$ cannot equal $($ or $(\neg$, nor $(\neg v$, where $v$ is a prefix of $s$; by induction, $v$ has at least as many left brackets as right brackets, and then $w$ has more left brackets than right brackets. Thus $w$ must equal $(\neg s)$.
        \item $wu = (s \circ w)$: Continue the string march. $w$ cannot equal $($, nor $(v$ by induction. Similarily, $w$ cannot equal $(s \circ$, nor $(s \circ v$, where $v$ is a substring of $w$, so $w$ must equal $(s \circ w)$.
    \end{enumerate}
    %
    Careful analysis of each case also shows that $w$ must have at least as many left brackets as right brackets.
\end{proof}

\begin{corollary}
    Every string in $\text{SL}(\Lambda)$ can be written uniquely as an atom $\Lambda$, or $(\neg s)$ and $(s \circ w)$, where $s$ and $w$ are elements of $\text{SL}(\Lambda)$. The unique connective is known as {\bf principal}.
\end{corollary}
\begin{proof}
    Such representations trivially exist. Suppose we have two representations. If one of the representations is an element of $\Lambda$, the other representation must have a string of length one, and is therefore equal. If we have two representations $(\neg s) = (\neg w)$, then by chopping off symbols, $s = w$. It is impossible to have two distinct representations $(s \circ w) = (\neg u)$, for no element of $\text{SL}(\Lambda)$ begins with $\neg$. Finally, suppose we have two representations $(s \circ w) = (u \circ v)$. Then either $u$ is a substring of $s$, or $s$ is a substring of $u$, and one is the prefix of the other. But both have balanced brackets, which implies $s = u$, and by chopping letters away, $w = v$. Thus representations are unique.
\end{proof}

The corollary above allows us to construct recursive definitions. Let $\Lambda = \{ A, B, C \}$, and $\Gamma = \{ X, Y, Z \}$. We would like to consider $\text{SL}(\Lambda)$ and $\text{SL}(\Gamma)$ the same, by considering a natural bijection. We extend the map
%
\[ X \mapsto A\ \ \ \ \ Y \mapsto B\ \ \ \ \ Z \mapsto C \]
%
by the definition
%
\[ f((s \circ w)) = (f(s) \circ f(w))\ \ \ \ \ f((\neg s)) = (\neg f(s)) \]
%
Such a map is well defined and injective, by the corollary.

Before we finish with syntax, it is interesting to discuss a less natural, but syntactically simpler method of forming sentences, called {\bf polish notation}, after its inventor Jan \L ukasiewicz. Rather than writing connectives in {\bf infix notation}, like $(u \wedge v)$ and $(u \Rightarrow v)$, we use {\bf prefix notation}, like $\wedge u v$ and $\Rightarrow u v$. We do not need brackets to parse statements anymore, but it is much more easy to read the statement $``((a \Rightarrow (b \vee c) \wedge d)''$, than $``\wedge \Rightarrow a \vee b c d''$.

\section{Semantics}

We can understand the discussion above without any understanding of what symbols mean. Now we want to interpret. A basic semantic method is to define whether a statement is `true'.

\begin{definition}
    A {\bf truth assignment} on a set $\Lambda$ is a map $f: \Lambda \to \{ \top, \bot \}$.
\end{definition}

\begin{example}
    An $n$-ary {\bf boolean function} is a truth assignment, where $\Lambda = \{ \top, \bot \}^n$. It is common to define such functions by truth tables. For $n$-ary boolean functions, we form a table with $n + 1$ columns, and $2^{n}$ rows. In each row, we fill in a particular truth assignment in $\{ \top, \bot \}^n$, and in the last column, the value of image of the truth assignment under $f$. One may combine multiple $n$-ary truth functions into the same table for brevity. There are $2^{2^{n}}$ $n$-ary truth functions.
    %
    \begin{center}
    \begin{tabular}{| c | c | c | c | c | c | c |}
        \hline $x$ & $y$ & $H_\wedge(x,y)$ & $H_\vee(x,y)$ & $H_\Rightarrow(x,y)$ & $H_\Leftrightarrow(x,y)$ & $H_\neg(x)$ \\
        \hline $\bot$ & $\bot$ & $\bot$ & $\bot$ & $\top$ & $\top$ & $\top$ \\
        $\bot$ & $\top$ & $\bot$ & $\top$ & $\top$ & $\bot$ & $\top$ \\
        $\top$ & $\bot$ & $\bot$ & $\top$ & $\bot$ & $\bot$ & $\bot$ \\
        $\top$ & $\top$ & $\top$ & $\top$ & $\top$ & $\top$ & $\bot$ \\
        \hline
    \end{tabular}
    \end{center}
    %
    This table defines the boolean functions $H_\circ$.
\end{example}

Homomorphisms between two rings $R$ and $S$, extend to homomorphisms between the polynomial rings $R[X]$ and $S[X]$. Similarily, we may extend truth assignments $f$ on $\Lambda$ to assignments $f_*$ on $\text{SL}(\Lambda)$. This is done recursively. Let $f$ be an arbitrary truth assignment. Define
%
\[ f_*(\neg s) = H_\neg(f_*(s))\ \ \ \ \ \ \ \ \ \ f_*(s \circ w) = H_\circ(f_*(s), f_*(w)) \]
%
then $f_*$ is defined on all of $\text{SL}(\Lambda)$.

\begin{definition}
    Let $s \in \text{SL}(\Lambda)$ be a statement. $s$ is a
    %
    \begin{enumerate}
        \item {\bf tautology}, if, for any truth assignment $f$ on $\Lambda$, $f_*(s) = \top$.
        \item {\bf contadiction} if $f_*(s) = \bot$.
        \item {\bf cotingent statement} if $s$ is neither a contradiction nor a tautology.
        \item {\bf satisfiable} if it is not a contradiction.
    \end{enumerate}
\end{definition}

We summarize the statement ``$s$ is a tautology'' by $\vDash s$. We say a statement $s$ {\bf semantically implies} $w$, written $s \vDash w$, if $\vDash s \Rightarrow w$, or correspondingly, if $f_*(w) = \top$ whenever $f_*(s) = \top$.

Suppose that we wish to verify whether $s \in \text{SL}(\Lambda)$ is a tautology. We say a variable $x \in \Lambda$ {\bf occurs} in $s$ if it is an element of the string. There can only be a finite number of variables $x_1, \dots, x_n$ which occur in $s$. Define a boolean function $g: \{ 0, 1 \}^n \to \{ 0, 1 \}$,
%
\[ g(y_1 \dots, y_n) = f^{(y_1, \dots, y_n)}_*(s) \]
%
where $f^{(y_1, \dots, y_n)}$ is the truth assignment formed by mapping $x_i$ to $y_i$. This is well defined, because if $h$ and $k$ are two truth assignments which agree on the $x_i$, then they agree at $s$. Then one verifies that $s$ is a tautology if and only if $g(y_1, \dots, y_n) = \top$ for all choices of the $y_i$. One then simply builds a truth table. To prevent errors, it is best to construct a truth table for the subformulas in the principal connective of a formula before constructing the truth table of the formula. This may be done side by side, in the same table.

\begin{example}
    For instance, for any variable $A \in \Lambda$, $A \vee \neg A$ is a tautology, $A \wedge \neg A$ is a contradiction, and $\neg A$ is contingent, which is verified by the truth table
    %
    \begin{center}
    \begin{tabular}{| c | c | c | c | c | c | c |}
        \hline $A$ & $A \vee \neg A$ & $A \wedge \neg A$ & $\neg A$ \\
        \hline $\top$ & $\top$ & $\bot$ & $\bot$ \\
               $\bot$ & $\top$ & $\bot$ & $\top$ \\
        \hline
    \end{tabular}
    \end{center}
    %
    The first formula is the {\bf law of excluded middle}, the second {\bf reductio ad absurdum}.
\end{example}

\begin{example}
    Let $A \vDash B$ be a tautology, and suppose that $A$ and $B$ have no variables in common. Then $A$ is a contradiction, or $B$ is a tautology, for if there is a truth assignment on the variables of $A$ which make the statement true, and a truth assignment on $B$ which cause the statement to be false, then we may combine the truth assignments to create a truth assignment in which $A \Rightarrow B$ is false.
\end{example}

\begin{theorem}[Semantic Modus Ponens]
    If $\vDash s$ and $s \vDash w$, then $\vDash w$.
\end{theorem}
\begin{proof}
    Let $f$ be a truth assignment. Then $f_*(s) = \top$ and
    %
    \[ f_*(s \Rightarrow w) = H_\Rightarrow(f_*(s),f_*(w)) = H_\Rightarrow(\top, f_*(w)) = \top \]
    %
    This holds only when $f_*(w) = \top$.
\end{proof}

\begin{definition}
    Let $x = (x_1, x_2, \dots, x_n)$ be a finite sequence of variables, and $s = (s_1, \dots, s_n)$ a sequence of statements. If $w \in \text{SL}(\Lambda)$, we shall define {\bf substitution}, denoted $w[s_1/x_1, \dots, s_n/x_n]$, or $w[s/x]$, recursively by the formulae
    %
    \[ x_j[s/x] = s_j\ \ \ \ \ y [s/x] = y\ \ \ \ \  \text{If $y \in \Lambda$, $y \not \in x$} \]
    %
    \[ (u \circ w)[s/x] = (u[s/x] \circ w[s/x])\ \ \ \ \ (\neg w)[s/x] = (\neg w[s/x]) \]
\end{definition}

\begin{theorem}
    If $\vDash w$, then $\vDash w[s/x]$
\end{theorem}
\begin{proof}
    Consider a truth assignment $f$. We shall define another truth assignment $\tilde{f}$ such that $f(v[s/x]) = \tilde{f}(v)$ for all $v$. Define $\tilde{f}(x_i) = f(s_i)$, and if $y \not \in x$, define $\tilde{f}(y) = f(y)$. Our base case, where $v$ is a variable, satisfies the claim by construction. Then, by induction, if $v = (u \circ w)$, then
    %
    \[ \tilde{f}_*(v) = H_\circ(\tilde{f}_*(u), \tilde{f}_*(w)) = H_\circ(f_*(u[s/x]), f_*(w[s/x])) = f_*(v[s/x]) \]
    %
    A similar proof answers the case where $u = (\neg v)$. Since $w$ is a tautology,
    %
    \[ f_*(w[s/x]) = \tilde{f}_*(w) = \top \]
    %
    So $w[s/x]$ is a tautology.
\end{proof}

\begin{corollary}
    If $v \vDash w$, then $v[s/x] \vDash w[s/x]$.
\end{corollary}

\begin{theorem}[Craig Interpolation]
    Let $A \vDash B$, and let the set of variables shared by $A$ and $B$ be $x_1, \dots, x_n$. Then there is a statement $C$, known as the {\bf iterpolant}, containing only the variables $x_i$, such that $A \vDash C$ and $C \vDash B$.
\end{theorem}
\begin{proof}
    We proceed by induction on the number of variables in $A$ which do not occur in $B$. If every variable in $A$ occurs in $B$, let $C = A$. In the general case, fix some variable $x$ in $A$ but not in $B$, take $y$ in both $A$ and $B$, and define
    %
    \[ D = A[(y \wedge \neg y)/x] \vee A[(y \vee \neg y)/x] \]
    %
    If $f_*(A) = \top$, and $f_*(x) = \bot$, then $f_*(A[(y \wedge \neg y)/x]) = \top$. If $f_*(x) = \top$, then $f_*(A[(y \vee \neg y)/x]) = \top$, so $A \vDash D$. In addition, $D \vDash B$. Let $f_*(D) = \top$. Then $f_*(A[(y \wedge \neg y)/x]) = \top$, or $f_*(A[(y \vee \neg y)/x]) = \top$. In the first case, we modify the truth assignment $f$ so that $f_*(x) = \bot$ (without changing the values of $B$ or $D$). Then $f_*(A) = \top$, so $f_*(B) = \top$. The other case follows by letting $f_*(x) = \top$. By induction, there is an iterpolant $C$ for which $D \vDash C$ and $C \vDash B$, and then $A \vDash C$, since $(x \Rightarrow y) \wedge (y \Rightarrow z) \vDash x \Rightarrow z$.
\end{proof}

\begin{example}
    Consider $A = (y_1 \Rightarrow x) \wedge (x \Rightarrow y_2)$ and $B = (y_1 \wedge z) \Rightarrow (y_2 \wedge z)$. Then $A \vDash B$. We shall find an interpolant.
    %
    \begin{align*}
        &C = [(y_1 \Rightarrow (y_1 \vee \neg y_1)) \wedge ((y_1 \vee \neg y_1) \Rightarrow y_2)]\\
        &\ \ \ \ \ \ \vee [(y_1 \Rightarrow (y_1 \wedge \neg y_1)) \wedge ((y_1 \wedge \neg y_1) \Rightarrow y_2)]
    \end{align*}
    %
    This equation can be simplified to $\neg y_1 \vee y_2$.
\end{example}

\section{Truth Functional Completeness}

We hope that propositional logic can model all notions of truth. Here we argue why our logic can model all such notions.

\begin{definition}
    Let $\Lambda$ be a set of boolean functions. The {\bf clone} of $\Lambda$ is the smallest set containing $\Lambda$ and all projections $\pi^n_k : \{ 0, 1 \}^n \to \{ 0, 1 \}$, defined
    %
    \[ \pi^n_k(x_1, \dots, x_n) = x_k \]
    %
    and in addition, if $g: \{ 0, 1 \}^n \to \{ 0, 1 \}$ and $f_1, \dots, f_n : \{ 0, 1 \}^m \to \{ 0, 1 \}$ are in the clone, then so is $g(f_1, \dots, f_n): \{ 0, 1 \}^m \to \{ 0, 1 \}$. $\Lambda$ is {\bf truth functionally complete} if its clone is the set of all boolean functions.
\end{definition}

\begin{example}
    $\{ H_\neg, H_\wedge, H_\vee \}$ is a truth functionally complete, since every formula can be put in {\bf conjunctive normal form}, for we may write the `true and false' functions
    %
    \[ \top(x) = \top = H_\vee(H_\neg(x), x)\ \ \ \ \ \bot(x) = \bot = H_\wedge(H_\neg(x), x) \]
    %
    and then we write
    %
    \[ f(x_1, \dots, x_n) = \bigvee_{\substack{(y_1, \dots, y_n) \in \{ 0, 1 \}^n\\f(y_1, \dots, y_n) = \top}}\ \  \bigwedge_{i = 1}^n H_\Leftrightarrow(x_i, y_i) \]
    %
    where we identify an element of $\{ \top, \bot \}$ with the constant boolean function, define $\bigcirc_{i = 1}^n f_i = H_\circ(f_n, \bigcirc_{i = 1}^{n-1})$, and take
    %
    \[ H_\Leftrightarrow(x,y) = H_\wedge(H_\Rightarrow(x,y), H_\Rightarrow(y,x)) = H_\wedge(H_\vee(H_\neg(x), y), H_\vee(H_\neg(y), x)) \]
    %
    This can be further reduced, by noticing that
    %
    \[ H_\wedge(x,y) = H_\neg(H_\vee(H_\neg(x), H_\neg(y))) \]
    %
    which is a truth functional form of Boole's inequality, implying $\{ \neg, \vee \}$ is truth functionally complete. We can also consider {\bf disjunctive normal form}, left to the reader to define.
\end{example}

\begin{example}
    The mathematician Henry M. Sheffer found a single truth function which is truth functionally complete. Consider the {\bf Sheffer stroke} $x|y$, also known as {\bf NAND}, defined by the truth table
    %
    \begin{center}
    \begin{tabular}{| c | c | c |}
        \hline $x$ & $y$ & $H_|(x,y)$\\
        \hline $\bot$ & $\bot$ & $\top$\\
        $\bot$ & $\top$ & $\top$\\
        $\top$ & $\bot$ & $\top$\\
        $\top$ & $\top$ & $\bot$\\
        \hline
    \end{tabular}
    \end{center}
    %
    Then $H_\neg(x) = H_|(x,x)$, and $H_\vee(x,y) = H_|(H_\neg(x), H_\neg(y))$, which implies, since this set is truth functionally complete, that the sheffer stroke is truth functionally complete.
\end{example}

The previous example is incredibly important to circuit design. Logical statements can be represented by boolean functions. Since all truth functions can be built from the sheffer stroke, we need only make an atomic circuit for the sheffer stroke, and then all other circuits are constructed by combining sheffer strokes. This is why computers consist of millions of NAND gates.




\section{Deduction}

When mathematicians want to derive whether a statement is true, they do not construct truth functions. Instead, they argue {\it why} the statement is true. Here we shall provide the mechanics for modelling a mathematical argument. We will show that truth tables and arguments are equivalent -- a statement is a tautology if and only if it can be proved.

\begin{definition}
    The set of thorems of $\text{SL}(\Lambda)$ is the smallest set such that for any $s,w,u \in \text{SL}(\Lambda)$,
    %
    \begin{enumerate}
        \item Any axiom is a theorem, where the axioms are the statements
        %
        \[ s \Rightarrow (w \Rightarrow s) \]
        %
        \[ (s \Rightarrow (w \Rightarrow u)) \Rightarrow ((s \Rightarrow w) \Rightarrow (s \Rightarrow u)) \]
        %
        \[ (\neg s \Rightarrow \neg w) \Rightarrow ((\neg s \Rightarrow w) \Rightarrow s) \]
        %
        The axioms are known as (A1), (A2), and (A3) respectively.
        \item Modus Ponens holds in our system. If $s \Rightarrow w$ and $s$ are theorems, then $w$ is a theorem. When we apply modus ponens, we may write that the theorem was obtained by (MP).
    \end{enumerate}
    %
    We shall write $\vdash s$ to state that $s$ is a theorem.
\end{definition}

For statements, the `smallness' characterization gives us structural induction. For theorems, we obtain the notion of a `proof'. A statement $s$ is a theorem of $\text{SL}(\Lambda)$ if and only if there is a sequence of formulae $(s_1, \dots, s_n)$ such that $s_n = s$, and each $s_i$ is either an axiom, or is obtained from some $s_j$ and $s_k$ by modus ponens, where $j,k < i$. This sequence is known as a proof. Often, we list a proof from top to bottom, where we reference how we obtained each element of the sequence alongside the proof.

\begin{example}
    Let us construct a proof of $\vdash s \Rightarrow s$, for any $s \in \text{SL}(\Lambda)$.
    %
    \[ \fitchprf{\pline{s \Rightarrow s}}{
        \pline[1.]{(s \Rightarrow ((s \Rightarrow s) \Rightarrow s))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (A1)]\\
        \pline[2.]{(s \Rightarrow ((s \Rightarrow s) \Rightarrow s)) \Rightarrow ((s \Rightarrow (s \Rightarrow s)) \Rightarrow (s \Rightarrow s))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (A2)]\\
        \pline[3.]{((s \Rightarrow (s \Rightarrow s)) \Rightarrow (s \Rightarrow s))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1),(2),(MP)]\\
        \pline[4.]{(s \Rightarrow (s \Rightarrow s))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (A1)]\\
        \pline[5.]{s \Rightarrow s}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3),(4),(MP)]
    } \]
    %
    In future proofs, we shall be able to use $\vdash s \Rightarrow s$ implicitly, since we now know the statement is a theorem. We will denote its application by $(I)$.
\end{example}

In other proof systems, we add additional axioms, and prove results from these axioms. If $\Gamma$ is a subset of $\text{SL}(\Lambda)$, then we may consider each member of $\Gamma$ to be an axiom. We write $\Gamma \vdash s$ if one can prove $s$ assuming all elements of $\Gamma$ are axioms. That is, we may write a sequence $(s_1, \dots, s_n)$, where $s_n = s$, and each $s_i$ is either an axiom, an element of $\Gamma$, or is obtained by modus ponens from previous elements of the sequence.

\begin{theorem}[Deduction Theorem]
    If $\Gamma \cup \{ s \} \vdash w$, then $\Gamma \vdash s \Rightarrow w$.
\end{theorem}
\begin{proof}
    We prove the theorem by induction of the size of the proof of $w$. Consider a particular proof $(s_1, \dots, s_n)$ of $w$ from $\Gamma \cup \{ s \}$. Suppose that $n = 1$. Then $s_1 = w$, and $w$ must either be an axiom, an element of $\Gamma$, or equal to $s$. In the first and second case, the proof is equally valid in $\Gamma$, and so $\Gamma \vdash s \Rightarrow w$ follows from the axiom $(w \Rightarrow (s \Rightarrow w))$. If $s = w$, Then we have shown that $\Rightarrow w \Rightarrow w$, so obviously $\Gamma \vdash s \Rightarrow w$.

    Now we consider the problem proved for $m < n$. $s_n = w$ is either an axiom, an element of $\Gamma$, equal to $s$, or proved by modus ponens from $s_i = (u \Rightarrow w)$ and $s_j = u$. We have already shown all but the last case. By induction, $\Gamma \vdash s \Rightarrow (u \Rightarrow w)$ and $\Gamma \vdash s \Rightarrow u$. But
    %
    \[ (s \Rightarrow (u \Rightarrow w)) \Rightarrow ((s \Rightarrow u) \Rightarrow (s \Rightarrow w)) \]
    %
    is an axiom, so $\Gamma \vdash s \Rightarrow w$.
\end{proof}

\begin{example}
    For any statements $s$ and $w$, $\{ s \Rightarrow w, w \Rightarrow u \} \vdash s \Rightarrow u$. This follows from a basic application of (A2). But this implies the two cut rules, that
    %
    \[ \vdash (s \Rightarrow w) \Rightarrow ((w \Rightarrow u) \Rightarrow (s \Rightarrow u)) \]
    %
    \[ \vdash (w \Rightarrow u) \Rightarrow ((s \Rightarrow w) \Rightarrow (s \Rightarrow u)) \]
    %
    which is much more tricky to prove.
\end{example}

\begin{example}
    Let us prove double negation elimination $(\neg \neg E)$, $\vdash \neg \neg s \Rightarrow s$, by proving $\neg \neg s \vdash s$.
    %
    \[
    \fitchprf{\pline{\neg \neg s \Rightarrow s}}{
        \subproof{\pline[1.]{\neg \neg s}} {
            \pline[2.]{((\neg s \Rightarrow \neg \neg s)) \Rightarrow ((\neg s \Rightarrow \neg s) \Rightarrow s)}[(A3)]\\
            \pline[3.]{(\neg \neg s) \Rightarrow (\neg s \Rightarrow \neg \neg s)}[(A1)]\\
            \pline[4.]{(\neg s \Rightarrow \neg \neg s)}[(1),(3),(MP)]\\
            \pline[5.]{(\neg s \Rightarrow \neg s) \Rightarrow s}[(2),(4),(MP)]\\
            \pline[6.]{\neg s \Rightarrow \neg s}[(I)]\\
            \pline[7.]{s}[(5),(6),(MP)]
        }
        \pline[8.]{\neg \neg s \Rightarrow s}[(1-7),(DT)]
    }
    \]
    %
    Now lets prove $\vdash s \Rightarrow \neg \neg s$.
    %
    \[
    \fitchprf{\pline{s \Rightarrow \neg \neg s}}{
        \pline[1.]{\neg \neg \neg s \Rightarrow \neg s}[\ \ \ \ \ \ \ $(\neg \neg E)$]\\
        \subproof{\pline[2.]{s}} {
            \pline[3.]{(\neg \neg \neg s \Rightarrow \neg s) \Rightarrow ((\neg \neg \neg s \Rightarrow s) \Rightarrow \neg \neg s)}[\ \ \ \ \ \ \ (A3)]\\
            \pline[4.]{(\neg \neg \neg s \Rightarrow s) \Rightarrow \neg \neg s}[\ \ \ \ \ \ \ (1), (3), (MP)]\\
            \pline[5.]{s \Rightarrow (\neg \neg \neg s \Rightarrow s)}[\ \ \ \ \ \ \ (A1)]\\
            \pline[6.]{\neg \neg \neg s \Rightarrow s}[\ \ \ \ \ \ \ (2),(5),(MP)]\\
            \pline[7.]{\neg \neg s}[\ \ \ \ \ \ \ (4),(6),(MP)]
        }
        \pline[8.]{s \Rightarrow \neg \neg s}[\ \ \ \ \ \ \ (2-7), (DT)]
    }
    \]
\end{example}

\begin{example}
    Lets prove $\neg w \vdash w \Rightarrow u$, by proving $\neg w, w \vdash u$.
    %
    \[
    \fitchprf{\pline{\neg w \Rightarrow (w \Rightarrow u)}}{
        \subproof{\pline[1.]{\neg w}} {
            \subproof{\pline[2.]{w}} {
                \pline[3.]{(\neg u \Rightarrow \neg w) \Rightarrow ((\neg u \Rightarrow w) \Rightarrow u)}[(A3)]\\
                \pline[4.]{\neg w \Rightarrow (\neg u \Rightarrow \neg w)}[(A1)]\\
                \pline[5.]{\neg u \Rightarrow \neg w}[(1),(4), (MP)]\\
                \pline[6.]{(\neg u \Rightarrow w) \Rightarrow u}[(3),(5),(MP)]\\
                \pline[7.]{w \Rightarrow (\neg u \Rightarrow w)}[(A1)]\\
                \pline[8.]{\neg u \Rightarrow w}[(2),(7),(MP)]\\
                \pline[9.]{u}[(6),(8),(MP)]
            }
            \pline[10.]{w \Rightarrow u}[(2-9),(DT)]
        }
        \pline[11.]{\neg w \Rightarrow (w \Rightarrow u)}[(1-10), (DT)]
    }
    \]
    %
    This is a proof of {\bf the law of contradiction}, denoed $(LC)$.
\end{example}

\begin{example}
    Consider the following proof.
    %
    \[
    \fitchprf{\pline{(s \Rightarrow w) \Rightarrow (\neg w \Rightarrow \neg s)}}{
        \subproof{\pline[1.]{s \Rightarrow w}} {
            \subproof{\pline[2.]{\neg w}}{
                \pline[3.]{\neg w \Rightarrow (\neg \neg s \Rightarrow \neg w)}[(A1)]\\
                \pline[4.]{\neg \neg s \Rightarrow \neg w}[(2),(3),(MP)]\\
                \pline[5.]{(\neg \neg s \Rightarrow \neg w) \Rightarrow ((\neg \neg s \Rightarrow w) \Rightarrow \neg s)}[(A3)]\\
                \pline[6.]{(\neg \neg s \Rightarrow w) \Rightarrow \neg s}[(4),(5),(MP)]\\
                \subproof{\pline[7.]{\neg \neg s}}{
                    \pline[8.]{\neg \neg s \Rightarrow s}[$(\neg \neg E)$]\\
                    \pline[9.]{s}[(7),(8),(MP)]\\
                    \pline[10.]{w}[(1),(9),(MP)]
                }
                \pline[11.]{\neg \neg s \Rightarrow w}[(7-10), (DT)]\\
                \pline[12.]{\neg s}[(6),(11), (MP)]
            }
            \pline[13.]{\neg w \Rightarrow \neg s}[(2),(12),(MP)]
        }   
        \pline[11.]{(s \Rightarrow w) \Rightarrow (\neg w \Rightarrow \neg s)}[(1-10), (DT)]
    }
    \]
    %
    This is the {\bf law of contraposition} $(LCP)$.
\end{example}

\begin{example}
    Lets prove $\vdash (s \Rightarrow w) \Rightarrow ((\neg s \Rightarrow w) \Rightarrow w)$.
    %
    \[
    \fitchprf{\pline{(s \Rightarrow w) \Rightarrow ((\neg s \Rightarrow w) \Rightarrow w)}}{
        \subproof{\pline[1.]{s \Rightarrow w}} {
            \pline[2.]{(s \Rightarrow w) \Rightarrow (\neg w \Rightarrow \neg s)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (LCP)]\\
            \pline[3.]{\neg w \Rightarrow \neg s}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1),(2),(MP)]\\
            \subproof{\pline[4.]{\neg s \Rightarrow w}} {
                \pline[5.]{(\neg w \Rightarrow \neg \neg s)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (4),(LCP)]\\
                \pline[6.]{\neg \neg s \Rightarrow s}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $(\neg \neg E)$]\\
                \pline[7.]{(\neg w \Rightarrow \neg \neg s) \Rightarrow ((\neg \neg s \Rightarrow s) \Rightarrow (\neg w \Rightarrow s))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (\text{CUT})]\\
                \pline[8.]{(\neg \neg s \Rightarrow s) \Rightarrow (\neg w \Rightarrow s)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (5),(7),(MP)]\\
                \pline[9.]{\neg w \Rightarrow s}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6),(8),(MP)]\\
                \pline[10.]{(\neg w \Rightarrow \neg s) \Rightarrow ((\neg w \Rightarrow s) \Rightarrow w)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (A3)]\\
                \pline[11.]{(\neg w \Rightarrow s) \Rightarrow w}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $(3),(10),(MP)$]
            }
            \pline[13.]{(\neg s \Rightarrow w) \Rightarrow w}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $(2-9),(DT)$]
        }
        \pline[14.]{(s \Rightarrow w) \Rightarrow ((\neg s \Rightarrow w) \Rightarrow w)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $(1-10), (DT)$]
    }
    \]
    %
    This essentially proves that $(s \vee \neg s) \Rightarrow w$ implies $w$.
\end{example}

We shall verify that proofs never lead to contradiction.

\begin{theorem}
    If $\vdash s$, then $\vDash s$.
\end{theorem}
\begin{proof}
    This is a trivial proof by structural induction. Prove that all axioms are tautologies, and that the set of tautologies are closed under modus ponens.
\end{proof}

This theorem shows that there are some statements which are not provable in our system. In fact, if $s$ is provable, then $\neg s$ is not provable. We call an axiom like this {\bf absolutely consistant}. We shall show that all tautologies are provable, which shows the system is {\bf complete}.

\begin{lemma}
    Let $x_1, \dots, x_n \in \Lambda$ be variables in $s \in \text{SL}(\Lambda)$. Let $f$ be a truth assignment, and define $s' = s$ if $f_*(s) = \top$, or $s' = \neg s$ if $f_*(s) = \bot$. Then
    %
    \[ x_1', \dots, x_n' \vdash s' \]
\end{lemma}
\begin{proof}
    We prove by structural induction. If $s = x_1$, then $x_1' = s'$, and $s \vdash s$ is a trivial theorem. If $s = \neg w$, we consider two cases. If $w' = w$, then $s' = \neg \neg w$, and we have already shown $w \vdash \neg \neg w$, hence $x_1', \dots, x_n' \vdash s'$. If $w' = \neg w$, then $s' = w'$, and the theorem is trivial. If $s = w \Rightarrow u$, then either $f_*(w) = \top$ and $f_*(u) = \top$, or $f_*(w) = \bot$. In the first case, we have $x_1', \dots, x_n' \vdash u$, from which $x_1', \dots, x_n' \vdash w \Rightarrow u$ follows. In the second case, $x_1', \dots, x_n' \vdash \neg w$, and we have $\neg w \vdash (w \Rightarrow u)$.
\end{proof}

\begin{corollary}[Completeness Theorem]
    If $\vDash s$, $\vdash s$.
\end{corollary}
\begin{proof}
    Let $x_1, \dots, x_n$ be the variables in $s$. By the last lemma, we have
    %
    \[ x_1, \dots, x_n \vdash s\ \ \ \ \ \ \ \ \ \ x_1, \dots, \neg \neg x_n \vdash s \]
    %
    By the deduction theorem
    %
    \[ x_1, \dots, x_{n-1} \vdash x_n \Rightarrow s\ \ \ \ \ \ \ \ \ \ x_1, \dots, x_{n-1} \vdash \neg x_n \Rightarrow s \]
    %
    But then, since $\vdash (x_n \Rightarrow s) \Rightarrow ((\neg x_n \Rightarrow s) \Rightarrow s)$,
    %
    \[ x_1, \dots, x_{n-1} \vdash s \]
    %
    By induction, we find $\vdash s$.
\end{proof}

Before we finish our discussion of semantics, we note that there are many other axioms systems which can be used to define a propositional calculus (in the sense that they prove all tautologies). Of most interest to us is the axiom system whose only connective is the sheffer stroke, and whose only axiom schema is
%
\[ (B|(C|D))|[\{E|(E|E)\}|[(F|C)|((B|F)|(B|F))]] \]
%
and whose rule of inference is to infer $D$ from $B|(C|D)$, and $B$. Of course, it is outlandish to attempt proofs in such a system.

\section{Multivalued Logics and Independence}

In classical logic, there are two truth values, $\bot$ and $\top$. We build proof systems which `preserves' truth. That is, all axioms are tautologies, and anything we prove from these axioms are tautologies. However, there is nothing stopping us from consider a system with $n$ truth values $\{ T_0, T_1, \dots, T_n \}$. This may represent the degree of truth of a statement, or something subtler.Intuitionistic logic discards the law of contradiction. It distinguishes between statements which are justifiable -- that is, they are provable, and those statements which are never false, and these can be considered `multiple values' of truth. When creating proof systems for multi-valued logics, we hope the value of a statement which follows from other statements is less than or equal to the value of the original statement. For instance, consider Sorites paradox
%
\begin{quote}
    (1) ``A grain of sand is not a heap of sand''.\\
    (2) ``Adding a grain of sand to something which is not a heap does not make it into a heap''\\
    (3) ``A grain cannot be turned into a heap by adding grains.''
\end{quote}
%
We fix this argument by considering it in a multi-valued logic system. For each $k$, consider the statements
%
\begin{quote}
    (1.$k$) ``k grains of sand do not make a heap''.\\
    (2.$k$) ``Adding a grain to $k$ grains of sand doesn't make $k+1$ grains a heap''\\
    (3.$k$) ``$k+1$ grains do not make a heap.''
\end{quote}
%
Provided we choose the truth of $(2.k)$ to be less than the truth of $(1.k)$, when the truth of $(3.k)$ is less than the truth of $(1.k)$, then this argument is invalid. We are only allowed to conclude $(3.k)$ from $(1.k)$ and $(2.k)$ when $(3.k)$ is `less true' than $(1.k)$ and $(2.k)$. As $k \to \infty$, this implies that the statement eventually becomes completely false, unless we have infintely many truth values. This book is not intended to talk about non-standard logics in detail. Instead, we use the notion of multivalued logic to attack the notion of independance. Are the axioms of our Hilbert system minimal -- do some of the axioms apply some of the other axioms?

Let us start by discarding (A1) from our axiom system. We may only derive results from (A2), (A3), and (MP). Can we derive (A1) using these rules? We shall try and set a value system on this proof system which satisfies the decreasing property of proofs. We shall consider `truth assignments' based on the table
%
\begin{center}
    \begin{tabular}{| c | c | c | c | }
        \hline $x$ & $y$ & $H_\Rightarrow(x,y)$ & $H_\neg(x)$ \\
        \hline $0$ & $0$ & $0$ & $1$ \\
        $0$ & $1$ & $2$ & $1$ \\
        $0$ & $2$ & $2$ & $1$ \\
        $1$ & $0$ & $2$ & $1$ \\
        $1$ & $1$ & $2$ & $1$ \\
        $1$ & $2$ & $0$ & $1$ \\
        $2$ & $0$ & $0$ & $0$ \\
        $2$ & $1$ & $0$ & $0$ \\
        $2$ & $2$ & $0$ & $0$ \\
        \hline
    \end{tabular}
\end{center}
%
With this definition, we may consider truth values of formulae. Note that $x \Rightarrow (y \Rightarrow x)$ evaluates to 2 when $x$ is 1 and $y$ is 2. We shall show any provable formula from (A2) and (A3) always takes the value zero. One calculates that this is true of any instance of (A2) and (A3). Furthermore, if $s$ and $s \Rightarrow w$ is always zero, then $w$ is always zero. But this shows that $x \Rightarrow (y \Rightarrow x)$ can never be proven using (A2) and (A3).







\chapter{First Order Logic}

Remember our conclusion that ``Julie is a human'' and ``Laura is a human'' from the general statement ``All women are human'' in the first chapter? We shall attempt to formalize manipulation of these statements.






\part{Computability}

What is computation? Until the early 20th century, little had been done to address this fundamental hole in mathematical logic. It is often the case that precise definitions give rise to easy proofs of the most surprising consequence. In 1931, Kurt G\"{o}del proved all sufficiently complicated axiomatic systems had unprovable theorems, but a fundamental question remained; how do we decide whether a theorem can be proved? It took a decade for Alonzo Church and Alan Turing to deduce the impossibility of such a claim. Fifty years later, `theoretical computers' had become a common reality. We shall describe mathematical models of computation which have been developed to analyze the limitations of various computational methods.

Turing and Church's major breakthrough was precisely defining a `computational procedure'. Philosophically, one should be able to define a procedure without reference to a computer, for humans computed long before microchips. On the other hand, models should reflect physical reality, since one needs a physical mechanism in order to computer, whether electronic or mental. It is hoped that these models accurately reflect the limitations of computational models, and in the past 80 years no-one has disproved this claim.

We will begin by analyzing the automaton, a model of computation without stored memory. We will expand the amount of expression of the automaton by considering context-free grammars. Finally, we add memory by considering a Turing machine. It is the Church Turing theses that this is the ultimate model of computation -- any real world computation can be modelled as an action on a Turing machine. From this model, and with the hypothesis of Church and Turing, we can make precise, philisophically interesting statements about the nature of computation.

As in mathematical logic, the objects of study are strings of symbols over a certain alphabet. One can also understand the notion of computation syntactically. The main idea of computability theory is that a mental computation can be modelled as a {\bf decision problem} -- find a computational model which will `accept' certain strings over an alphabet. Suppose our problem is to add two numbers. We are given $a$ and $b$. The analgous decision problem decides, given $a$, $b$, and $c$, whether $a + b = c$. Our symbol set is $\{ 0, 1, \dots, 9, : \}$, and we wish to model a computation which accepts all strings of the form $``a:b:c''$, where $a$, $b$, and $c$ are decimal strings for which $a + b = c$.

A subset of the set of all strings over an alphabet will be known as a {\bf language}, and the goal of a decision problem is to design a machine which accepts only strings in the specified language. The goal of understanding computation reduces a languages structure.

As a more dynamic discipline than mathematical logic, we need more operations on strings to obtain languages from other languages. We obviously need concatenation, but also {\bf reversal}, which will be denoted $s^R$. These operations are extended to languages by applying the operations on a component by component basis:
%
\[ S \circ W = \{ s \circ w : s \in S, w \in W \}\ \ \ \ \ S^R = \{ s^R : s \in S \} \]
%
A {\bf palindrome} is a string $s$ for which $s^R = s$. If $\Sigma$ is a set of strings, we shall let $\Sigma_\varepsilon = \Sigma \cup \{ \varepsilon \}$.



\chapter{Finite State Automata}

Our initial model of computability is a computer with severely limited memory. Surprisingly, we shall still be able to compute a great many things. The idea of this model rests on a state diagram. Suppose we would like to describe an algorithm describing if a number is divisible by three. We shall represent a number by a string of dashes. For instance, $``-----''$ represents the number 5. We describe the algorithm in a flow chart below
%
\begin{center}
\begin{tikzpicture}[scale=0.2]
\tikzstyle{every node}+=[inner sep=0pt]
\draw [black] (39.8,-19.2) circle (3);
\draw [black] (39.8,-19.2) circle (2.4);
\draw [black] (46.4,-30) circle (3);
\draw [black] (34,-30) circle (3);
\draw [black] (39.8,-13.3) -- (39.8,-16.2);
\fill [black] (39.8,-16.2) -- (40.3,-15.4) -- (39.3,-15.4);
\draw [black] (41.36,-21.76) -- (44.84,-27.44);
\fill [black] (44.84,-27.44) -- (44.85,-26.5) -- (43.99,-27.02);
\draw (43.74,-23.32) node [right] {$-$};
\draw [black] (43.4,-30) -- (37,-30);
\fill [black] (37,-30) -- (37.8,-30.5) -- (37.8,-29.5);
\draw (40.2,-30.5) node [below] {$-$};
\draw [black] (35.42,-27.36) -- (38.38,-21.84);
\fill [black] (38.38,-21.84) -- (37.56,-22.31) -- (38.44,-22.78);
\draw (36.22,-23.43) node [left] {$-$};
\end{tikzpicture}
\end{center}
%
The algorithm proceeds as follows. Begin at the top node. For each dash, proceed clockwise around the triangle. If, at the end of our algorithm, we end up back at the top node, then the number is divisible by three. The basic idea of the finite automata is to describe computation via these flow charts -- we follow a string around a diagram, and if we end up at a `accept state', then we accept the string.

\begin{definition}
    A {\bf finite state automaton} is a 5-tuple $(Q, \Sigma, \Delta, q_0, F)$, where $Q$ is a finite set of states, $\Sigma$ is a finite alphabet, $q_0$ is the start state, $F \subset Q$ are the accept states, and
    %
    \begin{enumerate}
        \item In a {\bf deterministic finite state automata}, $\Delta: Q \times \Sigma \to Q$ is the transition function.
        \item In a {\bf non-deterministic finite state automata}, $\Delta: Q \times \Sigma_\varepsilon \to \mathcal{P}(Q)$ is the non-deterministic transition function.
    \end{enumerate}
\end{definition}

We shall begin by discussing deterministic automata. A finite state machine `works' exactly how our original algorithm worked. Draw a directed graph whose nodes are $Q$, and draw an edge between $s \in Q$ and $\Delta(s, \lambda)$, for each $\lambda \in \Sigma$. Take a string $s \in \Sigma^*$. We begin at the start state $q_0$. Sequentially, for each symbol in $s$, we take our current state, and using the transition function $\Delta$ move to the next state linked by the symbol in $s$ we are currently at. If, after we have gone through every symbol, we are at a state in $F$, then $s$ is an `accepted' string. Formally, what we are doing is extending $\Delta$ to $Q \times \Sigma^*$. We define $\Delta(\varepsilon, q) = q$, and, for $t \in \Sigma$, $\Delta(ts, q) = \Delta(t,\Delta(s,q))$.

\begin{definition}
    Let $M = (Q, \Sigma, \Delta, q_0, F)$ be a deterministic finite state automata. Then $M$ {\bf accepts} a string $s$ in $\Sigma^*$ if $\Delta(s,q_0) \in F$. We call the set of all accepting strings the {\bf language} of $M$, and denote the set as $L(M)$. A subset of $\Sigma^*$ which is a language of a deterministic finite state automata is known as a {\bf regular language}.
\end{definition}

\begin{example}
    Consider $\Sigma = \{ - \}$, as we have previously considered. Then the set of all `dashes divisible by three' is regular, as we have shown. Formally, if we take $Q = \mathbf{Z}_3$, $\Delta(x,-) = x+1$, $q_0 = 0$, and $F = \{ 0 \}$, then $(Q, \Sigma, \Delta, q_0, F)$ recognizes the language. The `graph' of the automata is exactly the graph we considered above.
\end{example}

The set of regular languages is conveniently closed under many operations. We shall find enough such operations that we can describe all regular languages as composed from basic languages from certain operations.

\begin{theorem}
    If $A, B \subset \Sigma^*$ are regular languages, then $A \cup B$ is regular.
\end{theorem}
\begin{proof}
    let $M = (Q, \Sigma, \Delta, q_0, F)$ and $N = (R, \Sigma, \Gamma, r_0, G)$ be automata recognizing $A$ and $B$ respectively. We shall define a finite automata recognizing $A \cup B$. Let $S = Q \times R$, and $(\Delta \times \Gamma): S \times \Sigma \to S$ by letting
    %
    \[ (\Delta \times \Gamma)(q,r,\lambda) = (\Delta(q), \Gamma(r)) \]
    %
    Let $H = \{ (q,r) \in S : q \in F\ \text{or}\ r \in G \}$. We contend $(S, \Sigma, \Delta \times \Gamma, (q_0, r_0), H)$ recognizes $A \cup B$, for, by induction, one verifies that $(\Delta \times \Gamma)(s, (q,r)) = (\Delta(s,q), \Gamma(s,r))$, and thus $(\Delta \times \Gamma)(s, (q_0,r_0)) \in H$ if and only if $s$ is in $L(M)$ or $s$ is in $L(N)$.
\end{proof}

In future proofs, we shall be less detailed in constructing machines.

\begin{theorem}
    If $A$ is a regular language, then $A^c$ is regular.
\end{theorem}
\begin{proof}
    Take the complement of the accepting states in a machine.
\end{proof}

\begin{corollary}
    If $A$ and $B$ are regular languages, then $A \cap B$ are regular.
\end{corollary}

\section{Non Deterministic Automata}

It shall be convenient to expand our definition of automaton, in a manner which we shall repeat with more complicated models. It is a version of a finite automata that can parallel process. This is the use of the non-deterministic automaton. We shall a say a state $t$ is {\bf $\varepsilon$ reachable} from $u$ if there is a sequence of states $t_0, t_1, \dots, t_n$ with $t_0 = t$, $t_n = u$, and $t_{i+1} \in \Delta(t_i, \varepsilon)$. The set of all such states will temporarily be denoted $E(u)$.

\begin{definition}
    In a non-deterministic finite state automata, we {\bf accept} a string $s$ if $s = q_1 \dots q_n$, where each $q_i \in \Sigma_\varepsilon$, and a sequence of states $t_0, \dots, t_{n+1}$, with $q_0 = t_0$ and $t_n \in F$, such that $t_{k+1} \in \Delta(t_k, s_k)$. The set of accepting strings of a machine $M$ form the language $L(M)$.
\end{definition}

We draw a graph with nodes $Q$, and with directed edges $v$ to $w$ if $w \in \Delta(v, \Sigma_{\varepsilon})$. We begin at $q_0$. For a string $s$, we attempt to find a path from $q_0$ to an accept state, by following edges whose corresponding symbol is in $s$ (or whose symbol is $\varepsilon$, in which we get for free). The string is accepted if such a path is possible.

It seems to be a much more complicated procedure to find if a string is accepted by a non-deterministic automata, but it turns out that every non-deterministic automata can be converted into a deterministic automata. The proof relies on the fact that we may exploit the operations of non-determinism, described using power sets of a set.

\begin{theorem}
    A language recognized by a non-deterministic finite state automata is regular.
\end{theorem}
\begin{proof}
    Let $M = (Q, \Sigma, \Delta, q_0, F)$ be a non-deterministic automata. Let
    %
    \[ N = (\mathcal{P}(Q), \Sigma, \Gamma, \{ q_0 \}, \{ S \in \mathcal{P}(Q) : S \cap F \neq \emptyset \} \]
    %
    where
    %
    \[ \Gamma(S, t) = \{ u \in Q : \exists s \in S : u \in \Delta(s,t) \} \]
    %
    Suppose $s$ is accepted by $N$. Then $\Delta(\{ q_0 \}, s) \cap F \neq \emptyset$. Thus $s = q_1 \dots q_n$, and there are states $t_0 \dots t_n$, with $t_0 = q_0$ and $t_{k+1} \in \Delta(t_k, q_k)$, and $q_n \in F$. Thus $s$ is accepted by $M$. Conversely, if $s$ is accepted by $M$, then $\Delta(\{ q_0 \}, s) \cap F \neq \emptyset$, so $s$ is accepted by $N$.
\end{proof}

We may now emply non-deterministic automata to understand deterministic automata.

\begin{theorem}
    If $A$ and $B$ are regular languages, then $A \circ B$ is regular.
\end{theorem}
\begin{proof}
    Let $M = (Q, \Sigma, \Delta, q_0, F)$ and $N = (R, \Sigma, \Gamma, r_0, G)$ be languages accepting $A$ and $B$ respectively. Without loss of generality, assume $Q$ is disjoint from $R$. Consider the machine
    %
    \[ O = (Q \cup R, \Sigma, \Pi, q_0, G) \]
    %
    Define
    %
    \[ \Pi(s,t) = \begin{cases} \{ \Delta(s,t) \} & s \in Q\text{, and}\ t \neq \varepsilon\ \text{or}\ s \not \in F \\ \{ \Delta(s,t), r_0 \} & s \in F, t = \varepsilon\\ \{ \Gamma(s,t) \} & s \in R\\
    \emptyset & \text{otherwise} \end{cases} \]
    %
    We verify $sw \in L(O)$ for $s \in L(M)$, $w \in L(N)$. Now $r_0 \in \Pi(q_0, s)$, since $\Delta(t,w) \subset \Pi(t,w)$ for all $t \in Q$, and if $\Pi(t, w) \cap F = \emptyset$, $r_0 \in \Pi(t, w)$ by the $\varepsilon$ closure property. But, similarily, $\Pi(r_0, w) \cap G \neq \emptyset$, from which it follows that $\Pi(t, sw) \cap G \neq \emptyset$.

    If $v \in L(O)$, there is some substring $k$ such that $\Pi(q_0, k) = r_0$ (for this is the only path from $q_0$ to $G$). If $k$ is the shortest string for which this occurs, then $k \in L(O)$, for either $k = \varepsilon$
\end{proof}

\begin{theorem}
    If $A$ is a regular language, $A^*$ is regular.
\end{theorem}
\begin{proof}
    s
\end{proof}

\section{Regular Expressions}

Automata are equivalent to a much more classical notion of computation -- regular expressions, which are much more easy to manipulate in some circumstances.

\begin{definition}
    A {\bf regular expression} over an alphabet $\Sigma$ is the smallest subset of $(\Sigma \cup \{ \emptyset, \cup, *, (, ) \})^*$ such that
    %
    \begin{enumerate}
        \item $\emptyset$ is a regular expression as are $s \in \Sigma^*$.
        \item If $s$ and $w$ are regular expressions, then so are $s^*, (s \cup w)$, and $sw$.
    \end{enumerate}
\end{definition}

Every regular expression describes a regular language in the following manner.

\begin{definition}
    A string $s \in \Sigma^*$ is recognized by an regular expression $t$ if
    %
    \begin{enumerate}
        \item $t \in \Sigma^*$, and $s = t$.
        \item $t = (t' \cup t'')$, and $s$ is recognized by $t'$ or by $t''$.
        \item $t = t't''$, and $s = s's''$, where $s'$ is recognized by $t'$, and $s''$ is recognized by $t''$.
        \item $t = t'*$, and $s = \varepsilon$ or $s = s_1 \dots s_n$, where $s_i$ is recognized by $t'$.
    \end{enumerate}
    %
    The regular language corresponding to a regular expression $t$ is $L(t)$, the set of strings recognized by $t$. The set of all regular expressions on a language will be denoted $R(\Sigma)$
\end{definition}

It is a simple consequence of our discourse that every language recognized by a regular expression {\it is actually} a regular language. In fact, we can show that every regular language is described by a regular expression. We shall describe an algorithm for converting a finite state automaton to a regular expression. We shall make a temporary generalization, by allowing non deterministic finite automata to have regular expressions in their transition functions.

\begin{definition}
    A generalized non-deterministic finite state automate is a 5-tuple $M = (Q,\Sigma, \Delta, q_0, F)$, where $\Delta: Q \times Q \to R(\Sigma)$. A string $s \in \Sigma^*$ is accepted by $M$ if we may write $s = s_1 \dots s_n$, and there are a sequence of states $q_0, \dots, q_{n+1}$ where $q_n \in F$, and $\Delta(q_i, q_{i+1})$ recognizes $s_i$.
\end{definition}

We shall also assume a generalized automaton has one one end state, and that there are no states connecting to the beginning state of the automaton, which we can always model a finite automaton to possess without affecting the accepting language.

\begin{theorem}
    Every generalized automaton is equivalent to a regular expression.
\end{theorem}
\begin{proof}
    We shall prove this theorem for the more general class of generalized automaton. If the generalized automaton has two states, then there is only one transition from start state to begin state, and this transition describes a regular expression for the automaton. We will reduce every automaton to this form by induction. Suppose an automaton has $n$ states. Fix a state $q \in Q$, which is neither the beginning or accepting state. Define a new automaton
    %
    \[ N = (Q - \{ q \}, \Sigma, \Delta', q_0, F) \]
    %
    where $\Delta'(a,b) = (\Delta(a,b) \cup \Delta(a,q) \Delta(q,q)^* \Delta(q,b))$
    %
    Then $N$ is equivalent to $M$, and has one fewer state, and is thus equivalent to a regular expression.
\end{proof}

\begin{corollary}
    Every regular language is described by a regular expression.
\end{corollary}


\section{Limitations of Finite Automata}

We've discovered a menagerie of different problems we can solve with finite automata, but it has already been foreshadowed that better machines await. Here we attack problems which cannot be solved by finite automata.

\begin{theorem}[Pumping Lemma]
    Let $L$ be a regular language. Then there is a number $p$, called the pumping length, such that if $s \in L$ satisfies $|s| \geq p$, then we may write $s = wuv$, where $|u| > 0$, $|wu| \leq p$, and $wu^iv \in L$ for all $i \geq 0$.
\end{theorem}
\begin{proof}
    Apply pidgeonhole principle.
\end{proof}

\begin{example}
    $L = \{ 0^k10^k : k \geq 0 \}$ is not regular. If it was regular, it would have a pumping length $p$. Then $0^p10^p$ is in $L$, so we may write $0^p10^p = wuv$, where $|wu| < p$. Then $u = 0^k$, for $k > 0$, and $wv = 0^{p-k}10^p \in L$, which is clearly not the case, contradicting regularity.
\end{example}
 derivable




\chapter{Context Free Languages}

Finite state machines are not a very general method of c of variablesomputation. We would like our notion of computability to decide on a much more complicated set of problems. Thus we must define new classes of computable language.

\begin{definition}
    A {\bf Context Free Grammar} is $(V,\Sigma,R,S)$, where $V$ is a set of variables, $\Sigma$ is a character set, disjoint from $V$, $R$ is a relation between $V$ and $(V \cup \Sigma)^*$, and $S \in V$ is the start variable.
\end{definition}

We say a string $w \in (V \cup \Sigma)^*$ is {\bf directly derivable} from $s \in (V \cup \Sigma)^*$ if $w = tvt'$, $s = tt''t'$ where $v \in V$, $t,t',t'' \in (V \cup \Sigma)^*$, and $(v,t'') \in R$. The smallest transitive relation containing the `directly derivable' relation is the derivability relation. To reiterate, a string $s$ is {\bf derivable} from $w$ if there is a sequence $s_0, \dots, s_n$, with $s_0 = s$, $s_n = w$, and $s_{i+1}$ is directly derivable from $s_i$. Such a sequence is known as a {\bf derivation}. The language of a grammar is the set of all strings in $\Sigma^*$ derivable from the start state $S$. As in finite automata, the language of a grammar $G$ will be denoted $L(G)$. A language is {\bf context-free} if it is the language of a context free grammar.

\begin{lemma}
    Let $G = (V,\Sigma,R,S)$ and $H = (V,\Sigma,R',S)$ be two different context free languages. If every $(v,s) \in R'$, is derivable in $G$, then $L(H) \subset L(G)$.
\end{lemma}
\begin{proof}
    If we can show that every direct derivation in $H$ is a derivation in $G$, then all derivations in $H$ are derivations in $G$, since derivations form the smallest transitive relation containing the direct derivations in $H$, and the derivations in $G$ certainly satisfy this. If $w$ is directly derived from $s$ in $H$, then there is a rule $(B,u)$, $s = rBt$, and $w = rut$. There is a sequence $s_0 \dots s_n$ deriving $u$ from $B$ in $G$. But then $(rs_0t) \dots (rs_nt)$ is a derivation of $w$ from $s$, so $w$ is derivable from $s$ in $G$.
\end{proof}

A string is {\bf ambiguous} if it can be derived in two different ways. A grammar is ambiguous if its language contains an ambiguous string. Ambiguity is unfortunate when parsing a language, since it means we may be able to interpret the language in two different ways.

\begin{example}
    First order logic can be defined by an unambiguous grammar. To develop the language, we took a bottom up approach, but we can also take a top up approach. Consider any particular first order language $\text{FO}(\Lambda, \{ \mathcal{P}^n \}, \{ \mathcal{F}^n \})$, where $\Lambda$ are variables, $\mathcal{P}^n$ are $n$-ary predicates, and $\mathcal{F}^n$ are $n$-ary functions. Then the terms of the language form a grammar. First, we must include function formation rules. For each $f \in \mathcal{F}^n$, and $x \in \Lambda$ we have the rules
    %
    \[ T \mapsto f(\underbrace{T, T, \dots, T}_{n\ \text{times}}) | x \]
    %
    Then the terms form an unambiguous grammar, as we showed. The formulas are then defined by the rules, for each $P \in \mathcal{P}^n$ and $x \in \Lambda$, by
    %
    \[ F \mapsto P(T, T, \dots, T) | T = T | (F \wedge F) | (F \vee F) | (\neg F) | (F \Rightarrow F) | (\forall x: F) | (\exists x: F) \]
    %
    Then this grammar is also unambiguous.
\end{example}

It took a lot of work to show that first order logic is unambiguous. This make sense, because in general it is impossible to decide whether a language is unambiguous.

\begin{definition}
    A grammar $(V,\Sigma,R,S)$ is in {\bf Chomsky Normal Form} if the only relations in $R$ are of the form $(A,BC)$, where $A,B,C \in V$, $B,C \neq S$, or $(A,a)$, where $a \in \Sigma$.
\end{definition}

\begin{theorem}
    Every context-free language can be recognized by a context-free grammar in Chomsky normal form.
\end{theorem}
\begin{proof}
    We shall reduce any context free grammar $G_0 = (V,\Sigma,R_0,S)$ to a context free grammar in normal form. We may assume that any rule does not link to $S$, by adding an additional start variable which links to $S$. First, we shall remove any $\varepsilon$ rules. Order the set of all variables in $V - \{ S \}$:
    %
    \[ A_1, \dots, A_n \]
    %
    Recursively define grammars $G_n = (V,\Sigma,R_n,S)$, where
    %
    \[ R_n = \left( R_{n-1} - \{ (A_n, \varepsilon) \} \right) \cup \{ (B: sw) : (B,sA_nw) \in R, sw \neq \varepsilon\ \text{or}\ \neg (\exists i \leq n : B = A_i) \} \]
    %
    Then $L(G_n) = L(G_{n-1})$. Certainly $L(G_n) \subset L(G_{n-1})$ by the lemma, since any rule in $G_n$ is derivable in $G_{n-1}$. Let $(s_0, \dots, s_n)$ be a derivation with $s_0 = S$.
\end{proof}

\end{document}