\documentclass[12pt]{report}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsopn}
\usepackage{kpfonts}
\usepackage{graphicx}
\usepackage{kbordermatrix}

% Standard tool for drawing diagrams.
\usepackage{tikz}
\usepackage{tkz-berge}

% Draws commutative diagrams in TIKZ.
\usepackage{tikz-cd}

%
\usepackage{multicol}

%
\usepackage{framed}

%
\usepackage{mathtools}

%
\usepackage{float}

%
\usepackage{subfig}

%
\usepackage{wrapfig}

%
\usepackage{mathabx}

% Used for generating `enlightening quotes'
\usepackage{epigraph}

% Forget what this is used for :P
\usepackage[utf8]{inputenc}

% Used for generating quotes.
\usepackage{csquotes}

% Used
\usepackage{diagrams}

% Allows what to generate links inside
% generated pdf files
\usepackage{hyperref}

% Allows one to customize theorem
% environments in mathematical proofs.
\usepackage{thmtools}

% Gives access to a proof
\usepackage{lplfitch}

% I forget what this is for.
\usepackage{accents}

% A package for drawing simple trees,
% as a substitute for unnesacary TIKZ code
\usepackage{qtree}

% Enables sequent calculus proofs
\usepackage{ebproof}






\setlength\epigraphwidth{8cm}

\usetikzlibrary{arrows, petri, topaths}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{exercise}{Exercise}

%\theoremstyle{remark}
\newtheorem*{example}{Example}
\newtheorem*{proof*}{Proof}

\theoremstyle{definition}
\newtheorem*{defi}{Definition}
\newenvironment{definition}
    {\begin{samepage}\begin{framed}\begin{defi}}
    {\end{defi}\end{framed}\end{samepage}}





\usepackage{hyperref} 
\hypersetup{
    colorlinks = true,
    linkcolor = black,
}

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\renewcommand*\contentsname{\hfill Table Of Contents \hfill}

\newcommand{\optionalsection}[1]{\section[* #1]{(Important) #1}}
\newcommand{\deriv}[3]{\left. \frac{\partial #1}{\partial #2} \right|_{#3}} % partial derivative involving numerator and denominator.
\newcommand{\lcm}{\operatorname{lcm}} % Lowest Common Multiple.
\newcommand{\im}{\operatorname{im}} % Image of a function.
\newcommand{\bint}{\mathbf{Z}} % Bold integer Symbol.
\newcommand{\gen}[1]{\langle #1 \rangle} % Generator of a group.
\newcommand{\End}{\operatorname{End}}
\newcommand{\Mor}{\operatorname{Mor}}
\newcommand{\Id}{\operatorname{id}}
\newcommand{\visspace}{\text{\textvisiblespace}}
\newcommand{\Gal}{\text{Gal}}
\newcommand{\coker}{\text{coker}}

\DeclareMathOperator{\Dom}{Dom}

% Sources: Abraham/Marsten Foundations of Mechanics



\title{On Molecules and Natural Numbers:\\
An Introduction to Ergodic Theory}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}

\maketitle

%\tableofcontents

\pagenumbering{arabic}

These notes introduce Ergodic theory, which is a beautiful branch of mathematics which isn't talk in undergraduate curriculums very often. It's such a shame, because of the beautiful visual results we find in the theory, so in the next 20 minutes hopefully these notes can show you why you need to go out into the world after this talk and start learning the subject!

Historically, the story of ergodic theory begins in the 17th century, as does most of analysis, with the invention of Newton's calculus. With it began the `quantitative age' of mathematics, which focused on techniques of integration and of the exact solution of differential equations arising in physics. But in the late 19th century, the quantitative techniques hit an impasse. It was shown that the differential equation defining the motion of three or more planets could not be solved numerically - a fact analogous to Galois' proof that one could not formulate general solutions to the quintic equation. Such a crisis could only be solved by one of the greats, and it is with a particular great, the Frenchman Henri Poincar\'{e}, that ergodic theory began to take its form.

The classical method of looking at a differential equation is algebraic. We define an equation
%
\[ \frac{d^2 y}{dt^2} = t^2 \frac{dy}{dt} - ty \]
%
the game is then to apply certain solution techniques to reduce the formula to a simpler statement, with the main goal of obtaining an expression for $y$ as a function of $t$, unique after obtaining certain initial conditions (in this case, the initial position $x_0$, velocity $\dot{x}_0$, and time $t$). A revolutionary view of differential equations emerges when we see a differential equation instead as an {\it operator} on the space of initial conditions of the equation, transforming the conditions into conditions in the future.

Poincare's methods are characterized by looking at a solution from a global point of view, rather than a local point of view. Instead of finding a solution for a particular initial condition, we try and find the action of the differential equation on the space of all initial conditions -- solving all equations simultaneously. It is important to note that the initial conditions must describe the infinitisimal changes in the individual variables at present. For instance, in this equation we know that the change in position (velocity), the change in velocity ($t^2 \dot{x}_0 - tx$), and the change in time (constant, since time passes uniformly). Therefore, a differential equation gives us a vector field on the `phase space' of conditions, in this case
%
\[ v(t,x,\dot{x}) = (1,\dot{x},t^2 \dot{x} - tx) \]
%
The theory of uniqueness and existence for differential equations tells us that, for each $(t,x,\dot{x})$, there is a unique function $g(t)$ solving the differential equation in a certain time interval. Such a function gives us a particular curve in phase space, taking the conditions of the function at a particular time point. By uniqueness, we can put all these curves together to obtain an `evolution' function $f$, which takes a certain initial condition $x_0$, and a certain number $t$, and gives us the position $f_t(x_0)$ of the curve in phase space, $t$ seconds after it begins to move. With this switch, it is much easier to describe `qualitative facts about the differential equation', since we have now boxed all solutions in a single object to study.

\begin{example}
    It's best to see this method in action. Consider the spring equation
    %
    \[ y'' = -y \]
    %
    The phase-space for this equation is $\mathbf{R}^2$ (one dimension for the position of the end of the spring, and one dimension for the velocity). We obtain the vector field
    %
    \[ v(x,\dot{x}) = (\dot{x}, -x) \]
    %
    If you draw out this vector field, it is easy to see that the action $f_t$ is just to rotate phase space clockwise by an angle $t$. Qualitatively, this tells us why the motion of the spring is periodic, and why the value
    %
    \[ x^2 + \dot{x}^2 \]
    %
    is preserved in a particular trajectory. The beauty of this approach is that we need no formula to see why this is true, we just look at the qualitative effects of the vector field.
\end{example}

\begin{example}
    If a particle is pushed from left to right on the number line at a uniform velocity, then the differential equation is
    %
    \[ y' = C \]
    %
    with corresponding vector field $v(x) = C$, and whose propogation operators are translations $f_t(x) = x + Ct$.
\end{example}

Ergodic theory is especially concerned with the long-term qualitative effects of dynamical systems. That is, properties of the limiting values of $f_t(x)$ as $t \to \infty$. In this case, we do not really need $t$ to take on all real values. We really need only consider
%
\[ f_0(x), f_1(x), f_2(x), \dots \]
%
It only takes a little bit of thinking to convince yourself that $f_t \circ f_s = f_{t+s}$ (if we wait $s$ seconds, and then wait $t$ seconds, we are waiting $t + s$ seconds). In this case
%
\[ f_n = f_1 \circ f_1 \circ \dots \circ f_1 = f_1^n \]
%
So we can consider Ergodic theory the study of a single map $T:X \to X$, and it's iterates
%
\[ T, T^2, T^2, \dots \]
%
The real fun begins when $T$ is invertible, which induces a group action from $\mathbf{Z}$ into $\text{Inv}(X)$.
%
\[ \dots, T^{-2}, T^{-1}, \mathbf{1}, T, T^2, \dots \]
%
which is occasionally useful.

Let us now return to the historical development of the subject, to the application of Poincare's new view of differential equation in physics. The standard equation of motion in a `Newtonian System' is
%
\[ m \ddot{x} = F(x) \]
%
In 1833, the scientist William Hamilton, discovered that by a change in coordinates one could discover a much more beautiful representation of classical mechanics. If $p = m \dot{x}$ denotes the momentum of a particle at a certain time, and we rename the position coordinate $x$ to $q$ (the reason being because that's what Hamilton did rather than for a logical reason), then we may express the kinetic energy of the particle as
%
\[ \frac{m\dot{q}^2}{2} = \frac{p^2}{2m} \]
%
Often, we find that there is a scalar function $V(q)$ such that $\nabla V(q) = - F(q)$; such a function is known as the potential energy of the system. With these definitions in hand, we define the total energy of the system, known as the Hamiltonian, as
%
\[ H(p,q) = \frac{p^2}{2m} + V(q) \]
%
$H$ is known as the Hamiltonian of a physical system. Now in this form, Newton's laws take the pleasant form
%
\[ \dot{p} = - \frac{\partial H}{\partial q}\ \ \ \ \ \ \ \ \ \ \dot{q} = \frac{\partial H}{\partial p} \]
%
Note that this {\it isn't} a partial differential equation to be solved, since $H$ is a known quantity, and we are solving for $p$ and $q$. The main reason to apply Hamilton's equations is that the approach generalizes much more simply to arbitrary coordinate systems, and it is often much easier to define $H$, rather than the forces (try defining the forces for a system of balls attached to each other by pulleys, rather than just defining the potential energy of the balls, and you'll get what I mean).

Now suppose we only know the initial position and momentum of an particle to a certain precision -- then the initial conditions of our object in phase space lie in a certain area $U$. If we watch $U$ evolve, we learn that the object must eventually lie in $f_t(U)$, for each $t$. But how does the precision of our measurements change over time? $f_t(U)$ might shrink in size, so that our measurements become more accurate, so they grow inaccurate.

\begin{lemma}
    If a vector field $v$ has divergence zero,
    %
    \[ \text{div}\ v = \sum \frac{\partial v^i}{\partial x^i} = 0 \]
    %
    Then the operators $f_t$ preserve volume.
\end{lemma}
\begin{proof}
    Let $D$ be a region of space, define $w(0)$ to be the volume of $D$, and more generally, define $w(t)$ to be the volume of $f_t(D)$. Now $f_t$ is a diffeomorphism of $D$, so by the change of variables formula
    %
    \[ w(t) = \int_{f_t(D)} 1 = \int_{D} |\text{det} (Df_t) | \]
    %
    and hence
    %
    \[ w'(t) = \int_D \frac{d |\text{Det}(Df_t)|}{dt} \]
    %
    Now for small values of $t$,
    %
    \[ f_t(x) = x + v(x)t + O(t^2) \]
    %
    Thus
    %
    \[ \left( \frac{\partial f^i_t}{\partial x^j} \right) = I + (Dv) t + O(t^2) \]
    %
    One has that for any matrix $M$,
    %
    \[ \text{det}(I + t M) = 1 + t\ \text{tr}(M) + O(t^2) \]
    %
    obtained by expanding factors, and ignoring factors including $t^2$. The trace of $Dv$ is exactly the divergence of $v$, so it follows that
    %
    \[ \frac{d|\text{det}(I + t Dv)|}{dt} = \text{div}(v) \]
    %
    proving the claim when $\text{div}(v) = 0$.
\end{proof}

Louiville observed that the vector field governing the motion in Hamiltonian mechanics has zero divergence (mixed partials are equal), which means

\begin{theorem}
    Volume is preserved by motion in momentum-position space, under the action of the Hamiltonian equations.
\end{theorem}

Thus we have a kind of `Heisenberg uncertainty principle' in classical mechanics. We can never infer a more accurate measurement from an initial measurement, without losing accuracy in other measurements in the process -- the `area' of the measurements must remain the same.

Now the great part about measure preserving transformations is that they cannot `stretch' or `squish' any part of space, since the volume of that portion of space must stay the same. In spaces with only finite volume, this gives us something to work with. The first theorem taking advantage of this is a continuous form of the pidgeonhole principle.

\begin{theorem}
    Let $T: X \to X$ be a measure preserving map on a space of finite volume. Given a fixed $U \subset X$ of positive volume, almost every point of $U$ returns to $U$ after some time.
\end{theorem}
\begin{proof}
    Let
    %
    \[ A = \{ x \in U : \text{$x$ never returns to $U$} \} \]
    %
    Then $T^{-n}(A)$ is disjoint from $T^{-m}(A)$, for each $n \neq m$, for if $x$ arrives in $A$ in both $n$ iterations and $m$ iterations, for $n < m$, then $T^n x$ arrives in $A$ after $m - n$ iterations, a contradiction to the fact that $T^n x \in A$. Thus $v(A) = 0$, for otherwise
    %
    \[ v(X) \geq v\left( \bigcup_{i = 0}^\infty T^{-i}(U) \right) = \sum_{i = 0}^\infty v(T^{-i}(U)) = \sum_{i = 0}^\infty v(U) > \infty \]
    %
    So almost every point in $U$ returns to $U$ once.
\end{proof}

\begin{example}
    Consider this picture of a cat, and take the map on $\mathbf{R}^2 / \mathbf{Z}^2$ defined by
    %
    \[ T(x,y) = (2x + y, x + y) = \begin{pmatrix} 2 & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} \]
    %
    $T$ is actually an invertible map on the unit square, with inverse
    %
    \[ T^{-1}(x,y) = \begin{pmatrix} 1 & -1 \\ -1 & 2 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} \]
    %
    and is area preserving, since the determinant of the matrix is 1. Thus we can apply Poincare recurrence to see that almost every point returns to a neighbourhood of itself. The map is actually periodic on the 
\end{example}

In the case that $T$ is injective, we can obtain explicit bounds on the time we will have to wait for a point in $U$ to return to $U$. If $U$, $T U, \dots T^k U$ are disjoint, then
%
\[ v(\bigcup_{i = 0}^k T^i U) = n v(U) \]
%
If $T^iU$ and $T^jU$ intersect, $T^{-j}T^iU = T^{i-j}U$ intersects $U$. Thus we find that, to avoid contradiction, some point in $U$ must return to $U$ in $\lceil v(X)/v(U) \rceil$ iterations.

\begin{example}
    Consider opening the lid on a bucket of air molecules in a sealed chamber. Poincare's theorem tells us that, paradoxically, to any desired margin of error, the air molecules will eventually return to their original position inside the bucket. Such a paradox is solved when we consider the `curse of dimensionality' for the particles. Suppose we only know the initial positions of the particles to within an error of $\pm \varepsilon$. Then the `volume of error' in phase space will be $(2\varepsilon)^{3n}$, and if the sealed chamber is a cube of length $m$, then the bound on the return time is on the order of
    %
    \[ \left( \frac{m}{2\varepsilon} \right)^{3n} \]
    %
    which grows exponentially as the number of particles increase. Thus to wait for 200,000 particles to return to their original position (a large underestimate on the number of particles, if we have a whole bucketful), we will likely have to wait until the end of the universe.
\end{example}

\begin{example}
    Consider the circle $S^1$, which has a canonical `angular measure' placed on it. Let $T:z \mapsto wz$, for a fixed $w \in S^1$. Then $T$ is a rotation of the circle which preserves measure. If $w^n z = z$, for some $x$, then $w^n = 1$, and is a root of unity. In this case the system of translations $T, T^2, \dots,$ is cyclic, and points return to their initial positions exactly after a certain amount of time. If $w$ is not a root of unity, then Poincare's recurrence theorem tells us something interesting -- for any $\varepsilon > 0$, there is $n$ such that $|w^n - 1| < \varepsilon$. Take the neighbourhood $U$ around $1$ which sweeps out an angle of $\varepsilon/2$ in each direction. Poincare's theorem tells us that there is $z \in U$ and $n$ for which $w^n z \in U$, so $|w^n z - z| = |w^n - 1| < \varepsilon$.

    Since the group $S^1$ is isomorphic to $\mathbf{R}/\mathbf{Z}$, Poincare's theorem tells us analogously that, if $x$ is an irrational number, then we may always choose $n$ such that the decimal values of $nx$ are as close to zero as desired; a very elegant argument for what is normally a very messy proof.
\end{example}

Now measure preserving maps cannot `squish and hide' parts of their domain, but they can still keep parts of the domain too still. 

\begin{example}
    Consider the numbers
    %
    \[ 1, 2, 4, 8, 16, 32, \dots, 2^k, \dots \]
    %
    in particular, take the most significant digits of these numbers. Do these digits follow some sort of regular pattern?
    %
    \[ 1, 2, 4, 8, 1, 3, 6, 1, 2, 5, \dots \]
    %
    Perhaps, but it is not obvious. Ergodic theory is not good at discovering `pointwise' patterns, but can help us discover the behaviour of phenomena `on average', or `in the limit'.

    The trick to discovering the properties of the power sequence is to look at the numbers in scientific notation
    %
    \[ 1 \cdot 10^0, 2 \cdot 10^0, 4 \cdot 10^0, 8 \cdot 10^0, 1.6 \cdot 10^1, 3.2 \cdot 10^1, \dots \]
    %
    In general, we may extract the first digit of a number of the form $z = x \cdot 10^y$, where $0 \leq x < 10$ in the following manner. We have
    %
    \[ \log(z) = y + \log(x) \]
    %
    Since $\log$ is an increasing function, the first digit of $x$ is $1$, if and only if $1 \leq x < 2$, so $\log(1) \leq x < \log(2)$. Similarily, the first digit of $x$ is $n$ if $\log(n) \leq x < \log(n+1)$. Now the act of taking a power of $2$ may stretch the area of the shape, but if we define an area function on $[0,1]$ by
    %
    \[ v([a,b]) = \log(b) - \log(a) \]
    %
    then we see that doubling preserves the area. The recurrence theorem then shows that every number from $1$ to $9$ occur as the first digit of some power of 2. More advanced theorems of ergodic theory tell us that the powers of two are logarithmically `equally distributed' on $[0,1]$ according to our new area measure: the probablity that a randomly selected power of 2 has first digit $n$ is the same the logarithm lies between $\log(n)$ and $\log(n+1)$, so the probability is $\log(n+1) - \log(n)$.
\end{example}

As a final example of our system, consider hitting a ball on a frictionless pool table. How will the angle of the shot impact the orbit of the ball? The recurrence theorem tells us that, if the slope of our shot is irrational, the points of contact of the ball on the side of the walls of the table are dense. Some more advanced ergodic theory tells us that the points of contact are equidistributed along the wall.

\end{document}