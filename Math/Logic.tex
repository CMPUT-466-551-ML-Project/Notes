\input{../style.tex}

\title{Metamathematics}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}
\maketitle
\tableofcontents

\part{Mathematical Logic}

\chapter{What's Logic All About?}

\pagenumbering{arabic}

In Mathematics, we use rigorous arguments, known as proofs, to discover new facts from prior assumptions. Metamathematics turns mathematics on its head, using the mathematical method to analyze mathematical methods themselves! Traditionally, metamathematics (or logic, as it was then called) was used to find valid forms of reasoning. But in the early 20th century, a desire for rigour unvealed a plague of foundational paradoxes in mathematics. It was in this firestorm where `hard' metamathematics was forged, and we shall begin our story here...

Decades before, a German mathematician had invented an entirely new way of thinking about mathematics. Instead of discussing particular numbers or shapes or functions, Georg Cantor decided instead to talk about collections of mathematical objects, known as sets. In a flourish, he solved a problem which had troubled mathematicians for centuries. A number is called {\it algebraic} if it is the root of a certain polynomial
%
\[ a_0 + a_1 X + \dots + a_n X^n \]
%
with coefficients in the rational numbers.  Since Archimedes, mathematicians had wondered whether all numbers were trancedental. Cantor answered this question in the negative, proving the existence of {\it trancendental numbers}. The basic technique involved counting the {\it set} of algebraic numbers, and counting the {\it set} of all numbers, and showing that there were far too many numbers to be contained in the algebraic numbers\footnote{Of course, the method is slightly more complicated than this, but we don't have time to cover all of set theory, of which we assume a naive understanding as prerequisite for reading this book (i.e. a basic knowledge of sets, cardinality, and ordinal theory).}. But by creating a theory powerful enough to encapsulate mathematical objects, Cantor opened the door for mathematicians to see mathematics encapsulated whole, and this is where paradoxes began to emerge.

\begin{example}[Cantor's Paradox]
    For any set $X$, Cantor proved that the cardinality of $\mathcal{P}(X)$ is strictly greater than the cardinality of $X$. But if $X$ is the set of all mathematical objects (including sets, and sets and sets, and so on and so forth), then $\mathcal{P}(X) \subset X$, which implies the cardinality of $\mathcal{P}(X)$ is less than or equal to the cardinality of $X$.
\end{example}

\begin{example}[Burali-Forti Paradox]
    Consider the set $\Omega$ of all ordinals. Then $\Omega$ is a well-ordered set, and therefore order isomorphic to some ordinal, and thus to a proper segment of itself. But no well-ordered set is order isomorphic to a proper subset of itself.
\end{example}

\begin{example}[Russell's Paradox]
    Consider the following set.
    %
    \[ X = \{ x: x \not \in x \} \]
    %
    Russell's paradox rests on an innocent question: Is $X$ in itself? If $X$ was in $X$, then by definition, we conclude that $X$ cannot be contained in itself. So we are lead to believe that $X \not \in X$. But then, by construction of $X$, we conclude $X$ is in $X$ after all! A more colloquial explanation of the paradox considers a town with a single barber, shaving everyone who does not shave themselves. Does the barber shave himself?
\end{example}

All of these paradoxes rely on the fact that set theory can discuss mathematical objects that are `too big' to be understood logically. Russell's paradox is the most mathematically elegant of the set-theoretic paradoxes, for it relies on no advanced knowledge of cardinals nor ordinals. However, if we take the proof of Cantor's theorem, and replace a general set with the set of all sets, then we end up with the set $\{ x : x \not \in x \}$, so the two paradoxes are certainly linked.

Henri Poincar\'{e} saw the paradoxes as proof that set theory was a ``plague on mathematics''. But paradoxes were not limited to set-theoretic concepts. In their work on the Continuum hypothesis, Julius K\"{o}nig and Jules Richard found paradoxes attacking the notion of mathematical definability, the ability to specify objects by phrases of english.

\begin{example}[Richard \& K\"{o}nig's Paradoxes]
    The set of all english expressions may be enumerated by alphabetic ordering, since there are finitely many expressions of a certain size, and those of a certain size can by ordered alphabetically. Since the real numbers cannot be enumerated, we conclude that some real numbers cannot be described in english. Consider a particular enumeration containing all describable real numbers. Based on this enumeration, construct the expression
    %
    \begin{center}
        ``The number constructed from Cantor's diagonal argument on the enumeration considered.''
    \end{center}
    %
    This number is described in english, though not considered in the enumeration. Similarily, if the real numbers could be well-ordered, we could consider the expression,
    %
    \begin{center}
        ``The least number not definable in english.''
    \end{center}
    %
    And this number is definable, a contradiction.
\end{example}

It seems obvious that a definition is simply a description of the qualities of an object, but if this were true, there would be no problem with the arguments above, so we are at an impasse. A precise definition of definability is a key discovery in metamathematics, from which we will obtain the beautiful results of G\"{o}del and Tarski. A related paradox results from self-reference.

\begin{example}[L\"{o}b]
    Consider the Preposition $B$, defined to be true when $B \implies A$ is true. If $B$ is true, then $B \implies A$ is true, so $A$ is true. But then $B \implies A$ is true by construction, so $B$ is true, and we conclude $A$ is true. Since $A$ was arbitrary, so we conclude that every logical statement is true!
\end{example}

It can be argued that L\"{o}b's paradox fails because there is a statement which discussed itself, but there is a deeper problem in the argument, for a set theoretical argument also shows this paradox, without explicit self-reference.

\begin{example}[Curry]
     Consider
    %
    \[ C = \{ x : (x \in x) \implies P(x) \} \]
    %
    Then $(C \in C)$ holds if and only if $(C \in C) \implies P(C)$ holds. We must have $C \in C$, for if $C \not \in C$, then $(C \in C) \implies P(C)$ holds vacantly. But this implies $(C \in C) \implies P(C)$, so $P(C)$ is true. We conclude $P(C)$ is true, irrespective of the content of the statement $P(C)$.
\end{example}

From these paradoxes, it became clear that the current state of logic was not sharp enough to fix these paradoxes; thus the field of `modern' metamathematics was formed. Our only hope is to apply the precise weapon of mathematical rigor. In the face of adversity, we do what mathematicians do best -- define and conquer.

\section{Formal Systems}

In order to analyze mathematics mathematically, a careful method must be employed to avoid circularity. The main trick is to construct abstract models which simulate the procedure of mathematics. These models are separated from what they model, so may be analyzed rigorously. To do this, we must first estimate the procedure of of a mathematician at work. First, she accepts some fundamental statements as `obviously true', known as axioms\footnote{What is obvious is a personal statment, determined by the person at hand. To a scientist, what is obvious is that determined by experiment. To a priest, what is obvious is that which is written in a holy book. In mathematics, we do not care how one determines that statements are obvious, only that the statements are accepted as obvious.}. Using accepted logical derivations, additional statements are obtained from the axioms. Mathematics is as simple as that. Thus all we need to formalize is what a {\it statement is}, and what a {\it logical derivation} is.

The basic object of study in metamathematics, from which we build all of our models, is deceptively simple. We take a set $\Lambda$, known as an {\bf alphabet}, and consider {\bf strings} over that alphabet, finite (possibly empty) sequences of elements in $\Lambda$. Rather than formalizing the thought process of a mathematician, we choose instead to formalize what is written down: the proof. Abstract strings have turned out to be the right formalization to understand this approach.

We will often denote a string $(v_1, \dots, v_n)$ as $v_1 \dots v_n$. When this might confuse the reader, we surround the string in quotation marks, like
%
\begin{center}
    ``$v_1 \dots v_n$''
\end{center}
%
The choice of alphabet will often make the quotation marks redundant. We shall identify an element in $\Lambda$ with the corresponding one letter string in $\Lambda^*$. The concatenation $sw$ of two strings $s = s_1 \dots s_n$ and $w = w_1 \dots w_m$, is the string $s_1 \dots s_n w_1 \dots w_m$. If we view concatenation as an associative algebraic operation, then $\Lambda^*$ is the smallest {\it monoid} containing $\Lambda$. Since most of our languages will be constructed from putting basic strings together, we may wish to take complicated strings, such as
%
\begin{center}
    ``It was the best of times, it was the worst of times''
\end{center}
%
and identify more basic substrings in them, such as ``worst''. A {\bf substring} of a string $s$ is a string $u$ such that $s = wuv$. Substrings are consecutive subsequences of characters in $s$. 

A {\bf language} is a subset of strings over an alphabet. It allows us to separate meaningful strings over the alphabet
%
\begin{center}
    ``The quick brown fox jumps over the lazy dog''
\end{center}
%
from meaninless garble, like
%
\begin{center}
    ``iovuiaesfpauaauupewbpvapbuib''
\end{center}
%
In mathematical logic, we may consider the language of meaningful formulae in a language, and then the sublanguage of truthful formulae. The construction of such `truthful formulae' is encapsulated in what is called a {\bf formal system}. The most general definition consists of a language $L$ over an alphabet $\Lambda$, a set of axioms, which form a subset of $L$, and a set of inference rules, pairs $(\Gamma, s)$ where $\Gamma \subset L$ is the premises of the inference, and $s$ is the conclusion. The {\bf theorems} of a formal system are members of the smallest set such that
%
\begin{enumerate}
    \item every axiom is a theorem
    \item if $(\Gamma, s)$ is an inference rule, and all elements of $\Gamma$ are theorems, then $s$ is a theorem.
\end{enumerate}
%
If $F$ is a formal system, we shall let $\vdash_F s$ state that $s$ is a theorem of $F$. Normally though, we will write $\vdash s$, for the formal system is clear from context. Pretty much all metamathematics is is proving facts about formal systems.

\section{Circulus in Probando}

Before we get to the real work though, how can we ensure the mathematical models we apply are robust enough to tell us about real mathematics? David Hilbert's plan, along with the rest of the formalist school, was to construct a powerful formal system, a system which could prove itself consistant in the model. If such a system could be constructed, Hilbert believed we could hide the rest of mathematics in this system, shielding mathematics from paradoxes. One reduces mathematics to abstract symbol pushing inside the system, but Hilbert did not see this as an issue; this symbol pushing is no different from the symbol pushing inside our minds when we solve a problem, albeit more explicit. Regardless of whether you believe in this approach, we shall find Hilbert's approach is doomed from the beginning, for no sufficiently advanced {\it consistant} formal system can prove itself consistant.

So how can we ensure that our formal systems give correct results about everyday mathematics? If you desire absolute truth, you will be dissapointed. A real life situation can never be modelled completely accurately. A physicists's models are ideals, carved from reality in all senses but experimental parameters. No model describes a systems evolution exactly, and it is myopic to suggest a model's perfection. In spite of this, physics still does a bloody good job! In metamathematics, we attempt to form a mathematical model of mathematical principles. Some principles are pinned down for examination, others lost. We hope this model has enough vitality to provide key insights into real-life mathematics. Whether the method is successful can only be determined by evidence in actual mathematics.

A source of confusion in physics is the stylistic treatment of assumptions as absolute facts. A physicist describes ``a planet moving according to the equation $\ddot{x} = -m/x^2$'' even if he is actually talking about the dynamical system whose evolution is described by the differential equation $\ddot{x} = -m/x^2$, which {\it models} the motion of a planet. Such expressions are unavoidable, since they are much more viceral, whereas eschewing the natural language makes the formal equivalent dry to the bone. Keep this principle in mind as we begin to build models of logic.






\chapter{Prepositional Logic}

We shall begin with propositional logic, the simplest formal system to analyze truth. To understand propositional logic, we construct a mathematical model, known as a formal language, which represents the language in which mathematics is performed. The formal language is then analyzed by common mathematical deduction rules. The standard formal language for logic is an analysis of strings, sequences of abstract symbols from a given alphabet. Strings represent mathematical statements; manipulating these strings models how a mathematician infers some mathematical statement from another. It is best to see the tool in action to understand its utility, so we proceed swiftly into the technicalities.

\section{Syntax}

Each abstract symbol in an alphabet represents a precise form in colloquial speech. We begin with prepositional logic, which analyzes basic notions of truth and falsity. Some statements are {\bf atomic}, because they cannot be divided into more base statements. ``Socrates is a man'' is an atomic statement, as is ``every woman is human''. ``Socrates is a man and every woman is a human'' is not atomic, for the statement consists of two separate statements, composed by the connective ``and''. In English, ``every woman is a human'' can be broken into statements such as ``Julie is a human'' and ``Laura is a human'', yet propositional logic still considers this statement as atomic; the model does not have the capability to precisely model understanding of these complex statements, which are the realm of predicate logic, discussed in the next chapter.

Let $\Lambda$ be a set disjoint from $\{ (, ), \wedge, \vee, \neg, \Rightarrow, \Leftrightarrow \}$. The {\bf propositional language with atoms in $\Lambda$}, denoted $\text{SL}(\Lambda)$, is the smallest subset of
%
\[ (\Lambda \cup \{ (, ), \wedge, \vee, \neg, \Rightarrow, \Leftrightarrow \})^* \]
%
such that
%
\begin{enumerate}
    \item $\Lambda \subset \text{SL}(\Lambda)$.
    \item If $s, w \in \text{SL}(\Lambda)$, then $(\neg s), (s \wedge w), (s \vee w), (s \Rightarrow w), (s \Leftrightarrow w) \in \text{SL}(\Lambda)$.
\end{enumerate}
%
An element of $\text{SL}(\Lambda)$ is called a {\bf formula} or {\bf statement}.

Each {\bf connective} represents a certain linguistical form. Later on, the connection of symbols to meaning will become clear. For now, they are abstract symbols without intrinsic meaning.
%
\begin{center}
\begin{tabular}{| c | c | c |}
    \hline Connective & Name of Connective & Meaning of statement \\
    \hline $\neg s$ & Negation & ``$s$ is {\it not} true''\\
    $s \wedge w$ & Conjuction & ``$s$ {\it and} $w$ is true''\\
    $s \vee w$ & Disjunction & ``$s$ {\it or} $w$ is true''\\
    $s \Rightarrow w$ & Implication & ``{\it If} $s$ is true, {\it then} $w$ is true''\\
    $s \Leftrightarrow w$ & Bicondition & ``$s$ is true, {\it if, and only if,} $w$ is true''\\
    \hline
\end{tabular}
\end{center}

Take care to notice that $\text{SL}(\Lambda)$ is the {\it smallest} set constructed, in the same way that most `smallest objects' exist in mathematics, because the intersection of sets satisfying the set of statements defining the set also satisfy the statements. This property leads to the most useful proof method in logic.

\begin{theorem}[Structural Induction]
    Consider a proposition that can be applied to $\text{SL}(\Lambda)$. Suppose the proposition is true of all elements of $\Lambda$, and that if the proposition is true of $s$ and $w$, then the proposition is also true of $(\neg s), (s \wedge w), (s \vee w), (s \Rightarrow w)$, and $(s \Leftrightarrow w)$. Then the proposition is true for all of $\text{SL}(\Lambda)$.
\end{theorem}
\begin{proof}
    Let $P$ be some property to consider. Note the set
    %
    \[ K = \{ s \in (\Lambda \cup \{ (, ), \wedge, \vee, \neg, \Rightarrow, \Leftrightarrow \})^* : P(s)\ \text{is true} \} \]
    %
    satisfies axioms (1) and (2) which define $\text{SL}(\Lambda)$, so $\text{SL}(\Lambda) \subset K$.
\end{proof}

The `formulas' of prepositional logic are just abstract sequences of symbols. They have no intrinsic grammatical structure. In the way we have defined it, the grammatical structure which appears obvious must be proved. Since we are working over formulas of arbitrary complexity, structural induction will be the most useful here.

\begin{theorem}
    Any sentence in $\text{SL}(\Lambda)$ contains as many left as right brackets.
\end{theorem}
\begin{proof}
    Any atom in $\Lambda$ contains no left brackets, and no right brackets, and thus the same number of each. Let $s$ have $n$ pairs of brackets, and let $w$ have $m$. Then $(\neg s)$ contains $n + 1$ pairs of brackets, and $(s \circ w)$, where $\circ \in \{ \wedge, \vee, \neg, \Rightarrow, \Leftrightarrow \}$, contains $n + m + 1$ brackets. By structural induction, we have proved our claim.
\end{proof}

We need a more in depth theorem to correctly parse statements. If statements can be parsed in two different ways, they become ambiguous. For instance, what is the value of $2 + 7 - 5 - 4$? Is it, by collecting factors in pairs, $9 - 1 = 8$, or $9 - 9 = 0$. This is the reason for parenthesis. One splits mathematical logic into syntax, studying strings, and semantics, interpreting strings. We are studying syntax to begin with, so that we may begin semantics. Equations must be understood before we calculate with them.

\begin{theorem}
    If $wu = s \in \text{SL}(\Lambda)$, where $w \neq \varepsilon$, then $w$ has at least as many left brackets as right brackets, and $w = s$ if and only if $w$ has the same number of left and right brackets.
\end{theorem}
\begin{proof}
    If $s \in \Lambda$, then $s$ is only one letter long, so $s = w$, and has no brackets. Now let $s$ and $s'$ satisfy the theorem. We split our proof into two cases.
    %
    \begin{enumerate}
        \item $wu = (\neg s)$: March through all cases. Suppose $w$ has the same number of left brackets than right. $w$ cannot equal $($ or $(\neg$, nor $(\neg v$, where $v$ is a prefix of $s$; by induction, $v$ has at least as many left brackets as right brackets, and then $w$ has more left brackets than right brackets. Thus $w$ must equal $(\neg s)$.
        \item $wu = (s \circ s')$: Continue the string march. $w$ cannot equal $($, nor $(v$ by induction. Similarily, $w$ cannot equal $(s \circ$, nor $(s \circ v$, where $v$ is a substring of $s'$, so $w$ must equal $(s \circ s')$.
    \end{enumerate}
    %
    Careful analysis of each case also shows that $w$ must have at least as many left brackets as right brackets.
\end{proof}

\begin{corollary}
    Every string in $\text{SL}(\Lambda)$ can be written uniquely as an atom $\Lambda$, or $(\neg s)$ and $(s \circ w)$, where $s$ and $w$ are elements of $\text{SL}(\Lambda)$. The unique connective in the representative is known as the {\bf principal connective} of the statement.
\end{corollary}
\begin{proof}
    Such representations trivially exist. Suppose we have two representations. If one of the representations is an element of $\Lambda$, the other representation must have a string of length one, and is therefore equal. If we have two representations $(\neg s) = (\neg w)$, then by chopping off symbols, $s = w$. It is impossible to have two distinct representations $(s \circ w) = (\neg u)$, for no element of $\text{SL}(\Lambda)$ begins with $\neg$. Finally, suppose we have two representations $(s \circ w) = (u \circ v)$. Then either $u$ is a substring of $s$, or $s$ is a substring of $u$, and one is the prefix of the other. But both have balanced brackets, which implies $s = u$, and by chopping letters away, $w = v$. Thus representations are unique.
\end{proof}

The corollary above allows us to construct recursive definitions. Let $\Lambda = \{ A, B, C \}$, and $\Gamma = \{ X, Y, Z \}$. We would like to consider $\text{SL}(\Lambda)$ and $\text{SL}(\Gamma)$ the same, by considering a natural bijection. We extend the map
%
\[ X \mapsto A\ \ \ \ \ Y \mapsto B\ \ \ \ \ Z \mapsto C \]
%
by the definition
%
\[ f((s \circ w)) = (f(s) \circ f(w))\ \ \ \ \ f((\neg s)) = (\neg f(s)) \]
%
Such a map is well defined on all of $\text{SL}(\Gamma)$ by the corollary, and injective.

It is useful to have a visual specification of the decomposition of formulae. Given a formula $s$, define the {\bf parse tree} of $s$ inductively:
%
\begin{enumerate}
    \item If $s$ is atomic, then the parse tree of $s$ consists of a single node, $s$ itself.
    \item If $s = w \circ u$, where $\circ$ is a binary connective, then the parse tree has a parent node $s$, descending into two subtrees, the first of which being the parse tree of $w$, and the second of which the parse tree for $u$.
    \item If $s = \neg w$, then the parse tree has a parent node $s$, with a single edge descending into the parse tree of $w$.
\end{enumerate}
%
the {\bf complexity} of a formula is the height of the parse tree. A {\bf subformula} of a formula $s$ is a formula associated to one of the nodes in the parse tree of $s$. It is easy to see that they are the only substrings of $s$ which are valid formulas in the language. An occurence of a formula $w$ in a formula $s$ is a node in the parse tree of $s$ whose associated string is $w$.

\begin{example}
    The parse tree for $(A \vee B) \wedge \neg (B \wedge C)$ is
    %
    \[
    \Tree [.{$(A \vee B) \wedge \neg (B \wedge C)$} [.{$A \vee B$} {$A$} !\qsetw{1in} {$B$} ] !\qsetw{1in} [.{$\neg (B \wedge C)$} [.{$B \wedge C$} {$B$} !\qsetw{1in} {$C$} ] ] ]
    \]
    %
    Thus the formula has complexity $3$, for the longest branch in the parse tree is three. The subformulas of the string are simply the nodes in the tree. The tree also tells us that there are two occurences of $B$ in $s$, whereas only one occurence of $(B \wedge C)$.
\end{example}

Before we finish with syntax, it is interesting to discuss a less natural, but syntactically simpler method of forming sentences, called {\bf polish notation}, after its inventor Jan \L ukasiewicz. Rather than writing connectives in {\bf infix notation}, like $(u \wedge v)$ and $(u \Rightarrow v)$, we use {\bf prefix notation}, like $\wedge u v$ and $\Rightarrow u v$. We do not need brackets to parse statements anymore, but it is much more easy to read the statement $``a \Rightarrow ((b \vee c) \wedge d)''$, than $``\wedge \Rightarrow a \vee b c d''$. Nonetheless, the languages are effectively the same, as can be seen as taking parse trees of corresponding formulae in the different languages.

\section{Semantics}

We can understand the discussion in the last section without any understanding of what symbols mean. Now we want to interpret the symbols, giving the symbols meaning. A basic semantic method is to define whether a statement is `true'. A {\bf truth assignment} on a set $\Lambda$ is a map $f: \Lambda \to \{ \top, \bot \}$.

\begin{example}
    An $n$-ary {\bf boolean function} is a truth assignment, where $\Lambda = \{ \top, \bot \}^n$. It is common to define such functions by truth tables. For $n$-ary boolean functions, we form a table with $n + 1$ columns, and $2^{n}$ rows. In each row, we fill in a particular truth assignment in $\{ \top, \bot \}^n$, and in the last column, the value of image of the truth assignment under $f$. One may combine multiple $n$-ary truth functions into the same table for brevity. There are $2^{2^{n}}$ $n$-ary truth functions.
    %
    \begin{center}
    \begin{tabular}{| c | c | c | c | c | c | c |}
        \hline $x$ & $y$ & $H_\wedge(x,y)$ & $H_\vee(x,y)$ & $H_\Rightarrow(x,y)$ & $H_\Leftrightarrow(x,y)$ & $H_\neg(x)$ \\
        \hline $\bot$ & $\bot$ & $\bot$ & $\bot$ & $\top$ & $\top$ & $\top$ \\
        $\bot$ & $\top$ & $\bot$ & $\top$ & $\top$ & $\bot$ & $\top$ \\
        $\top$ & $\bot$ & $\bot$ & $\top$ & $\bot$ & $\bot$ & $\bot$ \\
        $\top$ & $\top$ & $\top$ & $\top$ & $\top$ & $\top$ & $\bot$ \\
        \hline
    \end{tabular}
    \end{center}
    %
    This table defines the boolean functions $H_\circ$.
\end{example}

Homomorphisms between two rings $R$ and $S$ extend to homomorphisms between the polynomial rings $R[X]$ and $S[X]$. Similarily, we may extend truth assignments $f$ on $\Lambda$ to assignments $f_*$ on $\text{SL}(\Lambda)$. This is done recursively. Let $f$ be an arbitrary truth assignment. Define
%
\[ f_*(\neg s) = H_\neg(f_*(s))\ \ \ \ \ \ \ \ \ \ f_*(s \circ w) = H_\circ(f_*(s), f_*(w)) \]
%
then $f_*$ is defined on all of $\text{SL}(\Lambda)$.

Let $s \in \text{SL}(\Lambda)$ be a statement. $s$ is a {\bf tautology}, if, for any truth assignment $f$ on $\Lambda$, $f_*(s) = \top$. A {\bf contadiction} conversely satisfies $f_*(s) = \bot$ for all assignments $f$. We summarize the statement ``$s$ is a tautology'' by $\vDash s$. We say a statement $s$ {\bf semantically implies} $w$, written $s \vDash w$, if $\vDash s \Rightarrow w$, or correspondingly, if $f_*(w) = \top$ whenever $f_*(s) = \top$.

Suppose that we wish to verify whether $s \in \text{SL}(\Lambda)$ is a tautology. Let $x_1, \dots, x_n \in \Lambda$ be all the variables which occur in $s$. Define a boolean function $g: \{ 0, 1 \}^n \to \{ 0, 1 \}$,
%
\[ g(y_1 \dots, y_n) = f^{(y_1, \dots, y_n)}_*(s) \]
%
where $f^{(y_1, \dots, y_n)}$ is a truth assignment formed by mapping $x_i$ to $y_i$. This is well defined, because if $h$ and $k$ are two truth assignments which agree on the $x_i$, then they agree at $s$. If $f$ is an arbitrary truth assignment, then
%
\[ g(f_*(x_1), \dots, f_*(x_n)) = f_*(s) \]
%
which can be seen from an easy structural induction. Thus $s$ is a tautology if and only if
%
\[ g(y_1, \dots, y_n) = \top \]
%
for all choice of $y_i$. Therefore one need only construct the truth table of $g$ to confirm whether $s$ is a tautology or not. To prevent errors, it is best to construct a truth table containing all subformulas of $s$, so that one can verify that calculations are consistant with other calculations. This may be done side by side, in the same table.

\begin{example}
    For instance, for any variable $A \in \Lambda$, $A \vee \neg A$ is a tautology, $A \wedge \neg A$ is a contradiction, and $\neg A$ is contingent, which is verified by the truth table
    %
    \begin{center}
    \begin{tabular}{| c | c | c | c | c | c | c |}
        \hline $A$ & $A \vee \neg A$ & $A \wedge \neg A$ & $\neg A$ \\
        \hline $\top$ & $\top$ & $\bot$ & $\bot$ \\
               $\bot$ & $\top$ & $\bot$ & $\top$ \\
        \hline
    \end{tabular}
    \end{center}
    %
    The first formula is the {\bf law of excluded middle}.
\end{example}

\begin{example}
    Let $A \vDash B$ be a tautology, and suppose that $A$ and $B$ have no variables in common. Then $A$ is a contradiction, or $B$ is a tautology, for if there is a truth assignment on the variables of $A$ which make the statement true, and a truth assignment on $B$ which cause the statement to be false, then we may combine the truth assignments to create a truth assignment in which $A \Rightarrow B$ is false.
\end{example}

\begin{theorem}[Semantic Modus Ponens]
    If $\vDash s$ and $s \vDash w$, then $\vDash w$.
\end{theorem}
\begin{proof}
    Let $f$ be a truth assignment. Then $f_*(s) = \top$ and
    %
    \[ f_*(s \Rightarrow w) = H_\Rightarrow(f_*(s),f_*(w)) = H_\Rightarrow(\top, f_*(w)) = \top \]
    %
    This holds only when $f_*(w) = \top$.
\end{proof}

The next theorem relies on a useful string manipulation technique which shall later prove a useful formalism. If $s \in \Lambda^*$, $x = (x_1, \dots, x_n)$ are distinct letters of $\Lambda$, and $w = (w_1, \dots, w_n) \in \Lambda^*$, then we shall let
%
\[ s[w_1/x_1, \dots, w_n/x_n] = s[w/x] \]
%
be the {\bf substitution} of $s$, denoting the string in $\Lambda^*$ obtained from swapping the $x_i$ with $w_i$. It is customary to focus only on alphabets which are countable, but arguments precede in much the same way, and restricting ourselves only makes proofs more complicated, so we do not follow this custom except where it becomes necessary.

\begin{theorem}
    If $\vDash w$, then $\vDash w[s/x]$
\end{theorem}
\begin{proof}
    Consider a truth assignment $f$. We shall define another truth assignment $\tilde{f}$ such that $f(v[s/x]) = \tilde{f}(v)$ for all $v$. Define $\tilde{f}(x_i) = f(s_i)$, and if $y \not \in x$, define $\tilde{f}(y) = f(y)$. Our base case, where $v$ is a variable, satisfies the claim by construction. Then, by induction, if $v = (u \circ w)$, then
    %
    \[ \tilde{f}_*(v) = H_\circ(\tilde{f}_*(u), \tilde{f}_*(w)) = H_\circ(f_*(u[s/x]), f_*(w[s/x])) = f_*(v[s/x]) \]
    %
    A similar proof answers the case where $u = (\neg v)$. Since $w$ is a tautology,
    %
    \[ f_*(w[s/x]) = \tilde{f}_*(w) = \top \]
    %
    So $w[s/x]$ is a tautology.
\end{proof}

\begin{corollary}
    If $v \vDash w$, then $v[s/x] \vDash w[s/x]$.
\end{corollary}

The next theorem has useful applications in model checking\footnote{Model checking is the process of determining whether some constructed system satsifies formal requirements. In computing science, we might want a program to follow some required properties, and model checking allows us to prove that the algorithm does satisfy these properties.}. Say a formula $s$ is {\bf satisfiable}, in the sense that there is some truth assignment $f$ such that $f_*(s) = \top$.

\begin{theorem}[Craig Interpolation]
    Let $A \vDash B$, and let the set of variables shared by $A$ and $B$ be $x_1, \dots, x_n$. Then there is a statement $C$, known as the {\bf interpolant}, containing only the variables $x_i$, such that $A \vDash C$ and $C \vDash B$.
\end{theorem}
\begin{proof}
    We proceed by induction on the number of variables in $A$ which do not occur in $B$. If every variable in $A$ occurs in $B$, let $C = A$. In the general case, fix some variable $x$ in $A$ but not in $B$, take $y$ in both $A$ and $B$, and define
    %
    \[ D = A[(y \wedge \neg y)/x] \vee A[(y \vee \neg y)/x] \]
    %
    If $f_*(A) = \top$, and $f_*(x) = \bot$, then $f_*(A[(y \wedge \neg y)/x]) = \top$. If $f_*(x) = \top$, then $f_*(A[(y \vee \neg y)/x]) = \top$, so $A \vDash D$. In addition, $D \vDash B$. Let $f_*(D) = \top$. Then $f_*(A[(y \wedge \neg y)/x]) = \top$, or $f_*(A[(y \vee \neg y)/x]) = \top$. In the first case, we modify the truth assignment $f$ so that $f_*(x) = \bot$ (without changing the values of $B$ or $D$). Then $f_*(A) = \top$, so $f_*(B) = \top$. The other case follows by letting $f_*(x) = \top$. By induction, there is an iterpolant $C$ for which $D \vDash C$ and $C \vDash B$, and then $A \vDash C$, since $(x \Rightarrow y) \wedge (y \Rightarrow z) \vDash x \Rightarrow z$.
\end{proof}

Suppose we wish to verify that $s \Rightarrow w$ is satisfiable. Computationally, this may be difficult, for the truth table of a formula containing $n$ variables has $2^n$ rows. Nonetheless, we may be able to show $s \Rightarrow w$ is satisfiable by first finding the interpolant $u$ of $s$ and $w$ (which is defined irrespective of whether $s \vDash w$), and then verifying that $u \Rightarrow w$ is satisfiable, which may have many less variables.

\begin{example}
    Consider $A = (y_1 \Rightarrow x) \wedge (x \Rightarrow y_2)$ and $B = (y_1 \wedge z) \Rightarrow (y_2 \wedge z)$. Then $A \vDash B$. We shall find an interpolant.
    %
    \begin{align*}
        &C = [(y_1 \Rightarrow (y_1 \vee \neg y_1)) \wedge ((y_1 \vee \neg y_1) \Rightarrow y_2)]\\
        &\ \ \ \ \ \ \vee [(y_1 \Rightarrow (y_1 \wedge \neg y_1)) \wedge ((y_1 \wedge \neg y_1) \Rightarrow y_2)]
    \end{align*}
    %
    This equation can be simplified to $\neg y_1 \vee y_2$.
\end{example}

\section{Truth Functional Completeness}

We hope that propositional logic can model all notions of truth, such that all truth functions can be formed from our original set. Here we argue why our logic can model all such notions. Let $\Lambda$ be a set of boolean functions. The {\bf clone} of $\Lambda$ is the smallest set containing $\Lambda$ and all projections $\pi^n_k : \{ 0, 1 \}^n \to \{ 0, 1 \}$, defined
%
\[ \pi^n_k(x_1, \dots, x_n) = x_k \]
%
and in addition, if $g: \{ 0, 1 \}^n \to \{ 0, 1 \}$ and $f_1, \dots, f_n : \{ 0, 1 \}^m \to \{ 0, 1 \}$ are in the clone, then so is $g(f_1, \dots, f_n): \{ 0, 1 \}^m \to \{ 0, 1 \}$. $\Lambda$ is {\bf truth functionally complete} if its clone is the set of all boolean functions.

\begin{example}
    $\{ H_\neg, H_\wedge, H_\vee \}$ is a truth functionally complete, since every formula can be put in {\bf conjunctive normal form}, for we may write the `true and false' functions
    %
    \[ \top(x) = \top = H_\vee(H_\neg(x), x)\ \ \ \ \ \bot(x) = \bot = H_\wedge(H_\neg(x), x) \]
    %
    and then we write
    %
    \[ f(x_1, \dots, x_n) = \bigvee_{\substack{(y_1, \dots, y_n) \in \{ 0, 1 \}^n\\f(y_1, \dots, y_n) = \top}}\ \  \bigwedge_{i = 1}^n H_\Leftrightarrow(x_i, y_i) \]
    %
    where we identify an element of $\{ \top, \bot \}$ with the constant boolean function, define $\bigcirc_{i = 1}^n f_i = H_\circ(f_n, \bigcirc_{i = 1}^{n-1})$, and take
    %
    \[ H_\Leftrightarrow(x,y) = H_\wedge(H_\Rightarrow(x,y), H_\Rightarrow(y,x)) = H_\wedge(H_\vee(H_\neg(x), y), H_\vee(H_\neg(y), x)) \]
    %
    This can be further reduced, by noticing that
    %
    \[ H_\wedge(x,y) = H_\neg(H_\vee(H_\neg(x), H_\neg(y))) \]
    %
    which is a truth functional form of Boole's inequality, implying $\{ \neg, \vee \}$ is truth functionally complete. We can also consider {\bf disjunctive normal form}, left to the reader to define.
\end{example}

We see that conjunctive and normal disjunctive forms are a way to canonically represent certain truth functions. It is a standard computational problem to verify whether such a formula is satisfiable, as a great many problems can be reduced to satisfiability. For instance, say one wishes to verify whether a graph $(V,E)$ is $m$ colorable (that is, there is a function $f: V \to \{ 1, \dots, m \}$ such that if $(v,w) \in E$, $f(v) \neq f(w)$). For each vertex $v \in V$ and color $i \in \{ 1, \dots, m \}$, let $v_i$ be a variable. A graph is colorable if and only if the statement
%
\[ \bigwedge_{v \in V} \left( \bigvee_{i = 1}^m v_i \right) \wedge \bigwedge_{(v,w) \in E} \left( \bigwedge_{i = 1}^m (\neg v_i \vee \neg w_i) \right) \wedge \bigwedge_{v \in V} \left( \bigwedge_{i,j = 1}^m (v_i \vee \neg v_j) \right) \]
%
is satisfiable (the first big clause says that some color is assigned to each vertex, the last that the color is unique. The middle clause is the coloring constraint). If a formula is in disjunctive normal form, it is algorithmically easy to verify whether the formula is satisfiable. We just need to check whether one of the disjunctive clauses is consistant, which can be done in a time proportional to the size of the formula. Checking whether a formula is in conjunctive normal form is much more difficult -- in fact, in computability theory one discovers that almost all interesting problems can be reduced to a satisfiability problem, so if it was easy to determine if a CNF is solvable, then we could solve a great many problems easily, which sound unlikely. To determine if the problem is easy is the $P = NP$ problem, one of the most fundamental problems in computing science.

\begin{example}
    The mathematician Henry M. Sheffer found a single truth function which is truth functionally complete. Consider the {\bf Sheffer stroke} $x|y$, also known as {\bf NAND}, defined by the truth table
    %
    \begin{center}
    \begin{tabular}{| c | c | c |}
        \hline $x$ & $y$ & $H_|(x,y)$\\
        \hline $\bot$ & $\bot$ & $\top$\\
        $\bot$ & $\top$ & $\top$\\
        $\top$ & $\bot$ & $\top$\\
        $\top$ & $\top$ & $\bot$\\
        \hline
    \end{tabular}
    \end{center}
    %
    Then $H_\neg(x) = H_|(x,x)$, and $H_\vee(x,y) = H_|(H_\neg(x), H_\neg(y))$, which implies, since this set is truth functionally complete, that the sheffer stroke is truth functionally complete.
\end{example}

The previous example is incredibly important to circuit design. Logical statements can be represented by boolean functions. Since all truth functions can be built from the sheffer stroke, we need only make an atomic circuit for the sheffer stroke, and then all other circuits are constructed by combining sheffer strokes. Computers are constructed from millions of NAND gates.




\section{Deduction}

When mathematicians want to derive whether a statement is true, they do not construct truth functions. Instead, they argue {\it why} the statement is true. Here we shall provide the mechanics for modelling a mathematical argument. We will show that truth tables and arguments are equivalent -- a statement is a tautology if and only if it can be proved. This is known as a {\it completeness result}, for it says that our semantic understanding of a theory is the same as the deductive understanding.

First, we thin out the connectives in our theory. Since $\Rightarrow$ and $\neg$ are truth functionally complete, we can consider a system consisting only of these connectives, and reinterpret other formulas as semantically equivalent formulas in the reduced theorem. Next, we defined the {\bf theorems} of $\text{SL}(\Lambda)$, which are elements of the smallest set such that for any $s,w,u \in \text{SL}(\Lambda)$,
    %
    \begin{enumerate}
        \item Any axiom is a theorem, where the axioms are any statement of the form
        %
        \begin{align*}
            &(A1) & s \Rightarrow (w \Rightarrow s)\\
            &(A2) & (s \Rightarrow (w \Rightarrow u)) \Rightarrow ((s \Rightarrow w) \Rightarrow (s \Rightarrow u))\\
            &(A3) & (\neg s \Rightarrow \neg w) \Rightarrow ((\neg s \Rightarrow w) \Rightarrow s)
        \end{align*}
        \item Modus Ponens holds in our system. If $s \Rightarrow w$ and $s$ are theorems, then $w$ is a theorem. When we apply modus ponens, we may write that the theorem was obtained by (MP).
    \end{enumerate}
    %
We shall write $\vdash s$ to state that $s$ is a theorem.

For statements, the `smallness' characterization gives us structural induction. For theorems, smallness gives us a abstract notion of a `proof'. A statement $s$ is a theorem of $\text{SL}(\Lambda)$ if and only if there is a sequence of formulae $(s_1, \dots, s_n)$ such that $s_n = s$, and each $s_i$ is either an axiom, or is obtained from some $s_j$ and $s_k$ by modus ponens, where $j,k < i$. This sequence is known as a proof. Often, we list a proof from top to bottom, where we reference how we obtained each element of the sequence alongside the proof.

\begin{example}
    Let us construct a proof of $\vdash s \Rightarrow s$, for any $s \in \text{SL}(\Lambda)$.
    %
    \[ \fitchprf{\pline{s \Rightarrow s}}{
        \pline[1.]{(s \Rightarrow ((s \Rightarrow s) \Rightarrow s))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (A1)]\\
        \pline[2.]{(s \Rightarrow ((s \Rightarrow s) \Rightarrow s)) \Rightarrow ((s \Rightarrow (s \Rightarrow s)) \Rightarrow (s \Rightarrow s))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (A2)]\\
        \pline[3.]{((s \Rightarrow (s \Rightarrow s)) \Rightarrow (s \Rightarrow s))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1),(2),(MP)]\\
        \pline[4.]{(s \Rightarrow (s \Rightarrow s))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (A1)]\\
        \pline[5.]{s \Rightarrow s}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3),(4),(MP)]
    } \]
    %
    In future proofs, we shall be able to use $\vdash s \Rightarrow s$ implicitly, since we now know the statement can be proved in any of its forms. We will denote its application by $(I)$.
\end{example}

In mathematics, we work in systems where implicit assumptions are made. In group theory, we assume that operations are associative. In geometry, we assume there is a line between any two points. To perform mathematics, we add additional axioms to logic, and prove results from these axioms. If $\Gamma$ is a subset of $\text{SL}(\Lambda)$, then we may consider each member of $\Gamma$ to be an axiom. We write $\Gamma \vdash s$ if one may prove $s$ assuming all formulae in $\Gamma$ have already been proved. That is, we may write a sequence $(s_1, \dots, s_n)$, where $s_n = s$, and each $s_i$ is either an axiom, an element of $\Gamma$, or is obtained by modus ponens from previous elements of the sequence.

\begin{theorem}[Deduction Theorem]
    If $\Gamma \cup \{ s \} \vdash w$, then $\Gamma \vdash s \Rightarrow w$.
\end{theorem}
\begin{proof}
    We prove the theorem by induction of the size of the proof of $w$. Consider a particular proof $(s_1, \dots, s_n)$ of $w$ from $\Gamma \cup \{ s \}$. Suppose that $n = 1$. Then $s_1 = w$, and $w$ must either be an axiom, an element of $\Gamma$, or equal to $s$. In the first and second case, the proof is equally valid in $\Gamma$, and so $\Gamma \vdash s \Rightarrow w$ follows from the axiom $(w \Rightarrow (s \Rightarrow w))$. If $s = w$, Then we have shown that $\vdash w \Rightarrow w$, so obviously $\Gamma \vdash s \Rightarrow w$.

    Now we consider the problem proved for $m < n$. $s_n = w$ is either an axiom, an element of $\Gamma$, equal to $s$, or proved by modus ponens from $s_i = (u \Rightarrow w)$ and $s_j = u$. We have already shown all but the last case. By induction, $\Gamma \vdash s \Rightarrow (u \Rightarrow w)$ and $\Gamma \vdash s \Rightarrow u$. But
    %
    \[ (s \Rightarrow (u \Rightarrow w)) \Rightarrow ((s \Rightarrow u) \Rightarrow (s \Rightarrow w)) \]
    %
    is an axiom, so $\Gamma \vdash s \Rightarrow w$.
\end{proof}

\begin{example}
    For any statements $s$ and $w$, $\{ s \Rightarrow w, w \Rightarrow u \} \vdash s \Rightarrow u$. This follows from a basic application of (A2). But this implies the two cut rules, that
    %
    \begin{gather*}
        \vdash (s \Rightarrow w) \Rightarrow ((w \Rightarrow u) \Rightarrow (s \Rightarrow u))\\
        \vdash (w \Rightarrow u) \Rightarrow ((s \Rightarrow w) \Rightarrow (s \Rightarrow u))
    \end{gather*}
    %
    which are much more tricky to prove (though technically a proof may be constructed inductively from the proof of the deduction theorem).
\end{example}

\begin{example}
    Let us prove the double negation elimination axiom, $\vdash \neg \neg s \Rightarrow s$ by the deduction theorem.
    %
    \[
    \fitchprf{\pline{\neg \neg s \Rightarrow s}}{
        \subproof{\pline[1.]{\neg \neg s}} {
            \pline[2.]{((\neg s \Rightarrow \neg \neg s)) \Rightarrow ((\neg s \Rightarrow \neg s) \Rightarrow s)}[(A3)]\\
            \pline[3.]{(\neg \neg s) \Rightarrow (\neg s \Rightarrow \neg \neg s)}[(A1)]\\
            \pline[4.]{(\neg s \Rightarrow \neg \neg s)}[(1),(3),(MP)]\\
            \pline[5.]{(\neg s \Rightarrow \neg s) \Rightarrow s}[(2),(4),(MP)]\\
            \pline[6.]{\neg s \Rightarrow \neg s}[(I)]\\
            \pline[7.]{s}[(5),(6),(MP)]
        }
        \pline[8.]{\neg \neg s \Rightarrow s}[(1-7),(DT)]
    }
    \]
    %
    In future proofs, application of the statement will be denoted $(\neg \neg E)$. Now lets prove negation introduction, $\vdash s \Rightarrow \neg \neg s$.
    %
    \[
    \fitchprf{\pline{s \Rightarrow \neg \neg s}}{
        \pline[1.]{\neg \neg \neg s \Rightarrow \neg s}[\ \ \ \ \ \ \ $(\neg \neg E)$]\\
        \subproof{\pline[2.]{s}} {
            \pline[3.]{(\neg \neg \neg s \Rightarrow \neg s) \Rightarrow ((\neg \neg \neg s \Rightarrow s) \Rightarrow \neg \neg s)}[\ \ \ \ \ \ \ (A3)]\\
            \pline[4.]{(\neg \neg \neg s \Rightarrow s) \Rightarrow \neg \neg s}[\ \ \ \ \ \ \ (1), (3), (MP)]\\
            \pline[5.]{s \Rightarrow (\neg \neg \neg s \Rightarrow s)}[\ \ \ \ \ \ \ (A1)]\\
            \pline[6.]{\neg \neg \neg s \Rightarrow s}[\ \ \ \ \ \ \ (2),(5),(MP)]\\
            \pline[7.]{\neg \neg s}[\ \ \ \ \ \ \ (4),(6),(MP)]
        }
        \pline[8.]{s \Rightarrow \neg \neg s}[\ \ \ \ \ \ \ (2-7), (DT)]
    }
    \]
    %
    We shall denote this rule $(\neg \neg I)$.
\end{example}

\begin{example}
    Lets prove $\neg w \vdash w \Rightarrow u$, by proving $\neg w, w \vdash u$.
    %
    \[
    \fitchprf{\pline{\neg w \Rightarrow (w \Rightarrow u)}}{
        \subproof{\pline[1.]{\neg w}} {
            \subproof{\pline[2.]{w}} {
                \pline[3.]{(\neg u \Rightarrow \neg w) \Rightarrow ((\neg u \Rightarrow w) \Rightarrow u)}[(A3)]\\
                \pline[4.]{\neg w \Rightarrow (\neg u \Rightarrow \neg w)}[(A1)]\\
                \pline[5.]{\neg u \Rightarrow \neg w}[(1),(4), (MP)]\\
                \pline[6.]{(\neg u \Rightarrow w) \Rightarrow u}[(3),(5),(MP)]\\
                \pline[7.]{w \Rightarrow (\neg u \Rightarrow w)}[(A1)]\\
                \pline[8.]{\neg u \Rightarrow w}[(2),(7),(MP)]\\
                \pline[9.]{u}[(6),(8),(MP)]
            }
            \pline[10.]{w \Rightarrow u}[(2-9),(DT)]
        }
        \pline[11.]{\neg w \Rightarrow (w \Rightarrow u)}[(1-10), (DT)]
    }
    \]
    %
    This is a proof of {\bf the law of contradiction}, denoed $(LC)$.
\end{example}

\begin{example}
    Consider the following proof.
    %
    \[
    \fitchprf{\pline{(s \Rightarrow w) \Rightarrow (\neg w \Rightarrow \neg s)}}{
        \subproof{\pline[1.]{s \Rightarrow w}} {
            \subproof{\pline[2.]{\neg w}}{
                \pline[3.]{\neg w \Rightarrow (\neg \neg s \Rightarrow \neg w)}[(A1)]\\
                \pline[4.]{\neg \neg s \Rightarrow \neg w}[(2),(3),(MP)]\\
                \pline[5.]{(\neg \neg s \Rightarrow \neg w) \Rightarrow ((\neg \neg s \Rightarrow w) \Rightarrow \neg s)}[(A3)]\\
                \pline[6.]{(\neg \neg s \Rightarrow w) \Rightarrow \neg s}[(4),(5),(MP)]\\
                \subproof{\pline[7.]{\neg \neg s}}{
                    \pline[8.]{\neg \neg s \Rightarrow s}[$(\neg \neg E)$]\\
                    \pline[9.]{s}[(7),(8),(MP)]\\
                    \pline[10.]{w}[(1),(9),(MP)]
                }
                \pline[11.]{\neg \neg s \Rightarrow w}[(7-10), (DT)]\\
                \pline[12.]{\neg s}[(6),(11), (MP)]
            }
            \pline[13.]{\neg w \Rightarrow \neg s}[(2),(12),(MP)]
        }   
        \pline[11.]{(s \Rightarrow w) \Rightarrow (\neg w \Rightarrow \neg s)}[(1-10), (DT)]
    }
    \]
    %
    This is the {\bf law of contraposition} $(LCP)$.
\end{example}

\begin{example}
    Lets prove $\vdash (s \Rightarrow w) \Rightarrow ((\neg s \Rightarrow w) \Rightarrow w)$.
    %
    \[
    \fitchprf{\pline{(s \Rightarrow w) \Rightarrow ((\neg s \Rightarrow w) \Rightarrow w)}}{
        \subproof{\pline[1.]{s \Rightarrow w}} {
            \pline[2.]{(s \Rightarrow w) \Rightarrow (\neg w \Rightarrow \neg s)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (LCP)]\\
            \pline[3.]{\neg w \Rightarrow \neg s}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1),(2),(MP)]\\
            \subproof{\pline[4.]{\neg s \Rightarrow w}} {
                \pline[5.]{(\neg w \Rightarrow \neg \neg s)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (4),(LCP)]\\
                \pline[6.]{\neg \neg s \Rightarrow s}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $(\neg \neg E)$]\\
                \pline[7.]{(\neg w \Rightarrow \neg \neg s) \Rightarrow ((\neg \neg s \Rightarrow s) \Rightarrow (\neg w \Rightarrow s))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (\text{CUT})]\\
                \pline[8.]{(\neg \neg s \Rightarrow s) \Rightarrow (\neg w \Rightarrow s)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (5),(7),(MP)]\\
                \pline[9.]{\neg w \Rightarrow s}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6),(8),(MP)]\\
                \pline[10.]{(\neg w \Rightarrow \neg s) \Rightarrow ((\neg w \Rightarrow s) \Rightarrow w)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (A3)]\\
                \pline[11.]{(\neg w \Rightarrow s) \Rightarrow w}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $(3),(10),(MP)$]\\
                \pline[12.]{w}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (9), (11), (MP)]
            }
            \pline[13.]{(\neg s \Rightarrow w) \Rightarrow w}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $(2-9),(DT)$]
        }
        \pline[14.]{(s \Rightarrow w) \Rightarrow ((\neg s \Rightarrow w) \Rightarrow w)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $(1-10), (DT)$]
    }
    \]
    %
    This essentially proves that $(s \vee \neg s) \Rightarrow w$ implies $w$.
\end{example}

We shall verify that proofs never lead to contradiction.

\begin{theorem}
    If $\vdash s$, then $\vDash s$.
\end{theorem}
\begin{proof}
    This is a trivial proof by structural induction. Prove that all axioms are tautologies, and that the set of tautologies is closed under modus ponens.
\end{proof}

This theorem shows that there are some statements which are not provable in our system. In fact, if $s$ is provable, then $\neg s$ is not provable. We call an axiom system like this {\bf absolutely consistant}. We shall show that all tautologies are provable, which shows the system is {\bf complete}. A complete system is effectively one in which all theorems which were meant to be able to be proved, are able to be proved.

\begin{lemma}
    Let $x_1, \dots, x_n \in \Lambda$ be variables in $s \in \text{SL}(\Lambda)$. Let $f$ be a truth assignment, and define $s' = s$ if $f_*(s) = \top$, or $s' = \neg s$ if $f_*(s) = \bot$. Then
    %
    \[ x_1', \dots, x_n' \vdash s' \]
\end{lemma}
\begin{proof}
    We prove by structural induction. If $s = x_1$, then $x_1' = s'$, and $s \vdash s$ is a trivial theorem. If $s = \neg w$, we consider two cases. If $w' = w$, then $s' = \neg \neg w$, and we have already shown $w \vdash \neg \neg w$, hence $x_1', \dots, x_n' \vdash s'$. If $w' = \neg w$, then $s' = w'$, and the theorem is trivial. If $s = w \Rightarrow u$, then either $f_*(w) = \top$ and $f_*(u) = \top$, or $f_*(w) = \bot$. In the first case, we have $x_1', \dots, x_n' \vdash u$, from which $x_1', \dots, x_n' \vdash w \Rightarrow u$ follows. In the second case, $x_1', \dots, x_n' \vdash \neg w$, and we have $\neg w \vdash (w \Rightarrow u)$.
\end{proof}

\begin{corollary}[Completeness Theorem]
    If $\vDash s$, $\vdash s$.
\end{corollary}
\begin{proof}
    Let $x_1, \dots, x_n$ be the variables in $s$. By the last lemma, we have
    %
    \[ x_1, \dots, x_n \vdash s\ \ \ \ \ \ \ \ \ \ x_1, \dots, \neg \neg x_n \vdash s \]
    %
    By the deduction theorem
    %
    \[ x_1, \dots, x_{n-1} \vdash x_n \Rightarrow s\ \ \ \ \ \ \ \ \ \ x_1, \dots, x_{n-1} \vdash \neg x_n \Rightarrow s \]
    %
    But then, since $\vdash (x_n \Rightarrow s) \Rightarrow ((\neg x_n \Rightarrow s) \Rightarrow s)$,
    %
    \[ x_1, \dots, x_{n-1} \vdash s \]
    %
    By induction, we find $\vdash s$.
\end{proof}

Notice that every proof we have given leading to the completeness theorem is constructive -- that is, one could effectively write an algorithm which constructs the items in the proof. This means that propositional logic is {\it decidable}, there is an effective algorithm that takes a statement of propositional logic, and returns a proof of that statement, if such a proof exists. The completeness theorem also shows that the {\it decision} problem of propositional logic is also solvable. Given a statement of propositional logic, to determine whether we can prove the statement, we need only test whether the statement is true under all truth interpretations.

Before we finish our discussion of semantics, we note that there are many other axioms systems which can be used to define a propositional calculus (in the sense that they prove all tautologies). Most interesting is the axiom system whose only connective is the sheffer stroke, and whose only axiom schema is
%
\[ (B|(C|D))|[\{E|(E|E)\}|[(F|C)|((B|F)|(B|F))]] \]
%
and whose rule of inference is to infer $D$ from $B|(C|D)$, and $B$. Of course, it is outlandish to attempt proofs in such a system, hence why we did not attempt this chapter using the system.

\section{Sequent Calculi}

The complete formal system we have studied is styled in the sense of a great many formal systems. A Hilbert system just takes axioms, and deductive rules, and then forms proofs as sequences $(s_1, \dots, s_n)$. But there are a great many styles of formal systems, and this section I shall detail my personal favourite, natural deduction. Most actual proofs in mathematics do not follow a linear style. We instead form a proof by combining prior deductions in a non-linear way to reach the conclusion, the end of the proof. Thus natural deduction does not model a proof as a sequence $(s_1, \dots, s_n)$, but instead as a tree, whose root node is the conclusion we are attempting to form. The nodes of the tree will not consist of formulas, but instead of sequents, which we have almost already seen, which are pairs of sequences of formulas of the form
%
\[ s_1, s_2, \dots, s_n \vdash w_1, \dots, w_m \]
%
which we interpret as proving
%
\[ (s_1 \wedge s_2 \wedge \dots \wedge s_n) \Rightarrow (w_1 \vee w_2 \vee \dots \vee w_m) \]
%
Why the assymmetry? It turns out that this will give us symmetry in proofs, which we shall require later. A manifestation of this symmetry is that if $s \Rightarrow w$ is true, then both $(s \wedge u) \Rightarrow w$ and $s \Rightarrow (w \vee u)$ is true, so the sequents
%
\[ s, u \vdash w\ \ \ \ \ s \vdash u,w \]
%
may be derived from the sequent
%
\[ s \vdash w \]
%
we have already treated the semantics of propositional logic, so we may just state the axioms and deduction rules. Unlike a hilbert system, our system has far more deduction rules than axioms, which is why this system is more {\it natural} -- we more naturally deal with deduction rules. The only axioms are of the form
%
\[ s \vdash s \]
%
the deduction rules are the edges from which we form our tree,
%
\begin{center}
\begin{prooftree}
\Hypo{ \Gamma, s \vdash \Delta }
\Infer1[($\wedge L$)]{ \Gamma, s \wedge w \vdash \Delta }
\end{prooftree}
\ \ \ \ \ \ \ \ \ \
\begin{prooftree}
\Hypo{ \Gamma \vdash s, \Delta }
\Infer1[($\vee R$)]{ \Gamma \vdash w \vee s, \Delta }
\end{prooftree}
\end{center}

\begin{center}
\begin{prooftree}
\Hypo{ \Gamma, s \vdash \Delta }
\Hypo{ \Pi, w \vdash \Delta }
\Infer2[($\vee L$)]{ \Gamma, \Pi, s \vee w \vdash \Delta }
\end{prooftree}
\ \ \ \ \ \ \ \ \ \ 
\begin{prooftree}
\Hypo{ \Gamma \vdash s, \Delta }
\Hypo{ \Gamma \vdash w, \Pi }
\Infer2[($\wedge R$)]{ \Gamma \vdash s \wedge w, \Delta, \Pi }
\end{prooftree}
\end{center}

\begin{center}
\begin{prooftree}
\Hypo{ \Gamma \vdash s, \Delta }
\Hypo{ \Sigma, w \vdash \Pi }
\Infer2[($\Rightarrow L$)]{ \Gamma, \Sigma, s \Rightarrow w \vdash \Delta, \Pi }
\end{prooftree}
\ \ \ \ \ \ \ \ \ \ 
\begin{prooftree}
\Hypo{ \Gamma, s \vdash w, \Delta }
\Infer1[($\Rightarrow R$)]{ \Gamma \vdash s \Rightarrow w, \Delta }
\end{prooftree}
\end{center}

\begin{center}
\begin{prooftree}
\Hypo{ \Gamma \vdash s, \Delta }
\Infer1[($\neg L$)]{ \Gamma, \neg s \vdash \Delta }
\end{prooftree}
\ \ \ \ \ \ \ \ \ \ 
\begin{prooftree}
\Hypo{ \Gamma, s \vdash \Delta }
\Infer1[($\neg R$)]{ \Gamma \vdash \neg s, \Delta }
\end{prooftree}
\end{center}
%
We have additional deduction rules, known as structural rules, which help show that sequents are the same, without any real logical content.
%
\begin{center}
\begin{prooftree}
\Hypo{ \Gamma \vdash \Delta }
\Infer1[(P)]{ \Gamma' \vdash \Delta' }
\end{prooftree}
\end{center}
%
in the last rule, we mean that $\Gamma'$ is obtained from $\Gamma$ by permuting the sequence, the same for $\Delta'$, as well as removing or adding duplicates. Thus a proof of a sequent $\Gamma \vdash \Delta$ is a tree, whose root is $\Gamma \vdash \Delta$, whose leaves are axioms of the form $s \vdash s$, and such that each edge is annotated by the appropriate deduction rule, for which the deduction is accurate. It turns out it is fairly simple to form deductions, since we may work backwards in most cases to determine which edges to apply.

\begin{example}
    Consider a proof of Pierce's law, $(s \Rightarrow w) \Rightarrow s) \vdash s$. To prove this, we likely need to apply $(\Rightarrow L)$, so we must prove $\vdash (s \Rightarrow w), s$ and $s \vdash s$. Since sequent calculus is meant to model propositional logic, and we will soon prove this system complete, we {\it know} we can prove $\vdash (s \Rightarrow w), s$, and this is obtained from $(\Rightarrow R)$ from the sequent $s \vdash w, s$, and this is obtained from $(P)$ from the axiom $s \vdash s$. We obtain the following proof tree.
    %
    \begin{center}
    \begin{prooftree}
        \Hypo{s \vdash s}
        \Infer1[(P)]{s \vdash w, s}
        \Infer1[($\Rightarrow$\ I)]{\vdash (s \Rightarrow w), s}

        \Hypo{s \Rightarrow s}
        \Infer2[($\Rightarrow$\ L)]{(s \Rightarrow w) \Rightarrow s) \vdash s}
    \end{prooftree}
    \end{center}
    %
    Thus this is a theorem of propositional logic.
\end{example}

We shall require some other sequents to be provable, but we also desire a simple system, one which invokes the cut rule
%
\begin{center}
\begin{prooftree}
\Hypo{ \Gamma \vdash \Delta, s }
\Hypo{ s, \Sigma \vdash \Pi }
\Infer2[(CUT)]{ \Gamma, \Sigma \vdash \Delta, \Pi }
\end{prooftree}
\end{center}
%
But this is distinctly different from the other rules, for it removes complexity from formulas rather than adds complexity. Nonetheless, we shall prove that we do not need the cut rule -- it can always be removed from proofs (It cannot be proved in the Sequent calculi, just removed provided we start from the basic axioms). We must identify properties of rules which can be removed in above applications of the formulae.

Define the {\bf degree} of a formula $s$ to be the height of the parse tree of $s$, define the degree of a sequent to be the sum of the degrees of the nodes in the sequent which are not the root node (so the degree of a variable is 0). The degree of a cut is the sum of the degrees of the two sequents which form the premise of the deduction rule.

%
which applies when $\Sigma$ and $\Delta$ contain some common formula $s$, where $\Sigma^*$ and $\Delta^*$ both have this formula removed. The mix rule is a generalization of the cut rule, so obvious the cut rule can be proved in the mix rule. Conversely, the cut rule implies the mix rule, by a simple induction. The proof is nasty, and can be skipped --

\begin{theorem}
    If a sequent $\Gamma \vdash \Delta$ is provable using the cut rule, then it is provable without the use of the cut rule.
\end{theorem}
\begin{proof}
     The contraction measure of the cut is the number of variables removed by a (P) rule before the cut, and the rank of the cut is the...

    We perform a triple induction, first on the rank, then on the contraction measure, and then on the degree. It is clear the rank of a cut is at least 2. The base case occurs when the degree and contraction measure is zero. This implies the use of the cut rule occurs right after introduction of axioms, and the cut rule is of the form
    %
    \begin{center}
    \begin{prooftree}
    \Hypo{ s \vdash s }
    \Hypo{ s \vdash s }
    \Infer2[(CUT)]{ s \vdash s }
    \end{prooftree}
    \end{center}
    %
    clearly, such a derivation is redundant.


    If the degree of the cut is zero, then we apply a cut rule consisting only of propositional variables
    %
    \begin{center}
\begin{prooftree}
\Hypo{ w \vdash u, s }
\Hypo{ s, t \vdash v }
\Infer2[(CUT)]{ w, t \vdash u, v }
\end{prooftree}
\end{center}
\end{proof}

We first note that the cut rule is {\it not provable} using the axioms of the calculus, but just that it may always be replaced by a more complicated proof.

\begin{example}
    The sequent $s, s \Rightarrow w \vdash w$ is provable
    %
    \begin{center}
    \begin{prooftree}
        \Hypo{s \vdash s}
        \Hypo{w \vdash w}
        \Infer2[($\Rightarrow L$)]{s, s \Rightarrow w \vdash w}
    \end{prooftree}
    \end{center}
    %
    The sequent $\Rightarrow L$ is essentially modus ponens.
\end{example}

We desire to show this system is complete. To do this, we could cheat. Since we already have completeness of a Hilbert system, we just need to form an encoding of the Hilbert system in this system, and an encoding of the sequent calculus in the Hilbert system, such that all axioms are provable. Consider the translation $f$ of the sequent calculus into the Hilbert system, by the map
%
\[ f(``{s_1, \dots, s_n \vdash w_1, \dots, w_m}'') = (s_1 \wedge \dots \wedge s_n) \Rightarrow (w_1 \vee \dots \vee w_m) \]
%
and the translation $g$ of the Hilbert system into the sequent calculus, defined by
%
\[ g(s) = ``{\vdash s}'' \]
%
We define a sequent $s_1, \dots, s_n \vdash w_1, \dots w_m $ to be semantically true, if, under every truth assignment that makes each $s_i$ true, one of the $w_j$ is true. Proving the system is sound is fairly trivial. First, we prove that $f$ and $g$ preserve provability. If $S$ is provable in the sequent calculus, then $f(x)$ is provable in the Hilbert system, and if $s$ is provable in the Hilbert system, then $g(y)$ is provable in the Sequent calculus (we need only prove the axioms and deductions). Second, we prove that $f$ and $g$ preserve semantic completeness, that if $S$ is a semantically true sequent, then so if $f(S)$, and if $\vDash s$, then $g(s)$ is semantically true. This allows us to prove completeness, suppose that $s_1, \dots, s_n \vdash w_1, \dots, w_m$ is semantically true. Then
%
\[ \vDash (s_1 \wedge \dots \wedge s_n) \Rightarrow (w_1 \vee \dots \vee w_m) \]
%
By completeness of the Hilbert system,
%
\[ \vdash (s_1 \wedge \dots \wedge s_n) \Rightarrow (w_1 \vee \dots \vee w_m) \]
%
Hence the sequent
%
\[ \vdash (s_1 \wedge \dots \wedge s_n) \Rightarrow (w_1 \vee \dots \vee w_m) \]
%
is provable in the sequent calculus (note the last formula is completely different from this formula, it is unfortunate the equations coincide). We require that we have already shown that
%
\[ (s \Rightarrow w), s \vdash w \]
%
are provable sequents. This may be combined with the previous sequent, letting $s = (s_1 \wedge \dots \wedge s_n)$, $w = (w_1 \vee \dots \vee w_m)$, and applying the cut rule, we find
%
\[ (s_1 \wedge \dots \wedge s_n) \vdash (w_1 \vee \dots \vee w_m) \]


\chapter{First Order Logic}

It is logical to conclude that ``Julie is a human'' and ``Laura is a human'' from the general statement that ``All women are human''? In Propositional logic, we are unable to model this deduction. Predicate logic is a formal system modelling these derivations.

\section{Language}

The syntax of predicate logic is less homogenous, for our language must contain nouns, like ``Julie'' and ``Laura'', which are the things we talk about, and separate words we apply to nouns, obtaining truth values. These are known as {\bf terms} and {\bf quantifiers} respectively.

Terms should model both definite nouns, such as ``Julie'' and ``Laura'', as well as variables, such as $X$ and $Y$, which can stand for many definite nouns at once, together with relational nouns, such as ``The school $X$ went to'', a statement describing a noun which varies in interpretation based on the value of $X$. Definite nouns are known as {\bf constants}, and relational nouns are known as {\bf functions}. Functions will be separated based on their {\bf arity}, the number of arguments they take. ``$X$'s favourite $Y$''is a `2-ary' function, ``$X$'s birthday'' is a `1-ary' function. We shall now define the terms formally. Let $\Lambda$ be a set of variables, $\Delta$ a set of constants, and for each $n$, a set $\Psi_n$ of $n$-ary functions. The set of {\bf terms} is the smallest set $T(\Lambda, \Delta, \{ \Psi_n \})$ such that $\Lambda, \Delta \subset T(\Lambda, \Delta, \{ \Psi_n \})$, and if $s_1, \dots, s_n \in T(\Lambda, \Delta, \{ \Psi_n \})$, and $f$ is a function in $\Psi_n$, then $f(s_1, \dots, s_n) \in T(\Lambda, \Delta, \{ \Psi_n \})$.

It is fairly easy to construct truth functional statements from these term. In addition to the usual connectives of sentential logic, we also require {\bf predicates}, which are functions of nouns representing a statement about those nouns. For instance, ``X is a Human'' is a predicate. Predicates, like functions, are separated based on arity. For each $n$, let $\Pi_n$ be a set of $n$-ary predicates. Given $\Lambda, \Delta, \{ \Psi_n \}$, and $\{ \Pi_n \}$, we shall construct a first order language whose terms are as above. Fix a set of variables $\Lambda$, a set of constants $\Delta$, a family of functions $\{ \Psi_n \}$, and a family of predicates $\{ \Pi_n \}$. An {\bf atomic formula} is a string of the form $P(t_1, \dots, t_n)$, where $P \in \Pi_n$, and $t_1, \dots, t_n \in T(\Lambda, \Delta, \{ \Psi_n \})$. Then the first-order language $\text{FO}(\Lambda, \Delta, \{ \Psi_n \}, \{ \Pi_n \})$ is defined to be the smallest set containing all atomic formulae, which is also closed under the logical operations $\wedge, \vee, \Rightarrow, \neg, \Leftrightarrow$, as in sentential logic, and if $x \in \Lambda$ is a variable, and $s$ is a statement, then $(\forall x: s)$ and $(\exists x: s)$ are formulae in the language. We shall abbreviate the string $(\forall x_1 : (\forall x_2: \dots (\forall x_n: s)\dots))$ as $(\forall x_1, x_2, \dots, x_n : s)$. Its a bit of faff to verify unique interpretation. Given the readers experience, we leave them to fill in the syntactical details when needed. A modification of the brackets lemma used in the previous chapter will aid in this task.

In sentential logic, one may substitute arbitrary formulas into variables, and the meaning of the statement will not change. In first order logic, things are more complicated. Consider the statement
%
\begin{quotation}
    ``there exists $X$, such that if $X$ is a man, then $X$ is mortal''
\end{quotation}
%
First off, we cannot replace the initial $X$, for when we replace it with a definite noun the statement becomes nonsense. We may replace the other $X$'s, but this changes the meaning of the statement
%
\[ \text{``there exists $X$, such that if Laura is a man, then Laura is mortal''} \]
%
The problem results because the instances of $X$ are faulty. Another case results when we substitute $X$ for $Y$ in the formula
%
\[ \text{``there is $Y$ such that if $X$ is a man, then $Y$ is a dog''} \]
%
The resulting substitution is
%
\[ \text{``there is $Y$ such that if $Y$ is a man, then $Y$ is dog''} \]
%
We wish to perform these types of substitutions, but in a way which avoids changing the meaning of a statement. Let $s$ be an arbitrary string in a first order language. An occurrence of a variable $x$ is {\bf bound} in $s$ if it occurs in a subformula $(\forall x: w)$ or $(\exists x: w)$. An occurrence is {\bf free} if it is not bound, and a variable $y$ is {\bf free for $x$ in} $s$ if $x$ does not occur in any subformula of the form $(\forall y: w)$ or $(\exists y: w)$, where $y$ is a free variable in $s$. A substitution $w[s_1/x_1, \dots, s_n/x_n]$ is only valid when each $x_i$ is free for $s_i$. This avoids the interpretation problems above. In the first example, $X$ is bound, so cannot be substituted. In the second $X$ is free, but is not free for $Y$.

\section{Interpretation}

Languages are defined in terms of the subject manner we wish to study, but may be interpreted in many different ways. For instance, the axioms which define the logic of group theory may be interpreted relative to whichever group we interpret the axioms as agreeing with. We would hope that a statement is true if and only if it is true in every interpretation of the axioms. To begin discussing this, we must precisely define what we mean by interpretation, as formulated by Alfred Tarski.

An {\bf interpretation} $M$ of a first order language $\text{FO}(\Lambda, \Delta, \{ \Phi_n \}, \{ \Pi_n \})$ is a set $U_M$, known as the {\bf universe of discourse}, and an interpretation of the relations; for each constant $c \in \Delta$, we have an associated element $c_M \in U_M$, for each function $f \in \Phi_n$, we have a function $f_M: U_M^n \to U_M$, and for each proposition $P \in \Pi_n$, we have an $n$-ary relation $P_M$ on elements of $U_M$.

In sentential logic, when we assign a truth value to a set of variables, we may extend the definition of truth to all formulas. When we assign a meaning to each variable in a first order language, we may define a meaning on all terms, and from these meanings, assign truth to statements in the corresponding language. Consider a particular interpretation of a first order language with variables $\Lambda$, and consider an assignment $f: \Lambda \to U_M$. Define $f_*: T(\Lambda, \Delta, \{ \Psi_n \}) \to U_M$, by the recursive formulation. Let $x$ be an arbitrary variable, $c$ an arbitrary constant, and $g$ an arbitrary formula,
%
\[ f_*(x) = f(x)\ \ \ \ \ f_*(c) = c_M\ \ \ \ \ f_*(g(t_1, \dots, t_n)) = g_M(f_*(t_1), \dots, f_*(t_n)) \]
%
Using this definition, we may define whether a formula is satisfied in a model of a first order theory. We shall now define what it means for an assignment to {\bf satisfy} a formula in an interpretation. For simplicity, write $f[a/x]$ for the assignment
%
\[ f[a/x](y) = \begin{cases} f(y) & y \neq x \\ a & y = x \end{cases} \]
%
\begin{enumerate}
    \item $f$ satisfies $P(t_1, \dots, t_n)$ if $P_M(f_*(t_1), \dots, f_*(t_n))$ holds.
    \item $f$ satisfies $(\forall x: s)$ if $s$ is satisfied by $f[a/x]$, for all $a \in U_M$. $f$ satisfies $(\exists x: s)$ if there is some $a \in U_M$ such that $f[a/x]$ satisfies $s$.
    \item An assignment $f$ satisfies $s \circ u$ or $\neg s$, where $\circ$ and $\neg$ are logical connectives, if the truth evaluation of $s$ and $u$ is consistant with the connectives.
\end{enumerate}
%
If $M$ is an interpretation, then a formula $s$ is {\bf valid} for an interpretation, denoted $\vDash_M s$, if $s$ is true under every assignment under $M$. A statement is false if it is true under no interpretation, or alternatively, if the negation of the statement is true under the interpretation. An interpretation is a {\bf model} for a set of formulas $\Gamma$ if every formula in $\Gamma$ is true for the interpretation. We shall now explore some useful properties of interpretations.

\begin{lemma} If $\vDash_M s$ and $\vDash_M s \Rightarrow w$, then $\vDash_M w$. \end{lemma}

\begin{lemma} If a formula $s$ contains free variables $x_1, \dots, x_n$, and two assignments $f$ and $g$ agree on the free variables, then $f$ satisfies $s$ if and only if $g$ satisfies $s$. \end{lemma}
\begin{proof}
    We shall first verify that if $t$ is a term containing variables $x_1, \dots, x_n$, on which $f$ and $g$ agree, then $f_*(t) = g_*(t)$. If $t$ is a variable, or a constant, the proof is easy. But then by induction, for an $n$-ary formula $u$,we have
    %
    \begin{align*}
        f_*(u(t_1, \dots, t_n)) &= u_M(f_*(t_1), \dots, f_*(t_n))\\
        &= u_M(g_*(t_1), \dots, g_*(t_n))\\
        &= g_*(u(t_1, \dots, t_n))
    \end{align*}
    %
    Thus the theorem is verified by structural induction.

    If $s$ is an atomic formula $P(t_1, \dots, t_n)$, then $f$ satisfies $P(t_1, \dots, t_n)$ if and only if $g$ does, because $f_*(t_i) = g_*(t_i)$. If $s$ is $(\forall x: w)$, and $f$ satisfies $s$, then $f[a/x]$ satisfies $w$ for all $a \in U_M$. But then $g[a/x]$ agrees on all free variables of $f[a/x]$, so $g[a/x]$ satisfies $w$ by induction. It follows that $g$ satisfies $s$. The remaining cases are easily shown.
\end{proof}

\begin{theorem}
    If $s$ contains no free variables, then either $\vDash_M s$ or $\vDash_M \neg s$.
\end{theorem}
\begin{proof}
    This follows from the fact that any two assignments that agree on the free variables of a formula agree on the satisfiability. Thus if there are no free variables, all assignments agree, and in particular all satisfy the formula or all do not satisfy the formula.
\end{proof}

\begin{lemma} $\vDash_M (\exists x: s)$ if and only if $\vDash_M \neg (\forall x: \neg s)$. \end{lemma}
\begin{proof}
    If an assignment $f$ satisfies $(\exists x: s)$, then $s$ is satisfied by some $f[a/x]$. But then $f$ does not satisfy $(\forall x: \neg s)$ for $f[a/x]$ does not satisfy $\neg s$. Conversely, if an assignment $f$ does not satisfy $(\exists x: s)$, then every $f[a/x]$ satisfies $\neg s$.
\end{proof}

\begin{lemma}
    $\vDash_M s$ if and only if $\vDash_M (\forall x: s)$.
\end{lemma}
\begin{proof}
    If $\vDash_M s$, then every assignment $f$ satisfies $s$, so certainly every $f[a/x]$ satisfies $s$, and thus $f$ satisfies $(\forall x: s)$, hence $\vDash_M (\forall x: s)$. Conversely, suppose $\vDash_M (\forall x: s)$. Then, every assignment satisfies $(\forall x: s)$, and thus in particular satisfies $s$.
\end{proof}

Let $s$ contain free variables $x_1, \dots, x_n$. The {\bf closure} of $s$ is the string $(\forall x_1, x_2, \dots, x_n: s)$. The above proof shows a formula is satisfies if and only if its closure is.

\begin{theorem}
    Consider a form of sentential logic, whose variables are all atomic formulas, and formulas of the form $(\forall x: s)$, and $(\exists x: s)$. Then if a statement is a tautology, then it is satisfied under all interpretations.
\end{theorem}
\begin{proof}
    The connectives of predicate logic are exactly the connectives of sentential logic once we hide away the existential and universal quantifiers. If the statement is a tautology, then regardless of how we interpret the formula, the statement will be satisfied.
\end{proof}

\begin{lemma}
    If $t$ and $u$ are terms, $x$ is a variable, and $f$ is an assignment, then
    %
    \[ f[f_*(u)/x]_*(t) = f_*(t[u/x]) \]
\end{lemma}
\begin{proof}
    If $t$ is a variable unequal to $x$, or $t$ is a constant, then
    %
    \[ f[f_*(u)/x]_*(t) = f(t) = f_*(t[u/x]) \]
    %
    If $t = x$, then
    %
    \[ f[f_*(u)/x]_*(t) = f_*(u) = f_*(t[u/x]) \]
    %
    For a structural induction, let $t = g(t_1, \dots, t_n)$. Then
    %
    \begin{align*}
        f[f_*(u)/x]_*(t) &= g_M(f[u/x]_*(t_1), \dots, f[u/x]_*(t_n))\\
        &= g_M(f_*(t_1[u/x]), \dots, f_*(t_n[u/x])) = f_*(t[u/x])
    \end{align*}
    %
    Thus the theorem holds in general.
\end{proof}

\begin{lemma}
    $f$ satisfies $s[u/x]$ if and only if $f[f_*(u)/x]$ satisfies $s$.
\end{lemma}
\begin{proof}
    If $s$ is $P(t_1, \dots, t_n)$, then $s[u/x] = P(t_1[u/x], \dots, t_n[u/x])$, and
    %
    \[ P_M(f_*(t_1[u/x]), \dots, f_*(t_n[u/x])) = P_M(f[f_*(u)/x]_*(t_1), \dots, f[f_*(u)/x]_*(t_n)) \]
    %
    Which shows that $f$ satisfies $s[u/x]$ if and only if $f[f_*(u)/x]$ satisfies $s$. If $s$ is formed by standard sentential connectives, the theorem is trivial. If $s = (\forall y: w)$, where $y \neq x$, then $s[u/x] = (\forall y: w[u/x])$, and by definition, $f$ satisfies $s[u/x]$ if and only if $f[a/y]$ satisfies $w[u/x]$ for all $a$, which by induction implies that $f[f_*(u)/x][a/y]$ satisfies $w$ for all $a$, so $f[f_*(u)/x]$ satisfies $s$. Similar results hold if $s$'s primitive connective is the existential quantifier.
\end{proof}

\begin{theorem}
    For any formula $s$ and term $t$ free for $x$, $\vDash_M (\forall x : s) \Rightarrow s[t/x]$
\end{theorem}
\begin{proof}
    Let $f$ be an assignment satisfying $(\forall x: s)$. Then $f[f_*(t)/x]$ satisfies $s$, so $f$ satisfies $s[t/x]$.
\end{proof}

\begin{theorem}
    If $s$ does not contain $x$ as a free variable, then
    %
    \[ \vDash_M (\forall x: s \Rightarrow w) \Rightarrow (s \Rightarrow (\forall x: w)) \]
\end{theorem}
\begin{proof}
    If $f$ satisfies $(\forall x: s \Rightarrow w)$ and $s$, then $f[a/x]$ satisfies $s \Rightarrow w$, and since $s$ does not contain $x$ as a free variable, $f[a/x]$ also satisfies $s$, so $f$ satisfies $(\forall x : w)$.
\end{proof}

Given a first order language $F$, together with a specific interpretation with universe of discourse $U_M$, we enhance the first order language we are discussing to include $U$ as constants. Such an extension is denoted as $F(U_M)$. $U_M$ can be extended canonically to interpret $F(U_M)$, in the obvious manner. This allows us to discuss formulas of the form
%
\[ \vDash_M s[u_1/x_1, \dots, u_n/x_n] \]
%
Where $u_1, \dots, u_n \in U$ are formulas containing elements of $U$. If the free variables of a formula $s$ are $x_1, \dots, x_n$, then the set of $u_1, \dots, u_n$ such that $M$ models $s[u_1/x_1, \dots, u_n/x_n]$ will be called the relation relative to $s$.

A statement is {\bf logically valid} if it is true under all possible interpretations. A statement is {\bf satisfiable} if it is true under at least one interpretation, and {\bf contradictory} if it is false under every interpretation. A set of statements is satisfiable if they are all true under a single interpretation. A statement $s$ is a {\bf logical consequence} of a set of statements $\Gamma$ if every interpretation which satisfies every statement of $\Gamma$ also satisfies $s$.

What we have argued in this chapter is that certain rules preserve logical validity. Thus they are valid for a formal argument in predicate logic. In the next section, we will formally introduce these rules, and in the system, prove they are all the formulae needed to prove anything valid about predicate logic.






\section{First Order Formal Systems}

We have the syntax and semantics to begin discussing formal systems in the first-order languages. We would like to find axioms which only find theorems satisfied by every model of the system, a {\bf sound} axiom system. Even better, if we can find {\bf complete} axioms, which can proved anything satisfied by every model of the system. For simplicity, we consider only formulae in the connectives $\forall$, $\neg$, and $\Rightarrow$, since all other formulae are equivalent to formulas formed by these connectives. We shall use an axiom consisting of five schemata, the original (A1), (A2), and (A3) found in predicate logic, as well as two new first order schema (A4) and (A5). Let $s$ and $w$ be formula, and $t$ a term substitutable for a variable $x$. Then (A4) is
%
\[ (\forall x: s) \Rightarrow s[t/x] \]
%
provided that $t$ is free for substitution for $x$ in $s$, and (A5) is
%
\[ (\forall x: s \Rightarrow w) \Rightarrow (s \Rightarrow (\forall x: w)) \]
%
where $s$ contains no free occurences of $x$. Our rules of inference are modus ponens (MP), to infer $w$ from $s$ and $s \Rightarrow w$, as well as universal generalization (UG), inferring $(\forall x: s)$ from $s$. As in predicate logic, we shall let $\vdash s$ stand for $s$ is a theorem of the system. If $\Gamma$ is a set of formulae, we shall let $\Gamma \vdash s$ state that $s$ is provable assuming $\Gamma$ are axioms.

The rule of universal generalization is very subtle. One cannot put it into an axiomatic schema
%
\[ s \Rightarrow (\forall x: s) \]
%
for this statement is not sound in all models of predicate logic.

\begin{example}
    Consider the formula $P(x) \Rightarrow (\forall x: P(x))$. Take a model $M$ whose universe consists of two elements $a$ and $b$, and let $P_M$ be the relation only satisfied by $a$. Then, under the assignment $f(x) = a$, $P(x)$ is satisfied, but $(\forall x: P(x))$ is not. Thus $P(x) \Rightarrow (\forall x: P(x))$ is not semantically valid, and hence not provable in first order logic.
\end{example}

Nonetheless, if we can {\it prove} $s$, universal generalization allows us to conclude that $(\forall x: s)$ is also valid; if $s$ contains $x$ as a free variable, it is only an inbetween statement which we use to eventually conclude that $(\forall x: s)$ is true. It is this little annoyance which lead Moses Sch\"{o}nfinkel and Haskell Curry to invent combinatory logic, which is a logical system designed to avoid quantified variables. Nonetheless, common arguments are most easily adapted to our model of first order logic, so for the time being we will stick to this system.

From the theorems we proved about first order semantics, we already know that this axiom system is sound, for each axiom is valid under all interpretations, and our deductions preserve truth. We shall, after some work, conclude this system is complete -- one may prove a theorem if and only if it is valid under all interpretations of the theory. To begin with, we notice our system includes (A1), (A2), (A3), and (MP), all the rules of propositional calculus, which justifies a very useful property of our new formal system.

\begin{theorem}
    Let $s$ be a formula in first order logic, and replace it with a corresponding formula in sentential logic by replacing all the main occurences of the quantifiers $(\forall x: s)$ with variables separate from the rest of the variables occuring in $s$. Then if the formula obtained is a tautology, $s$ is provable in first order logic.
\end{theorem}

We shall denote an application of the deduction theorem by $(\text{TAUT})$. There are many useful schemas obtained from this, that are trivial to prove.
%
\begin{itemize}
    \item Negation Elimination (NE): $\vdash \neg \neg s \Rightarrow s$
    \item Negation Introduction (NI): $\vdash s \Rightarrow \neg \neg s$
    \item Conjunction Elimination (CE): $\vdash s \wedge w \Rightarrow s$, $\vdash s \wedge w \Rightarrow w$.
    \item Conjunction Introduction (CI): $\vdash s \Rightarrow (w \Rightarrow (s \wedge w))$.
\end{itemize}
%
one just makes an application of the tautology theorem.

\begin{example}
    If $t$ is free for $x$ in $s$. Then $s[t/x] \Rightarrow (\exists x: s)$ is a theorem of first order logic, a formalization of existential introduction.
    %
        \[
    \fitchprf{\pline{s[t/x] \Rightarrow \neg (\forall x: \neg s)}}{
        \pline[1.]{(\forall x: \neg s) \Rightarrow \neg s[t/x]}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (A4)]\\
        \pline[2.]{((\forall x: \neg s) \Rightarrow \neg s[t/x]) \Rightarrow (s[t/x] \Rightarrow \neg (\forall x: \neg s))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (TAUT)]\\
        \pline[3.]{s[t/x] \Rightarrow \neg (\forall x: \neg s)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1),(2),(MP)]
    }
    \]
    %
    Without the tautology theorem, this deduction would be much longer. It will be useful in further proofs to note that we never applied $(UG)$ to any formulae. We denote an application of this proof by $(EI)$.
\end{example}

The deduction theorem cannot be carried directly over to first order logic, for assumptions in proofs and premises in logical statements are interpreted differently in certain circumstances. $s \vdash (\forall x: s)$ is always true for any statement $s$, yet $\vdash s \Rightarrow (\forall x: s)$ is not always a theorem.

The problem is, of course, the free variables again causing trouble. In a proof $(s_1, \dots, s_n)$ of $\Gamma \vdash s_n$, we say $s_i$ {\bf depends on} $w \in \Gamma$ if $s_i$ is $w$, and is obtained as an axiom, or $s_i$ is inferred from $s_j$ and $s_j$ depends on $w$.

\begin{theorem}
    Suppose that $\Gamma, s \vdash w$, and there is a proof of $w$ from $s$ which involves no application of universal generalization on a formula which depends on $w$, by a variable $x$ which is free in $w$. Then $\Gamma \vdash s \Rightarrow w$.
\end{theorem}
\begin{proof}
    We perform induction on the length of proofs. If $s$ can be proved in one statement, then $s$ is either an instance of an axiom, or an element of $\Gamma \cup \{ w \}$. If $s \neq w$, then $\Gamma \vdash s$ since the proof is equally valid here. If $s = w$, then $w \Rightarrow w$ is a tautology, so $\Gamma \vdash w \Rightarrow s$.

    Suppose we have a proof $(s_1, \dots, s_{n+1})$. By induction, for each $s_i$, we have $\Gamma \vdash (w \Rightarrow s_i)$. If $s_{n+1}$ is an axiom, an element of $\Gamma$, or equal to $w$, we may apply the last paragraph. Suppose $s_{n+1}$ was inferred by modus ponens from $s_i$ and $s_j$, where $s_j$ has the form $s_i \Rightarrow s_{n+1}$. Then $\Gamma \vdash (w \Rightarrow s_i)$, and $\Gamma \vdash (w \Rightarrow (s_i \Rightarrow s_{n+1}))$ by induction. The statement
    %
    \[ u = ((w \Rightarrow (s_i \Rightarrow s_{n+1})) \Rightarrow (w \Rightarrow s_i)) \Rightarrow (w \Rightarrow s_{n+1}) \]
    %
    is a tautology, so $\Gamma \vdash u$, and by modus ponens, we find $\Gamma \vdash (w \Rightarrow s_{n+1})$. Otherwise $s_{n+1}$ is of the form $(\forall x: s_i)$, obtained from some $s_i$ by universal generalization. By induction, $\Gamma \vdash w \Rightarrow s_i$, so $\Gamma \vdash (\forall x: w \Rightarrow s_i)$. By assumption, $x$ does not occur as a free variable of $w$, so we have the axiom $(\forall x: w \Rightarrow s_i) \Rightarrow (w \Rightarrow (\forall x: s_i))$, and we conclude $\Gamma \vdash w \Rightarrow (\forall x: s_i)$.
\end{proof}

\begin{example}
    We have the theorem $\vdash (\forall x, \forall y: s)) \Rightarrow (\forall y, \forall x: s)$, for any statement $s$. To see this, we apply our newly established deduction theorem.
    %
    \[
    \fitchprf{\pline{(\forall x, \forall y: s) \Rightarrow (\forall y, \forall x: s)}}{
        \subproof{\pline[1.]{(\forall x, \forall y: s)}} {
            \pline[2.]{(\forall x, \forall y: s) \Rightarrow (\forall y: s)}[(A4)]\\
            \pline[3.]{(\forall y: s)}[(1),(2),(MP)]\\
            \pline[4.]{(\forall y: s) \Rightarrow s}[(A4)]\\
            \pline[5.]{s}[(3),(4),(MP)]\\
            \pline[6.]{(\forall x: s)}[(5),(UG)]\\
            \pline[7.]{(\forall y, \forall x: s)}[(6),(UG)]
        }
        \pline[8.]{((\forall x, \forall y: s) \Rightarrow (\forall y, \forall x: s)}[$(1-7), (DT)$]
    }
    \]
    %
    Care needs to be taken in order to ensure these steps are accurate, and do not apply universal generalization on a free variable.
\end{example}

\begin{example}
    For any $s$ and $w$, $\vdash (\forall x: s \Leftrightarrow w) \Rightarrow ((\forall x: s) \Leftrightarrow (\forall x: w))$.
    %
    \[
    \fitchprf{\pline{(\forall x: s \Leftrightarrow w) \Rightarrow ((\forall x: s) \Leftrightarrow (\forall x: w))}}{
        \subproof{\pline[1.]{(\forall x: s \Leftrightarrow w)}} {
            \pline[2.]{(s \Leftrightarrow w)}[(A4)]\\
            \subproof{\pline[3.]{(\forall x: s)}}{
                \pline[4.]{s}[(A4), (3), (MP)]\\
                \pline[5.]{w}[(2),(4),(MP)]\\
                \pline[6.]{(\forall x: w)}[(UG)]
            }
            \pline[7.]{(\forall x: s) \Rightarrow (\forall x: w)}[(3-6),(DT)]\\
            \dots\\
            \pline[8.]{(\forall x: s) \Rightarrow (\forall x: w)}\\
            \pline[9.]{(\forall x: s) \Leftrightarrow (\forall x: w)}
        }
        \pline[10.]{(\forall x: s \Leftrightarrow w) \Rightarrow ((\forall x: s) \Leftrightarrow (\forall x: w))}
    }
    \]
    %
    The proof involved in the elipses is exactly the same as (3) through (6).
\end{example}

A useful theorem is an immediate consequence of the deduction theorem, left to the reader.

\begin{theorem}
    If $s$ has no free occurrences of $y$, then
    %
    \[ \vdash ((\forall x: s) \Leftrightarrow (\forall y: s[y/x])) \]
\end{theorem}

The next theorem is more involved, but very useful. We define substitution on formulas in the following way. Given formulae $s,w,u$, we define the formula $s[w/u]$, obtained from swapping $u$ with $w$, by the base case $u[w/u] = w$, and the recursive case by delving into subformulas, {\it provided the formula isn't just $w$}.

\begin{theorem}
    Let $x_1, \dots, x_n$ be all free variables of $s$ that occurs as a bound variable in $u$ or $w$. Then
    %
    \[ (\forall x_1, \dots, x_n: w \Leftrightarrow u) \Rightarrow (s \Leftrightarrow s[w/u]) \]
    %
    The theorem also holds if $s[w/u]$ is replaced with a formula obtained only be swapping some (but perhaps not all) occurences of $u$.
\end{theorem}
\begin{proof}
    We prove, like always, by structural induction. If no occurrences are swapped, we are left with the formula
    %
    \[ (\forall x_1, \dots, x_n: w \Leftrightarrow u) \Rightarrow (s \Leftrightarrow s) \]
    %
    which is a tautology, hence trivial. Thus we may assume that $u$ does occur in $s$. If $s$ is an atomic formula, then we are left with the case that $s = u$, and then
    %
    \[ (\forall x_1, \dots, x_n: w \Leftrightarrow u) \Rightarrow (w \Leftrightarrow u) \]
    %
    is an instance of (A4).
\end{proof}

It is also useful in mathematics to make arguments of the following form. Suppose we have a theorem of the form $(\exists x: s)$. We introduce a new constant $c$, which is not used in any axiom of the formal system, and consider the formula $s[c/x]$. If we end up with a formula $w[c/x]$, we conclude that $(\exists x: s)$ implies $(\exists x: w)$. Though our system is not capable of expressing these arguments, it is satisfying to know that such arguments do not increase the amount of theorems one may prove in first order logic. Temporarily, we shall write $\vdash_C s$ for a proof of this form. Formally, we write $\vdash_C s$ if there is a sequence of formulas $(s_1, \dots, s_n)$ such that each $s_i$ is an axiom, is inferred by (MP) or (UG) from a previous formula, or there is a preceding formula $s_j = (\exists x: w)$, where $s_i = w[c/x]$, and $c$ is a new constant which does not occur in any prior formulae or explicitly in axioms of the formal system. We require that no application of $(UG)$ is made by a variable free in some $(\exists x: s)$ by which the new rule is applied. Finally, we require that $s_n$ does not contain any of the constants introduced by the final rule. To prove that this method can be applied to our formal system, we require a certain formula, proved now, and integral to the proof that $\vdash_C$ is redundant.

\begin{example}
    If $x$ is not free in $w$, $((\exists x: s) \Rightarrow w) \Leftrightarrow (\forall x: s \Rightarrow w)$.
    %
    \[
    \fitchprf{\pline{((\exists x: s) \Rightarrow w) \Leftrightarrow (\forall x: s \Rightarrow w)}}{
        \subproof{\pline[1.]{(\exists x: s) \Rightarrow w}} {
            \subproof{\pline[2.]{s}}{
                \pline[3.]{(\exists x: s)}[(EI)]\\
                \pline[4.]{w}[(1),(3),(MP)]
            }
            \pline[5.]{s \Rightarrow w}[(2-4),(DT)]\\
            \pline[6.]{(\forall x: s \Rightarrow w)}[(UG)]
        }
        \pline[6.]{((\exists x: s) \Rightarrow w) \Rightarrow (\forall x: s \Rightarrow w)}[(1-6),(DT)]\\

        \subproof{\pline[7.]{(\forall x: s \Rightarrow w)}}{
            \pline[8.]{s \Rightarrow w}[(A4)]\\
            \pline[9.]{\neg w \Rightarrow \neg s}[(8),(\text{TAUT}),(MP)]\\
            \pline[10.]{(\forall x: \neg w \Rightarrow \neg s)}[(UG)]\\
            \pline[11.]{\neg w \Rightarrow (\forall x: \neg s)}[(10),(A5),(MP)]\\
            \pline[12.]{\neg (\forall x: \neg s) \Rightarrow w}[(11),(TAUT),(MP)]
        }
        \pline[13.]{(\forall x: s \Rightarrow w) \Rightarrow ((\exists x: s) \Rightarrow w)}[(7-12),(DT)]
    }
    \]
\end{example}

\begin{theorem}
    If $\vdash_C s$, then $\vdash s$.
\end{theorem}
\begin{proof}
    Let $(s_1, \dots, s_n)$ be a proof of $s$, and suppose that
    %
    \[ s_{i_1} = (\exists x_1: w_1)\ \ \ \ \ s_{i_2} = (\exists x_2: w_2)\ \ \ \ \ \dots\ \ \ \ \ s_{i_m} = (\exists x_m: w_m) \]
    %
    are all existence formulae used in the proof, from which new constants $c_1, \dots, c_m$ are introduced. Certainly
    %
    \[ w_1[c_1/x_1], \dots, w_m[c_m/x_m] \vdash s \]
    %
    By the universal generalization condition on $\vdash_C$, we can apply the deduction theorem to conclude that
    %
    \[ w_1[c_1/x_1], \dots, w_{m-1}[c_{m-1}/x_{m-1}] \vdash w_m[c_m/x_m] \Rightarrow s \]
    %
    In the proof of this statement, replace all instances of $c_m$ with a new variable $y_m$. This is then still a valid proof (for variables can be operated on in at least the same capacity as constants), hence we obtain
    %
    \[ w_1[c_1/x_1], \dots, w_{m-1}[c_{m-1}/x_{m-1}] \vdash w_m[y_m/x_m] \Rightarrow s \]
    %
    Hence
    %
    \[ w_1[c_1/x_1], \dots, w_{m-1}[c_{m-1}/x_{m-1}] \vdash (\forall y_m: w_m[y_m/x_m] \Rightarrow s) \]
    %
    applying the recently proved example, since we know $y_m$ does not occur in $s$ at all, we conclude
    %
    \[ w_1[c_1/x_1], \dots, w_{m-1}[c_{m-1}/x_{m-1}] \vdash (\exists y_m: w_m[y_m/x_m]) \Rightarrow s \]
    %
    And by induction, we may assume that
    %
    \[ w_1[c_1/x_1], \dots, w_{m-1}[c_{m-1}/x_{m-1}] \vdash (\exists y_m: w_m[y_m/x_m]) \]
    %
    which implies, by a particular use of (MP), that
    %
    \[ w_1[c_1/x_1], \dots, w_{m-1}[c_{m-1}/x_{m-1}] \vdash s \]
    %
    And we may recursively prove that $\vdash s$ applies.
\end{proof}

\section{Completeness Theorem}

The completeness theorem is best understood in the context of {\bf First Order Theories}, which are subsets of formulas in a first order system, which we think of as axioms. An {\bf extension} of a theory is just a theory which is a superset of the original theory. If $\Gamma$ is a theory, we let $\Gamma^{\mathfrak{d}}$ denote the {\bf deductive closure} of $\Gamma$, which is the extension consisting of all formulas $s$ such that $\Gamma \vdash s$. Recall that a theory $\Gamma$ is {\bf consistant} if for any $s$, we do not have both $\Gamma \vdash s$ and $\neg s \in \Gamma$. A {\bf complete} theory is a theory in which $s \in \Gamma$ or $\neg s \in \Gamma$ always holds. If some theory is consistance or complete, so is its logical closure.

At times, we shall consider extensions which lie in extensions of first order languages, in the sense of adding additional prepositions, constants, and variable symbols to the language. It is simple to see that if $\mathcal{B}$ is a first order language extending $\mathcal{A}$, if $\Gamma$ is a theory in $\mathcal{B}$, and if $\Gamma_\mathcal{B}$ is the deductive closure of $\Gamma$ in $\mathcal{B}$, $\Gamma_{\mathcal{A}}$ the deductive closure of $\Gamma$ in $\mathcal{A}$, then $\Gamma_\mathcal{A} = \Gamma_\mathcal{B} \cap \mathcal{A}$. Surely the left is included in the second. Alternatively, suppose there is a proof $(s_1, \dots, s_n)$ in $\mathcal{B}$ of $\Gamma \vdash s_n$, with $s_n \in \mathcal{A}$. For each variable and constant in the proof which is not in $\mathcal{A}$, swap it out with a unique unused variable in $\mathcal{A}$. Since variables can do everything a constant can do, the proof is still valid. Similarily, swap each proposition in the proof which is not in $\mathcal{A}$ with some proposition in $\mathcal{A}$. This might be a little harder to see, but one verifies by induction on proof length that this works out, and shows that we may prove $s_n$ solely in $\mathcal{A}$. Thus it is consistant to talk of extensions of theories that add new variables without introducing additional information.

\begin{lemma}
    If a theory has a model, then it is consistant.
\end{lemma}
\begin{proof}
    If $\Gamma$ has a model, and $\Gamma \vdash s$ and $\Gamma \vdash \neg s$, then $s$ and $\neg s$ must both be valid in the model, which is clearly impossible.
\end{proof}

\begin{example}
    The theory of groups is based on the language consisting of some variables, the constant $e$, a $2-ary$ function $*$, and the $2-ary$ preposition $=$ (applied in infix notation), with the axiom system
    %
    \[ x * (y * z) = (x * y) * z \]
    \[ e * x = x\ \ \ \ \ x * e = x \]
    \[ (\forall x, \exists y: x * y = e \wedge y * x = e) \]
    %
    together with the equality axioms
    %
    \[ (x = y) \Rightarrow (y = x)\ \ \ \ \ ((x = y) \wedge (y = z)) \Rightarrow (x = z)\ \ \ \ \ (x = y) \Rightarrow (x * z = x * y) \]
    %
    This set of statements is a first order theory. It is also interesting to note that the theory is {\bf finitely axiomatizable}, in the sense that the theory $\Gamma$ is a finite set. Any model of this theory is just a plain old group. The completeness theorem will show that every theorem proved in this formal system is valid in any group. This theory is consistant, but certainly not complete, for we may add the abelian axiom
    %
    \[ x * y = y * x \]
    %
    and we certainly have groups which do not satisfy this axiom.

    Models of algebraic systems are interesting as an example, but it also is used for interesting applications of logic in algebra. For instance, a simple classification of the algebraically closed fields is obtained through model theoretic methods.
\end{example}

\begin{theorem}
    If we cannot prove $s$ or $\neg s$ in a consistant theory $\Gamma$, then $\Gamma \cup \{ s \}$ is a consistant theory.
\end{theorem}
\begin{proof}
    Suppose that $\Gamma \cup \{ s \} \vdash w$ and $\Gamma \cup \{ s \} \vdash \neg w$. Without loss of generality, we may assume that $s$ contains no free variables, for the logical closure of $\Gamma \cup \{ s \}$ is the same as the logical closure of $\Gamma \cup \{ (\forall x_1, \dots, x_n: s) \}$, because of axiom (A4) and (UG). But then, by the deduction theorem,
    %
    \[ \Gamma \vdash s \Rightarrow w\ \ \ \ \ \ \ \ \ \ \Gamma \vdash s \Rightarrow \neg w \]
    %
    But then $\Gamma \vdash \neg s$, because
    %
    \[ ((s \Rightarrow w) \wedge (s \Rightarrow \neg w)) \Rightarrow \neg s \]
    %
    is a tautology, a contradiction.
\end{proof}

\begin{lemma}[Lindenbaum]
    If $\Gamma$ is a consistant theory, then there is a complete consistant extension of $\Gamma$.
\end{lemma}
\begin{proof}
    Let $\mathcal{K}$ be the set of all consistant extensions of $\Gamma$, ordered by inclusion. If $\mathcal{A}$ is a chain of consistant extensions of $\Gamma$, then we claim $\bigcup \mathcal{A}$ is consistant. Suppose that
    %
    \[ \bigcup \mathcal{A} \vdash s\ \ \ \ \ \ \ \ \bigcup \mathcal{A} \vdash \neg s \]
    %
    Then there is a proof $(s_1, s_2, \dots, s_n)$ of $s$ from $\bigcup \mathcal{A}$, and a proof $(w_1, w_2, \dots, w_n)$ of $\neg s$. Since each side is finite, each uses only finitely many axioms, which implies that there is $\Gamma \in \mathcal{A}$ containing all axioms. But then
    %
    \[ \Gamma \vdash s\ \ \ \ \ \ \ \ \Gamma \vdash \neg s \]
    %
    Which implies $A$ is not consistant, a contradiction. By Zorn's lemma, there is a maximal consistant extension $\Gamma$. $\Gamma$ is complete, by the last lemma. If $\Gamma \not \vdash \neg s$, then $\Gamma \cup \{ \neg s \}$ is consistant, implying $\Gamma \cup \{ \neg s \} = \Gamma$, so $s \in \Gamma$, implying $\Gamma \vdash s$.
\end{proof}

A term is {\bf closed} if it has no free variables. A theory $\Gamma$ is {\bf scapegoat} if for any formula $s$ which only has one free variable $x$, there is a closed term $t$ for which
%
\[ \Gamma \vdash (\exists x: \neg s) \Rightarrow \neg s[t/x] \]
%
Scapegoat theorys are useful for proving the completeness theorem, for it is easier to move constants into interpretations than with plain formulas.

\begin{lemma}
    Every consistent theory $\Gamma$ has a consistant scapegoat extension $\Gamma$ which has the same cardinality as $\Gamma$ if $\Gamma$ is infinite, or is denumerable if $\Gamma$ is finite.
\end{lemma}
\begin{proof}
    The deductive closure of $\Gamma$ has the same cardinality as $\Gamma$ in the infinite case, or is denumerable in the finite case. Since the deductive closure extends $\Gamma$, we may, without loss of generality, assume $\Gamma$ is deductively closed. Let $\mathcal{X}$ be the set of all variables in all formulas of $\Gamma$ which have exactly one free variable, and biject them with a set $\mathcal{C}$ disjoint from all preexisting characters in the first order language of $\Gamma$. Let $c_x$ be the element of $\mathcal{C}$ in bijection with $\mathcal{X}$. Let $\tilde{\Gamma}$ be the theory obtained from $\Gamma$ by adding $\mathcal{C}$ as constants to the theory, together with the formulas
    %
    \[ (\exists x: \neg s) \Rightarrow \neg s[c_x/x] \]
    %
    $\tilde{\Gamma}$ is surely a scapegoat by construction. We claim that $\tilde{\Gamma}$ is consistant and scapegoat. It suffices, since every proof is finite, to show that every finite subextension is consistant. Consider any particular variables $x_1, x_2, \dots, x_n \in \mathcal{X}$, and let $\Gamma_0 = \Gamma$, and $\Gamma_k$ be the theory obtained from $\Gamma_{k-1}$ be adding the contant $c_{x_k}$ and the axiom $(\exists x_k: \neg s_k) \Rightarrow \neg s_k[c_x/x]$. We prove this by induction. Suppose $\Gamma_{k-1}$ is consistant, and $\Gamma_k$ is inconsistant. Then we may prove all statements in $\Gamma_{k-1}$ in $\Gamma_k$. In particular,
    %
    \[ \Gamma_k \vdash \neg ((\exists x_k: \neg s_k) \Rightarrow \neg s_k[c_{x_k}/x_k]) \]
    %
    Since this formula is closed (because $x$ is the only free variable in $s_k$), we may apply the deduction theorem to conclude
    %
    \[ \Gamma_{k-1} \vdash ((\exists x_k: \neg s_k) \Rightarrow \neg s_k[c_{x_k}/x_k]) \Rightarrow \neg ((\exists x_k: \neg s_k) \Rightarrow \neg s_k[c_{x_k}/x_k]) \]
    %
    Which allows us to conclude (by the tautology $(A \Rightarrow \neg A) \Rightarrow \neg A$), that
    %
    \[ \Gamma_{k-1} \vdash \neg ((\exists x_k: \neg s_k) \Rightarrow \neg s_k[c_{x_k}/x]) \]
    %
    Hence
    %
    \[ \Gamma_{k-1} \vdash (\exists x_k: \neg s_k)\ \ \ \ \ \ \ \ \Gamma_{k-1} \vdash s_k[c_{x_k}/x_k] \]
    %
    Since $c_{x_k}$ does not occur in $\Gamma_0, \Gamma_1, \dots, \Gamma_{k-2}$, we may conclude that $\Gamma_{k-1} \vdash s_k[y_k/x_k]$, by replacing all occurences of $c_{x_k}$ in the proof of $s_k[c_{x_k}/x_k]$ by a new variable $y_k$ which doesnot occur in the proof. But then
    %
    \[ \Gamma_{k-1} \vdash (\forall y_k: s_k[y_k/x_k]) \]
    %
    so $\Gamma_{k-1} \vdash s_k$, hence $\Gamma_{k-1} \vdash (\forall x_k: s_k)$, contradicting preestablished theorems. Thus we have shown $\Gamma$ is consistant.
\end{proof}

\begin{lemma}
    Let $\Gamma$ be a consistant, complete, scapegoat theory. Then $\Gamma$ has a model $M$ whose universe of discourse is the set of closed terms in $\Gamma$.
\end{lemma}
\begin{proof}
    For each constant $c$ in the language, let $c^M = c$. For each function $f$, let $f^M(t_1, \dots, t_n) = f(t_1, \dots, t_n)$ (by assumption, each $t_i$ is closed). For each predicate $P$, let $(t_1, \dots, t_n) \in P^M$ if and only if $\vdash P(t_1, \dots, t_n)$. We shall show that $M \vDash s$ if and only if $\Gamma \vdash s$, for any closed term $s$. This implies that $M$ is a model of $\Gamma$, for then, if $w$ is any axiom of $\Gamma$, then $\Gamma \vdash (\forall x_1, \dots, x_n: w)$, hence $M \vDash (\forall x_1, \dots, x_n: w)$, which occurs if and only if $M \vDash w$. Thus $M$ models all axioms. We will prove our statement by structural induction.
    %
    \begin{enumerate}
        \item If $s$ is $P(t_1, \dots, t_n)$, the statement is trivial by construction.

        \item Suppose $s$ is $\neg w$. If $M \vDash w$, then by induction $\Gamma \vdash w$, which implies by consistancy that $\Gamma$ cannot prove $\neg w$, and $M$ cannot satisfy $\neg w$. Conversely, if $M \vDash \neg w$, then $M \not \vDash w$, so the theorem follows again by consistancy.

        \item If $s$ is $w \Rightarrow u$, since $s$ is closed, $w$ and $u$ are closed. If $M \not \vDash w$, then $M \vDash w \Rightarrow u$, and by induction $\Gamma \not \vdash w$, so $\Gamma \vdash \neg w$, and then $\Gamma \vdash w \Rightarrow u$ by tautology. The remaining cases are again treated by tautology and completeness.

        \item If $s$ is $(\forall x: w)$, then either $w$ is closed, or $w$ has a single free variable. If $w$ is closed, the statement follows from tautologies and completeness fairly easily. We shall treat the other case in detail. Suppose that $M \vDash (\forall x: w)$, yet $\Gamma \not \vdash (\forall x: w)$. Thus $\Gamma \vdash \neg (\forall x: w)$. Since $\Gamma$ is scapegoat, there is a constant $c$ such that $\Gamma \vdash \neg w[c/x]$, from which we conclude $M \vDash \neg w[c/x]$ by induction, contradicting that $M \vdash (\forall x: w)$. Conversely, suppose $M \not \vDash (\forall x: w)$, yet $\Gamma \vdash (\forall x: w)$. Then $\Gamma \vdash w[t/x]$ for any term $t$, so by induction, $M \vDash w[t/x]$ for all closed terms $t$. Let $f$ be an assignment on $M$ such that which does not satisfy $w$. Let $f(x) = t$. Since $t$ is a closed term in the interpretation, $f_*(t) = t$, and $f = f[f_*(t)/x]$, so a previous lemma implies that $f$ cannot satisfy $w[t/x]$. Thus $M \not \vDash w[t/x]$, a contradiction.
    \end{enumerate}
    %
    We have addressed all cases, so our proof is complete.
\end{proof}

\begin{theorem}
    Every consistant theory has a model whose cardinality is the same as the theory itself, unless the theory is finite, in which the model is denumerable.
\end{theorem}
\begin{proof}
    Let $\Gamma$ be a consistant theory. Extend $\Gamma$ to a consistant, complete, scapegoat theory $\Gamma'$, which is denumerable if $\Gamma$ is finite or denumerable, or else $\Gamma$ has the same cardinality as $\Gamma'$. But then $\Gamma'$ has a model consisting of closed terms in $\Gamma'$, whose cardinality is the same as the $\Gamma$.
\end{proof}

We are now ready for the utmost regarded ``G\"{o}del's completeness theorem''. Once you know it, you can brag to all your friends that you understand it...

\begin{theorem}[G\"{o}del (Our proof is Henkin's)]
    A formula is provable in a theory if and only if it is true under all interpretations.
\end{theorem}
\begin{proof}
    Let $\Gamma$ be a theory. Without loss of generality, assume $\Gamma$ is consistant. If $\Gamma \vdash s$, then we have already shown $s$ is true in all models. If $\Gamma \not \vdash s$, then $\Gamma \cup \{ \neg s \}$ is consistant, so $\Gamma \cup \{ \neg s \}$ has a model $M$, but then $M \vDash \neg s$, so $M \not \vdash s$, and $M$ is a model for $\Gamma$.
\end{proof}

Thus syntax and semantics coincide. We actually proved something much stronger, that a formula is provable in a theory if and only if it is true under all interpretations whose cardinality is that theories cardinality, or denumarable if the theory is finite. Nonetheless, we find our proof is non-constructive. We did not construct a formal proof given that the formula was true under all interpretations. This foreshadows the general result that there is no constructive procedure for generating a proof of a the formula, proved by G\"{o}del a decade later.

\section{The Compactness Theorem}

\begin{theorem}[The Compactness Theorem]
    $\Gamma$ is a consistant theory if and only if every finite subset of $\Gamma$ is consistant. Conversely, $\Gamma$ has a model if and only if every finite subset of $\Gamma$ has a model.
\end{theorem}
\begin{proof}
    If $\Gamma$ is consistant, than a finite subset is consistant. Conversely, suppose $\Gamma$ is inconsistant, and consider proofs of two statements $\Gamma \vdash s$ and $\Gamma \vdash \neg s$. These proofs only use finitely many axioms in $\Gamma$, so there is some finite subset $\Delta$ of $\Gamma$ such that $\Delta \vdash s$ and $\Delta \vdash \neg s$, so some finite subset is inconsistant.
\end{proof}

Consider the space of all consistant theories in a first order logic. Consider the family
%
\[ U_s = \{ \Gamma : \Gamma \vdash s \} \]
%
Then $U_s$ defines a basis for a topology, for $U_s \cap U_w = U_{s \wedge w}$. A net $\{ \Gamma_i \}$ converges to $\Gamma$ in this topology if and only if every statement provable in $\Gamma$ is eventually provable in the net. Every element of the basis is clopen, for $U_s^c = U_{\neg s}$. The compactness theorem is equivalent to the set $\mathcal{C}$ of complete, consistant theories being topologically compact, for inconsistant theories correspond to open covers of $\mathcal{C}$. If $\Gamma$ is an inconsistant theory, then
%
\[ \{ U_{\neg s} : s \in \Gamma \} \]
%
is a cover of $\mathcal{C}$. This follows for any complete theory not in any $U_{\neg s}$ must be able to prove every element of $\Gamma$, and therefore be inconsistant. Conversely, suppose $\mathcal{U}$ is an open cover of $\Gamma$ by sets of the form $U_s$. Then $\{ \neg s : U_s \in \mathcal{U} \}$ is inconsistant, for otherwise it is contained in a complete, consistant, deductively closed theory $\Gamma$, and $\Gamma \not \in U_{\neg s}$ for any $U_{\neg s} \in \mathcal{C}$.

Now suppose $\mathcal{C}$ was compact, and let $\Gamma$ be an inconsistant theory. Then
%
\[ \{ U_{\neg s} : s \in \Gamma \} \]
%
forms a cover of $\mathcal{C}$.  Thus $U_{\neg s}$ has a finite subcover. This corresponds to a finite inconsistant subtheory of $\Gamma$. We shall prove $\mathcal{C}$ is compact using the compactness theorem. Let $\mathcal{U}$ be an open cover of $\mathcal{C}$. Without loss of generality, assume each element of $\mathcal{U}$ is of the form $U_s$. Then $\Gamma = \{ \neg s : U_s \in \mathcal{U} \}$ is an inconsistant theory, and therefore has a finite inconsistant subtheory, which corresponds to a finite subcover of $\mathcal{C}$.

Similarily, consider the class $\mathcal{M}$ of all models of a first order system, and take a topology on it by considering the family
%
\[ V_s = \{ M \in \mathcal{M} : M \vDash s \} \]
%
of open neighbourhoods. Then $M_i \to M$ only when any formula $s$ satisfied by $M$ is eventually satisfied in $M_i$. For any model $M$, define $\text{Th}(M)$ to be the set of all {\it closed} formulas $s$ for which $M \vDash s$. Then $\text{Th}(M)$ is a consistant theory, which we claim is also complete. Let $w$ be any formula, and let $w'$ be its closure. Then either $M \vDash w'$ or $M \vDash \neg w'$, which implies either $w$ or $w'$ is in $\text{Th}(M)$, but then either $\text{Th}(M) \vdash w$ or $\text{Th}(M) \vdash \neg w$, by universal elimination. We claim that the map $\text{Th}: M \mapsto \text{Th}(M)$, from $\mathcal{M}$ to $\mathcal{C}$, is continuous. Given a formula $s$, consider all $M$ such that $\text{Th}(M) \vdash s$. If $s'$ is the closure of $s$, then $U_s = U_{s'}$, so we may assume $s$ is closed. But then either $M \vDash s$ or $M \vDash \neg s$, and since $\text{Th}(M)$ is consistant, we must have $M \vDash s$. If $M \vDash s$, then obviously $\text{Th}(M) \vdash s$, so
%
\[ \text{Th}^{-1}(U_s) = V_s \]
%
We know $\text{Th}$ is surjective, but the next section will show that it is impossible for $\text{Th}$ to be injective, unless we limit our models to only having a certain cardinality. Since $\text{Th}(U_s) = V_s$ for each closed formula $s$, the map is also open, implying $\mathcal{M}$ is compact. A useful corollary is that

\begin{theorem}
    Any sequence of models $M_i \in \mathcal{M}$ contains a convergent subsequence.
\end{theorem}

This theorem has interesting consequences. First, note that if $\Gamma$ is a theory, then the set of all models of a theory $\Gamma$ is closed in $\mathcal{M}$, since the set of all models $M$ which model $\Gamma$ are also all models such that $\text{Th}(\Gamma)$ proves all of $\Gamma$, which is
%
\[ \text{Th}^{-1} \left(\bigcap_{s \in \Gamma} U_s \right) \]
%
and each $U_s$ is closed, since its complement is open.

\begin{example}
    Supplement the standard definition of $\mathbf{N}$ by adding an additional constant $c$ to the theory. Technically, this increases the number of models of the theory, but any model of this theory can be restricted to an interpretation of Peano arithmetic. For each $n \in \mathbf{N}$, let $\mathbf{N}_n$ be the model of extended peano arithmetic consisting of the standard $\mathbf{N}$, but with $c$ interpreted as $n$. By compactness, there is a subsequence $\mathbf{N}_{n_i}$ converging to some theory $\mathbf{N}_{n_*}$ of extended peano arithmetic, where $n_*$ is the interpretation of $c$ in this model. For each $k$, since $M_{n_i} \vDash c > k$ for all $i$ sufficiently large, $M_{n_*} \vDash c > k$ for all $k$, so $n_* > k$. Thus $M_{n_*}$ is a peano model which has `infinitely large' elements.

    Alternatively, we may consider the theory
\end{example}

\section{Skolem-L\"{o}wenheim}

\begin{theorem}[Skolem-L\"{o}wenheim]
    Any theory with a model has a model whose cardinality is the same as the theory, or is denumerable if the theory is finite.
\end{theorem}
\begin{proof}
    If a theory has a model, then it is consistant, from which the statement follows from Lindenbaum's lemma.
\end{proof}

We are actually able to prove a much strong proposition.

\begin{corollary}
    If $\omega$ is an infinite cardinality greater than or equal to the cardinality of a consistant theory $\Gamma$, then there is a theory of size $\omega$.
\end{corollary}
\begin{proof}
    Let $M$ be a model of $\Gamma$. Fix $x \in U^M$. Extend $U^M$ to a set $U^N$ of cardinality $\omega$. Each new element will behave exactly like $x$. We define $f^N(t_1, \dots, t_n) = f^M(t_1', \dots, t_n')$, where $t_k' = t_k$ if $t_1 \in U^M$, else $t_k' = x$. Similarily, let $(t_1, \dots, t_n) \in P^N$ if and only if $(t_1', \dots, t_n') \in P^M$. Define constants the same as constants are defined in $U^M$. Then $N$ is a model of $\Gamma$ of cardinality $\omega$.
\end{proof}

This theorem appears to contradict various classical results, such as the fact that any complete, ordered fields are isomorphic (and thus $\mathbf{R}$ should be `the only model' of such fields, which contradicts the cardinality argument). Remember that first order theories are only a model of real mathematics, and thus do not sufficiently encapsulate all mathematical proofs. We therefore learn from the L\"{o}wenheim theorem that one cannot prove that $\mathbf{R}$ is the only complete ordered field.

The Skolem L\"{o}wenheim theorem holds because first order logic is not sufficiently powerful enough to distinguish `infinities'. The semantics cannot sufficiently describe what it means for two elements to be equal, so we can hide various model-theoretic `elements' of a theory inside a single semantic element of a theory. Perhaps, if we add additional equality semantics, can we prevent the Lowenheim theorem from occuring.

\section{Theories with Equality}

We now specialize our study, adding additional axioms that occur in almost every useful first order model. We shall say a first order system possesses equality if the theory has a binary predicate $=$, possessing the additional axioms (A6),
%
\[ x = x \]
%
and (A7),
%
\[ (x = y) \Rightarrow (s \Rightarrow s') \]
%
for any variables $x$ and $y$, and formulae $s$, where $s'$ is obtained from $s$ by swapping some numbers of occurences of $x$ with occurences of $y$.

\begin{example}
    In any theory with equality, $t = t$ for any term $t$. This follows from $(A6)$ by substitution. Similarily, $t = s \Rightarrow s = t$ holds.
    %
    \[
    \fitchprf{\pline{(t = s \Rightarrow s = t)}}{
        \subproof{\pline[1.]{(x = y)}}{
            \pline[2.]{(x = y) \Rightarrow ((x = x) \Rightarrow (y = x))}[(A7)]\\
            \pline[3.]{(x = x) \Rightarrow (y = x)}[(1),(2),(MP)]\\
            \pline[4.]{(x = x)}[(A6)]\\
            \pline[5.]{(y = x)}[(3),(4),(MP)]
        }
        \pline[6.]{(x = y) \Rightarrow (y = x)}[(1-6),(DT)]\\
        \pline[7.]{(\forall x,y : (x = y) \Rightarrow (y = x))}[(UG)]\\
        \pline[8.]{(t = s \Rightarrow s = t)}[(7),(A4),(MP)]
    }
    \]
    %
    We shall also need the theorem $t = s \Rightarrow (s = r \Rightarrow t = r)$, which is an instance of (A7) after some universal generalization and specification.
\end{example}

We have already seen the theory of groups, which is a first order theory with equality. Here is another example.

\begin{example}
    The theory of fields is built on a first order theory with constants $0$ and $1$, and functions $+$ and $\cdot$. The new axioms (in addition to the axioms of equality) are
    %
    \[ x + (y + z) = (x + y) + z\ \ \ \ \ \ \ \ x + 0 = x\ \ \ \ \ \ \ \ (\forall x, \exists y: x + y = 0) \]
    \[ x + y = y + x\ \ \ \ \ \ \ \ x \cdot (y \cdot z) = (x \cdot y) \cdot z\ \ \ \ \ \ \ \ x \cdot (y + z) = (x \cdot y) + (x \cdot z) \]
    \[ x \cdot y = y \cdot x\ \ \ \ \ \ \ \ x \cdot 1 = x\ \ \ \ \ \ \ \ x \neq 0 \Rightarrow (\exists y: x \cdot y = 1)\ \ \ \ \ \ \ \ 0 \neq 1 \]
    %
    If we add another binary predicate symbol $<$, and add axioms
    %
    \[ x < y \Rightarrow x + z < y + z\ \ \ \ \ \ \ \ x < y \wedge 0 < z \Rightarrow x \cdot z< y \cdot z \]
    %
    Then we obtain the theory of ordered fields.
\end{example}

\begin{example}
    One of the most important historical axiom systems was a system for geometry. The new predicates of the system are $I$, $P$, and $L$. $P(x)$ means $x$ is a point, $L(x)$ means $x$ is a line, and $I(x,y)$ means $x$ lies on $y$ (is incident to). The new axioms are
    %
    \[ P(x) \Rightarrow \neg L(x)\ \ \ \ \ I(x,y) \Rightarrow P(x) \wedge L(y)\ \ \ \ \ L(x) \Rightarrow (\exists y \neq z: I(y,x) \wedge I(z,x)) \]
    \[ P(x) \wedge P(y) \wedge x \neq y \Rightarrow (\exists z: L(z) \wedge I(x,z))\ \ \ \ \ (\exists x,y,z: P(x) \wedge P(y) \wedge P(z) \wedge \neg C(x,y,z)) \]
    %
    where $C(x,y,z)$ is the collinear relation
    %
    \[ (\exists u: L(u) \wedge L(x,u) \wedge L(y,u) \wedge L(z,u) \]
    %
    This is the theory of geometry, which extends to Euclidean, Projection, and Hyperbolic geometry, so that the theory is not complete.
\end{example}

In theories of equality it is possible to define new symbols which are useful for abreviating formulae. We define $(\exists ! x: s)$ to means that there is only one element $x$ satisfying $s$. That is, the symbol is short for
%
\[ (\exists x: s) \wedge (\forall x,y: s[x/x] \wedge s[y/x] \Rightarrow x = y) \]
%
Similarily, for each integer $n$, we may define the symbol $(\exists_n x: s)$. $(\exists_1 x: s)$ is the same as $(\exists ! x: s)$.

In any model $M$ of a theory with equality, the relation $=^M$ is an equivalence relation. The model $M$ is {\bf normal} if the equivalence relation is trivial. Any model $M$ can be contracted into a normal model $M'$. Given a model $M$, we quotient $U^M$ by $=^M$ to obtain $U^{M'}$. For a function $f$ and predicate $P$, define
%
\[ f^{M'}([t_1],[t_2],\dots,[t_n]) = [f^M(t_1, \dots, t_n)] \]
%
\[ ([t_1],[t_2],\dots,[t_n]) \in P^{M'}\ \ \ \text{iff}\ \ \ (t_1, \dots, t_n) \in P^{M'} \]
%
These definitions are well defined, since the model interprets equality correctly. All axioms are correctly interpreted by the model as well. We obtain an extension to last chapter.

\begin{prop}
    Any consistant theory of equality has a normal model whose cardinality is less than or equal to the cardinality of the theory in question.
\end{prop}
\begin{proof}
    Contract any particular model.
\end{proof}

And now, the true L\"{o}wenheim-Skolem theorem, since the other proof relied on a big cheat, which we cannot rely on in normal models.

\begin{corollary}[L\"{o}wenheim-Skolem]
    Any theory with equality with an infinite normal model has a model whose size is the same as the theory if the theory is infinite, or is countable otherwise.
\end{corollary}
\begin{proof}
    Let $\Gamma$ be a consistant theory with an infinite normal model $M$. First, let us treat the case where the cardinality of $\Gamma$ is infinite, but less than or equal to the cardinality of the theory. Add to $\Gamma$ new constants $c_\alpha$, with axioms $c_\alpha \neq c_\beta$ if $\alpha \neq \beta$, forming a new theory $\Gamma'$. We claim $\Gamma'$ is consistant. Suppose that $\Gamma' \vdash s \wedge \neg s$, which is without loss of generality also a formula in the first order theory related to $\Gamma$. The proof of $s \wedge \neg s$ only used a finite number of axioms of the form $c_{\alpha_1} \neq c_{\beta_1}, \dots, c_{\alpha_m} \neq c_{\beta_m}$. Let $M$ be an infinite model of $\Gamma$. Since $M$ is infinite, we may extend the model to a model $M'$, with the same universe of discourse, interpreting $c_{\alpha_i}$ and $c_{\beta_i}$ in such a way that $c_{\alpha_i} \neq c_{\beta_i}$ is satisfied in $M$. But then $M'$ is a model for
    %
    \[ \Gamma \cup \{ c_{\alpha_1} \neq c_{\beta_1}, \dots, c_{\alpha_m} \neq c_{\beta_m} \} \]
    %
    which therefore must be consistant, a contradiction. Thus $\Gamma'$ is consistant, and has an infinite normal model.
\end{proof}

\section{New Function Letters and Constants}



\part{Computability}

In 1931, Kurt G\"{o}del proved all sufficiently complicated axiomatic systems had unprovable theorems, but a fundamental question remained; how was one to decide whether a theorem could be proved? It took a decade for Alonzo Church and Alan Turing to deduce the impossibility of such a claim. Fifty years later, `theoretical computation' had become a common reality. We shall describe mathematical models of computation which have been developed to analyze the limitations of various computational methods.

Turing and Church's major breakthrough was precisely defining a `computational procedure'. It is often the case that precise definitions give rise to easy proofs of the most surprising consequence. Philosophically, one should be able to define a procedure without reference to a computer, for humans computed long before microchips. On the other hand, models should reflect physical reality, since one needs a physical mechanism in order to computer, whether electronic or mental. If your computational model is too strong or too weak, it will not accurately represent our limitations.

We will begin by analyzing the automaton, a model of computation without stored memory. We will expand the amount of expression of the automaton by considering context-free grammars. Finally, we add memory by considering a Turing machine. It is the Church Turing thesis that this is the ultimate model of computation -- any real world computation can be modelled as an action on a Turing machine. From this model, and with the hypothesis of Church and Turing, we can make precise, philisophically interesting statements about the nature of computation in the real world.

As in mathematical logic, the objects of study are strings of symbols over a certain alphabet. One studies the notion of computation syntactically. One of the main ideas of computability theory is that a mental decision can be modelled as a {\bf decision problem} -- find a computational model which will `accept' certain strings over an alphabet. Suppose our problem is to verify whether the addition of two numbers is correct. We are given $a$, $b$, and $c$, and we must decide whether $a + b = c$. Our symbol set is $\{ 0, 1, \dots, 9, : \}$, and we wish to model a computation which accepts all strings of the form $``a:b:c''$, where $a$, $b$, and $c$ are decimal strings for which $a + b = c$. Thus we must design machine to accept strings in a specified language, and determining whether a problem is solvable reduces to studying the structure of languages.

As a more dynamic discipline than mathematical logic, we need more operations on strings to obtain languages from other languages. We obviously need concatenation, but also {\bf reversal}, which will be denoted $s^R$. These operations are extended to languages by applying the operations on a component by component basis:
%
\[ S \circ W = \{ s \circ w : s \in S, w \in W \}\ \ \ \ \ S^R = \{ s^R : s \in S \} \]
%
A {\bf palindrome} is a string $s$ for which $s^R = s$. If $\Sigma$ is a set of strings, we shall let $\Sigma_\varepsilon = \Sigma \cup \{ \varepsilon \}$.



\chapter{Finite State Automata}

Our initial model of computability is a computer with severely limited, finite amount of memory. Surprisingly, we shall still be able to compute a great many things. The idea of this model rests on explicitly representing memory as a finite amount of states, whose behaviour is uniquely determined by the state upon which it stands at a point of time. We can represent this process via a state diagram. Suppose we would like to describe an algorithm determining if a number is divisible by three. We shall represent a number by a string of dashes. For instance, $``-----''$ represents the number 5. We describe the algorithm in a flow chart below
%
\begin{center}
\begin{tikzpicture}[scale=0.2]
\tikzstyle{every node}+=[inner sep=0pt]
\draw [black] (39.8,-19.2) circle (3);
\draw [black] (39.8,-19.2) circle (2.4);
\draw [black] (46.4,-30) circle (3);
\draw [black] (34,-30) circle (3);
\draw [black] (39.8,-13.3) -- (39.8,-16.2);
\fill [black] (39.8,-16.2) -- (40.3,-15.4) -- (39.3,-15.4);
\draw [black] (41.36,-21.76) -- (44.84,-27.44);
\fill [black] (44.84,-27.44) -- (44.85,-26.5) -- (43.99,-27.02);
\draw (43.74,-23.32) node [right] {$-$};
\draw [black] (43.4,-30) -- (37,-30);
\fill [black] (37,-30) -- (37.8,-30.5) -- (37.8,-29.5);
\draw (40.2,-30.5) node [below] {$-$};
\draw [black] (35.42,-27.36) -- (38.38,-21.84);
\fill [black] (38.38,-21.84) -- (37.56,-22.31) -- (38.44,-22.78);
\draw (36.22,-23.43) node [left] {$-$};
\end{tikzpicture}
\end{center}
%
The algorithm proceeds as follows. Begin at the top node. we proceed clockwise around the triangle, moving one spot for each dash we see. If, at the end of our algorithm, we end up back at the top node, then the number of dashes we have seen is divisible by three. The basic idea of the finite automata is to describe computation via these flow charts -- we follow a string around a diagram, and if we end up at a `accept state', then we accept the string. A mathematical model for this description is a finite state automaton.

A {\bf deterministic finite state automaton} is a 5-tuple $(Q, \Sigma, \Delta, q_0, F)$, where $Q$ is a finite set of states, $\Sigma$ is a finite alphabet, $q_0 \in Q$ is a start state, $F \subset Q$ are the accept states, and $\Delta: Q \times \Sigma \to Q$ is the transition function. A finite state machine `works' exactly how our original algorithm worked. Draw a directed graph whose nodes are states, and draw an edge between a state $q$ and each related state $\Delta(q, \sigma)$, for each symbol $\sigma \in \Sigma$. Take a string $s \in \Sigma^*$. We begin at the start state $q_0$. Sequentially, for each symbol in $s$, we follow the directed edge from our current state to the state on the edge related to the current symbol in $s$. If, we end at an accept state in $F$, then $s$ is an `accepted' string. Formally, we define this method by extending $\Delta$ to $Q \times \Sigma^*$. We define, for $s \in \Sigma^*$, $t \in \Sigma$,
%
\[ \Delta(q, \varepsilon) = q\ \ \ \ \ \Delta(q, st) = \Delta(\Delta(q,s), t) \]
%
A state machine $M$ {\bf accepts} a string $s$ in $\Sigma^*$ if $\Delta(s,q_0) \in F$. We call the set of all accepting strings the {\bf language} of $M$, and denote the set as $L(M)$. A subset of $\Sigma^*$ which is a language of a deterministic finite state automata is known as a {\bf regular language}.

\begin{example}
    Consider $\Sigma = \{ - \}$. Then the set of all `dashes divisible by three' is regular, as in the introductory diagram. Formally, take
    %
    \[ Q = \mathbf{Z}_3\ \ \ \ \ \Delta(x,-) = x+1\ \ \ \ \ q_0 = 0\ \ \ \ \ F = \{ 0 \} \]
    %
    then $(Q, \Sigma, \Delta, q_0, F)$ recognizes dashes divisible by three. The `graph' of the automata is exactly the graph we've already drawn.
\end{example}

Arithmetic is closed under certain operations. Given two numbers, we can add them, subtract them and multiplication, and what results is still a number. In the theory of computation, the operations have a different flavour, but are nonetheless just as important. We shall find that all regular languages can be described from very basic languages under certain compositional operators, under which the set of regular languages is closed.

\begin{theorem}
    If $A, B \subset \Sigma^*$ are regular languages, then $A \cup B$ is regular.
\end{theorem}
\begin{proof}
    let $M = (Q, \Sigma, \Delta, q_0, F)$ and $N = (R, \Sigma, \Gamma, r_0, G)$ be automata recognizing $A$ and $B$ respectively. We shall define a finite automata recognizing $A \cup B$. Define a function $(\Delta \times \Gamma) : (Q \times R) \times \Sigma \to (Q \times R)$, by letting
    %
    \[ (\Delta \times \Gamma)(q,r,\sigma) = (\Delta(q,\sigma), \Gamma(r,\sigma)) \]
    %
    Consider
    %
    \[ H = \{ (q,r) \in S : q \in F\ \text{or}\ r \in G \} \]
    %
    We contend that
    %
    \[ ((Q \times R), \Sigma, \Delta \times \Gamma, (q_0, r_0), H) \]
    %
    recognizes $A \cup B$. By induction, one verifies that for any $s \in \Sigma^*$,
    %
    \[ (\Delta \times \Gamma)(q,r,s) = (\Delta(q,s), \Gamma(r,s)) \]
    %
    Thus $(\Delta \times \Gamma)(q_0, r_0, s) \in H$ if and only if $\Delta(q_0, s) \in F$ or $\Gamma(r_0, s) \in G$.
\end{proof}

\begin{theorem}
    If $A$ is a regular language, then $A^c$ is regular.
\end{theorem}
\begin{proof}
    If $M = (Q,\Sigma,\Delta,q_0,F)$ recognizes $A$. Then define a new machine $N = (Q,\Sigma,\Delta,q_0,F^c)$. The transition $\Delta(q_0, s)$ is in $F^c$ if and only if $\Delta(q_0, s)$ is not in $F$.
\end{proof}

\begin{corollary}
    If $A$ and $B$ are regular languages, then $A \cap B$ are regular.
\end{corollary}
\begin{proof} $A \cap B = (A^c \cup B^c)^c$. \end{proof}

\section{Non Deterministic Automata}

An important concept in computability theory is the introduction of non-determinism. Deterministic machines must follows a set protocol when understanding input. Non deterministic machines can execute one of many different specified protocols. If any of the protocols accepts the input, then the entire machine accepts the input. Thus non-deterministic machines are said to multitask, for they can be seen to run every protocol specified at once, checking one of a great many protocols to see a pass. An alternative viewpoint is that the machines make a lucky guess -- they always seem to choose the write protocol which results in an accepted string.

A {\bf non-deterministic finite state automaton} is a 5-tuple $(Q, \Sigma, \Delta, q_0, F)$, where $Q$ is a finite set of states, $\Sigma$ is a finite alphabet, $q_0$ is the start state, $F \subset Q$ are the accept states, and $\Delta: Q \times \Sigma_\varepsilon \to \mathcal{P}(Q)$ is the non-deterministic transition function.

In a non-deterministic finite state automata, we {\bf accept} a string $s$ if $s = s_1 \dots s_n$, where each $s_i \in \Sigma_\varepsilon$, and there are a sequence of states $t_0, \dots, t_{n+1}$, with $q_0 = t_0$ and $t_n \in F$, such that $t_{k+1} \in \Delta(t_k, s_k)$. The set of accepting strings of a machine $M$ form the language $L(M)$. We draw a graph with nodes $Q$, and with directed edges $v$ to $w$ if $w \in \Delta(v, \Sigma_{\varepsilon})$. We begin at $q_0$. For a string $s$, we attempt to find a path from $q_0$ to an accept state, by following edges whose corresponding symbol is in $s$ (or whose symbol is $\varepsilon$, in which we get for free). The string is accepted if such a path is possible. Some call non-deterministic methods a lucky guess methods, since they always make a lucky guess of which deterministic path to take to accept a string.

There is a nicer criterion of acceptance than described above, which is easier to work with in proofs. First, assume there are no $\varepsilon$-transitions in a machine $M$; that is, $\Delta(q, \varepsilon) = \emptyset$ for all states $q$. We may then extend $\Delta: Q \times \Sigma \to \mathcal{P}(Q)$ to $\Delta: \mathcal{P}(Q) \times \Sigma^* \to \mathcal{P}(Q)$ recursively by
%
\[ \Delta(X, \varepsilon) = X \ \ \ \ \ \Delta(X, st) = \Delta(\Delta(X,s), t) \]
%
where $\Delta(X,s) = \{ \Delta(x,s) : x \in X \}$. A string $s$ is accepted by $M$ if and only if an accept state is an element of $\Delta(q_0, s)$. If $s = s_1 \dots s_n$, and there are $t_0, \dots, t_n$ with $t_0 = q_0$, $t_n$ an accept state, and $t_{k+1} \in \Delta(t_k, s_k)$, then by induction one verifies that $t_n \in \Delta(q_0, s)$. Conversely, an induction on $s$ verifies that if $q \in \Delta(q_0, s)$, and if $s = s_1 \dots s_n$, then there is a state $q'$ with $q' \in \Delta(q_0, s_1 \dots s_{n-1})$, $q \in \Delta(q', s_n)$. But this implies that if there is an accept state in $\Delta(q_0, s)$, then $s$ is accepted by $M$. We can always perform this trick, for we may always remove $\varepsilon$ transitions.

\begin{lemma}
    Every non-deterministic automata is equivalent to a non deterministic automata without $\varepsilon$ transitions, in the sense that they both recognize the same language.
\end{lemma}
\begin{proof}
    Consider a non-deterministic automata.
    %
    \[ M = (Q, \Sigma, \Delta, q_0, F) \]
    %
    Define a state $u$ to be $\varepsilon$ reachable from $t$ if there is a sequence of states $q_0, \dots, q_n$ with $q_0 = t$, $q_n = u$, and $q_{i + 1} \in \Delta(q_i, \varepsilon)$. Let $E(u)$ be the set of all states $\varepsilon$ reachable from $u$. Define
    %
    \[ N = (Q, \Sigma, \Delta', q_0, F) \]
    %
    Where
    %
    \[ \Delta'(q, s) = \begin{cases} \bigcup_{u \in E(q)} \Delta(u, s) & s \neq \varepsilon \\ \emptyset & s = \varepsilon \end{cases} \]
    %
    Then it is easily checked that $L(N) = L(M)$, for we may skip over $\varepsilon$ transitions in $N$.
\end{proof}

It seems to be a much more complicated procedure to find if a string is accepted by a non-deterministic automata, but it turns out that every non-deterministic automata can be converted into a deterministic automata. The proof relies on the fact that we may exploit the operations of non-determinism, described using power sets of a set.

\begin{theorem}
    A non-deterministic finite state automata language is regular.
\end{theorem}
\begin{proof}
    Let $M = (Q, \Sigma, \Delta, q_0, F)$ be a non-deterministic automata. Assume $M$ has no $\varepsilon$ transitions, without loss of generality. Let
    %
    \[ N = (\mathcal{P}(Q), \Sigma, \Gamma, \{ q_0 \}, \{ S \in \mathcal{P}(Q) : S \cap F \neq \emptyset \} \]
    %
    where
    %
    \[ \Gamma(S, t) = \Delta(S, t) \]
    %
    We have already verified this deterministic machine recognizes $L(M)$, for $\Delta(\{ q_0 \}, s)$ contains an accept state if and only if $s$ is accepted.
\end{proof}

It is now fair game to use non-deterministic automata to understand regular languages, for the language of every non-deterministic automata is regular.

\begin{theorem}
    If $A$ and $B$ are regular languages, then $A \circ B$ is regular.
\end{theorem}
\begin{proof}
    Let $M = (Q, \Sigma, \Delta, q_0, F)$ and $N = (R, \Sigma, \Gamma, r_0, G)$ be deterministic automata accepting $A$ and $B$ respectively. Without loss of generality, assume $Q$ is disjoint from $R$. Consider the non-deterministic machine
    %
    \[ O = (Q \cup R, \Sigma, \Pi, q_0, G) \]
    %
    Define
    %
    \[ \Pi(s,t) = \begin{cases} \{ \Delta(s,t) \} & s \in Q\text{, and}\ t \neq \varepsilon\ \text{or}\ s \not \in F \\ \{ \Delta(s,t), r_0 \} & s \in F, t = \varepsilon\\ \{ \Gamma(s,t) \} & s \in R\\
    \emptyset & \text{otherwise} \end{cases} \]
    %
    We quickly verify that if $s \in L(M)$ and $w \in L(N)$, then $sw \in L(O)$. If $v \in L(O)$, there is some substring $k$ such that $r_0 \in \Pi(q_0, k)$ (for this is the only path from $q_0$ to $G$). If we choose $k$ to be the shortest such string, then $k$ must also be an accept string in $L(M)$, since any substring of $k$ cannot map to states in $R$, and thus $k$ must map via the $\varepsilon$ transition from an accept state of $M$. If $v = kr$, then $\Pi(r_0, r)$ is an accept state in $N$, so $r \in L(N)$. Thus $v \in L(M) \circ L(N)$.
\end{proof}

\begin{theorem}
    If $A$ is a regular language, $A^*$ is regular.
\end{theorem}
\begin{proof}
    Let $M = (Q, \Sigma, \Delta, q_0, F)$ be a deterministic language which accepts $A$. Form a non-deterministic automata $N = (Q \cup \{ i \}, \Sigma, \Gamma, i, F)$, where
    %
    \[ \Gamma(q,s) = \begin{cases} \{ \Delta(q,s) \} & s \neq \varepsilon \\ \{ q_0 \} & s = \varepsilon, q = i \\ \{ i \} & s = \varepsilon, q \in F \\ \emptyset & \text{otherwise} \end{cases} \]
    %
    Then $L(N) = A^*$, for if $s = a_1 \dots a_n$, with $a_i \in A$, then we may go from $i$ to $q_0$, then to an accept state via $a_1$, return to $i$ with a $\varepsilon$ transition, and continue, so $s$ is accepted. If we split a string accepted to $L(N)$ into when $\varepsilon$ transitions to $i$ are used, then we obtain strings in $A$.
\end{proof}

In turns out that the operations of concatenation, union, and `kleene starification' are enough to describe all regular languages. Thus we may take an algebraic approach to understanding regular languages, using the symbology of regular expressions.

\section{Regular Expressions}

Automata are equivalent to a much more classical notion of computation -- regular expressions.

\begin{definition}
    A {\bf regular expression} over an alphabet $\Sigma$ is the smallest subset of $(\Sigma \cup \{ \emptyset, \cup, *, (, ) \})^*$ such that
    %
    \begin{enumerate}
        \item $\emptyset$ is a regular expression, as are $s \in \Sigma^*$.
        \item If $\lambda$ and $\gamma$ are regular expressions, then so are $\lambda^*, (\lambda \cup \gamma)$, and $(\lambda \circ \gamma)$.
    \end{enumerate}
\end{definition}
%
Every regular expression describes a language. A string $s \in \Sigma^*$ is recognized by an regular expression $\lambda$ if
%
\begin{enumerate}
    \item $\lambda \in \Sigma^*$, and $s = \lambda$.
    \item $\lambda = (\lambda \cup \gamma)$, and $s$ is recognized by $\lambda$ or by $\gamma$.
    \item $\lambda = \gamma \circ \delta$, and $s = wl$, where $w$ is recognized by $\gamma$, and $\delta$ recognizes $l$.
    \item $\lambda = \gamma^*$, and $s = \varepsilon$ or $s = s_1 \dots s_n$, where $s_i$ is recognized by $\gamma$.
\end{enumerate}
%
The regular language corresponding to a regular expression $\lambda$ is $L(\lambda)$, the set of strings recognized by $t$. The set of all regular expressions on an alphabet $\Sigma$ will be denoted $R(\Sigma)$.

It is a simple consequence of our discourse that every language recognized by a regular expression {\it is actually} a regular language. In fact, we can show that every regular language is described by a regular expression. We shall describe an algorithm for converting a finite state automaton to a regular expression. We shall make a temporary generalization, by allowing non deterministic finite automata to have regular expressions in their transition functions. A {\bf generalized non-deterministic finite state automaton} is a 5-tuple $M = (Q,\Sigma, \Delta, q_0, f_0)$, where $\Delta: Q - \{ f_0 \} \times Q - \{ q_0 \} \to R(\Sigma)$ is the generalized transition function, and $q_0$ is the start state, $f_0$ is the end state. A generalized automaton accepts  $s \in \Sigma^*$ if we may write $s = s_1 \dots s_n$, and there are a sequence of states $q_0, \dots, q_n$ where $q_n = f_0$, and $\Delta(q_i, q_{i+1})$ recognizes $s_i$.

\begin{theorem}
    Any generalized finite-state automaton describes the language of a regular expression.
\end{theorem}
\begin{proof}
    If the generalized automaton has two states, then there is only one transition from start state to begin state, and this transition describes a regular expression for the automaton. We will reduce every automaton to this form by induction. Suppose an automaton has $n$ states. Fix a state $q \in Q$, which is neither the beginning or accepting state. Define a new automaton
    %
    \[ N = (Q - \{ q \}, \Sigma, \Delta', q_0, f_0) \]
    %
    where $\Delta'(a,b) = (\Delta(a,b) \cup \Delta(a,q) \Delta(q,q)^* \Delta(q,b))$
    %
    Then $N$ is equivalent to $M$, and has one fewer state, and is thus equivalent to a regular expression.
\end{proof}

\begin{corollary}
    Every regular language is described by a regular expression.
\end{corollary}
\begin{proof}
    Clearly, every DFA and NFA is equivalent to a generalized NFA, for by adding new states, we may ensure no states map back to the start state, and that there is only one end state.
\end{proof}


\section{Limitations of Finite Automata}

We've discovered a menagerie of different problems we can solve with finite automata, but it has already been foreshadowed that better machines await. Here we discover methods which attack languages, showing that they cannot be recognized by regular expressions, not finite automata.

\begin{theorem}[Pumping Lemma]
    Let $L$ be a regular language. Then there is a number $p$, called the pumping length, such that any $s \in L$ which satisfies $|s| \geq p$, then we may write $s = wuv$, where $|u| > 0$, $|wu| \leq p$, and $wu^iv \in L$ for all $i \geq 0$.
\end{theorem}
\begin{proof}
    Let $L$ be a regular language, and $M$ a deterministic automata recognizing $L$ with $p$ states. Let $s$ be a string with $|s| \geq p$, with $s \in L(M)$. Write $s = s_1 \dots s_n$ and let $q_k = \Delta(q_0, s_1, \dots, s_k)$. Then we obtain $|s| + 1$ states $q_0, q_1, \dots, q_{|s|}$. By the pidgeonhole principle, since $q_i$ equals some $q_j$, for $i < j$. Let $w = s_1 \dots s_i$, $u = s_{i+1} \dots s_j$, $v = s_{j+1} \dots s_{|s|}$. Then $\Delta(\Delta(q_0, w), u) = \Delta(q_0, w)$, so
    %
    \[ \Delta(q_0, wu^iv) = \Delta(q_0, wuv) \]
    %
    So $wu^iv \in L$ for all $i$.
\end{proof}

\begin{example}
    $L = \{ 0^k10^k : k \geq 0 \}$ is not regular. If it was regular, it would have a pumping length $p$. Since $0^p10^p$ is in $L$, so we may write $0^p10^p = wuv$, where $|wu| \leq p$, and $wu^iv \in L$. Then $u = 0^k$, for $k > 0$, and $wv = 0^{p-k}10^p \in L$, which is clearly not in the language, contradicting regularity.
\end{example}

\begin{example}
    $L = \{ 1^{n^2} : n \in \mathbf{N} \}$ is not regular. Suppose we had a pumping length $p$. Then $1^{p^2} \in L$, so there is $0 < k \leq p$ with $1^{p^2 + k} \in L$. But
    %
    \[ (p + 1)^2 = p^2 + 2p + 1 > p^2 + k \]]
    %
    And there is no perfect square between $p^2$ and $(p+1)^2$, a contradiction.
\end{example}

\begin{example}
    The language $L = \{ 0^i1^j : i > j  \}$ is not regular. If we had a pumping lenght $p$, then $0^p1^{p-1} \in L$. But then there is $0 < k \leq p$ such that $0^{p + (i - 1)k}1^{p-1} \in L$ for all natural numbers $i$. In particular, for $i = 0$, we find $0^{p - k}1^{p-1} \in L$ But $p - k \leq p -1$, a contradiction.
\end{example}

There is a much more mathematically elegant and complete way of separating regular languages from non-regular ones, discovered by John Myhill and Anil Nerode. Consider an alphabet $\Sigma$, and a particular language $L \subset \Sigma^*$. Call two strings $a$ and $b$ in $\Sigma^*$ $L$-indistinguishable if $az \in L$ if and only if $bz \in L$ for any $z \in \Sigma^*$. This forms an equivalence relation on $\Sigma^*$. We shall define the index of $L$, denoted $\text{Ind}\ L$, to be the cardinality of the partition.

\begin{theorem}[Myhill-Nerode]
    $L$ is regular if and only if $\text{Ind}\ L < \infty$, and $\text{Ind}\ L$ is the number of states of the smallest finite state machine to recognize $L$.
\end{theorem}
\begin{proof}
    Let $M$ be a deterministic finite state machine recognizing $L$ with $n$ states, transition function $\Delta$, and start state $q_0$. Let $a_1, \dots, a_{n+1}$ be $n + 1$ strings in $\Sigma^*$. We claim that at least one pair is indistinguishable. if we take $q_i = \Delta(q_0, a_i)$, then some $q_i = q_j$. These two strings are then indistinguishable in $L$. Conversely, suppose that $\text{Ind}\ L < \infty$. Let $A_1, \dots, A_n$ be the equivalence classes of $\Sigma^*$. If $s \in A_i$ is contained in $L$, then every other $w \in A_i$ is in $L$, for otherwise $s$ and $w$ can be distinguished. Build a finite state machine whose states are $A_i$, whose start state is $[\varepsilon]$, and whose transition function is
    %
    \[ \Delta([s], t) = [st] \]
    %
    $[s]$ is accepted if and only if $s \in L$. This finite state machine recognizes $s$.
\end{proof}

In some circumstances, the Myhill Nerode theorem is very powerful.

\begin{example}
    For $\Sigma = \{ a, b, \dots, z \}$, consider the set $L$ of words $w$ whose last letter has not appeared before. For example, the words ``apple'', ``google'', ``k'', and $\varepsilon$ are in $L$, but the words ``potato'' and ``nutrition'' are not. Is this language regular? We apply Myhill Nerode. If the letters in one word are different to the letters in another word, these words are distinguishable. If the letters in one word are the same as the letters in another word, and both are not accepted or both are not accepted, these words are indistinguishable. Thus the index of the language in consideration is the same as the number of different subsets of the set of letters in a word, counted twice for repeated and non-repeated characters. Subtracting one from the fact that $\varepsilon$ need only be counted once, we find that $2 \cdot 2^{26} - 1 = 2^{27} - 1$. Since this is finite, the language is regular, and this is the minimal number of sets in a finite state machine recognizing the language.
\end{example}

\section{Function representation}

Decision problems are sufficient to model a large variety of computational models, but for completeness, we should at least mention how we determine whether problems with multiple outputs can be. The counterpart of a finite-state automata is a finite-state transducer. A {\bf finite state transducer} is a 5 tuple $(Q, \Sigma, \Lambda, \Delta, \Gamma, q_0)$, where $Q$ is the set of states, $\Sigma$ is the input alphabet, $\Lambda$ is the output alphabet, $\Delta: Q \times \Sigma \to Q$ is the transition function, $\Gamma: Q \times \Sigma \to \Lambda_\varepsilon$ is the transduction function, and $q_0$ is the start state. Each finite-state transducer $M$ gives rise to a function $f_M: \Sigma^* \to \Lambda^*$, defined by
%
\[ f_M(\varepsilon) = \varepsilon\ \ \ \ \ f_M(st) = f_M(s) \circ \Gamma(\Delta(q_0,s), t) \]
%
A {\bf regular function} is one computable by a finite state transducer.

\begin{example}
    Consider an alphabet $\mathbf{F}_2$ and consider the function $f$ taking and returning strings over $\mathbf{F}_2$, inverting the strings on even positions, and leaving the strings on the odd positions. Then $f$ is a regular function, for it may be computed by the automata below.
\end{example}










\chapter{Context Free Languages}

Finite state machines are useful on a specific set of problems, but have limitations. We would like our notion of computability to decide on a much more complicated set of problems. Thus we must define new classes of computable languages.

\section{Context Free Grammars}

\begin{definition}
    A {\bf Context Free Grammar} is $(V,\Sigma,R,S)$, where $V$ is a set of variables, $\Sigma$ is a character set, disjoint from $V$, $R$ is a relation between $V$ and $(V \cup \Sigma)^*$, and $S \in V$ is the start variable. We call elements of $R$ derivation rules, and write $(a,s) \in R$ as $a \rightarrow s$.
\end{definition}

A string of the form $uvw$ is {\bf directly derivable} from $uAw$ if $A \rightarrow v$ is a derivation rule. The smallest transitive relation containing the `directly derivable' relation is the derivability relation. To reiterate, a string $u$ is {\bf derivable} from $w$ if there is a sequence of direct derivations
%
\[ w \rightarrow s_0 \rightarrow \dots \rightarrow s_n \rightarrow u \]
%
Such a sequence is known as a {\bf derivation}. The language of a grammar is the set of all strings in $\Sigma^*$ derivable from the start state $S$. As in finite automata, the language of a grammar $G$ will be denoted $L(G)$. A language is {\bf context-free} if it is the language of a context free grammar.

\begin{lemma}
    Let $G = (V,\Sigma,R,S)$ and $H = (V,\Sigma,R',S)$ be two different context free languages. If every $(v,s) \in R'$ is a derivation in $G$, then $L(H) \subset L(G)$.
\end{lemma}
\begin{proof}
    If we can show that every direct derivation in $H$ is a derivation in $G$, then all derivations in $H$ are derivations in $G$, since derivations form the smallest transitive relation containing the direct derivations in $H$, and the derivations in $G$ certainly satisfy this. If $w$ is directly derived from $s$ in $H$, then there is a rule $(B,u)$, $s = rBt$, and $w = rut$. There is a sequence $s_0 \dots s_n$ deriving $u$ from $B$ in $G$. But then $(rs_0t) \dots (rs_nt)$ is a derivation of $w$ from $s$, so $w$ is derivable from $s$ in $G$.
\end{proof}

A leftmost derivation is a derivation which always swaps out the leftmost variable. A string is {\bf ambiguous} if it has two different leftmost derivations. A grammar is ambiguous if its language contains an ambiguous string. Ambiguity is unfortunate when parsing a language, since it means we may be able to interpret elements of the language in two different ways.

\begin{example}
    First order logic can be defined as an unambiguous grammar. To develop the language, we took a bottom up approach, but a top up approach can also be taken. We must take a finite alphabet to develop the language, which we take to be
    %
    \[ \{ (, ), \forall, \exists, \neg, \wedge, \vee, \Rightarrow, x, f, P, 0, 1, \dots, 9 \} \]
    %
    we cannot take an `infinite number of variables', in the sense of an infinite number of symbols, for then formal language theory does not apply. We must instead assume our variables, predicates, and functions are themselves words in a finite alphabet. For instance, we will enumerate our variables
    %
    \[ \Lambda = \{ x, x_0, x_{00}, \dots, x_{0000000000}, \dots \} \]
    %
    The trick to forming functions and predicates is to put $n$ ones after an $f$ or a $P$ to denote that is is an $n$-ary function, so
    %
    \[ \mathcal{F}^n = \{ \left.f^{111 \dots 11}\right., \left.f^{111\dots11}\right._0, \dots, \left.f^{111 \dots 11}\right._{000000000}, \dots \} \]
    %
    \[ \mathcal{P}^n = \{ P^{1111\dots11}, \dots \} \]
    %
    Then we may form predicate logic as a context free grammar. The variables are easiest to form
    %
    \[ X \rightarrow {x}\ |\ {X_0} \]
    %
    Terms are tricky to define because we must form functions as well. The trick is to introduce new variables $Y$ and $Z$ which add enough terms to the function.
    %
    \begin{align*}
        T &\rightarrow {X}\ |\ {f^1U)}\\
        U &\rightarrow {V(T}\ |\ {^1U, T}\\
        V &\rightarrow \varepsilon\ |\ V_0
    \end{align*}
    %
    Finally, we form the formulas of the calculus. Again, the only trick part are the atomic formulae
    %
    \begin{align*}
        F &\rightarrow {(F \wedge F)}\ |\ {(F \vee F)}\ |\ {(\neg F)}\ |\ {(F \Rightarrow F)}\ |\ {(\forall x: F)}\ |\ {(\exists x: F)}\ |\ {P^1Z)}\\
        Y &\rightarrow {Z(T}\ |\ {^1Y, T}\\
        Z &\rightarrow \varepsilon\ |\ Z_0
    \end{align*}
    %
    We showed the formulas are derived unambiguously, but this took a lot of hard work. It is impossible to find a general procedure to decide whether a language is ambiguous, which is what makes verifying ambiguity so difficult.
\end{example}

It is useful to put grammars in a simple form for advanced theorems. A grammar $(V,\Sigma,R,S)$ is in {\bf Chomsky Normal Form} if the only relations in $R$ are of the form
%
\[ S \rightarrow \varepsilon\ \ \ \ \ A \rightarrow BC\ \ \ \ \ A \rightarrow a \]
%
where $A,B,C \in V$ and $B,C \neq S$, and $a \in \Sigma$.

\begin{theorem}
    Every context-free language can be recognized by a context-free grammar in Chomsky normal form.
\end{theorem}
\begin{proof}
    We shall reduce any context free grammar $G = (V,\Sigma,R,S)$ to a context free grammar in normal form in a systematic fashion, adding each restriction one at a time.

    \begin{enumerate}
        \item {\bf No derivation rules map to the start variable}:\\
        Create a new start variable mapping onto the old start variable.

        \item {\bf There are no $\varepsilon$-transition rules except from the start variable}:\\
        Define a variable $A \in V$ to be nullable if we may derive $\varepsilon$ from $A$. Let $W$ be the set of all nullable variables. Define a new language $G'(V,\Sigma,R',S)$ such that, if $A \rightarrow A_1 \dots A_n$ is a derivation rule in $G$, and $A_{i_1}, \dots, A_{i_m}$ are nullable, then we add $2^m$ new rules to $G'$ by removing some subset of the $A_{i_k}$. Then $L(G') = L(G) - \{ \varepsilon \}$, so that if $\varepsilon \in L(G)$, we need only add an $\varepsilon$ rule to $S$ to make the two languages equal.

        We will prove that if $A$ is a variable in $G'$, then $A$ can derive $w \in \Sigma^*$ in $G'$ if and only if it can derive it in $G$ and $w \neq \varepsilon$. One way is trivial, the other a proof by induction on the length of the derivation. Suppose we have a derivation in $G$
        %
        \[ A \rightarrow s_0 \rightarrow \dots \rightarrow s_n \rightarrow w \]
        %
        Let $s_0 = A_1 \dots A_n$. Then each $A_i$ derives $w_i$ in $G$, where $w = w_1 \dots w_n$. We can choose such a derivation to be shorter than the derivation of $G$. But this implies that $A_i$ derives $w_i$ in $G'$, provided $w_i \neq \varepsilon$. Let $w_{i_1} \dots w_{i_m} \neq \varepsilon$. Then $m \neq 0$, since $w \neq \varepsilon$. We have a corresponding production rule $A \rightarrow A_{i_1} \dots A_{i_n}$ in $G'$, since the other variables are nullable. Thus, by induction, $A$ can derive $w$.

        \item {\bf There are no derivation rules $A \rightarrow B$, where $B$ is a variable}:\\
        Call $B$ unitarily derivable from $A$ if there are a sequence of derivation rules
        %
        \[ A \rightarrow V_1 \rightarrow \dots \rightarrow V_n \rightarrow B \]
        %
        Define a new grammar $G' = (V,\Sigma,R',S)$. If $B$ is directly derivable from $A$, and $B$ has a derivation rule $B \rightarrow s$, then $G'$ has a production rule $A \rightarrow s$, provided that $s$ is not a variable. Then $G'$ has no rules of the form $A \rightarrow B$, and generates the same language. This is fairly clear, and left to the reader to prove.

        \item {\bf Every rule is of the form $A \rightarrow AB$ or $A \rightarrow a$}:\\
        If we have a rule in $G$ of the form $A \rightarrow s$, where $s = s_1 \dots s_n$, and $s_{i_1}, \dots, s_{i_m} \in \Sigma$, then add new unique variables $A_{s_{i_k}}$ for each $s_i \in \Sigma$, and replace the rule with new rules of the form
        %
        \[ A \rightarrow s_1 \dots A_{s_{i_1}} \dots A_{s_{i_m}} \dots s_n \]
        %
        \[ A_{s_{i_m}} \rightarrow s_{i_m} \]
        %
        Thus we may assume every rule of the form $A \rightarrow A_1 \dots A_n$ (where we may assume $n \geq 2$) only maps to new variables. But then we may add new variables $V_2 \dots V_{n-1}$, and swap this rule with rules of the form
        %
        \[ A \rightarrow V_{n-1} A_n \]
        %
        \[ V_k \rightarrow V_{k-1} A_k \]
        %
        \[ V_2 \rightarrow A_1 A_2 \]
        %
    \end{enumerate}
    %
    Now every derivation rule is in the correct form, and we have reduced every grammar to Chomsky normal form.
\end{proof}

Chomsky normal form allows us to prove a CFG pumping lemma.

\begin{theorem}
    If $L$ is a context free language, then there is $p > 0$, such that if $s \in L$, and $|s| \geq p$, then we may write $s = uvwxy$, where $|vwx| \leq p$, $v,x \neq \varepsilon$, and for all $i \geq 0$, $uv^iwx^iy \in L$.
\end{theorem}
\begin{proof}
    Let $A$ be the language of the grammar $G$, which we assume to be in Chomsky normal form. Let there be $v$ variables in $G$. If the parse tree of $s \in A$ has height $k$, then $|s| \leq 2^{k-1}$, which follows because the tree branches in two except at roots, so there is at most $2^{k-1}$ roots. If $|s| \geq 2^{3v}$, then every parse tree of $s$ has height greater than $v$. Pick a particular parse tree of smallest size. There is a sequence of variables $A_1, A_2, \dots, A_{3v}$, such that $A_i$ is the parent of $A_{i+1}$. Because of how many variables there are, some variable must occur at least 3 times (for otherwise we may remove the variables in pairs, to conclude that $3v - 2v = v$ variables contain no variables, a contradiction). What's more, they much occur within a height of $3v$ of each other. Let $A_i = A_j = A_k$, for $i < j < k$. Let $A_i$ produce $a A_j b$, let $A_j$ produce $x A_k y$, and let $A_k$ produce $r$. Write $s = m a x r y b n$. By virtue of the minimality of the tree, we may assume that $a$ or $b$ is nonempty, and one of $x$ or $y$ is nonempty. First, if both $ax$ and $yb$ are assumed non-empty, then we may pump these strings up, and have proved our lemma. So suppose $ax$ is empty. Then $b$ and $y$ are nonempty, and $s = mrybn$. Since $A_i$ produces $A_j b$, and $A_j$ produces $A_k y$, $A_i$ may be pumped to produce $A_j b^i$, $A_j$ may be pumped to produce $A_k y^i$, and $mry^ib^in$ is in the context free language, so we have non-empty strings to pump. The proof is simlar if $by$ is empty, for then $a$ and $x$ are non-empty. The constraint $|vxy| \leq k$ is satisfies for $yb$ and $ax$, for the $A_i$ and $A_j$ lie within $3v$ of each, so the string produces in this production is at most as long as $2^{3v}$, which is less than or equal to the pumping length.
\end{proof}

\section{Pushdown Automata}

Regular languages have representations as the languages of regular expressions or as finite automata. Context-free languages also have dual representations, as `machines' or as abstraction operations. It is good to represent a language as a machine for it may hint as to what hardware capabilities a computer must have to be able to solve problems related to the languages. The key machine component for a context-free language is a stack. A pushdown automata is a finite state automata with the addition of a stack.

\begin{definition}
    A {\bf (non-deterministic) pushdown automata} is a tuple $(Q, \Sigma, \Lambda, \Delta, q_0, F)$, where $Q$ is a finite set of states, $\Sigma$ is an alphabet, $\Lambda$ is the stack alphabet, $\Delta: Q \times \Sigma_\varepsilon \times \Lambda_\varepsilon \to \mathcal{P}(Q \times \Lambda_\varepsilon)$ is the state transition function, $q_0 \in Q$ is the start state, and $F \subset Q$ are the accept states.
\end{definition}

It turns out that deterministic pushdown automata are less powerful than non-deterministic automata, so we do not discuss deterministic automata. It is interesting to note that the languages of deterministic automata are connected to unambiguous grammars, though we will not have time to discuss this further.

Let us describe a pushdown automata intuitively. The automata has a stack of symbols from $\Lambda$, which it can push and pull from when deciding how to move through the machine. A stack is a string in $\Lambda^*$. Thus, formally, a string $s$ is {\bf accepted} by a push-down automata $M$ if there are a sequence of states $q_0, \dots, q_n$, and stacks $w_0, \dots, w_n \in \Lambda^*$ such that $q_n \in F$, $w_0 = \varepsilon$, and if we write $w_i = w \lambda$, with $\lambda \in \Lambda_\varepsilon$, then $w_{i+1} = w_i \lambda'$, with $(q_{i+1}, \lambda') \in \Delta(q_i,\lambda)$. Thus we pop and pull off the rightmost character in the string when moving between states.

Pushdown automata have enough versatile memory to recognize context free languages. The stack can `remember' variables it has yet to parse, and check when symbols are used. We shall allow a mild generalization of pushdown automata, which can push multiple symbols to the stack at a time. This is fine, without loss of generality, because we could have instead introduced new states that take nothing from the stack, and push the symbols on one at a time. We shall also assume a pushdown automata starts with a $\$$ symbol at the bottom of its stack, which is fine, because we could have added another start state to the automata which pushes the $\$$ on as we begin running the machine.

\begin{theorem}
    Every context free language is accepted by a pushdown automata.
\end{theorem}
\begin{proof}
    Consider a context free language $(V, \Sigma, R, S)$. Consider a pushdown automata $(Q,\Sigma, \Lambda,\Delta,q_0,F)$, with the stack language $\Lambda = \Sigma \cup V$, and $Q$ just two states $q_0$ and $f_0$. For each derivation $A \rightarrow s \in R$ we have
    %
    \[ (q_0, s) \in \Delta(q_0, \varepsilon, A) \]
    %
    And for each $a \in \Sigma$, we have
    %
    \[ (q_0, \varepsilon) \in \Delta(q_0, a, a) \]
    %
    And a finale transition
    %
    \[ (f_0, \varepsilon) \in \Delta(q_0, \varepsilon, \$) \]
    %
    It is clear that this automata parses the context free language.
\end{proof}

A converse also holds, so that pushdown automata are equivalent computers of context free languages. To do this, we assume that the automata pushes everything off its stack before it finishes, and has a single accept state $f_0$. In addition, we shall assume that a state only pops and pulls in one action, and doesn't do both at the same time. Adding additional states means this is no loss of generality.

\begin{theorem}
    Each pushdown automata language is context free.
\end{theorem}
\begin{proof}
    The gist of our approach is as follows. Let $(Q, \Sigma, \Gamma, \delta, q_0, \{ f_0 \})$ be a pushdown automata. We shall define a context grammar with variables $A_{pq}$, with $p,q \in Q$. This variable should be able to generate all possible strings which can start in $p$ with an empty stack, and end up in $q$ with an empty stack. Our start variable will then be $A_{q_0, f_0}$. The first rules are most basic
    %
    \[ A_{pp} \rightarrow \varepsilon \]
    %
    Such a path may end up empty halfway through the path, so we have these rules, for each $p,q,r \in Q$,
    %
    \[ A_{pq} = A_{pr} A_{rq} \]
    %
    We can also pop something on the stack, and save it for a long time later. If $t \in \Sigma$, and $(r, t) \in \Delta(p, a, \varepsilon)$, and $(q, \varepsilon) \in \Delta(s, b, t)$, then we add the derivation rule
    %
    \[ A_{pq} = a A_{r,s} b \]
    %
    We claim these rules describe all possible derivations we could make in the pushdown automata. It is clear that all such derivations in this context language are accepted in the pushdown automata.

    We shall prove that if we can move from $p$ to $q$ using a string $x$, both with an empty stack, then $A_{pq} \rightarrow x$. This is done by induction on the number of steps to accept the string in the automata. If we do this in one step, then the string is empty, and we have a rule $(q, \varepsilon) \in \Delta(p, \varepsilon)$, or the string consists of a single letter, and we have a rule $(q, \varepsilon) \in \Delta(p, t)$. In the first case, we have a derivation $A_{p, q} \rightarrow \varepsilon$, and in the second, we have a derivation
    %
    \[ A_{p,q} \rightarrow t A_{q,q} \varepsilon \rightarrow t \varepsilon \varepsilon = t \]
    %
    Now consider a machine that runs for a length $n$. Suppose the stack empties at some state $k$, after running through $x_1$ of the string, for $x = x_1 x_2$. Then we have, by induction, a derivation
    %
    \[ A_{pq} \rightarrow A_{pk} A_{kq} \rightarrow x_1 x_2 \]
    %
    Thus we may assume that the stack never empties except at beginning and end. Then the first action must be to push a symbol $t$ to the stack, and the last action to remove $t$. If we move from $p$ to $r$ in the first action by reading $a$, and from $s$ to $q$ in the last action by reading $b$, then we may write $x = acb$, and by induction, we have the derivation
    %
    \[ A_{pq} \rightarrow a A_{rs} b \rightarrow acb = x  \]
    %
    Thus we have verified the equivalence of the pushdown automata and context free language.
\end{proof}

Pushdown automata are easy to connect to their finite state cousins.

\begin{corollary}
    Every regular language is context free.
\end{corollary}



\chapter{Turing Machines and Uncomputability}

Finite state machines are good at modelling machines with a small amount of memory, and pushdown automata are good at modelling stack processes. Nonetheless, there are many problems that `should be computable', since we can solve them which these automata cannot solve. In this chapter, we introduce a much more robust model, the {\bf Turing machine}, which satisfies this criteria. Every known algorithm can be implemented in this model.

The basic idea is that a Turing machine runs off of an infinite tape, which the machine can scan through, swerving left and right to read off the tape. The tape begins with the input
%
\[ q_0 x_1 x_2 \dots x_n \]
%
A notation which implies that the machine is in state $q_0$, and the tape head is looking at $x_1$, and the tape consists of the letters $x_1, x_2 \dots, x_n$, and then the rest of the tape consists of blank spaces.

The Turing machine has finitely many states, which describe how the machine reacts when it sees a current character -- it chooses to swap the current character with a different character, and moving either left or right one space. We may also choose to accept the string or reject the string at any time.

Formally, a turing machine can be described as a tuple
%
\[ (Q, \Sigma, \Gamma, \Delta, q_0, q_{\text{accept}}, q_{\text{reject}}) \]
%
where $Q$ is a set of states, $\Sigma$ is an input alphabet, $\Gamma$ is a tape alphabet (which we assume includes the blank space $-$), $q_0 \in Q$ is the start state, $q_{\text{accept}} \in Q$ is the accept state, and $q_{\text{reject}} \in Q$ is the reject state, and $\Delta: Q \times \Gamma \to Q \times \Gamma \times \{ L , R \}$. We interpret this as given a state and a currently read letter, we move to a new state, we replace the letter with a new character, and we move left or right.

To compute the operation of a Turing machine on a string, we recursively define a computation on a string. A specific state of the machine will be represented by a string $sqw$, where $q \in Q$, $s,w \in \Gamma^*$. We say that $sqtw$ {\bf yields} $suq'w$ if $(q', u, R) = \Delta(q,t)$.

\chapter{Complexity Theory}

We have now discovered what it means to solve a problem algorithmically. We formalize the problem into a certain language over an alphabet, then find a Turing machine over that alphabet which terminates on every input, and accepts exactly theres strings that are an element of a language. But just because a problem is solvable, does not mean that it is {\it feasibly solvable}; there may be an algorithm which will eventually solve a problem, but if it takes centuries to find the answer, the solution is not effective. In this section, we restrict our attention to the computable problems, and describe the degree of difficulty of certain problems. First, we must find a measure of a problem's difficulty, so that we can classify problems based on how difficult they are.

\section{Measuring Complexity}

Let $M$ be a turing machine over an alphabet $\Sigma$, which halts on all inputs. Then, for each string $s \in \Sigma^*$, $M$ deterministically executes a certain number of steps, eventually terminating. Define $t_M(s) \in \mathbf{N}$ to be the number of configurations $M$ steps through before it halts on input $s$. The {\bf time complexity} of $M$ is the function
%
\[ f_M(n) = \max \{ t_M(s): s \in \Sigma^*, |s| = n \} \]
%
It could be argued that a better notion of complexity is the average running time over inputs of a certain length, but we rarely know how often certain inputs will occur. Often, in practice, slow inputs occur much more often that fast inputs, so worst time analysis better reflects an algorithm's speed. Furthermore, worst time analysis gives us an alegant and useful theory which gives meaningful results, which without this simplification would be impossible. We should not be dogmatic in this approach, because some algorithms do benefit from an average case analysis (for instance, the ellipsoid algorithm solving the simplex problem), but the approach has turned out to be the most useful.

We rarely specify a turing machine exactly, and thus have difficulty expressing $f_M$ to an exact number. Even if we described a machine exactly, $f_M$ likely will not have a simple formula specifying an algorithms speed. Thus we apply the theory of asymptotics. Recall that a real-valued function $f$, the `big O' set $O(f)$ consists of all functions $g$ for which $|g| \leq c|f|$ {\it eventually} holds, where $c$ is some positive constant. Similarily, the `little o' set $o(f)$ consist of all functions $g$ satisfying the sharper relation
%
\[ \lim_{x \to \infty} \left|\frac{g(x)}{f(x)}\right| = 0 \]
%
Less important to our needs, but canonically include are the $\Omega(f)$ and $\omega(f)$ sets, which consist of functions $g$ such that $f \in O(g)$ and $f \in o(g)$ respectively. We define $\Theta(f) = \Omega(f) \cap O(f)$, the class of functions which are asymptotically equal to $f$.

\begin{example}
    Let $P \in \mathbf{R}[X]$ be a polynomial of degree $m$. For $x \geq 1$, and $i \leq j$, the inequality $x^i \leq x^j$ holds, so if $P$ is expressed in coefficients as
    %
    \[ P = \sum c_i X^i \]
    %
    then for $x \geq 1$,
    %
    \[ |P(x)| \leq \sum |c_i| x^i \leq \left( \sum |c_i| \right) x^m \]
    %
    and we have shown that $P \in O(x^m)$.
\end{example}

\begin{example}
    Since $O(f)$ is a subset of a space of functions, they can be manipulated under real-valued operations. As an example, we consider the set $2^{O(\log n)}$, which consts of all functions of the form $2^f$, where $f \in O(\log n)$. We contend this is just the set of all functions $g$ in $O(n^c)$ for some $c$. First note that if
    %
    \[ f(x) \leq c \log x \]
    %
    then because the exponential function is monotone, we obtain that
    %
    \[ 2^{f(x)} \leq 2^{c \log x} = x^c \]
    %
    so if the first inequality eventually holds, the second inequality eventually holds. Conversely, suppose $f \in O(x^c)$. Suppose that eventually
    %
    \[ f(x) \leq K x^c \]
    %
    Then if $x \geq e$,
    %
    \[ \log_2 f(x) \leq \log_2 K + c \log_2 x \leq \frac{c + 2 \log_2 K}{\log 2} \log x \]
    %
    which implies $\log_2 f \in O(\log n)$, and $f = 2^{\log_2 f} \in 2^{O(\log n)}$.
\end{example}

\begin{example}
    A commonly used relation is that $O(f) + O(g) = O(f + g)$. Let $h$ and $k$ be functions with a constant $K$ and $n_0$, such that
    %
    \[ h \leq Kf\ \ \ \ \ k \leq Kg \]
    %
    eventually holds. Then
    %
    \[ h + k \leq K(f + g)\]
    %
    eventually holds as well, so $h + k \in O(f + g)$. Conversely, suppose $h \in O(f + g)$, and let
    %
    \[ h \leq K(f + g) \]
    %
    Define
    %
    \[ h_1 = \min(h,Kf)\ \ \ \ \ h_2 = h - h_1 \]
    %
    Then $h_1 + h_2 = h$. It is easy to see $h_1 \in O(f)$. Now if $n \geq n_0$, then
    %
    \[ h(n) - h_1(n) \leq Kf + Kg - h_1(n) \leq Kg \]
    %
    Thus $h_2 \in O(g)$, and we have verified the equality. Similar arguments show
    %
    \[ O(fg) = O(f)O(g) \]
\end{example}

Certainly, a machine $M$ with $f_M(n) = n^2$ runs much faster than a machine $N$ with $f_N(n) = 2^8 n^2$, and certainly a machine $L$ with $f_L(n) = 2^n$ runs faster than $f_N$ for the first 20 input sizes, but regardless of the constant, the exponential machine will quickly become much slower even if we choose a much larger constant ($2^{100} n^2$ becomes smaller than $2^n$ only after the input size increases to 125). What's more, we have a theorem that, by constructing a new machine, we can always decrease the constant in an algorithm, with at most a linear increase in time.

\begin{theorem}
    For any two-tape machine $M$, and $\varepsilon > 0$, there is an equivalent two-tape machine $N$ such that $f_N$ is eventually less than $\varepsilon f_M + (1 + \varepsilon) n$.
\end{theorem}
\begin{proof}
    Consider the following strategy. Let $M$ have tape alphabet $\Sigma$ and tape alphabet $\Sigma \cup \Sigma^n$. Construct $N$ with tape alphabet $\Sigma \cup \Sigma^m$, and with states $Q \times \Sigma^m \cup Q'$. Our machine takes the first $m$ characters $w_1, \dots, w_m$ in input, and places $(w_1, w_2, \dots, w_m) \in \Sigma^m$ on the second tape. This takes $m$ steps. If the input does not contain $m$ strings, we assume that we write in whitespace instead. Continue this until we reach whitespace. After $m \lceil n/m \rceil$ steps, we have a condensed input. After this preprocessing, we can execute $m$ steps of the original machine in 6 steps of our algorithm. Our state in $N$ will now be of the form $(q,t)$, where $q$ is a state in $M$, and $t$ is the current position in the tuple $(w_1, \dots, w_m)$ we are pointing at in the second string that the simulated machine should be at. We move left once, and then right twice, in order to see the states to the left and right of our current states. Given this information, we can predict whether we will end up on the left or right after $m$ steps of the simulated machines, and what the other states will look like. Only states in the current tuple, and the left or right tuple (but not both) will be changed. Based on our computation, we move left or right, changing the states we are currently in, and, if needed, move again to change the states in our new tuple. Thus in six steps, we have simulated $m$ steps. If $M$ takes $k$ steps to halt on some input, $N$ takes $m \lceil n/m \rceil + \lceil k/m \rceil$. For $n$ large enough,
    %
    \[ m \lceil n/m \rceil \leq (1 + \varepsilon) n \]
    %
    which implies that eventually
    %
    \[ f_N \leq \lceil f_M/m \rceil + m \lceil n/m \rceil \]
    %
    If we choose $m$ large enough, we obtain the inequality desired.
\end{proof}

If we only use one tape, then the increase becomes $\varepsilon f_M + 2 n^2 + 2$. We also note that programs which run in linear time can only improved to $(1 + \varepsilon) n$, for some $\varepsilon$. This makes sense, for if some program runs in time bounded by $cn$ for $c < 1$, then for large inputs the program must eventually not even look at all the input, which is unfeasible in most problems.

Since we do not describe Turing machines formally, we rely on heuristic requirements to determine upper bounds for certain algorithms. Our intuition, guided by our knowledge of formal Turing machines, should carry us through, provided we describe algorithms in enough detail that the underlying Turing machine can be seen in enough detail.

We now have the formality to classify problems by how long it takes to compute them. Given a function $g$, define $\mathbf{TIME}(g)$ to be the class of all languages $L$ which are recognized by a machine $M$, and $f_M \in O(g)$. Thus $\mathbf{TIME}(g)$ consists of all problems which can be computed `roughly in time with $g$'. This is the first example of a {\bf complexity class}, a set of languages characterized by how slow it takes to decide upon the language.

\begin{example}
    Consider the language $A = \{ 0^k1^k : k \geq 0 \}$. Determine a turing machine deciding the language from the algorithm
    %
    \begin{enumerate}
        \item Scan across input, checking whether the input is of the form $0^i 1^j$ for some $i,j$. We ensure the string only contains 0s and 1s, and that the string contains no 0s after it contains 1s. Reject in any other circumstance. Afterwords, return to the beginning of the tape.
        \item While there are still unmarked 0s and 1s on the tape, tick off a new 0 on the tape and a new 1. If there are no 0s but still 1s, or 0s but no 1s, reject the input.
        \item We have ticked off one 0 for each 1, so there are the same number of zeroes as ones. Accept the input.
    \end{enumerate}
    %
    We analyze the three steps individually, putting the asymptotics together once we're done. Step 1 takes roughly a constant number of steps for each element of the input, for we perform the same operation on each character. Thus the time to compute step 1 is in $O(n)$. The number of times step 2 is executed is at most half the characters in the input, and each iteration is in $O(n)$, for the iteration involes moving from the beginning to the end of the tape a finite number of times. Thus step 2 is in
    %
    \[ O(n/2)O(n) = O(n)^2 = O(n^2) \]
    %
    Finally, step 3 is a constant time operation, and is therefore in $O(1)$. Thus the entire algorithm is in
    %
    \[ O(n) + O(n^2) + O(1) = O(n^2 + 2n + 1) = O(n^2) \]
    %
    Thus $A \in \mathbf{TIME}(n^2)$. A divide and conquer variation of this algorithm shows that $A \in \mathbf{TIME}(n \log n)$, left as an exercise.
\end{example}

\section{Models of Complexity}

We have considered various variants of Turing machines. All turn out to decide the same class of languages, hence the adoption of the machine as an applicable model of real world computation. But we run into issues when applying this argument to complexity theory, for using a different model of turing machine may reduce the time complexity of an algorithm.

\begin{example}
    Consider the $0^k1^k$ problem. The best algorithm we found ran in $O(n \log n)$ time. But consider a 2-tape Turing machine, which first shifts all 1s in the input to a 2nd tape, then procedurally checks off 0s and 1s. This runs in $O(n)$ time, for we need only scan over the tape a constant number of times. Later, we shall show that the asymptotics of the $0^k1^k$ problem cannot be improved on a single tape machine, so multi-tape turing machines are asymptotically faster than one tape machines.
\end{example}

We may take comfort in discovering that changing the model does not {\it drastically} affect the computation time of an algorithm. This is what this section sets out to address.

\begin{theorem}
    If a multi-tape turing machine has time complexity $f$, where $f \geq n$, then the multi-tape turing machine has an equivalent single-tape turing machine with time complexity in $O(f^2)$.
\end{theorem}
\begin{proof}
    We have already described a procedure which simulates a $k$-tape turing machine. We shall compute an asymptotic analysis of this simulation. Suppose the multitape turing machine takes $g(n)$ steps before terminating on a particular input of size $n$. Let us analyze each step
    %
    \begin{enumerate}
        \item The algorithm first takes the input $w_1 \dots w_n$, and manipulates it into the form
        %
        \[ \# \dot{w_1} \dots w_n \# \dot{\visspace} \# \dot{\visspace} \# \dot{\visspace} \# \dots \# \dot{\visspace} \]
        %
        where we have $k$ hashtags. This takes $O(n + k)$ steps.

        \item For each of the $g(n)$ steps the multitape machine takes, we must pass through our simulation tape, recognizing the current state. We then perform a second pass to move dots left or right when needed. In each step, we add at most $k$ new elements to our tape, so the size of the tape is always bounded by $n + k g(n)$. Therefore the number of steps in the first pass is in $O(n + kg(n))$. The second pass moves through the tape, and pushes at most $k$ new symbols into the tape, otherwise just moving the dots in the tape back and forth. A push moves at most $n + kg(n)$ symbols to the right, and thus the number of steps we take is in $O(k(n + kg(n)))$.
    \end{enumerate}
    %
    Step 2 is applied $g(n)$ times, so overall, the speed of the algorithm is in
    %
    \begin{align*}
        O(n + k) + O(n + kg(n)) + g(n) O(n + kg(n)) = O(ng(n) + g^2(n))
    \end{align*}
    %
    assuming that $n \in O(g)$, then the speed is in $O(g^2)$.
\end{proof}

Thus, though multitape machines may compute faster, they only introduce do things quadratically faster than single tape machines. One of the biggest issues with complexity theory is that this is not true of non-deterministic machines.

First off, we must debate how long a non-deterministic machine takes to compute. We cannot simply count all steps the non-deterministic machine takes, because the machine takes many branches, some of which may be infinite. Since we can see non-deterministic computation as some sort of parallel processing, we could define the time to be the length of the shortest branch of processesing which yields termination. Mathematically, however, we will see that it is more convenient to define the run time to be the length of the longest branch. We are then able to define the time complexity of a non-deterministic automata.

\begin{theorem}
    If a non-deterministic machine runs in $O(f)$ time, then there is an equivalent deterministic automata which runs in $2^{O(f)}$ time.
\end{theorem}
\begin{proof}
    Perform a time analysis on the turing machine which simulates a non-deterministic machine.
\end{proof}

Thus non-deterministic algorithms are fundamentally connected to exponential deterministic algorithms. This makes sense, for in general, if a problem can be divided into exponentially many cases to check, each verifiable in a linear amount of time, then a non-deterministic algorithm can split into each possible case, exponentially dividing, and then check each case in a polynomial amount of time, giving us a polynomial time algorithm. Thus the time of Non-deterministic turing machines is bounded by the time it takes to verify a single case.

Now we get to the fun stuff. Define the complexity class
%
\[ \textbf{P} = \bigcup_{k = 0}^\infty \textbf{TIME}(n^k) \]
%
these are all problems computable in polynomial time on a deterministic automata.

\end{document}

\section{Multivalued Logics and Independence}

The most basic assumption of boolean logic is that there are two truth values, $\bot$ and $\top$. Everything is either true or false. But what about the statement ``God exists''? Until the skys open up, it is impossible to determine whether this statement is true or false. If we model the statement with boolean values, we must decide whether the statement is true and false. It is the principle of three-valued logic, invented by Jan \L ukasiewicz, who decided that some statements have a third `truth value', which indicates that the classical truth of the statement is unknown. After this introduction, there is nothing stopping us from considering a system with even more `truth values', which evaluate statements based on the degree of truth, representing false, possible, justifiable, likely, and so on.

In classical logic, proof systems are built to `preserve truth'. All axioms are tautologies, and anything deduced from tautologies are tautologies. How then, are we to evaluate proof systems for multi-valued logics? When creating proof systems for multi-valued logics, we hope the value of a statement which follows from other statements is less than or equal to the value of the original statement. We shouldn't be able to prove some `more true' than what we know. For instance, consider Sorites paradox
%
\begin{quote}
    (1) ``A grain of sand is not a heap of sand''.\\
    (2) ``Adding a grain of sand to something which is not a heap does not make it into a heap''\\
    (3) ``A grain cannot be turned into a heap by adding grains.''
\end{quote}
%
We fix this argument by considering it in a multi-valued logic system. For each $k$, consider the statements
%
\begin{quote}
    (1.$k$) ``k grains of sand do not make a heap''.\\
    (2.$k$) ``Adding a grain to $k$ grains of sand doesn't make $k+1$ grains a heap''\\
    (1.$k+1$) ``$k+1$ grains do not make a heap.''
\end{quote}
%
we are only allowed to conclude $(1.k+1)$ from $(1.k)$ and $(2.k)$ when $(1.k+1)$ is `less true' than $(1.k)$ and $(2.k)$. As $k \to \infty$, if we continue the argument, then we find the statement eventually becomes completely false, which is true, for eventually a certain number of grains do become a heap. This book is not intended to talk about non-standard logics in detail. Instead, we use the notion of multivalued logic to attack the notion of independance. Are the axioms of our Hilbert system minimal -- do some of the axioms apply some of the other axioms? The trick to finding whether an axiom is independent is to find a truth system in which the other axioms are suitable, but the removed axiom is not. It follows that any statements which follows from the axioms are not removed.

Let us start by discarding (A1) from our axiom system. We may only derive results from (A2), (A3), and (MP). Can we derive (A1) using these rules? We shall try and set a value system on this proof system which satisfies the decreasing property of proofs. We shall consider `truth assignments' based on the table
%
\begin{center}
    \begin{tabular}{| c | c | c | c | }
        \hline $x$ & $y$ & $H_\Rightarrow(x,y)$ & $H_\neg(x)$ \\
        \hline $0$ & $0$ & $0$ & $1$ \\
        $0$ & $1$ & $2$ & $1$ \\
        $0$ & $2$ & $2$ & $1$ \\
        $1$ & $0$ & $2$ & $1$ \\
        $1$ & $1$ & $2$ & $1$ \\
        $1$ & $2$ & $0$ & $1$ \\
        $2$ & $0$ & $0$ & $0$ \\
        $2$ & $1$ & $0$ & $0$ \\
        $2$ & $2$ & $0$ & $0$ \\
        \hline
    \end{tabular}
\end{center}
%
With this definition, we may consider truth values of formulae. Note that $x \Rightarrow (y \Rightarrow x)$ evaluates to 2 when $x$ is 1 and $y$ is 2. We shall show any provable formula from (A2) and (A3) always takes the value zero. One calculates that this is true of any instance of (A2) and (A3). Furthermore, if $s$ and $s \Rightarrow w$ is always zero, then $w$ is always zero. But this shows that $x \Rightarrow (y \Rightarrow x)$ can never be proven using (A2) and (A3).

To prove that (A2) is independent of (A1) and (A3), take the truth table
%
\begin{center}
    \begin{tabular}{| c | c | c | c | }
        \hline $x$ & $y$ & $H_\Rightarrow(x,y)$ & $H_\neg(x)$ \\
        \hline $0$ & $0$ & $0$ & $1$ \\
        $0$ & $1$ & $2$ & $1$ \\
        $0$ & $2$ & $1$ & $1$ \\
        $1$ & $0$ & $0$ & $0$ \\
        $1$ & $1$ & $2$ & $0$ \\
        $1$ & $2$ & $0$ & $0$ \\
        $2$ & $0$ & $0$ & $1$ \\
        $2$ & $1$ & $0$ & $1$ \\
        $2$ & $2$ & $0$ & $1$ \\
        \hline
    \end{tabular}
\end{center}
%
Then everything proved from( (A1) and (A3) takes the value zero always, yet (A2) takes the value $2$ when $s$ is 0, $w$ is 0, and $u$ is 1.

We prove the independence of (A3) using a different trick, related to the removal of (A3) in intuitionistic logic. If $s$ is a statement, consider $h(s)$, which is defined by
%
\[ h(x) = x\ \ \ \ \ h((x \Rightarrow y)) = (h(x) \Rightarrow h(y))\ \ \ \ \ h((\neg x)) = x \]
%
Then $h$ removes all negations signs from $s$. If $s$ is an instance of (A1) or (A2), then $s$ is a tautology, and $h(s)$ is a tautology. Furthermore, if $h(w \Rightarrow u)$, and $h(w)$ are tautologies, then $h(w \Rightarrow u) = h(w) \Rightarrow h(u)$ is a tautology. Yet
%
\[ h((\neg s \Rightarrow \neg w) \Rightarrow ((\neg s \Rightarrow w) \Rightarrow s)) = (s \Rightarrow w) \Rightarrow((s \Rightarrow w) \Rightarrow w) \]
%
which is not a tautology, and is thus an instance of (A3) which cannot be proved by (A1) and (A2).