\input{../style.tex}

\title{The Representation Theory of Lie Algebras}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}

\maketitle

\tableofcontents

\pagenumbering{arabic}

\chapter{Basic Definitions}

If $K$ is a field, then a Lie algebra over $K$ is a $K$ vector-space $\mathfrak{g}$ equipped with an alternating, bilinear form $[\cdot, \cdot]$, known as the {\bf Lie bracket}, satisfying the Jacobi identity
%
\[ [X,[Y,Z]] + [Y,[Z,X]] + [Z,[X,Y]] = 0 \]
%
for any $X,Y,Z \in \mathfrak{g}$. The majority of Lie algebras emerge in geometry and some parts of number theory (normally where $K = \mathbf{R}$ or $K = \mathbf{C}$), where a Lie group $G$ gives rise to a natural Lie algebra structure on the space $\mathfrak{g}$ of tangent vectors at the origin (There is a natural theory of `Lie groups' for fields other than the real and complex numbers, though we won't discuss them here). We obtain the Lie algebra by differentiating the operation on the group, so we can think of a Lie algebra as an infinitisimal linear approximation to the action of the group. Indeed, most algebraic concepts about Lie groups have `infinitisimal formulations' on Lie algebras. Because of the correspondence between $G$ and $\mathfrak{g}$, we normally denote a Lie algebra corresponding to a Lie group by the `frakturized' name of the group.

\begin{example}
    If we differentiate the Lie group $GL_n(K)$ of invertible matrices with coefficients in $K$, we obtain our first example of a Lie algebra. The tangent space at the origin can be identified with the space of all matrices, since $GL_n(K)$ is an open submanifold of $M_n(K)$, and with some work we show that the Lie bracket takes the form $[X,Y] = XY - YX$. In general, the set of invertible endomorphisms $GL(V)$ on a finite dimensional vector space forms a Lie group, the Lie algebra can be identified with the set of all endomorphisms $T: V \to V$, and the Lie bracket takes the form $[T,S] = T \circ S - S \circ T$.
\end{example}

\begin{example}
    In general, if $A$ is any associative algebra over $K$, then the commutator $[X,Y] = XY - YX$ gives a Lie algebra structure on $A$, denoted $\mathfrak{a}$. From the perspective of differential geometry, if $A$ is an arbitrary finite-dimensional algebra, then $A$ has the structure of a differential manifold since it is a vector space, and $U(A)$ is an open neighbourhood of the origin, hence a Lie group. The induced Lie algebra structure on $U(A)$ can then be identified with the Lie bracket on $A$ defined by $[X,Y] = XY - YX$. Note, however, that this is no more general than our discussion of the general linear group, because any finite dimensional algebra $A$ can be faithfully represented as endomorphisms $\text{End}(A)$ over the vector space structure of $A$ by the map $\rho: A \to \text{End}(A)$, $\rho(X)(Y) = XY$, and this essentially identifies $U(A)$ as a subalgebra of $\mathfrak{gl}(V)$.
\end{example}

\begin{example}
    If $\mathfrak{g}$ is any vector space, then the trivial bracket $[X,Y] = 0$ gives $\mathfrak{g}$ a Lie algebra structure. $\mathfrak{g}$ is known as a commutative Lie algebra, because then $[X,Y] = [Y,X]$, and for Lie algebras over fields of characteristic $\neq 2$, this is the only circumstance in which this case occur. If a Lie group $G$ is abelian, than $\mathfrak{g}$ is abelian, and the converse holds provided $G$ is connected.
\end{example}

\begin{example}
    On any field $K$, the space of linear endomorphisms on $K[X]$ forms an associative algebra over $K$ under composition. Of particular interest are the operators
    %
    \[ f(X) \mapsto g(X) f(X) \]
    %
    for $g \in K[X]$, which we shall identify with the polynomial $g$, and the differentiation operators
    %
    \[ f(X) \mapsto f'(X) \]
    %
    which we denote $\partial_X$. If we consider the space of `differential operators with polynomial coefficients', operators which can be written in the form
    %
    \[ \sum_{k = 0}^N g_k(X) \partial_X^k \]
    %
    with $g_k(X) \in K[X]$, for some $N$, then we find these form a subalgebra of the space of endomorphisms, since we have the identity
    %
    \[ (\partial_X X)(f) = X \partial_X(f) + f \partial_X(X) = X \partial_X(f) + f \]
    %
    so that $\partial_X X = X \partial_X + 1$, and we may use this identity to rearrange the product of any two differential operators to coincide with an operator of the form above. As an algebra, the space of differential operators has essentially no more relations. If
    %
    \[ \sum_{k = 0}^N g_k(X) \partial_X^k = 0 \]
    %
    Then successively applying the operator to the monomials $X^m$ gives
    %
    \[ g_0(X) = g_1(X) = \dots = g_N(X) = 0 \]
    %
    provided we are working over a field of characteristic 0, in which case we have an isomorphism of the ring of operators with $K \langle X, Y \rangle / (YX - XY - 1)$. In this form, the algebra is useful in quantum mechanics, where the operators represent certain physical measurements, which are non-commutative. If a field has finite characteristic $p$,  this method only allows us to determine that
    %
    \[ g_0(X) = g_1(X) = \dots = g_{p-1}(X) = 0 \]
    %
    but $\partial_X^p = 0$ in this case, so if we add this additional relation $Y^p = 0$, we obtain another isomorphism with a quotient of $K \langle X, Y \rangle$. In general, the ring of differential operators in $n$ variables is called the $n$'th Weyl algebra. It is obtained from $K\langle X_1, \dots, X_n, Y_1, \dots, Y_n \rangle$ modulo the relations $Y_i X_j - X_j Y_i = \delta_{ij}$ and $Y_i^p = 0$, which makes sense if we view $Y_i$ as the operator which is partial differentiation in the $i$'th variable. The commutator on these algebras gives a particularly interesting Lie algebra structure.
\end{example}

\begin{example}
    If $A$ is an associative algebra over $K$, then $M_n(A)$ is an algebra over $K$, and therefore a Lie algebra, denoted $\mathfrak{gl}_n(A)$. A particularly interesting example occurs if $A = \mathbf{C}[X,X^{-1}]$, in which case we call $\mathfrak{gl}_n(A)$ a Loop Lie algebra. These algebras have applications in various fields of theoretical physics, including String theory.
\end{example}

\begin{example}
    Given a (possibly non associative) algebra $A$, a derivation on $A$ is a linear map $d: A \to A$ satisfying $d(xy) = xd(y) + d(x)y$. Given two derivations $d$ and $d'$, $d \circ d'$ may not be a derivation, but the commutator
    %
    \[ [d_1, d_2] = d_1 \circ d_2 - d_2 \circ d_1 \]
    %
    is always a derivation, because
    %
    \begin{align*}
        (d_1 \circ d_2 - &d_2 \circ d_1)(fg) = d_1(f d_2(g) + d_2(f) g) - d_2(d_1(f) g + f d_1(g))\\
        &= [d_1(f) d_2(g) + f (d_1 \circ d_2)(g) + d_2(f) d_1(g) + (d_1 \circ d_2)(f) g]\\
        &\ - [(d_2 \circ d_1)(f) g + d_1(f) d_2(g) + d_2(f) d_1(g) + f (d_2 \circ d_1)(g)]\\
        &= f(d_1 \circ d_2 - d_2 \circ d_1)(g) - (d_1 \circ d_2 - d_2 \circ d_1)(f) g
    \end{align*}
    %
    Thus the set of derivations on $A$, denoted $\text{der}(A)$, forms a Lie algebra. We should expect the space of derivations to play a fundamental role in the study of Lie algebras, because if $X,Y,Z$ are elements of any Lie algebra, then the Jacobi identity tells us that
    %
    \[ [X,[Y,Z]] = - [Y,[Z,X]] - [Z,[X,Y]] = [Y,[X,Z]] + [[X,Y],Z] \]
    %
    Introducing the {\bf adjoint map} $\text{adj}_X: \mathfrak{g} \to \mathfrak{g}$ defined by $\text{adj}_X(Y) = [X,Y]$, we can restate the equation above as
    %
    \[ \text{adj}_X[Y,Z] = [Y, \text{adj}_X Z] + [\text{adj}_XY, Z] \]
    %
    so that $\text{adj}_X$ is actually a derivation on $\mathfrak{g}$. The map $X \mapsto \text{adj}_X$ is actually a representation of $\mathfrak{g}$ on $\text{der}(\mathfrak{g})$, known as the {\bf adjoint representation} because
    %
    \begin{align*}
        [\text{adj}_X, \text{adj}_Y](Z) &= [\text{adj}_X \text{adj}_Y - \text{adj}_Y \text{adj}_X](Z)\\
        &= [X,[Y,Z]] - [Y,[X,Z]]\\
        &= [X,[Y,Z]] + [Y,[Z,X]]\\
        &= [[X,Y],Z]\\
        &= \text{adj}_{[X,Y]}(Z)
    \end{align*}
    %
    The kernel of the representation being the centre of $\mathfrak{g}$, $Z(\mathfrak{g})$. In general, a bilinear skew-symmetric map is a Lie bracket if and only if the corresponding adjoint maps are all derivations; thus the Jacobi identity is exactly the derivation equation in disguise, harkening back to the definition of vector fields as derivations on the space of $C^\infty$ functions on the Lie group.
\end{example}

\begin{example}
    The formula
    %
    \[ \det(I + tM) = 1 + t\ \text{tr}(M) + o(t^2) \]
    %
    which can be proved by using the Leibnitz formula to expand the determinant out algebraically, and then only taking into the account the first-order terms. The tangent space of $SL_n(K)$ can be identified as a subspace of the tangent bundle on $GL_n(K)$, and since elements of $SL_n(K)$ are defined by the equation $\det(X) = 1$, the set of tangent vectors at the origin are exactly those which annihilate the determinant -- i.e., those matrices $M$ such that
    %
    \[ \left. \frac{\det(I + tM)}{dt} \right|_{t = 0} = 0 \]
    %
    This implies that the Lie algebra $\mathfrak{sl}_n(K)$ consists of the matrices $M \in \mathfrak{gl}_n(K)$ with trace zero. Algebraically, we can verify that $\mathfrak{sl}_n(K)$ is a Lie subalgebra of $\mathfrak{gl}_n(K)$, because it is closed under the Lie bracket. Because $\text{tr}(XY) = \text{tr}(Y,X)$,
    %
    \[ \text{tr}(XY - YX) = \text{tr}(XY) - \text{tr}(YX) = 0 \]
    %
    Though this follows automatically because $SL_n(K)$ is a subgroup of $GL_n(K)$.
\end{example}

\begin{example}
    The orthogonal group $O_n(K)$ consists of matrices in $GL_n(K)$ satisfying $X^tX = I$. Since
    %
    \[ (I + hX)^t(I + hX) = I + h(X^t + X) + h^2X^tX \]
    %
    $X$ is a tangent vector at the origin for $O_n(K)$ if and only if $X^t = -X$. If $\text{char}\ K \neq 2$, then $X^t = -X$ implies that $X$ vanishes along the diagonal, hence $\mathfrak{o}_n(K) \subset \mathfrak{sl}_n(K)$. This essentially follows because the special orthogonal Lie group $SO_n(K)$ of orthogonal matrices of determinant one forms a connected component of $O_n(K)$, hence $\mathfrak{so}_n(K) = \mathfrak{o}_n(K)$. Algebraically, $\mathfrak{o}_n(K)$ is a subalgebra of $\mathfrak{gl}_n(K)$ because if $X,Y \in \mathfrak{o}_n(K)$, then
    %
    \[ (XY - YX)^t = Y^tX^t - X^tY^t = YX - XY = -(XY - YX) \]
    %
    so the Lie bracket preserves orthogonality.
\end{example}

\begin{example}
    If $n$ is even, $n = 2m$, then matrices $M \in M_n(K)$ can represent operators from $K^n \times K^n$ to $K^n \times K^n$. Consider the canonical symplectic form $\omega$ on $K^n \times K^n$ (a non-degenerate, antisymmetric bilinear form), defined by
    %
    \[ \omega(x_0 + y_0, x_1 + y_1) = \omega(x_0,y_1) + \omega(y_0,x_1) = \sum (x_0^i y_1^i - y_0^i x_0^i) \]
    %
    We let the symplectic group $SP_n(K)$ be the set of operators preserving this symplectic form. In matrix form, if we define the matrix $J \in M_n(K)$ by
    %
    \[ J = \begin{pmatrix} 0 & I_m \\ -I_m & 0 \end{pmatrix} \]
    %
    then $SP_n(K)$ consists exactly of those matrices $M$ such that $M^tJM = J$, or $M^tJMJ^t = I$. If we consider a first order approximation at the origin
    %
    \[ (I + hM)^tJ(I + hM)J^t = I + h(M^t + JMJ^t) + h^2M^tJMJ^t \]
    %
    we see the tangent vectors consist of matrices $M$ such that $M^t = -JMJ^t$, or $M^tJ + JM = 0$. This is the symplectic Lie algebra $\mathfrak{sp}_n(K)$. If we write
    %
    \[ X = \begin{pmatrix} A & B \\ C & D \end{pmatrix} \]
    %
    Then
    %
    \[ X^tJ = \begin{pmatrix} -C^t & A^t \\ -D^t & B^t \end{pmatrix}\ \ \ \ \ JX = \begin{pmatrix} C & D \\ -A & -B \end{pmatrix} \]
    %
    Implying that $M$ is in $\mathfrak{sp}_n(K)$ if and only if $C^t = C$, $A^t= -D$, and $B^t = B$.
\end{example}

\begin{example}
    The Heisenberg group $H_n(K)$ is the Lie group of matrices of the form
    %
    \[ \begin{pmatrix} 1 & a & c \\ 0 & I_n & b \\ 0 & 0 & 1 \end{pmatrix} \]
    %
    where $a \in K^n$ is a row vector, $c \in K$, and $b \in K^n$ is a column vector. It's corresponding Lie algebra $\mathfrak{h}_n(K)$ consists of matrices of the form
    %
    \[ \begin{pmatrix} 0 & a & c \\ 0 & 0 & b \\ 0 & 0 & 0 \end{pmatrix} \]
    %
    which is flat, since the Heisenberg group is essentially flat.
\end{example}

One of the main purposes of the abstract study of Lie algebras is that a Lie group's Lie algebra gives most of the required information about the structure of the Lie group. If we can characterize the structure of all Lie algebras, we are well on our way to understanding the structure of Lie groups. The standard way of breaking apart some algebraic structure is, like in most algebraic categories, by considering subalgebras and quotients. A homomorphism $f$ of Lie algebras is, of course, a linear map preserving the Lie bracket operation. As with many other algebraic objects, a basic way to understand a Lie algebra is to factor it into two simpler algebras $\mathfrak{h}$ and $\mathfrak{k}$ by considering a short exact sequence
%
\[ 0 \to \mathfrak{h} \to \mathfrak{g} \to \mathfrak{k} \to 0 \]
%
Then we can identity $\mathfrak{g}$ with the vector space $\mathfrak{h} + \mathfrak{k}$ equipped with a Lie bracket formed from a `twisted product'
%
\[ [(H_0,K_0), (H_1,K_1)] = ([H_0,H_1], [K_0,K_1] + A(H_0,K_1) - A(H_1,K_0) + B(H_0,H_1)) \]
%
for some bilinear $A,B: \mathfrak{g}^2 \to \mathfrak{g}$. In the nicest case, we will have $A = B = 0$, in which case we call $\mathfrak{g}$ the direct sum of $X$ and $Y$, and denote it by $\mathfrak{h} \oplus \mathfrak{k}$. We have then literally decomposed $\mathfrak{g}$ into more simple Lie algebras.

The standard way to form a short exact sequence for a Lie algebra $\mathfrak{g}$ is to find a (two-sided) ideal $\mathfrak{a}$, which is a subspace of $\mathfrak{g}$ such that $[X,Y]$ and $[Y,X]$ are both in $\mathfrak{a}$ for any $X \in \mathfrak{a}$, $Y \in \mathfrak{g}$. We can then consider the quotient Lie algebra $\mathfrak{g}/\mathfrak{a}$, and we have an exact sequence
%
\[ 0 \to \mathfrak{a} \to \mathfrak{g} \to \mathfrak{g}/\mathfrak{a} \to 0 \]
%
The other way to form an exact sequence is to consider a surjective homomorphism $f: \mathfrak{g} \to \mathfrak{h}$, in which case if we let $\mathfrak{k} = \ker f$, then
%
\[ 0 \to \mathfrak{k} \to \mathfrak{g} \to \mathfrak{h} \to 0 \]
%
is exact. As in most algebraic categories, these two processes are essentially the same by the first isomorphism theorem. In fact, we have a version of all standard isomorphism theorems in the category of Lie algebras.

\begin{theorem}[The First Isomorphism Theorem]
    The kernel of a Lie algebra homomorphism $f: \mathfrak{g} \to \mathfrak{h}$ is an ideal, and the kernel $\mathfrak{a}$ forms an ideal of $\mathfrak{g}$ inducing an injective map $\tilde{f}: \mathfrak{g}/\mathfrak{a} \to \mathfrak{h}$, such that
    %
    \begin{center}
    \begin{tikzcd}
        \mathfrak{g} \arrow{r}{f} \arrow{d} & \mathfrak{h}\\
        \mathfrak{g}/\mathfrak{h} \arrow{ru}[below]{\tilde{f}}
    \end{tikzcd}
    \end{center}
    %
    commutes.
\end{theorem}

\begin{theorem}[The Second Isomorphism Theorem]
    The set of subalgebras of $\mathfrak{h}$ of $\mathfrak{g}/\mathfrak{a}$ is one to one with the class of subalgebras of $\mathfrak{g}$ containing $\mathfrak{a}$, and the correspondence maps ideals to ideals, where the subalgebra corresponding to $\mathfrak{a} \subset \mathfrak{h}$ is denoted $\mathfrak{h}/\mathfrak{a}$. If $\mathfrak{h}$ is an ideal, then $(\mathfrak{g}/\mathfrak{a})/(\mathfrak{b}/\mathfrak{a})$ is isomorphic to $\mathfrak{g}/\mathfrak{b}$.
\end{theorem}

\begin{theorem}[The Third Isomorphism Theorem]
    If $\mathfrak{a} \subset \mathfrak{b}$ are ideals of $\mathfrak{g}$, then $\mathfrak{a} + \mathfrak{b}$ and $\mathfrak{a} \cap \mathfrak{b}$ are ideals of $\mathfrak{g}$, and $(\mathfrak{a} + \mathfrak{b})/\mathfrak{a}$ is isomorphic to $\mathfrak{b}/(\mathfrak{a} \cap \mathfrak{b})$.
\end{theorem}

\begin{example}
    Consider the homomorphism $f: \mathfrak{gl}_n(K) \to K$ obtained by taking the trace of the matrix, which is a homomorphism because
    %
    \[ \text{tr}\ [x,y] = \text{tr}(xy - yx) = 0 = [\text{tr}(x), \text{tr}(y)] \]
    %
    The kernel is the special linear group, which is therefore an ideal of the general linear group. Thus we have an exact sequence
    %
    \[ 0 \to \mathfrak{sl}_n(K) \to \mathfrak{gl}_n(K) \to K \to 0 \]
    %
    Hence the vector space structure of $\mathfrak{gl}_n(K)$ is the direct sum of $\mathfrak{sl}_n(K)$ and $K \cdot I$, and since $K \subset Z(K)$, the direct sum agrees with the Lie algebra operations as well.
\end{example}





\section{Solvable Lie Algebras}

On Lie groups $G$, we can consider brackets of normal subgroups $H$,
%
\[ [H_0,H_1] = \langle h_0h_1h_0^{-1}h_1^{-1} : h_0 \in H_0, h_1 \in H_1 \rangle \]
%
and this subgroup of $G$ will also be normal. Of particular interest in the derived subgroup $[G,G]$, and when we take the quotient we have an abelian group $G_{text{ab}} := G/[G,G]$. On Lie algebras, we can take brackets of ideals to form new ideals
%
\[ [\mathfrak{a}, \mathfrak{b}] = \text{span} \{ [X,Y] : X \in \mathfrak{a}, \mathfrak{b} \} \]
%
which is essentially the infinitisimal version of the product of two groups. Analogous to the commutator subgroups of a group is the derived subalgebra $\mathfrak{g}' = D\mathfrak{g} = [\mathfrak{g}, \mathfrak{g}]$, which is the smallest ideal such that $\mathfrak{g}/\mathfrak{g}'$ is trivial (we call this the {\it abelianization} of $\mathfrak{g}$, denoted $\mathfrak{g}_{\text{ab}}$). Since we have the exact diagram
%
\[ 0 \to \mathfrak{g}' \to \mathfrak{g} \to \mathfrak{g}_{\text{ab}} \to 0 \]
%
we can write $\mathfrak{g} = \mathfrak{g}_{\text{ab}} + \mathfrak{g}'$, where
%
\[ [(X_0,Y_0),(X_1,Y_1)] = ([X_0,X_1], A(X_0,Y_1) - A(X_1,Y_0) + B(X_0,X_1)) \]
%
for some Bilinear $A$ and $B$. Thus we can think of the elements of $\mathfrak{g}'$ as infinitisimals, since they have no impact on the Lie bracket structure of $\mathfrak{g}_{\text{ab}}$, existing somewhat `beneath the surface' of the calculations. If we consider $\mathfrak{g}'' = (\mathfrak{g}')'$, then we can compute the abelian approximation `to a second order', writing $\mathfrak{g} = \mathfrak{g}_{\text{ab}} + \mathfrak{g}'_{\text{ab}} + \mathfrak{g}''$. Continuing this process, we hope to write $\mathfrak{g}$ as the successive product of abelian infinitisimals, $\mathfrak{g} = \mathfrak{g}^{(1)}_{\text{ab}} + \mathfrak{g}^{(2)}_{\text{ab}} + \dots + \mathfrak{g}^{(n)}_{\text{ab}}$, where $\mathfrak{g}^{(n)}$ is the $n$'th element of the {\bf derived series}. For this to work, we require that $\mathfrak{g}^{(n+1)} = 0$ for some $n$, in which case we say $\mathfrak{g}$ is a {\bf solvable} Lie algebra. This is equivalent to the existence of an abelian tower, and by refinement, a cyclic tower as well. Solvability on Lie algebras is a tower property, in the sense that if $\mathfrak{g}/\mathfrak{h}$ and $\mathfrak{h}$ are solvable, then $\mathfrak{g}$ is also solvable. It is natural to consider the {\bf lower central series}
%
\[ \mathfrak{g}_1 = \mathfrak{g}\ \ \  \mathfrak{g}_2 = [\mathfrak{g}, \mathfrak{g}]\ \ \ \mathfrak{g}_3 = [\mathfrak{g}, [\mathfrak{g}, \mathfrak{g}]]\ \ \  \dots \]
%
where the derived series is slightly finer, $\mathfrak{g}^{(n)} \subset \mathfrak{g}_n$, yet the infinitisimals have the additional property that if $X \in \mathfrak{g}$ and $Y \in \mathfrak{g}_n$, then $[X,Y] \in \mathfrak{g}_{n+1}$ (for the derived series, we can only guarantee that $[X,Y] \in \mathfrak{g}^{(1)}$ if $X \in \mathfrak{g}$). If the lower central series eventually terminates, we call the algebra {\bf nilpotent}. We then have a decomposition
%
\[ \mathfrak{g} = (\mathfrak{g}_1/\mathfrak{g}_2) \times (\mathfrak{g}_2/\mathfrak{g}_3) \times \dots \times (\mathfrak{g}_n/\mathfrak{g}_{n+1}) \]
%
Of course, this is a very strong condition for a Lie algebra to have. An important example of a nilpotent Lie algebra is the set of strictly upper triangular matrices.

As a corollary to this, we see that any Lie algebra $\mathfrak{g}$ has a maximum solvable ideal. If $\mathfrak{a}$ is a {\it maximal} solvable ideal, and $\mathfrak{b}$ is any other solvable ideal, then $\mathfrak{a} + \mathfrak{b}$ is solvable, hence $\mathfrak{a} + \mathfrak{b} = \mathfrak{a}$, and so $\mathfrak{b} \subset \mathfrak{a}$. We shall denote the maximum solvable ideal of a Lie algebra by $\text{rad}(\mathfrak{g})$, and call it the {\bf radical} of the algebra. The radical essentially separates the approximately commutative section of the algebra from the non-commutative section. The most non-commuatative Lie algebras are the {\bf semi-simple} ones, such that $\text{rad}(\mathfrak{g}) = (0)$. For any algebra $\mathfrak{g}$, $\mathfrak{g}/\text{rad}(\mathfrak{g})$ is semisimple, so the radical efficiently extracts the commutative section of the algebra. It is an essential tool in the description of the finite dimensional Lie algebras, because it means we can write any Lie algebra as the product of a semisimple algebra and a solvable one.

The problem with using nilpotency to verify properties of a group is that it is not as amenable to algebraic manipulations than solvability. Just because $\mathfrak{a}$ and $\mathfrak{g}/\mathfrak{a}$ is nilpotent, this does not imply that $\mathfrak{g}$ is nilpotent. Thus we can't prove properties as easily by induction on the size of towers. Thus solvability is more natural. In conclusion, we have broken down study of an arbitrary Lie algebra $\mathfrak{g}$ into the study of a solvable Lie algebra $\text{rad}(\mathfrak{g})$, and a semisimple algebra $\mathfrak{g}/\text{rad}(\mathfrak{g})$. Most of the study of the classification of Lie algebras concerns itself with understanding the solvable and semisimple Lie algebras.

A {\bf simple} Lie algebra is a non-abelian Lie algebra $\mathfrak{g}$ having no ideals other than the trivial ideals $(0)$ and $\mathfrak{g}$. We shall soon prove that semi-simple Lie algebras break down into the direct sum of simple Lie algebras, so that we need only study the simple Lie algebras to understand the semisimple ones. As we've stated before, the classification theorem for simple Lie algebras over the complex numbers gives a very simple set of families to understand, $\mathfrak{sl}_n$, $\mathfrak{so}_n$, $\mathfrak{sp}_n$, and some `eccentric' algebras $\mathfrak{e}_6$, $\mathfrak{e}_7$, $\mathfrak{e}_8$, $\mathfrak{f}_4$, and $\mathfrak{g}_2$. This will require us to know some very sophisticated theorems of representation theory.






\chapter{Matrix Lie Algebras}

An important class of Lie algebras are those which occur as subalgebras of $\mathfrak{gl}(V)$, for some vector space $V$. This is not only an important class of examples of Lie algebras, but provides a suitable training ground for the understanding of the representation theory of Lie algebras -- the idea being that we can completely characterize a Lie algebra by it's actions on vector spaces. This theory will become a powerful tool to classify all the Lie algebras. For now, we assume our Lie algebras occur as matrix subalgebras of $\mathfrak{gl}(V)$, but we note that our theory will soon be used to talk about general actions of Lie algebras on vector spaces.

Eigenvectors and eigenvalues are incredibly important to the classification of linear operators over a finite dimensional vector space. They also play an important part in understanding the action of a Lie algebra on a vector space. Given a Lie algebra $\mathfrak{g}$ acting on a vector space $V$, we define an eigenvector of $\mathfrak{g}$ to be a vector $v \in V$ such that $Xv$ is a scalar multiple of $v$ for each $X \in \mathfrak{g}$. The scalar multiple may differ depending on the $X$ we choose. For instance, the eigenvectors for the subalgebra of diagonal matrices in $\mathfrak{gl}_n(K)$ consist of exactly the basis vectors $e_i$. The eigenvectors essentially allow us to simultaneously diagonalize all the operators in a subalgebra at once. Dual to the concept of eigenvectors are a sort of `generalized eigenvalue'. If $v$ is an eigenvector for $\mathfrak{g}$, then we may define a function $\lambda: \mathfrak{g} \to K$ by $Xv = \lambda(X) v$. $\lambda$ is a linear map, because
%
\[ \lambda(X + Y)v = (X + Y)v = Xv + Yv = [\lambda(X) + \lambda(Y)]v \]
%
We call such a map a {\bf weight} for $\mathfrak{g}$. In general, for a linear functional $\lambda \in \mathfrak{g}^*$, we define $V_\lambda$ to be the set of vectors $v$ such that $Xv = \lambda(X)v$ holds for all $X \in \mathfrak{g}$. A weight is precisely a linear functional for which $V_\lambda$ is non-trivial.

\begin{example}
    As we have shown, if $\mathfrak{g}$ is the subalgebra of diagonal matrices in $\mathfrak{gl}_n(K)$, then the eigenvectors consist of the basis vectors $e_i$, and the corresponding weights are precisely the weights $\varepsilon_i(X) = X_{ii}$, because $Xe_i = X_{ii} e_i$. If the eigenvectors of a matrix subalgebra $\mathfrak{g}$ over $V$ can form a basis of $V$, then $\mathfrak{g}$ is isomorphic to the set of diagonal matrices over $V$.
\end{example}

\begin{example}
    The vector $e_1$ is an eigenvector for the subalgebra of upper triangular matrices, and the corresponding weight is $\varepsilon(X) = X_{11}$. The corresponding weight space $V_\varepsilon$ is just the span of $e_1$, because if $v$ is any vector such that $Xv = X_{11}v$, then $v_1 \neq 0$, for otherwise $Xv$ could not depend on $X_{11}$. If $v_i \neq 0$ for $i = 1$, we could change the $i$'th column of $X$, which would not adjust $X_{11}$, but it would change $Xv$.
\end{example}

The weight spaces have nice invariance properties which, like for the Jordan normal form, allow us to decompose algebras into subalgebras.

\begin{lemma}
    If $\mathfrak{a}$ is an ideal of a subalgebra $\mathfrak{g}$ on $\mathfrak{gl}(V)$, the space of vectors $v \in V$ such that $Xv = 0$ for all $X \in \mathfrak{a}$ is $\mathfrak{g}$ invariant.
\end{lemma}
\begin{proof}
    If $Xv = 0$ for all $X \in \mathfrak{a}$, then for any $Y \in \mathfrak{g}$, $[X,Y]v = 0$, hence
    %
    \[ XYv = YXv = Y0 = 0 \]
    %
    hence we have verified $\mathfrak{g}$-invariance.
\end{proof}

We can generalize this result to the case where $v$ is an eigenvector, and the eigenvalue is non-zero. This is very important to the decomposition results of Lie algebras we will prove in the future.

\begin{theorem}
    Over a field $K$ of characteristic zero, let $\mathfrak{g}$ be a subalgebra of $\mathfrak{gl}(V)$ with ideal $\mathfrak{a}$. Then the weight space of any $\lambda: \mathfrak{a} \to K$ is $\mathfrak{g}$ invariant.
\end{theorem}
\begin{proof}
    If $Xv = \lambda(X)v$ for all $X \in \mathfrak{a}$, then we must show $XYv = \lambda(X)Yv$ for any $Y \in \mathfrak{g}$. Note that
    %
    \[ XYv = YXv + [X,Y]v = \lambda(X) Yv + \left( \lambda[X,Y] \right) v \]
    %
    Thus it suffices to verify that $\lambda[X,Y] = 0$. Consider the span of $\{ v, Yv, \dots, Y^nv \}$. We claim that for any $Z \in \mathfrak{a}$,
    %
    \[ ZY^n v = \lambda(Z) (Y^n v) + \text{span}(v,Yv, \dots, Y^{n-1}v) \]
    %
    For $n = 0$, the claim is assumed. In general, since $[Z,Y] \in \mathfrak{a}$, we have
    %
    \begin{align*}
        ZY^nv &= YZY^{n-1}v + [Z,Y]Y^{n-1}v\\
        &= \lambda(X) Y^n v + [Z,Y]Y^{n-1}v + \text{span}(v,Yv, \dots, Y^{n-2}v)\\
        &= \lambda(X) Y^n v + \text{span}(v,Yv, \dots, Y^{n-1}v)
    \end{align*}
    %
    The matrix representation of $\text{adj}_Z$ with respect to this basis has trace $n\lambda(Z)$. If we let $Z = [X,Y]$, then we find $n\lambda[X,Y] = 0$, hence $\lambda[X,Y] = 0$.
\end{proof}

Here's a simple consequence.

\begin{theorem}
    If $X, Y \in \mathfrak{gl}(V)$ commute with $[X,Y]$, then $[X,Y]$ is nilpotent.
\end{theorem}
\begin{proof}
    Since we are working over the complex numbers, it suffices to show that the only eigenvalue of $[X,Y]$ is 0. Let $\lambda$ be an eigenvalue of $[X,Y]$, and let $\mathfrak{g}$ be the subalgebra of $\mathfrak{gl}(V)$ generated by $X$, $Y$, and $[X,Y]$. Then the span of $[X,Y]$ is a Lie algebra ideal of $\mathfrak{g}$, hence the space $V_\lambda$ of vectors $v$ satisfying $[X,Y]v = \lambda v$ is $\mathfrak{g}$ invariant. This implies that $X$ and $Y$ restricted to operators on $V_\lambda$, and the restriction of $[X,Y]$ to $V_\lambda$ is the same as $XY - YX$, hence $[X,Y]$ has trace zero. But $[X,Y]$ is diagonalizable on $V_\lambda$, with trace $n \lambda$ if $V_\lambda$ is $n$-dimensional, hence $\lambda = 0$.
\end{proof}

\section{Engel's Theorem}

In linear algebra, we form classification theorems for linear operators. The theorems show the existence of certain basis elements on the vector space, such that the corresponding matrix representation of the operators have nice structure. But given a family of linear operators, it is a much more difficult problem to find a basis such that {\it all} the matrix representations of the family of linear operators all have nice structure.

Any nilpotent operator on a vector space has a basis with respect to which the matrix representation is upper triangular. We would like to know which families of nilpotent linear operators can be simultaneously `strictly upper triangularized'. Also natural is to know which families of linear operators can be simultaneously `upper triangularized'. In the case that these families form a Lie subalgebra of operators, then these results essentially constitute Engel's theorem and Lie's theorem.

\begin{lemma}
    If $\mathfrak{g}$ is a Lie subalgebra of nilpotent transformations on $V$, then there is $v \neq 0$ for which $Xv = 0$ for all $X \in \mathfrak{g}$.
\end{lemma}
\begin{proof}
    We proceed by induction on the dimension of $\mathfrak{g}$. If $\mathfrak{g}$ is zero dimensional, the theorem is trivial. If $\mathfrak{g}$ is one-dimensional, spanned by some $X \neq 0$, then there is $n$ such that $X^n \neq 0$, $X^{n+1} = 0$, and if $X^n v \neq 0$, then $X(X^n v) = 0$, so $v$ satisfies the theorem.

    In general, let $\mathfrak{a}$ be a maximal proper Lie subalgebra of $\mathfrak{g}$. For any $X \in \mathfrak{a}$, the adjoint map $\text{adj}_X: \mathfrak{g} \to \mathfrak{g}$ descends to a map from $\varphi_X: \mathfrak{g}/\mathfrak{a} \to \mathfrak{g}/\mathfrak{a}$, because if $Y - Z \in \mathfrak{a}$, then $[X,Y] - [X,Z] = [X,Y-Z] \in \mathfrak{a}$. Thus $\varphi(\mathfrak{a})$ is a subalgebra of $\mathfrak{gl}(V)$, and $\dim \varphi(\mathfrak{a}) < \dim \mathfrak{g}$. To apply the induction hypothesis, we require that $\varphi(X)$ is nilpotent when $X$ is nilpotent, but this follows because $\text{adj}_X$ is nilpotent when $X$ is nilpotent, because we have
    %
    \[ \text{adj}^m_X(Y) = \sum_{k = 0}^m (-1)^k {m \choose k} (X^kYX^{m-k}) \]
    %
    and if $X^n = 0$, then $\text{adj}^m_X(Y) = 0$ when $m \geq 2n$. Thus, by induction, there is $Y + \mathfrak{a} \in \mathfrak{g}/\mathfrak{a}$ with $Y \not \in \mathfrak{a}$ such that $[X,Y] \in \mathfrak{a}$ for all $X \in \mathfrak{a}$. This implies that $\mathfrak{a} + K Y$ is a subalgebra of $\mathfrak{g}$, and by maximality, $\mathfrak{a} + K Y = \mathfrak{g}$. But $\mathfrak{a}$ is an ideal of $\mathfrak{a} + K Y$, hence $\mathfrak{a}$ is an ideal of $\mathfrak{g}$ with codimension 1.

    Now we apply induction again to conclude that the space $W$ of $v \in V$ with $Xv = 0$ for all $X \in \mathfrak{a}$ is non-trivial. Now $W$ is invariant under multiplication by $\mathfrak{g}$, so in particular $YW \subset W$. Since $Y$ is nilpotent, we find a non-zero $w \in W$ with $Yw = 0$, and then $(\mathfrak{a} + KY)w = 0$.
\end{proof}

\begin{theorem}[Engel]
    For any algebra $\mathfrak{g}$ of nilpotent linear transformations on $\mathfrak{gl}(V)$, there is a basis which simultaneously upper triangularizes all elements of the algebra.
\end{theorem}
\begin{proof}
    We adapt the proof strategy in the case of a single transformation. First, we find a vector $v$ such that $Xv = 0$ for all $X \in \mathfrak{g}$. If $W = K v$, then each $X$ descends to a linear endomorphism on $V/W$. The image of $\mathfrak{g}$ under this descending process satisfies the hypothesis of Engel's theorem, and therefore by induction there is a basis $v_1 + W, \dots, v_m + W$ of $V/W$ in which all $X$ are upper triangular. Then $v, v_1, \dots, v_m$ is a basis for $V$, But then
    %
    \[ Xv_i \in \text{span}(v_1, \dots, v_{i-1}) + \text{span}(v) \]
    %
    and so the $X$ are upper triangular in this new space.
\end{proof}

\begin{corollary}
    A Lie algebra $\mathfrak{g}$ is nilpotent iff $\text{adj}_X$ is nilpotent for all $X \in \mathfrak{g}$.
\end{corollary}
\begin{proof}
    A Lie algebra $\mathfrak{g}$ is nilpotent if and only if there is a value $n$ such that
    %
    \[ \text{adj}_{X_1} \circ \text{adj}_{X_2} \dots \circ \text{adj}_{X_n} = 0 \]
    %
    for any $X_i \in \mathfrak{g}$. In particular, $\text{adj}_X^n = 0$ for all $X$. Conversely, if the $\text{adj}_X$ are nilpotent, then Engel's theorem implies the existence of a basis of $X_i$ such that all $\text{adj}$ are strictly upper triangular. But if the Lie algebra is $m$ dimensional, this implies that the nilpotency condition above holds for $n = m$.
\end{proof}

Engel's theorem holds for Lie algebras over arbitrary fields. The analogous theorem for solvable Lie algebras is not so easy to generalize.

\begin{lemma}
    If $\mathfrak{g} \subset \mathfrak{gl}(V)$ is a solvable Lie algebra over the complex numbers, and $V \neq 0$, there there is $v \neq 0$ which is an eigenvector for every element of $\mathfrak{g}$.
\end{lemma}
\begin{proof}
    We prove this theorem by induction on the dimension of $\mathfrak{g}$. If $\mathfrak{g}$ has dimension 1, it suffices to show that every matrix has an eigenvector, and this follows because $\mathbf{C}$ is a complete field. For an arbitrary $\mathfrak{g}$, we note that $\mathfrak{g}'$ is a proper subalgebra of $\mathfrak{g}$, hence we may enlarge it to a subalgebra $\mathfrak{a}$ of $\mathfrak{g}$ of codimension 1, choosing $X$ such that $\mathfrak{g} = \mathfrak{a} + \mathbf{C} X$ and by induction this subalgebra has an eigenvector $v$ with corresponding weight $\lambda: \mathfrak{a} \to \mathbf{C}$. We know that $V_\lambda$ is $\mathfrak{g}$ invariant, so that there is a vector $w \in V_\lambda$ which is an eigenvector of $X$ with eigenvalue $\gamma$. We claim that this implies that $w$ is an eigenvector for all elements of $\mathfrak{g}$, because if we write $Z = \alpha X + \beta Y$, where $Y \in \mathfrak{a}$, then $Zw = (\alpha \gamma + \beta \lambda(Y))w$.
\end{proof}

The work of Lie's theorem is essentially the same techniques as Engel's theorem.

\begin{theorem}
    If $\mathfrak{g} \subset \mathfrak{gl}(V)$ is a solvable Lie algebra, then there is a basis for $V$ such that the matrix representations of $\mathfrak{g}$ are upper triangular.
\end{theorem}
\begin{proof}
    We choose an eigenvector $v$ for $\mathfrak{g}$, and then consider $V/W$ for $W = \mathbf{C}v$. Every element of $\mathfrak{g}$ descends to an operator on $V/W$, and by induction we can write each element as an upper triangular matrix with a certain basis $v_1 + W, \dots, v_n + W$. Passing back up, we find that each element of $\mathfrak{g}$ is upper triangular with respect to the basis $v, v_1, \dots, v_n$.
\end{proof}

If $\mathfrak{g}$ is a complex solvable Lie algebra, which is a subset of some $\mathfrak{gl}(V)$, then Lie tells us there is a basis of $V$ in which every element of $\mathfrak{g}$ is upper triangular. If $A$ and $B$ are upper triangular matrices, then
%
\[ (AB - BA)_{ii} = \sum_{j = 1}^n A_{ij}B_{ji} - B_{ij}A_{ji} = A_{ii}B_{ii} - B_{ii}A_{ii} = 0 \]
%
Hence the matrix representations of $\mathfrak{g}'$ are strictly upper triangular, and therefore nilpotent! Conversely, if $\mathfrak{g}'$ is nilpotent, then $\mathfrak{g}'$ is solvable, and $\mathfrak{g}/\mathfrak{g}'$ is abelian, hence $\mathfrak{g}$ is solvable. Thus for Lie algebras which are concretely represented as subalgebras f $\mathfrak{gl}(V)$, $\mathfrak{g}$ is solvable if and only if $\mathfrak{g}'$ is nilpotent.


\chapter{Representation Theory}

In this chapter, we view abstract Lie algebras as subalgebras of endomorphisms over a finite dimensional vector space. Just as groups can be represented as automorphisms over some category, reflecting their construction as the mathematical representation of symmetries, Lie algebras act on vector spaces, reflecting their construction as left-invariant vector fields on a Lie group. We define a {\bf representation} of a Lie algebra $\mathfrak{g}$ to be a homomorphism $\rho: \mathfrak{g} \to \mathfrak{gl}(V)$. The homomorphism induces an {\bf action} of $\mathfrak{g}$ on $V$, defined by $Xv = \rho(X)(v)$. Conversely, if we have an action $\mathfrak{g} \times V \to V$ of $\mathfrak{g}$ on $V$, satisfying
%
\[ (\lambda X + \gamma Y)v = \lambda Xv + \gamma Yv \]
\[ X(\mu v + \nu w) = \mu Xv + \nu Xw \]
\[ [X,Y] v = X(Yv) - Y(Xv) \]
%
Then this induces a representation of $\mathfrak{g}$ on $\mathfrak{gl}(V)$. A representation is {\bf faithful} if it is injective, and {\bf transitive} if we can map any $v \in V$ to any $w \in V$ by some $X \in \mathfrak{g}$.

\begin{example}
    We have already seen that the adjoint map $\text{ad}: \mathfrak{g} \to \mathbf{gl}(\mathfrak{g})$ is a representation of $\mathfrak{g}$. It occurs very often in the theory. The kernel of the homomorphism is the center $Z(\mathfrak{g})$.
\end{example}

A {\bf submodule} of a $\mathfrak{g}$ module $V$ is a subspace which is closed under multiplication by elements of $\mathfrak{g}$. As examples, the submodules of $\mathfrak{g}$ are exactly the ideals. Lie's theorem says that every module over a complex solvable Lie algebra has a one dimensional submodule. Given a submodule $W$, we can form the factor module $V/W$.

We wish to break modules down into their simpler representations, so that the action of $\mathfrak{g}$ on the modul becomes clear. A module $V$ is called {\bf irreducible}, or {\bf simple}, if it has no submodules other than $(0)$ and $V$ itself. If $V$ is not irreducible, we can find a submodule $W$ of minimal dimension. Then $V/W$ will have an irreducible submodule, and we may proceed inductively. Thus the irreducible submodules essentially form building blocks for all modules. Another reason why irreducible modules are useful is that they are precisely the modules for which the action of the Lie algebra is transitive. The Lie algebras which are irreducible modules over their adjoint representation are exactly the simple ones. The only irreducible modules over a complex solvable algebra are one dimensional.

If we can write a module as the direct sum of other submodules, then we have said to decompose the module. A module is {\bf indecomposable} if it cannot be broken down into a direct sum. A module is {\bf completely reducible} if it can be written as the direct sum of irreducible submodules. This is the ideal situation to be in for understanding the structure of the action.

As can be expected, a module homomorphism between two $\mathfrak{g}$ modules is exactly one preserving the action of the algebra. The three isomorphism theorems continue to hold here. Homomorphisms provide the best way at determining the structure of modules, and it is natural to begin looking at homomorphisms of irreducible modules. Since the image of a module homomorphism is always a submodule of that module, these homomorphisms must either be trivial, or isomorphisms.

\begin{lemma}[Schur]
    If $\mathfrak{g}$ is a complex Lie algebra, and $V$ is a finite dimensional irreducible $\mathfrak{g}$ module, then the only module endomorphisms on $V$ are scalar multiples of the identity.
\end{lemma}
\begin{proof}
    Let $T: V \to V$ be a Lie algebra homomorphism. Then if $T \neq 0$, then $T$ has an eigenvector $v$, satisfying $Tv = \lambda v$. Then $T - \lambda$ is a module homomorphism mapping $v$ to zero, hence $T - \lambda = 0$, so $T = \lambda$.
\end{proof}

It follows that if $X \in Z(\mathfrak{g})$, and $V$ is an irreducible $\mathfrak{g}$ algebra, then there exists $\lambda$ such that $Xv = \lambda v$ for all $v \in V$, because $X$ acts as a module endomorphism. We can also conclude that the only irreducible $\mathbf{C}$ modules are one-dimensional.




\section{Representations of $\mathfrak{sl}_2(\mathbf{C})$}

Many of the ideas which occur in the general representation theory of complex Lie algebras occurs in the theory of representations of $\mathfrak{sl}_2(\mathbf{C})$. What's more, we will find that the representations of this Lie algebra control a large part of the representation theory. Recall the basis $E_{12}$, $E_{11} - E_{22}$, and $E_{21}$ spans the algebra. We begin by constructing a family of irreducible representations.

For each $n$, define $V_n$ to be the subspace of $\mathbf{C}[X,Y]$ generated by the homogenous polynomials with degree $n$
%
\[ X^n, X^{n-1}Y, \dots, XY^{n-1}, Y^n \]
%
$V_n$ is therefore a space of dimension $n+1$. We represent $\mathfrak{sl}_2(\mathbf{C})$ on $\mathfrak{gl}(V_n)$ with a homomorphism $\rho: \mathfrak{sl}_2(\mathbf{C}) \to \mathfrak{gl}(V_n)$ by defining
%
\[ \rho \begin{pmatrix} a & b \\ c & -a \end{pmatrix} = a\left( X\frac{\partial}{\partial X} - Y \frac{\partial}{\partial Y} \right) + b X \frac{\partial}{\partial Y} + c Y \frac{\partial}{\partial X} \]
%
These are well defined operators since they carry degree $n$ monomials into degree $n$ monomials. Note that now $\rho(h)(X^aY^b) = (a - b) X^aY^b$, so the $X^aY^b$ are eigenvectors of $\rho(h)$. By construction, $\rho$ is linear. To verify that brackets are preserved, we need only verify this for the basis. We calculate $[E_{12}, E_{21}] = E_{11} - E_{22}$, and using the relations we calculated for the Weyl algebra, we find
%
\begin{align*}
    [\rho(E_{12}),\rho(E_{21})] &= \left[ X \frac{\partial}{\partial Y}, Y \frac{\partial}{\partial X} \right] = X \frac{\partial}{\partial Y} Y \frac{\partial}{\partial X} - Y \frac{\partial}{\partial X} X \frac{\partial}{\partial Y}\\
    &= \left(1 + Y \frac{\partial}{\partial Y} \right) \left( X \frac{\partial}{\partial X} \right) - \left( 1 + X \frac{\partial}{\partial X} \right) \left( Y \frac{\partial}{\partial Y} \right)\\
    &= X \frac{\partial}{\partial X} - Y \frac{\partial}{\partial Y} = \rho(E_{11} - E_{22})
\end{align*}
%
Next, we calculate $[E_{12}, E_{11} - E_{22}] = -2 E_{12}$, and find
%
\begin{align*}
    [\rho(E_{12}), \rho(E_{11} - E_{22})] &= \left[ X \frac{\partial}{\partial Y}, X \frac{\partial}{\partial X} - Y \frac{\partial}{\partial Y} \right]\\
    &= X^2 \frac{\partial}{\partial X} \frac{\partial}{\partial Y} - X \left(1 + X \frac{\partial}{\partial X} \right) \frac{\partial}{\partial Y}\\
    &- X \left( 1 + Y \frac{\partial}{\partial Y} \right) \frac{\partial}{\partial Y} + X Y \frac{\partial^2}{\partial Y^2}\\
    &= -2 X \frac{\partial}{\partial Y} = -2 \rho(E_{12})
\end{align*}
%
Finally, $[E_{21}, E_{11} - E_{22}] = 2E_{21}$, and we can reduce the calculation of the product to the calculation above by exchanging variables, so
%
\begin{align*}
    [\rho(E_{21}), \rho(E_{11} - E_{22})] &= \left[ Y \frac{\partial}{\partial X}, X \frac{\partial}{\partial X} - Y \frac{\partial}{\partial Y} \right] = - \left[ Y \frac{\partial}{\partial X}, Y \frac{\partial}{\partial Y} - X \frac{\partial}{\partial X} \right]\\
    &= 2 Y \frac{\partial}{\partial X} = 2 \rho(E_{21})
\end{align*}
%
In the basis $X^n,X^{n-1}Y, \dots, Y^n$, we have matrix representations
%
\[ \rho(E_{12}) = \begin{pmatrix} 0 & 1 & 0 & \dots & 0 \\ 0 & 0 & 2 & \dots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \dots & n \\ 0 & 0 & 0 & \dots & 0 \end{pmatrix}\ \ \ \ \ \rho(E_{21}) = \begin{pmatrix} 0 & 0 & \dots & 0 & 0 \\ n & 0 & \dots & 0 & 0 \\ 0 & n-1 & \dots & 0 & 0 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & \dots & 1 & 0 \end{pmatrix} \]
\[ \rho(E_{11} - E_{22}) = \begin{pmatrix} n & 0 & \dots & 0 & 0 \\ 0 & n-2 & \dots & 0 & 0 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & \dots & 2 - n & 0 \\ 0 & 0 & \dots & 0 & -n \end{pmatrix} \]
%
It is easy to see that any submodule containing a monomial contains the entire space, since we can use the representations of the basis elements to permute the monomials around, so the submodule contains all monomials, and hence the entire space. In fact, each $V_n$ is irreducible.

\begin{theorem}
    $V_n$ is an irreducible $\mathfrak{sl}_n(\mathbf{C})$ module.
\end{theorem}
\begin{proof}
    Let $W$ be a non-zero submodule of $V_n$. An eigenvector of $\rho(E_{11} - E_{22})$ lies in $W$, because $\rho(E_{11} - E_{22})$ restricts to an operation on this space. But this implies that $W$ contains a monomial, since the eigenspaces of $\rho(E_{11} - E_{22})$ are all one dimensional.
\end{proof}

It turns out that the class of $V_n$ represent all irreducible modules of $\mathfrak{sl}_2(\mathbf{C})$. The trick, given a general module $V$, is to look at the eigenvectors of $E_{11} - E_{22}$.

\begin{lemma}
    If $v \in V$ is an eigenvector of $E_{11} - E_{22}$ with eigenvalue $\lambda$, then
    %
    \begin{enumerate}
        \item[(i)] $E_{12} v = 0$, or $E_{12} v$ is an eigenvector of $E_{11} - E_{22}$ with eigenvalue $\lambda + 2$.
        \item[(ii)] $E_{21} v = 0$, or $E_{21} v$ is an eigenvector of $E_{11} - E_{22}$ with eigenvalue $\lambda - 2$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    (i) is proved by calculation,
    %
    \begin{align*}
        (E_{11} - E_{22})(E_{12} v) &= E_{12}(\lambda v) + [E_{11} - E_{22}, E_{12}] v\\
        &= \lambda E_{12} v + 2E_{12}v = (\lambda + 2) E_{12} v
    \end{align*}
    %
    and essentially the same calculation shows (ii).
\end{proof}

\begin{lemma}
    If $V$ is a finite dimensional $\mathfrak{sl}_2(\mathbf{C})$ module, then $V$ contains an eigenvector $v$ for $E_{11} - E_{22}$ such that $E_{12} v = 0$.
\end{lemma}
\begin{proof}
    Using the last lemma, we find that if $v$ is an eigenvector with eigenvalue $\lambda$, then either $E_{12} v = 0$, or $E_{12} v$ is an eigenvector with eigenvalue $\lambda + 2$, and since the eigenvalue is different, $E_{12} v$ is independent of $v$. Continuing this process, we either find that $E_{12}^m v$ is an eigenvector for $v$ with eigenvalue $\lambda + 2m$ for all $m$, or there is an eigenvector $w$ satisfying $E_{12} w = 0$. But if each $E_{12}^m v$ is an eigenvector, then they form an infinite family of independent vectors, contradicting finite dimensionality.
\end{proof}

\begin{theorem}
    If $V$ is a finite dimensional irreducible $\mathfrak{sl}_2(\mathbf{C})$ module, then $V$ is isomorphic to some $V_d$.
\end{theorem}
\begin{proof}
    Find an eigenvector $v$ for $E_{11} - E_{22}$ such that $E_{12} v = 0$, with eigenvalue $\lambda$. Consider the sequence $v, E_{21} v, E_{21}^2 v, \dots$. The last lemma essentially implies that $E_{21}^{m+1} v = 0$, $E_{21}^m v \neq 0$. We claim that $v, E_{21} v, \dots, E_{21}^m v$ form a basis for a submodule of $V$. They are certainly linearly independant, since they are eigenvectors of $E_{11} - E_{22}$ with different eigenvalues. It remains to show that $E_{12}(E_{21}^k v) \in \text{span}(E_{21}^l v)$, for $l \leq k$. For $k = 0$, we know $E_{12} v = 0$. For the induction, we find
    %
    \begin{align*}
        E_{12}(E_{21}^k v) &= (E_{21} E_{12} + (E_{11} - E_{22})) (E_{21}^{k-1} v)\\
        &= E_{21}(E_{12}(E_{21}^{k-1} v)) + (\lambda + 2(k-1)) E_{21}^{k-1} v
    \end{align*}
    %
    and by induction, $E_{12}(E_{21}^{k-1} v) \in \text{span}(E_{21}^l v)$, for $l < k$, and therefore $E_{12}(E_{21}^{k-1} v) \in \text{span}(E_{21}^l v)$, for $l \leq k$.

    By irreducibility, $V$ is the span of the $E_{21}^k v$. The matrix of $E_{11} - E_{22}$ with respect to the basis $E_{21}^k v$ is diagonal, with trace
    %
    \[ \lambda + (\lambda - 2) + \dots + (\lambda - 2m) = (m+1) \lambda - m(m+1) \]
    %
    hence $\lambda = m$, since the image of $E_{21}$ is in the derived subgroup, and therefore has trace zero.

    We now have enough information to provide an explicit homomorphism with $V_m$. Note that $V_m$ is spanned by $X^m$, $E_{21} X^m, \dots, E_{21}^m X^m$. If we set $\psi(E_{21}^k v) = E_{21}^k X^m$ this defines a vector space isomorphism which commutes with the action of $E_{11} - E_{22}$ and $E_{21}$. It remains to show that $\psi$ commutes with $E_{12}$, we use induction. For $k = 0$,
    %
    \[ E_{12}X^m = 0 = \psi(E_{12} v) \]
    %
    Now by induction,
    %
    \begin{align*}
        E_{12}(E_{21}^k X^m) &= E_{21}(E_{12}(E_{21}^{k-1} X^m)) + (E_{11} - E_{22})(E_{21}^{k-1} X^m)\\
        &= \psi(E_{21}(E_{12}(E_{21}^{k-1} v))) + (\lambda - 2(k-1)) E_{21}^{k-1} X^m \\
        &= \psi(E_{12}(E_{21}^k v)) - \psi((E_{22} - E_{11}) E_{21}^{k-1} v) + (\lambda - 2(k-1)) E_{21}^{k-1} X^m\\
        &= \psi(E_{12}(E_{21}^k v)) + (\lambda - 2(k-1))(E_{21}^{k-1} X^m - \psi(E_{21}^{k-1} v))\\
        &= \psi(E_{12}(E_{21}^k v))
    \end{align*}
    %
    and this completes the correspondence.
\end{proof}

\begin{corollary}
    If $V$ is a finite dimensional $\mathfrak{sl}_2(\mathbf{C})$ and $v \in V$ is an eigenvector of $E_{11} - E_{22}$ such that $E_{12}v = 0$, then $E_{21}v = mv$ for some integer $m$, and the submodule of $V$ generated by $v$ is isomorphic to $V_m$.
\end{corollary}
\begin{proof}
    We have argued that $v, E_{21} v, \dots, E_{21}^m v$ span the submodule generated by $v$. Now we just apply irreducibility to conclude that the module generated is isomorphic to $V_m$.
\end{proof}

The vector $v$ we constructed with the largest eigenvalue is known as the {\bf heighest weight vector}. The associated eigenvalue $m$ is known as the {\bf highest weight}.

\section{Weyl's Theorem}

\begin{theorem}[Weyl]
    If $\mathfrak{g}$ is a complex, semisimple Lie algebra, then every finite dimensional module over $\mathfrak{g}$ is completely reducible.
\end{theorem}

\section{Cartan's Criterion}

From the definition, it is very difficult to verify that an algebra is semisimple. In this chapter, we develop simple methods to decide if an algebra is semisimple, or, on the other extreme, solvable.

Recall that every linear operator $T$ on a finite dimensional complex can be uniquely written $T = D + N$, where $D$ is a diagonalizable operator, $N$ is a nilpotent operator, and $D$ and $N$ commute. $N = 0$ precisely when $T$ is diagonalizable, in which case we say $T$ is {\bf semisimple}.

If $\mathfrak{g}$ is a solvable Lie algebra, which is a subalgebra of $\mathfrak{gl}(V)$, then we know there is a basis in which all elements of $\mathfrak{g}$ have upper triangular matrix representations, and the representations of $\mathfrak{g}'$ are strictly upper triangular. It follows that $\text{tr}(XY) = 0$ for all $X \in \mathfrak{g}$ and $Y \in \mathfrak{g}'$, because
%
\[ \sum_{i,j} X_{ij}Y_{ji} = \sum_{\substack{i \leq j\\j < i}} X_{ij} Y_{ji} = 0 \]
%
Thus we have a necessary condition for a concrete subalgebra to be solvable. Cartan found that this is essentially a sufficient condition.

\begin{theorem}
    If $\mathfrak{g}$ is a complex Lie subalgebra of $\mathfrak{gl}(V)$ such that $\text{tr}(XY) = 0$ for $X,Y \in \mathfrak{g}$ then $\mathfrak{g}$ is solvable.
\end{theorem}
\begin{proof}
    We prove that every linear operator in $\mathfrak{g}'$ is nilpotent. Engel's theorem then says that $\mathfrak{g}'$ is a nilpotent Lie algebra, and therefore $\mathfrak{g}$ is solvable. Using the Jordan decomposition, write $X \in \mathfrak{g}'$ as a sum $D + N$ of a diagonal operator and a nilpotent operator. Fix a basis where $D$ is diagonal, and $N$ is strictly upper triangular. Let $D$ have entries $\lambda_1, \dots, \lambda_n$. It will suffice to show that $\sum |\lambda_i|^2 = 0$, so that $D = 0$, and so $X$ is nilpotent. If we consider the diagonal matrix $\overline{D}$ obtained by taking the complex conjugate on the diagonal, then $\sum |\lambda_i|^2$ is just the trace of $\overline{D}X$. Write
    %
    \[ X = \gamma_1 [Y_1, Z_1] + \dots + \gamma_m [Y_m, Z_m] \]
    %
    It suffices to show that the trace of $\overline{D}[Y_i,Z_i]$ is zero. But since
    %
    \[ \text{tr}(\overline{D}(Y_iZ_i - Z_iY_i)) = \text{tr}([\overline{D}, Y_i]Z_i) \]
    %
    provided we can show that $[\overline{D},Y_i] \in \mathfrak{g}$, we can apply our hypothesis to conclude the trace is zero. Note that the Jordan decomposition of $\text{adj}_X$ is $\text{adj}_D + \text{adj}_N$, and therefore there is a polynomial $f \in \mathbf{C}[X]$ with $f(\text{adj}_X) = \overline{adj_D} = \text{adj}_{\overline{D}}$. Since $\text{adj}_X$ maps $\mathfrak{g}$ into itself, so too does $\text{adj}_{\overline{D}}$. Thus the proof is completed.
\end{proof}

Since $\mathfrak{g}$ is solvable if and only if it's image in the adjoint representation is solvable, we conclude that $\mathfrak{g}$ is solvable if and only if $\text{tr}(\text{adj}_X \circ \text{adj}_Y) = 0$ for all $X \in \mathfrak{g}$, $Y \in \mathfrak{g}'$, for this guarantees that $\mathfrak{g}'$ is solvable, and therefore $\mathfrak{g}$ is solvable as well.

Because of this discussion, it appears that the symmetric, bilinear form on $\mathfrak{g}$ defined by
%
\[ \kappa(X,Y) = \text{tr}(\text{adj}_X \circ \text{adj}_Y) \]
%
is of interest. It is known as the {\bf Killing form}. Another very nice property of the Killing form is that it is in fact associative,
%
\[ \kappa([X,Y],Z) = \kappa(X,[Y,Z]) \]
%
which follows from the trace identity. The theorem above can be stated that $\mathfrak{g}$ is solvable if and only if $\kappa(X,Y) = 0$ for all $X \in \mathfrak{g}$ and $Y \in \mathfrak{g}'$. This is known as Cartan's first criterion.

\begin{example}
    Consider the two dimensional non-abelian Lie algebra with basis $X,Y$ such that $[X,Y] = X$. Then the adjoint have matrix representations
    %
    \[ \text{adj}_X = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}\ \ \ \ \ \text{adj}_Y = \begin{pmatrix} -1 & 0 \\ 0 & 0 \end{pmatrix} \]
    %
    and by looking at these representations we see $\kappa = Y^* \otimes Y^*$. The Killing form vanishes on the span of $X$, and therefore our algebra is solvable. Indeed, the derived subalgebra is abelian.
\end{example}

The Killing form descends on ideals consistantly

\begin{lemma}
    If $\mathfrak{a}$ is an ideal of $\mathfrak{g}$, then the Killing form on $\mathfrak{a}$ is the restriction of the Killing form on $\mathfrak{g}$.
\end{lemma}
\begin{proof}
    Let $\kappa$ denote the Killing from on $\mathfrak{g}$, and $\kappa_\mathfrak{a}$ the Killing form on $\mathfrak{a}$. If $X \in \mathfrak{a}$, then $\text{adj}_X: \mathfrak{g} \to \mathfrak{g}$ maps $\mathfrak{g}$ into $\mathfrak{a}$, and therefore if we take a basis $v_1, \dots, v_n$ for $\mathfrak{a}$ and extend it to a basis on $\mathfrak{g}$, the matrix representation of $\text{adj}_X$ will be
    %
    \[ \begin{pmatrix} A_X & B_X \\ 0 & 0 \end{pmatrix} \]
    %
    If we then consider $\text{adj}_Y: \mathfrak{g} \to \mathfrak{g}$, for $Y \in \mathfrak{a}$, then the matrix representation will be
    %
    \[ \begin{pmatrix} A_Y & B_Y \\ 0 & 0 \end{pmatrix} \]
    %
    and so
    %
    \[ \text{adj}_X \circ \text{adj}_Y = \begin{pmatrix} A_XA_Y & A_XB_y \\ 0 & 0 \end{pmatrix} \]
    %
    and so the trace of the restriction will agree with the trace of the adjoint on all of $\mathfrak{g}$.
\end{proof}

The Killing form not only gives us a criterion for solvability, but also a criterion for semisimplicity. We say $\kappa$ is {\bf non-degenerate} if there is no $X \in \mathfrak{g}$ such that $\kappa(X,Y) = 0$ for all $Y$.

\begin{lemma}
    A Lie algebra is semisimple if and only if it has no non-zero abelian ideals.
\end{lemma}
\begin{proof}
    If an algebra $\mathfrak{g}$ has a non-zero abelian ideal, the ideal is solvable, and thus $\text{rad}(\mathfrak{g}) \neq 0$. Note that $Z(\mathfrak{g})$ is always an ideal of $\mathfrak{g}$, because if $[X,Y] = 0$ for all $Y$, then $[X,Y] = 0 \in Z(\mathfrak{g})$. Thus in a semisimple algebra, $Z(\mathfrak{g}) = 0$. If $\mathfrak{a}$ is an ideal of $\mathfrak{g}$, then $Z(\mathfrak{a})$ is an ideal of $\mathfrak{g}$, because if $[X,Z] = 0$ for all $Z \in \mathfrak{a}$, then for any $Y$, $[Y,Z] \in \mathfrak{a}$, and hence
    %
    \[ [[X,Y],Z] = [X,[Y,Z]] + [Y,[Z,X]] = 0 + [Y,0] = 0 \]
    %
    In any solvable Lie algebra $\mathfrak{h}$, $\mathfrak{h}'$ is nilpotent, hence $\mathfrak{h}'_{k+1} = [\mathfrak{h}, \mathfrak{h}_k] = 0$ for some $k$, implying that $Z(\mathfrak{h}')$ is a non-trivial abelian subalgebra of $\mathfrak{h}$, and we have also show it is in fact an ideal of $\mathfrak{h}$.
\end{proof}

\begin{theorem}[Cartan's Second Criterion]
    $\mathfrak{g}$ is semisimple if and only if the Killing form is nondegenerate.
\end{theorem}
\begin{proof}
    The set of $X \in \mathfrak{g}$ such that $\kappa(X,Y) = 0$ for all $Y$ forms an ideal of $\mathfrak{g}$ because the Killing form is associative. Since $\kappa_\alpha = 0$, we conclude that this ideal is solvable. Thus if $\mathfrak{g}$ is semisimple, $\alpha = 0$, and so $\kappa$ is nondegenerate. Conversely, if $\mathfrak{g}$ has a non-trivial abelian ideal $\mathfrak{a}$, and if $Y \in \mathfrak{a}$ is non-zero, then $\text{adj}_Y \circ \text{adj}_X \circ \text{adj}_Y = 0$ for any $X \in \mathfrak{g}$, hence $(\text{adj}_Y \circ \text{adj}_X)^2 = 0$. Nilpotent maps have trace zero, so $\kappa(Y,X) = 0$ for all $X$, and so $\kappa$ is degenerate.
\end{proof}

These criterions are incredibly powerful to deriving structural results for Lie algebras. We will start by showing that every semisimple Lie algebra is the direct sum of simple Lie algebras (so the algebras really determine the name semisimple). Define the perpendicular $\mathfrak{h}^\perp$ of a subalgebra $\mathfrak{h}$ of a $\mathfrak{g}$ to be the perpendicular with respect to the Killing form
%
\[ \mathfrak{h}^\perp = \{ x \in \mathfrak{g} : (\forall y \in \mathfrak{h}: \kappa(y,x) = 0) \} \]

\begin{lemma}
    If $\mathfrak{g}$ is a complex semisimple Lie algebra, and $\mathfrak{a}$ is an ideal, then $\mathfrak{g} = \mathfrak{a} \oplus \mathfrak{a}^\perp$. $\mathfrak{a}$ is also a semisimple Lie algebra.
\end{lemma}
\begin{proof}
    Because $\kappa: \mathfrak{a} \cap \mathfrak{a}^\perp \to \mathfrak{a} \cap \mathfrak{a}^\perp$ is equal to zero, $\mathfrak{a} \cap \mathfrak{a}^\perp$ is solvable, hence $\mathfrak{a} \cap \mathfrak{a}^\perp = (0)$, and by dimension counting we certainly have the vector space structure of $\mathfrak{g}$ as a direct sum of $\mathfrak{a}$ and $\mathfrak{a}^\perp$. If $X \in \mathfrak{a}$, and $Y \in \mathfrak{a}^\perp$, then $[X,Y] = 0$, because $\mathfrak{a}$ and $\mathfrak{a}^\perp$ are both ideals so $[X,Y] \in \mathfrak{a} \cap \mathfrak{a}^\perp$. Thus $\mathfrak{g}$ really is the Lie algebra sum of the two ideals. Finally, if $\mathfrak{a}$ was not semisimple, then $\kappa_\mathfrak{a}$ would be degenerate, but this would imply the degeneracy of $\kappa$.
\end{proof}

We verify that $\mathfrak{a}^\perp$ is also an ideal of $\mathfrak{g}$, so by recursively performing this calculation, we can break any semisimple Lie algebra down into the direct sum of simple Lie algebras. Conversely, if we consider a lie algebra $\mathfrak{g} = \mathfrak{g}_1 \oplus \mathfrak{g}_2 \oplus \dots \oplus \mathfrak{g}_m$, where each $\mathfrak{g}_m$ is simple, and $\mathfrak{a} \subset \mathfrak{g}$ is a solvable ideal of $\mathfrak{g}$, then $[\mathfrak{a}, \mathfrak{g}_i] \subset \mathfrak{a} \cap \mathfrak{g}_i$ is a solvable ideal of $\mathfrak{g}_i$. But this implies $[\mathfrak{a}, \mathfrak{g}_i] = 0$, hence
%
\[ [\mathfrak{a}, \mathfrak{g}] = [\mathfrak{a}, \mathfrak{g}_1 + \mathfrak{g}_2 + \dots + \mathfrak{g}_m] = [\mathfrak{a}, \mathfrak{g}_1] + \dots + [\mathfrak{a}, \mathfrak{g}_m] = 0 \]
%
hence $\mathfrak{a} \subset Z(\mathfrak{g})$. But $Z(\mathfrak{g}) = \bigoplus Z(\mathfrak{g}_i) = \bigoplus 0 = 0$, hence $\mathfrak{a} = 0$. Using very similar ideals, we can prove that the quotient of any semisimple Lie algebra is semisimple, because if $\mathfrak{a}$ is any ideal of $\mathfrak{g}$, then $\mathfrak{g} = \mathfrak{a} \oplus \mathfrak{a}^\perp$, and $\mathfrak{g}/\mathfrak{a}$ is isomorphic to $\mathfrak{a}^\perp$, which is semisimple.

\begin{theorem}
    If $\mathfrak{g}$ is a finite dimensional complex semisimple Lie algebra, then the adjoint representation of $\mathfrak{g}$ is an isomorphism with the space of all derivations on $\mathfrak{g}$.
\end{theorem}
\begin{proof}
    Denote the image of $\mathfrak{g}$ by $\text{adj}\ \mathfrak{g}$. This subalgebra is actually an ideal, for if $d$ is any derivation, then
    %
    \[ [\text{adj}_X, d](Y) = [X,dY] - d[X,Y] = -[dX,Y] = \text{adj}_{-dX}(Y) \]
    %
    Thus $\text{adj}\ \mathfrak{g}$ is an ideal on $\text{Der}\ \mathfrak{g}$. This is true for any Lie algebra.

    For a complex semisimple Lie algebra, the adjoint representation is injective. The Killing form on $\text{adj}\ \mathfrak{g}$ is the restriction of the killing form on $\text{Der}\ \mathfrak{g}$. Since $\text{adj}\ \mathfrak{g}$ is semisimple, $(\text{adj}\ \mathfrak{g}) \cap (\text{adj}\ \mathfrak{g})^\perp = 0$. Thus $\text{Der}\ \mathfrak{g} = (\text{adj}\ \mathfrak{g}) \oplus (\text{adj}\ \mathfrak{g})^\perp$. This implies that if $d \in (\text{adj}\ \mathfrak{g})^\perp$, then $[d,\text{adj}_X] = \text{adj}_{dX}$ for all $X$, hence $dX = 0$ for all $X$, because the center of $\mathfrak{g}$ is trivial, hence $d = 0$.
\end{proof}

We can also use the Killing form and its structure of semisimple Lie algebras to classify the representations of the algebra as operators. An important property of the semisimple Lie algebras is that the `Jordan decomposition' is invariant of the representation. Note that in general, the Jordan decomposition of a representation can be fairly arbitrary. For instance, the representations of $\mathbf{C}$ over some vector space $V$ are obtained my mapping $1$ to an arbitrary element of $V$.

\begin{theorem}
    If $\mathfrak{g}$ is a complex Lie algebra, and $D \in \text{Der}(\mathfrak{g})$ has Jordan decomposition $D = A + N$, then $A$ and $N$ are both derivations on $\mathfrak{g}$.
\end{theorem}
\begin{proof}
    For each $\lambda \in \mathbf{C}$, let $\mathfrak{g}_\lambda$ be the set of $X$ such that $(D - \lambda)^mX = 0$ for some $m$. Then the vector space structure of $\mathfrak{g}$ decomposes into the sum of the subspaces $\mathfrak{g}_\lambda$. We have $[\mathfrak{g}_\lambda, \mathfrak{g}_\gamma] \subset \mathfrak{g}_{\lambda + \gamma}$, because
    %
    \[ (D - (\lambda + \gamma))^m(XY - YX) = \sum {m \choose k} [(D - \lambda)^k X, (D - \gamma)^{m-k} Y] \]
    %
    Since $A$ is diagonalizable, the $\lambda$ eigenspace for $A$ is $\mathfrak{g}_\lambda$. If $X \in \mathfrak{g}_\lambda$ and $Y \in \mathfrak{g}_\gamma$, then $[X,Y] \in \mathfrak{g}_{\lambda + \gamma}$, hence
    %
    \[ A[X,Y] = (\lambda + \gamma)[X,Y] = [AX, Y] + [X,AY] \]
    %
    hence $A$ is a derivation. $N$ is then a derivation, because it is the difference of two derivations.
\end{proof}

\begin{theorem}
    If $\mathfrak{g}$ is semisimple, then any $X \in \mathfrak{g}$ can be written uniquely as $D + N$, where $\text{adj}_D$ is diagonalizable, $\text{adj}_N$ is nilpotent, and $[D,N] = 0$. If $Y$ commutes with $X$, then $[D,Y] = [N,Y] = 0$.
\end{theorem}
\begin{proof}
    We apply the last theorem to $\mathfrak{g}$, viewed as $\text{adj}\ \mathfrak{g}$ by an isomorphism. Given $X$, we have the Jordan decomposition $\text{adj}_X = \text{adj}_D + \text{adj}_N$ for some $D,N \in \mathfrak{g}$, and hence $X = D + N$. Since $\text{adj}_D$ and $\text{adj}_N$ commute, $[D,N] = 0$. If $Y$ commutes with $X$, then $\text{adj}_X(Y) = 0$, then because $\text{adj}_D$ and $\text{adj}_N$ are polynomials in $\text{adj}_X$, we see
    %
    \[ \text{adj}_N = \sum c_k \text{adj}^k_X \]
    %
    hence $\text{adj}_N(Y) = c_0$. But since $\text{adj}_N$ is nilpotent, $c_0 = 0$.
\end{proof}

We call the decomposition $X = D + N$ the {\bf abstract Jordan decomposition} of $X$.

\begin{theorem}
    If $\rho: \mathfrak{g} \to \mathfrak{gl}(V)$ is a representation of a complex semisimple Lie algebra, and $X \in \mathfrak{g}$ has a Jordan decomposition $D + N$, then the (concrete) Jordan decomposition of $\rho(X)$ is $\rho(D) + \rho(N)$.
\end{theorem}
\begin{proof}
    The image of any representation of a complex semisimple Lie algebra is semisimple, since if $\mathfrak{h}$ is the kernel of $\rho$, then $\mathfrak{g}/\mathfrak{h}$ is isomorphic to $\rho(\mathfrak{g})$. We can therefore talk about the abstract Jordan decomposition of elements of $\rho(\mathfrak{g})$. The concrete decomposition of $\rho(X)$ agrees with the asbtract decomposition, because if $Y$ is nilpotent/diagonalizable it implies $\text{adj}_Y$ is as well. If $\text{adj}_D$ is diagonalizable, then it is diagonalizable on $\mathfrak{h}$ by some basis $X_1, \dots, X_n$, and if we extend this basis of eigenvectors to the whole space, $X_1, \dots, X_n, Y_1, \dots, Y_m$, then, viewed as a map on the quotient $\mathfrak{g}/\mathfrak{h}$, we have $[D, Y_i + \mathfrak{h}] = \lambda_i Y_i + \mathfrak{h}$. If $\text{adj}_N$ is nilpotent, then $\text{adj}_N$ is nilpotent on the quotient as well. This verifies that $\rho(X)$ has abstract Jordan decomposition $\rho(D) + \rho(N)$, because the decomposition is unique.
\end{proof}








\chapter{Root Systems}

Recall the proof that $\mathfrak{sl}_2(\mathbf{C})$ is the only simple 2 dimensional Lie algebra.
%
\begin{enumerate}
    \item We found $X \in \mathfrak{sl}_2(\mathbf{C})$ such that $\text{adj}_X$ is diagonalizable.
    \item We took a basis of $\mathfrak{sl}_2(\mathbf{C})$ consisting of eigenvectors for $\text{adj}_X$, and showed that the corresponding structural constants
\end{enumerate}
%
To classify the semisimple Lie algebras, we will utilize a very important generalization of this technique. In order to apply this technique to higher dimensional Lie algebras, we will need to generalize the first step to finding a sufficiently large simultaneously diagonalizable subalgebra of the Lie algebra. This subalgebra is known as the Cartan subalgebra.

Let us try and generalize the technique to $\mathfrak{sl}_n(\mathbf{C})$. Consider the subalgebra $\mathfrak{h}$ of $\mathfrak{sl}_n(\mathbf{C})$ consisting of the diagonal matrices. Since
%
\[ [E_{ij}, E_{kl}] = \delta_j^k E_{il} - \delta_l^i E_{kj} \]
%
We find that for any diagonal matrix $X = (\sum a_i E_{ii})$, if $m \neq n$, then
%
\[ [X,E_{mn}] = \sum a_i [E_{ii}, E_{mn}] = (a_m - a_n) E_{mn} \]
%
Since the diagonal matrices form an abelian subalgebra of $\mathfrak{sl}_n(\mathbf{C})$, we can extend the $E_{ij}$ to a full basis diagonalizing all diagonal matrices. In particular, we see the weights for this action are precisely the maps $\varepsilon_{ij}(M) = M_{ii} - M_{jj}$. The weight space for $\varepsilon_{ij}$ for $i \neq j$ corresponds precisely to the span of $e_{ij}$, so the space is 1 dimensional, and for $i = j$, the weight space is precisely the space of diagonal matrices, which is $n - 1$ dimensional. Since the weight spaces form $\mathfrak{h}$-invariant subspaces, we have a module decomposition
%
\[ \mathfrak{sl}_n(\mathbf{C}) = \mathfrak{h} \oplus \bigoplus_{i \neq j} \mathbf{C} e_{ij} \]
%
which effectively characterizes the action of the adjoint maps from $\mathfrak{h}$.

In general, our aim is to find a maximal abelian subalgebra consisting only of elements $X$ with $\text{adj}_X$ diagonalizable (we call these {\bf semisimple} elements), and then to decompose $\mathfrak{g}$ into the corresponding weight spaces. We shall call a maximal abelian subalgebra containing only semisimple elements a {\bf Cartan subalgebra}.

It is natural to ask why abelian subalgebras consisting of semisimple elements are useful. Given any particular abelian subalgebra $\mathfrak{h}$, the adjoint maps $\text{adj}_X$ for $X \in \mathfrak{h}$ commute with one another -- if $X,Y \in \mathfrak{h}$, then
%
\[ [X,[Y,Z]] = [[X,Y],Z] + [Y,[X,Z]] = [Y,[X,Z]] \]
%
hence $\text{adj}_X \circ \text{adj}_Y = \text{adj}_Y \circ \text{adj}_X$. Since each $\text{adj}_X$ is diagaonalizable, the elements of $\mathfrak{h}$ are {\it simultaneously diagonalizable} -- there is a basis of $\mathfrak{g}$ consisting of eigenvectors. We thus have a decomposition of $\mathfrak{g}$ as
%
\[ \mathfrak{g} = \mathfrak{h} \oplus \bigoplus_{\lambda \neq 0} \mathfrak{g}_\lambda \]
%
where $\lambda$ ranges over the set of weights for $\mathfrak{h}$. Given $\mathfrak{g}_\lambda$ and $\mathfrak{g}_\gamma$, $[\mathfrak{g}_\lambda, \mathfrak{g}_\gamma] \subset \mathfrak{g}_{\lambda + \gamma}$, because if $X \in \mathfrak{g}_\lambda$, $Y \in \mathfrak{g}_\gamma$, then by using the Jacobi identity we find that for any $Z \in \mathfrak{h}$,
%
\begin{align*}
    [Z,[X,Y]] &= [[Z,X],Y] + [X,[Z,Y]]\\
    &= \lambda(Z)[X,Y] - \gamma(Z)[Y,X]\\
    &= (\lambda(Z) + \gamma(Z))[X,Y]
\end{align*}
%
If $\lambda + \gamma \neq 0$, then $\kappa(\mathfrak{g}_\lambda, \mathfrak{g}_\gamma) = 0$. Indeed, if $X \in \mathfrak{g}_\lambda$ and $Y \in \mathfrak{g}_\gamma$, and $Z \in \mathfrak{h}$
%
\[ \lambda(Z) \kappa(X,Y) = \kappa([Z,X],Y) = - \kappa(X,[Z,Y]) = -\gamma(Z) \kappa(X,Y) \]
%
If we choose $Z$ such that $\lambda(Z) + \gamma(Z) \neq 0$, we conclude $\kappa(X,Y) = 0$. This shows that $\kappa$ is non-degenerate on $\mathfrak{g}_0$, because if there is $X \in \mathfrak{g}_0$ such that $\kappa(X,Y) = 0$ for all $Y \in \mathfrak{g}_0$, then any $Z \in \mathfrak{g}$ can be expanded as $\sum Z_\lambda$, with $Z_\lambda \in \mathfrak{g}_\lambda$, and $\kappa(X,Z) = \kappa(X,Z_0)$, so that $X$ annihilates all elements of $\mathfrak{g}$, and hence $X = 0$. It follows that $\mathfrak{h}$ is semisimple.

If $\mathfrak{h}$ is small, then the weight decomposition of $\mathfrak{g}$ is likely to be rather coarse. If we choose $\mathfrak{h}$ too small, then $\mathfrak{h}$ may be a proper subset of $\mathfrak{g}_0$, hence the decomposition and nice properties of the Killing form may be not so useful.

\begin{example}
    If $\mathfrak{h}$ is the Lie subalgebra of $\mathfrak{sl}_n(\mathbf{C})$ spanned by $E_{11} - E_{22}$, then $\mathfrak{g}_0$ consists precisely of the matrices $X$ with $X_{1n} = X_{n1} = 0$ for $n \neq 1$, and $X_{2n} = X_{2n} = 0$ for $n \neq 2$, so $\mathfrak{g}_0 = \mathfrak{h}$ precisely when $n = 2$. The $\mathfrak{g}_0$ has dimension $n^2 - 1 - 2(n-1) - 2(n-2) = n^2 - 4n + 5$, so the decomposition is very coarse.
\end{example}

\begin{theorem}
    Complex semisimple Lie algebras have a non-zero Cartan subalgebra $\mathfrak{h}$. If $X \in \mathfrak{h}$ minimizes the dimension of $C_\mathfrak{g}(X)$, then every $Y \in \mathfrak{h}$ is central in $C_\mathfrak{g}(X)$, and so $C_\mathfrak{g}(X) \subset C_\mathfrak{g}(Y)$ and therefore $C_\mathfrak{g}(X) = C_\mathfrak{g}(\mathfrak{h})$.
\end{theorem}
\begin{proof}
    Nah.
\end{proof}

\begin{theorem}
    If $\mathfrak{h}$ is a Cartan subalgebra of $\mathfrak{g}$ then $C_\mathfrak{g}(\mathfrak{h}) = \mathfrak{h}$, hence $\mathfrak{h}$ is self centralizing, and if we consider the weight decomposition with respect to $\mathfrak{h}$, then $\mathfrak{g}_0 = \mathfrak{h}$.
\end{theorem}

Thus a general semisimple Lie algebra $\mathfrak{g}$ has a Cartan subalgebra $\mathfrak{h}$, and since $\mathfrak{h} = C_\mathfrak{g}(\mathfrak{h})$, we may write $\mathfrak{g}$ via the {\bf root space decomposition}
%
\[ \mathfrak{g} = \mathfrak{h} \oplus \bigoplus_{\alpha \in \Phi} \mathfrak{g}_\alpha \]
%
where $\Phi$ is the set of roots of $\mathfrak{g}$, the set of non-zero weights. The roots and weights depend on the Cartan subalgebra $\mathfrak{h}$ we choose, but for the classical algebras the Cartan subalgebra is canonical.

\begin{lemma}
    If $\alpha$ is a non-zero root, then $-\alpha$ is a root, and for every non-zero $X \in \mathfrak{g}_\alpha$ there is $Y \in \mathfrak{g}_{-\alpha}$ such that $\text{span}(X,Y,[X,Y])$ is a Lie subalgebra of $\mathfrak{g}$ isomorphic to $\mathfrak{sl}_2(\mathbf{C})$.
\end{lemma}





\section{Cartan Subalgebras as Inner Product Spaces}

\begin{lemma}
    If $X \in \mathfrak{h}$ is non-zero, then there is a root $\alpha$ with $\alpha(X) \neq 0$, and the roots span $\mathfrak{h}^*$.
\end{lemma}
\begin{proof}
    If $\alpha(X) = 0$ for all roots $X$, then for any $Y \in \mathfrak{g}$, we can write $Y = Y_0 + \sum_{\alpha \in \Phi} Y_\alpha$, then $[X,Y] = \sum \alpha(X) Y = 0$, hence $X \in Z(\mathfrak{g}) = 0$, hence $X = 0$. That the roots span $\mathfrak{h}^*$ is essentially a restatement of what we have proved.
\end{proof}

Since the Killing form on $\mathfrak{h}$ is non-degenerate, it induces a canonical isomorphism of $\mathfrak{h}$ with $\mathfrak{h}^*$, and therefore the Killing form can be applied to elements of the dual as well.





\section{Root Spaces for the Classical Lie Algebras}

It will be important for us to consider the Cartan subalgebras and root spaces of the classical Lie algebras, that occur as spaces of matrices.

We have already seen the Cartan subalgebra $\mathfrak{h}$ of $\mathfrak{sl}_n(\mathbf{C})$, which consists of the diagonal matrices. Indeed, if $XY = YX$ for all diagonal matrices $Y \in \mathfrak{sl}_n(\mathbf{C})$, then in particular
%
\[ 0 = [X,E_{ii} - E_{(i+1)(i+1)}] = \sum_j X_{ji} E_{ji} - X_{ij} E_{ij} - X_{j(i+1)} E_{j(i+1)} + X_{(i+1)j} E_{(i+1)j} \]
%
which implies $X_{ij} = 0$ for $i \neq j$, hence $\mathfrak{h}$ is maximal. For $i \neq j$,
%
\[ \left[\sum a_k E_{kk} ,E_{ij} \right] = (a_i - a_j) E_{ij} \]
%
so the weights consist of the maps $\varepsilon_{ij}(X) = X_{ii} - X_{jj}$, for $i \ne j$.



\end{document}











\section{Low Dimensional Classifications}

There is a useful criterion for verifying some abstractly defined bracket is actually a Lie bracket, which will enable us to classify the low dimensional Lie algebras. Given a vector space $\mathfrak{g}$, after fixing a basis $E_1, \dots, E_n$, we find an arbitrary bracket (not necessarily a Lie bracket) $[\cdot, \cdot]: \mathfrak{g} \to \mathfrak{g}$ can be written as
%
\[ [X,Y] = \sum f_k(X,Y) E_k \]
%
where each $f_i$ is a bilinear form, and so we may find constants $a_{ij}^k \in K$ such that
%
\[ f_k = \sum a^k_{ij} ( E_i^* \otimes E_j^* ) \]
%
which is the same as saying $[E_i, E_j] = \sum a_{ij}^k E_k$. If $[\cdot, \cdot]$ was actually a Lie bracket, then the alternating property of the Lie algebras implies that each $f_k$ is a skew-symmetric Bilinear form, hence $a_{ij}^k = -a_{ji}^k$. The Jacobi identity takes the form
%
\[ \sum_l a_{jk}^l a_{il}^m + a_{ki}^l a_{jl}^m + a_{ij}^l a_{kl}^m = 0 \]
%
for every fixed $i,j,k,m$. Conversely, given a set of constants $a^k_{ij}$ over some vector space satisfying these equations, the corresponding bilinear map is a Lie bracket, because if $X = \sum a^i e_i$, $Y = \sum b^i e_i$, and $Z = \sum c^i e_i$, then
%
\begin{align*}
    [X,[Y,Z]] + &[Y,[Z,X]] + [Z,[X,Y]]\\
    &= \sum_{i,j,l} a^i b^j c^l \left( [e_i,[e_j,e_l]] + [e_j,[e_l,e_i]] + [e_l,[e_i,e_j]] \right)\\
    &= 0
\end{align*}
%
This coordinatization is particular applicable in low dimensions, where the calculations are relatively trivial, and allows us to classify the algebras up to isomorphism, because two algebras $\mathfrak{g}$ and $\mathfrak{h}$ are isomorphic if and only if we can specify a basis in such a way that the structure constants obtained are the same. In the one dimensional case, the only skew symmetric matrix is trivial, so that every one dimensional Lie algebra is abelian. The calculation of the two and three dimensional cases gets progressively more and more involved.

Before we do this though, we should mention a little trick to help verify the Jacobi identity for certain abstract Lie brackets. Given a bilinear map $\omega: \mathfrak{g}^2 \to \mathfrak{g}$, we consider the trilinear map $T\omega: \mathfrak{g}^3 \to \mathfrak{g}$ defined by
%
\[ (T \omega)(X,Y,Z) = \omega(X, \omega(Y,Z)) + \omega(Y,\omega(Z,X)) + \omega(Z,\omega(X,Y)) \]
%
Then $T \omega = 0$ if and only if $\omega$ satisfies the Jacobi identity. By linearity, each coefficient of $\omega(X,\omega(Y,Z))$ can be written as a polynomial in the variables $X^i$, $Y^j$, and $Z^k$,
%
\[ \omega(X, \omega(Y,Z)) = \sum b_{ijk}^l X^i Y^j Z^k E_l \]
%
and
%
\[ T\omega = \sum (b_{ijk}^l + b_{jki}^l + b_{kij}^l) X^i Y^j Z^k E_l \]
%
Applying the theory of multivariate polynomials, $T \omega = 0$ if and only if $b_{ijk}^l + b_{jki}^l + b_{kij}^l = 0$ for each $l, i, j, k$. In most cases, we will find that only two of the three coefficients will be non-zero, and they will be the negation of one another. Thus we can verify the Jacobi identity by `pairing cycles' in the coefficients of $\omega(X,\omega(Y,Z))$.

\begin{theorem}
    There is a single nonabelian two dimensional Lie algebra.
\end{theorem}
\begin{proof}
    If $\mathfrak{g}$ is a two dimensional Lie algebra, and if $(X,Y)$ is a basis, then $[X,Y]$ spans the derived subalgebra, hence the derived subalgebra is one dimensional if the space is nonabelian. Fix a non-zero $X$ in the derived subalgebra, and extend $X$ to a basis $(X,Y)$. Then $[X,Y] \neq 0$, for otherwise the bracket is abelian. Write $[X,Y] = \lambda X$. By scaling $Y$, we may actually assume $[X,Y] = X$. This shows that there can be at most one non-abelian Lie algebra, because we have found a basis with a particular set of structural constants, and provided these constants specify a Lie algebra, we have determined that there is a single two dimensional Lie algebra. The bracket $[X,Y] = X$ does actually give a Lie algebra structure, since
    %
    \[ [a_1X + a_2Y, [b_1X + b_2Y, c_1X + c_2Y]] = (a_2b_2c_1 - a_2b_1c_2) X \]
    %
    And $a_2b_2c_1$ is obtained from $a_2b_1c_2$ by a cycle permutation in the variables $a,b$, and $c$, hence the Jacobi holds.
\end{proof}

The three dimensional case is more involved. We split our discussion into three cases, where the derived subalgebra has dimension 0,1,2, and 3. The zero dimensional case is obviously the abelian Lie algebra, and needs no further discussion.

\begin{theorem}
    The Heisenberg Lie algebra is the unique three dimensional Lie algebra whose derived subalgebra is one dimensional, and contained within the center of the algebra.
\end{theorem}
\begin{proof}
    Given such a Lie algebra $\mathfrak{g}$, take $X$ and $Y$ such that $[X,Y] = Z \neq 0$. Then $X$, $Y$, and $Z$ are linearly independant, for if $aX + bY + cZ = 0$, then
    %
    \[ [aX + bY + cZ, X] = b[Y,X] = -bZ = 0\ \ \ \ \ [aX + bY + cZ, Y] = aZ = 0 \]
    %
    implying $a = b = 0$, hence $c = 0$. Thus $X,Y$, and $Z$ are a basis of the Lie algebra. The structural constants of this Lie algebra are identical to the structural constants of the Heisenberg Lie algebra, where $X = E_{12}$, $Y = E_{23}$, and $Z = E_{33}$.
\end{proof}

The remaining dimension 1 case occurs when the derived subalgebra is not contained within the center. If we consider the direct product the nonabelian two-dimensional Lie algebra $\mathfrak{g}$ with the field $K$, denoted $\mathfrak{g} \oplus K$, then the derived subalgebra of this Lie algebra is not contained within the centre, because
%
\[ (\mathfrak{g} \oplus K)' = KX \oplus (0) \]
\[ Z(\mathfrak{g} \oplus K) = (0) \oplus K \]
%
We shall now prove that this is the defining three dimensional Lie algebra whose derived subalgebra is one-dimensional, and is not contained within the centre.

\begin{theorem}
    There is a unique three dimensional Lie algebra with one dimensional derived subalgebra not contained within the center of the algebra.
\end{theorem}
\begin{proof}
    Let $\mathfrak{h}$ be such an algebra. Pick $X \neq 0$ spanning the derived $\mathfrak{h}'$, then pick $Y$ with $[X,Y] = X$. Then $X$ and $Y$ are linearly independant, and we may consider some basis $\{ X, Y, Z \}$. There must $a$ and $b$ such that $[X,Z] = aX$, $[Y,Z] = bX$. If we consider the equations
    %
    \begin{align*}
        (\lambda X + \gamma Y + \eta Z, X) &= -(\gamma + \eta a) X\\
        (\lambda X + \gamma Y + \eta Z, Y) &= (\lambda - \eta b) X\\
        (\lambda X + \gamma Y + \eta Z, Z) &= (a \lambda + b \gamma) X
    \end{align*}
    %
    And if we consider the matrix
    %
    \[ \begin{pmatrix} 0 & -1 & -a \\ 1 & 0 & -b \\ a & b & 0 \end{pmatrix} \]
    %
    we notice the determinant is $0$, hence the matrix is non-invertible, and the nullspace must contain some non-zero $(\lambda, \gamma, \eta)$, i.e. $Z(\mathfrak{g})$ is non trivial, and contains some non-zero $\lambda X + \gamma Y + \eta Z$. If $\eta = 0$, then the equations above would imply $\gamma = \lambda = 0$, which is impossible, hence $\eta \neq 0$, and we have found a nonzero $w \in Z(\mathfrak{h})$ not contained in the span of $x$ or $y$, hence we have the decomposition
    %
    \[ \mathfrak{h} = \langle x, y \rangle \oplus Kw \cong \mathfrak{g} \oplus K \]
    %
    Hence $\mathfrak{g} \oplus K$ is the unique such Lie algebra.
\end{proof}

There is actually an infinite family of non-isomorphic Lie algebras whose derived algebra is two-dimensional in the case where $K$ is the complex numbers. but we have a nice classification of this family. We take a basis $(Y,Z)$ of the derived subalgebra, and complete it to a basis $(X,Y,Z)$ of the entire algebra. We claim that the derived subalgebra is always abelian. To prove this, it suffices to show $[Y,Z] = 0$. Consider the adjoint map $\text{adj}_Y$, which can be written in matrix form with respect to the basis given as
%
\[ \begin{pmatrix} 0 & 0 & 0 \\ * & 0 & \alpha \\ * & 0 & \beta \end{pmatrix} \]
%
for some constants $\alpha$ and $\beta$. We claim that the trace of any adjoint operator from a derived element is zero, so that $\beta = 0$. By consider the adjoint map $\text{adj}_Z$, we conclude that $\alpha = 0$. Thus the derived subalgebra is abelian.

\begin{lemma}
    For any Lie algebra $\mathfrak{g}$, and $X \in \mathfrak{g}'$, $\text{tr}(\text{adj}_X) = 0$.
\end{lemma}
\begin{proof}
    If $X = [Y,Z]$, then
    %
    \[ \text{adj}_{[Y,Z]} = [\text{adj}_Y, \text{adj}_Z] \in \mathfrak{gl}(\mathfrak{g})' = \mathfrak{sl}(\mathfrak{g}) \]
    %
    Hence the trace the adjoint is zero.
\end{proof}

In fact, $\text{adj}_X$ is an isomorphism of $\mathfrak{g}'$, because $[X,Y]$ and $[X,Z]$ span the derived subalgebra, hence the map is surjective, and therefore an isomorphism. Over $\mathbf{C}$, we may apply the theory of the Jordan normal form, and break our derivation down into two cases.

\begin{enumerate}
    \item If we can choose $Y$ and $Z$ such that $[X,Y] = \lambda Y$, $[X,Z] = \gamma Z$, then by scaling, we may assume $\lambda = 1$. For a fixed $\gamma$, this does indeed define a Lie algebra structure on $\mathbf{C}^3$, denoted $\mathfrak{g}_\gamma$, because
    %
    \begin{align*}
        [a_1X + a_2Y& + a_3Z, [b_1X + b_2Y + b_3Z, c_1X + c_2Y + c_3Z]]\\
        &= [a_1X + a_2Y + a_3Z, (b_1c_2 - b_2c_1) Y + \gamma (b_1 c_3 - b_3c_1) Z]\\
        &= (a_1b_1c_2 - a_1 b_2c_1) Y + \gamma^2 (a_1 b_1c_3 - a_1 b_3 c_1) Z
    \end{align*}
    %
    and $a_1b_1c_2$ is obtained from $a_1b_2c_1$, and $a_1b_1c_3$ is obtained from $a_1b_3c_1$ by a cycle permutation, hence the Jacobi identity holds. These form a class of distinct Lie algebras, for $\mathfrak{g}_\lambda$ is isomorphic to $\mathfrak{g}_\gamma$ if and only if $\lambda = \gamma$ or $\lambda = 1/\gamma$.

    \item Suppose our operator is not diagonalizable. Since we are working over $\mathbf{C}$, we have a Jordan normal form. That is, there exists an eigenvalue $\lambda$, and a basis $Y,Z$ such that $[X,Y] = \lambda Y + Z$, $[X,Z] = \lambda Z$. By scaling $X$, we may assume $\lambda = 1$, but then this defines a unique Lie algebra structure. Indeed, then
    %
    \begin{align*}
        &[a_1X + a_2Y + a_3Z, [b_1X + b_2Y + b_3Z, c_1X + c_2Y + c_3Z]]\\
        &= [a_1X + a_2Y + a_3Z, (b_1c_2 - b_2c_1) Y + (b_1c_3 - b_3c_1 + b_1c_2 - b_2c_1) Z]\\
        &= (a_1b_1c_2 - a_1 b_2c_1) (Y + Z) + (a_1 b_1c_3 - a_1 b_3 c_1 + a_1b_1c_2 - a_1b_2c_1) Z\\
        &= (a_1b_1c_2 - a_1b_2c_1) Y + (2a_1b_1c_2 - 2a_1b_2c_1 + a_1b_1c_3 - a_1b_3c_1) Z
    \end{align*}
    %
    and we have cycle pairs, hence the Jacobi identity holds.
\end{enumerate}

Finally, suppose that $\mathfrak{g}$ is a Lie algebra such that $\mathfrak{g}' = \mathfrak{g}$. We already know such an algebra exists, $\mathfrak{sl}_2(\mathbf{C})$, because if we consider the basis of the algebra $X = \left( \begin{smallmatrix} 0 & 1 \\ 0 & 0 \end{smallmatrix} \right)$, $Y = \left( \begin{smallmatrix} 0 & 0 \\ 1 & 0 \end{smallmatrix} \right)$, and $Z = \left( \begin{smallmatrix} 1 & 0 \\ 0 & -1 \end{smallmatrix} \right)$, then
%
\[ [X,Y] = Z\ \ \ \ \ [X,Z] = -2X\ \ \ \ \ [Y,Z] = 2Y \]
%
which span $\mathfrak{sl}_2(\mathbf{C}$. We shall find this is the only such algebra up to isomorphism.

\begin{theorem}
    If a three dimensional Lie algebra $\mathfrak{g}$ satisfies $\mathfrak{g}' = \mathfrak{g}$, then $\mathfrak{g}$ is isomorphic to $\mathfrak{sl}_2(\mathbf{C})$.
\end{theorem}
\begin{proof}
Given such an algebra, if $X \in \mathfrak{g}$ is non-zero, then $\text{adj}_X$ has rank 2, because $\mathfrak{g}'$ is spanned by $[X,Y]$, $[X,Z]$ and $[Y,Z]$, which must be linearly independant, and $[X,Y]$ and $[X,Z]$ span the range of $\text{adj}_X$. This also implies that $[X,M] = 0$, then $M$ is in the span of $X$. We claim that we can choose $X$ such that $\text{adj}_X$ has a non-zero eigenvalue. If all the eigenvalues of $X$ are non-zero, applying the Jordan canonical form theorem we conclude that there is a basis $X_0, Y_0$, and $Z_0$ such that $[X,X_0] = 0$, $[X,Y_0] = X_0$, and $[X,Z_0] = Y_0$. By rescaling $X$, we may actually assume $X = X_0$, in which case $[Y_0,X_0] = -[X_0,Y_0] = -X_0$, so that $\text{adj}_{Y_0}$ has a non-zero eigenvalue.

If $\text{adj}_X$ has one non-zero eigenvalue, it must actually have two non-zero eigenvalues which are the negations of one another (because the trace of the adjoint is zero), and therefore $\text{adj}_X$ is diagonalizable. Thus we extend $X$ to a basis $X,Y,Z$ with $[X,Y] = \lambda Y$, $[X,Z] = -\lambda Z$, and $X,Y$, and $Z$ are a basis of the space. To fully describe the structure of the algebra, we need to determine $[Y,Z]$. Note that
%
\[ [X,[Y,Z]] = [[Z,X],Y] + [[X,Y],Z] = \lambda [Z,Y] + \lambda [Y,Z] = 0 \]
%
Which implies $[Y,Z]$ is a non-zero scalar multiple of $X$. We may assume that $X = [Y,Z]$ by rescaling $Y$. Finally, by rescaling $X$, we can change the value of $\lambda$ to an arbitrary value, and this shows that the Lie algebra structure is unique, because we have specified all structural constants exactly. For $\lambda = 2$, we obtain the special linear group constants we found above.
\end{proof}







































A useful (and standard) basis for $\mathfrak{gl}_n(K)$ are the matrices $E_{ij}$, which are only non-zero on row $i$ and column $j$, where the matrix coefficient has value 1. We note that
%
\[ [E_{ij}, E_{kl}] = \delta_j^k E_{il} + \delta_i^l E_{kj} \]
%
If we define $H_k = E_{kk} - E_{k+1\ k+1}$, then the $H_k$, together with the $E_{ij}$ for $i \neq j$, span $\mathfrak{sl}_n$, and
%
\[ [H_k, E_{ij}] = [E_{kk}, E_{ij}] - [E_{k+1\ k+1}, E_{ij}] = (\delta_k^i + \delta_k^j - \delta_{k+1}^i - \delta_{k+1}^j) E_{ij} \]
%
\[ [H_i, H_j] = [H_i, E_{jj}] - [H_i, E_{j+1\ j+1}] = 2(\delta_i^{j+1} - \delta_{i+1}^j) E_{jj} \]
%
Thus $E_{ij}$ is an eigenvector for the {\bf adjoint} map $\text{adj}(X)$

So $E_{ij}$ is an eigenvector for the {\bf adjoint} map $\text{adj}(X)$, defined by $\text{adj}(X)(Y) = [X,Y]$, where $X = H_k$.