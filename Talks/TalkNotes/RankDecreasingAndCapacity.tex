% LaTeX template for Summer Schools
%
% !!!!!!!!!!!!   INSTRUCTIONS  !!!!!!!!!!!!!!

% 0) Submit your work in tex format (not pdf). It will be pasted into the proceedings file.
%
% 1) please name your file yourlastname.tex, e.g. thiele.tex.
%
% As several files will be concatenated, please 
% use some discipline as to the following:
%
% 2) Whenever you use \label{} to get automatic cross references
% through \ref{} (this is the preferred option for cross references)
% please add your initials to the label such as 
% \label{SLEct} for the Stochastic Loewner equation
% with initials of author Christoph Thiele
% Same with other citations such as in bibitem.
%
% 3) Please STRONGLY avoid using \def or \newcommand unless really necessary.
% We do have macros for black board bold. If you use your favorite
% macros while preparing your summary, please expand them
% (replace by the original definition) everywhere in your file.
% This will save me the work of doing the very same thing.
% Thank you! If you have to use \def, please also add your
% initials to the definition.
%
% 4) if you want to compile the header of the document, 
% uncomment remove the corresponding paragraph signs below
%
% 5) There is a sample lecture below. For the header 
% it is best to keep most of the
% commands and just change the name, title, text. etc
%
% 6) Please follow the conventions below in terms of capitalization of 
% headings etc:
% Only the beginning of a sentence and names are capitalized.


\documentclass[12pt]{article}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{esint}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{comment}
\usepackage{bbm,dsfont}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{proposition}[theorem]{Proposition}

\newcommand{\talktitle}[1]{\section{#1}}
\newcommand{\talkafter}[1]{\textbf{After #1} \addcontentsline{toc}{subsection}{after #1}}
\newcommand{\talkspeaker}[2]{\begin{center}
\textit{A summary written by #1}
\end{center}
\addcontentsline{toc}{subsection}{#1, #2}
}

\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[hidelinks]{hyperref}

\newcommand*{\Z}{\mathbb{Z}}
\newcommand*{\Q}{\mathbb{Q}}
\def\C{\mathbb{C}}
\newcommand*{\N}{\mathbb{N}}
\newcommand*{\R}{\mathbb{R}}

% Absolute values and norms using mathtools.
% \\[lr][vV]ert produces correct spacing, as opposed to | and \|.
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}

\title{Brascamp--Lieb inequalities}

% \author{Summer School, Kopp}
% \thanks{supported by Hausdorff Center for Mathematics, Bonn}

% \date{September 2021}



\begin{document}

% \maketitle

% {
% \center{Organizers:}
% \center{
% Christoph Thiele, Universit\"at Bonn}
% \center{
% Pavel Zorin-Kranich, Universit\"at Bonn}
% \center{$\ $}
% }
% \newpage
% \tableofcontents

% \newpage


\talktitle{Capacity of Rank Decreasing Operators}
\talkafter{Gurvits \cite{gurv2004}}
\talkspeaker{Jacob Denson}{University of Madison, Wisconsin}

\setcounter{equation}{0}
\setcounter{theorem}{0}

\begin{abstract}
{ We give a simple necessary and sufficient condition on
the length of the gaps corresponding to one dimensional periodic
Schr\"odinger operators. }
\end{abstract}

% Operator Scaling Preserves the Rank Non-Decreasing Property
% Polynomial Identity Testing

Recall the classical result of Lieb (Lieb1990), which shows that a Brascamp-Lieb inequality of the form
%
\[ \int_{\mathbf{R}^n} \prod_{j = 1}^m |f_j(B_j x)|^{p_j}\; dx \leq \text{BL}(B,p) \cdot \prod_{j = 1}^m \| f_j \|_{L^1(\mathbf{R}^{n_j})}^{p_j} \]
%
holds over all choices of functions $f_1, \dots, f_m \in L^1(\mathbf{R}^{n_j})$ if and only if it holds whenever $f_1, \dots, f_m$ are Gaussian functions centered at the origin, and the optimum constant $\text{BL}(B,p)$ in the inequality is the same in both settings. If, for each $i$, we set $f_i(x) = e^{- \pi x^T X_i x}$, where $X_i$ is a positive definite matrix, then both sides of the inequality become the integrals of Gaussian, which can be easily calculated over all inputs, from which we find that
%
\begin{equation} \label{BrascampLiebSupremum}
    \text{BL}(B,p)^2 = \sup_{X_1, \dots, X_m \succ 0} \frac{\det \left( \sum_{j = 1}^m p_j B_j^* X_j B_j \right)}{\prod_{j = 1}^m \det(X_j)^{p_j}}.
\end{equation}
%
Here, and in the rest of this summary, for a square matrix $A$ we use the notation $A \succ 0$ and $A \succeq 0$ to communicate that $A$ is positive definite and positive semidefinite respectively. Another well known result of Bennett, Carbery, Christ, and Tao (BCCT08), shows that $\text{BL}(B,p)$ is finite if and only if $n = \sum_{j = 1}^m p_j n_j$, and for any subspace $V \subset \mathbf{R}^n$,
%
\begin{equation} \label{BrascampLiebMappingProperty}
    \dim(V) \leq \sum_{j = 1}^m p_j \dim(B_j V).
\end{equation}
%
Thus the finiteness of \eqref{BrascampLiebSupremum} acts as a guarantee for the mapping properties of the matrices $B_1, \dots, B_m$ given in \eqref{BrascampLiebMappingProperty}, and vice versa.

In these notes, we focus on an analogous result, emerging from a setting studied in combinatorial optimization and quantum information theory, originally investigated with the main goal of obtaining efficient algorithms for various problems emerging in these settings. By studying the connection between this setting, and the study of Brascamp-Lieb inequalities, (GGOW18) were able to find efficient algorithms which can approximate the Brascamp-Lieb constant to arbitrary precision, a fact that has important theoretical consequences to the theory of Brascamp-Lieb inequalities independent of practical application to computation of particular Brascamp-Lieb constants.

\section{Capacity of Rank-Decreasing Operators}

This connection is more fully explored in the subsequent talk. In this summary, we introduce the setting studied in computing science, only noting some similarities to Brascamp Lieb. Let $M(N)$ denote the space of all $N \times N$ complex-valued matrices. The main object of study here are the \emph{positive operators} $T: M(N) \to M(N)$, i.e. linear transformations between spaces of matrices such that if $X \succeq 0$, then $T(X) \succeq 0$.  There is a rich theory of such maps, connected to representation theory and the theory of free probability (see Bhatia for more information). A useful example to keep in mind, given any matrices $B_1,\dots, B_m$ and $p_1,\dots,p_m > 0$, is the positive operator
%
\begin{equation} \label{completelypositiveoperator}
    T(X) = \sum_{j = 1}^m p_j B_j^* X B_j
\end{equation}
%
With any positive operator $T$, we associate a statistic known as the \emph{capacity} of $T$, defined by setting
%
\begin{equation} \label{capacityequation}
    \text{Cap}(T) = \inf_{X \succ 0} \left( \frac{\det(TX)}{\det(X)} \right).
\end{equation}
%
The connection between the study of this statistic and Brascamp-Lieb may be hinted at by comparing \eqref{BrascampLiebSupremum} to \eqref{capacityequation} when $T$ is of the form given in $\eqref{completelypositiveoperator}$. A useful property of a positive operator $T$ is that it is \emph{rank non-decreasing}, i.e. for any $X \succeq 0$,
%
\begin{equation} \label{ranknondecreasing}
    \text{Rank}(TX) \geq \text{Rank}(X).
\end{equation}
%
As we noted above, condition \eqref{capacityequation} is similar to \eqref{BrascampLiebSupremum}. And condition \eqref{ranknondecreasing} seems roughly similar to \eqref{BrascampLiebMappingProperty}. And indeed, one has an equivalence between \eqref{capacityequation} and \eqref{ranknondecreasing}, analogous to the situation in the theory of Brascamp Lieb inequalities.

\begin{theorem}\cite{gurv2004} \label{rankdecreasingcapacitytheorem}
    $T$ is rank non-decreasing if and only if $\text{Cap}(T) > 0$.
\end{theorem}
\begin{proof}
    A simple family of positive operators are those of the form
    %
    \begin{equation} \label{basicoperator}
        TX = X_{11} A_1 + \dots + X_{NN} A_N,
    \end{equation}
    %
    where $A_1,\dots,A_N \succeq 0$. For such an operator, we can write
    %
    \[ \text{Cap}(T) = \inf_{\gamma_1, \dots, \gamma_N > 0} \left\{ \frac{\det \left( \sum_{j = 1}^N \gamma_j A_j \right)}{\gamma_1 \cdots \gamma_N} \right\}. \]
    %
    Results from a previous pair of papers by Gurvits prove Theorem \ref{rankdecreasingcapacitytheorem} in the special case of an operator defined by \eqref{basicoperator}. Assuming this, we will show how the general case can be reduced to this case.

    For each orthonormal basis $U = \{ u_1, \dots, u_N \}$, we define the decoherence operator $D_U(X) = \sum \langle Xu_i, u_i \rangle \cdot u_i u_i^*$, and then consider
    %
    \[ T_U = (T \circ D_U)(X) = \sum_{i = 1}^N \langle Xu_i, u_i \rangle \cdot T(u_i u_i^*). \]
    %
    This operator is, up to a change of basis in $M(N)$, the same as one of the operators described in the above paragraph. This we know $T_U$ is rank non-decreasing if and only if $\text{Cap}(T_U) > 0$. The proof follows from the following two properties:
    %
    \begin{enumerate}
        \item $T$ is rank non-decreasing if and only if $T_U$ is rank non-decreasing for all bases $U$.
        \item $\text{Cap}(T) = \inf_U \text{Cap}(T_U)$.
    \end{enumerate}
    %
    Assuming these properties, if $\text{Cap}(T) > 0$, then Property 2 implies $\text{Cap}(T_U) > 0$ for all $U$, so $T_U$ is rank non-decreasing for all $U$, and thus Property 1 implies $T$ is rank non-decreasing. The converse is similar.

    The proof of Properties 1 and 2 both rely on a simple trick. Given any $X \succeq 0$, we can find an orthonormal basis $U$ diagonalizing $X$, and then for such $U$ we have $T(X) = T_U(X)$. This immediately implies $T$ is rank non-decreasing if $T_U$ is rank non-decreasing for all $U$. The converse follows because $D_U$ is rank non-decreasing for all $U$ (all \emph{doubly stochastic} operators are rank non-decreasing, a family of operators we will study shortly). To show that $\text{Cap}(T) = \inf_U \text{Cap}(T_U)$, we note that for any $\gamma_1,\dots, \gamma_N$,
    %
    \begin{equation} \label{UCapacityUpperBound}
        \frac{\det \left( \sum_{j = 1}^N \gamma_j T(u_j u_j^*) \right)}{\gamma_1 \dots \gamma_N} = \frac{\det \left( T \left( \sum_{j = 1}^N \gamma_j \cdot u_j u_j^* \right) \right)}{\det \left( \sum_{j = 1}^m \gamma_j \cdot u_j u_j^* \right)} \geq \text{Cap}(T).
    \end{equation}
    %
    Taking infima over all $\gamma_1,\dots,\gamma_N$, and then over all $U$, gives $\inf_U C_U(T) \geq \text{Cap}(T)$. Conversely, given any $X \succeq 0$, pick $U$ with $T(X) = T_U(X)$, and then
    %
    \begin{equation} \label{UCapacityLowerBound}
        \frac{\det(TX)}{\det(X)} = \frac{\det(T_U X)}{\det(X)} \geq C_U(T) \geq \inf_U C_U(T).
    \end{equation}
    %
    Taking infima over all $X$ gives $\text{Cap}(T) \geq \inf_U C_U(T)$. Combining these two calculations gives $\text{Cap}(T) = \inf_U C_U(T)$, which proves Property 2.
\end{proof}

\section{Operator Scaling}

Being defined by a non-convex optimization procedure, it is nonobvious to explicitly compute the capacity of a positive operator. However, the techniques described in this section show that for each $\varepsilon > 0$, given a positive operator $T$ (with rational entries described using $b$ bits), there is an algorithm to compute a value $\text{Cap}_{\text{approx}}(T) \in [0,\infty)$ in $\lesssim (b \log(1/\varepsilon))^{O(1)}$ steps, such that
%
\[ \text{Cap}(T) \leq \text{Cap}_{\text{approx}}(T) \leq (1 + \varepsilon) \cdot \text{Cap}(T). \]
%
The tool allowing us to compute capacity is \emph{operator scaling}, introduced in this section.

Given any nonsingular matrices $A,B \in M(N)$, and any positive operator $T$, we define a \emph{scaled operator} $T_{AB}(X) = B \cdot T(AXA^*) \cdot B^*$. Then for any $X \succeq 0$, if $Y = AXA^*$, then
%
\[ \frac{\det(T_{AB}X)}{\det(X)} = \det(B)^2 \frac{\det(T(AXA^*))}{\det(X)} = \det(A)^2 \det(B)^2 \frac{\det(T(Y))}{\det(Y)}. \]
%
Taking infima over both sides of the equation gives
%
\[ \text{Cap}(T_{AB}) = \det(A)^2 \det(B)^2 \cdot \text{Cap}(T). \]
%
Thus operator scaling changes the capacity in a controlled manner.

The next key step is to consider \emph{doubly stochastic} matrices. The space of matrices $M(N)$ is naturally an inner product space, with the inner product $(X,Y) \mapsto \text{Tr}(XY^*)$. Thus given any positive operator $T: M(N) \to M(N)$, we can consider an adjoint operator $T^*: M(N) \to M(N)$. A positive operator $T$ is called \emph{doubly stochastic} if $T(I) = I$ and $T^*(I) = I$, where $I \in M(N)$ is the identity matrix. The reason these operators are interesting to us is that the capacity of any doubly stochastic operator is equal to one.

\begin{lemma} \label{doublystochasticcapacitytheorem}
    If $T$ is doubly stochastic, then $\text{Cap}(T) = 1$.
\end{lemma}
\begin{proof}
    Since $T(I) = I$, it is clear that $\text{Cap}(T) \leq 1$. Conversely, for any doubly stochastic $T$, and any $X \succeq 0$, $\det(T(X)) \geq \det(X)$. The verification of the second fact is somewhat involved, reducing the claim to the study of bistochastic matrices and then applying a theorem from the theory of such matrices. The proof can be found in [Watrous, Theory of Quantum Information (Page 250)], with the relevant background information in bistochastic matrices found in [Serre, Matrices, Page 156].
%    Indeed, for any such $X \succeq 0$, we can pick orthonormal bases $U = \{ u_1, \dots, u_N \}$ and $V = \{ v_1, \dots, v_N \}$ diagonalizing $X$ and $T(X)$, with corresponding eigenvalues $\lambda = (\lambda_1,\dots,\lambda_N)$ and $\gamma = (\gamma_1, \dots, \gamma_N)$. Then
    %
%    \[ \gamma_i = \sum_{j = 1}^n \lambda_j v_i^* T(u_j u_j^*) v_i, \]
    %
%    so if we define $A_{ij} = v_i^* T(u_j u_j^*) v_i$ then $A\lambda = \gamma$. The fact that $T$ is doubly stochastic implies that the \emph{matrix} $A$ is doubly stochastic, i.e. it is non-negative and the rows and columns of $A$ sum to one. But this means that $\gamma \geq \lambda$, which implies that $\det(T(X)) \geq \det(X)$.
\end{proof}

Thus, given an operator $T$, if we can find nonsingular $A,B \in M(N)$ such that $T_{A,B}$ is nonsingular, then it follows from Theorem \ref{doublystochasticcapacitytheorem} that $\text{Cap}(T) = \det(A)^2 \det(B)^2$. This is not quite possible to do for all $T$ with $\text{Cap}(T) > 0$ (it is only possible if the infinum defining $\text{Cap}(T)$ is attained by a particular input). Nonetheless, if $\text{Cap}(T) > 0$ then we can find doubly stochastic operators approximating $T$ arbitrarily closely. The capacity is a continuous function of the operator input, so this is good enough to approximate $\text{Cap}(T)$ rather than calculating the capacity exactly.

We remark that, before the theory of capacity was applied to the theory of Brascamp-Lieb in 2018, a variant of operator scaling was already being used in the theory. BLOOP GEOMETRIC BRASCAMP LIEB.

Our approximation algorithm is quite simple, an application of a similar method first used by Sinkhorn 

\begin{thebibliography}{3}

\bibitem{gurv2004}
    Leonid Gurvits,
    \emph{Classical Complexity and Quantum Entanglement},
    J. Comput. Syst. Sci. {\bf 69} (2004), 448--484.

\end{thebibliography}

\noindent \textsc{Jacob Denson, UW Madison}\\
\textit{email:} \texttt{jcdenson@wisc.edu}

\end{document}