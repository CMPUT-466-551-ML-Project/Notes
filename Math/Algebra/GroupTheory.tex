\input{../../style.tex}

\title{Group Theory}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}
\maketitle
\tableofcontents
\pagenumbering{arabic}

\chapter{What is Abstract Algebra?}

In mathematics, one often focuses on certain families of objects with a family of properties. In order to understand this property, it is natural to consider transformations between these objects which preserve the properties of the objects.

\begin{example}
In Euclidean Geometry, rotations and translations preserve the angles between lines and distances between points. If we only care about such properties, we can translate and rotate any particular figure to a canonical form to simplify the situation; a triangle can always be rotated and translated to one side lies horizontally, and if we don't care about distances, we can dilate space so that one line of the triangle has length one.
\end{example}

The technique of applying symmetries to simplify situations occurs over and over in mathematics, and so it is important to classify the general tools we can use when we meet new objects, and wish to understand their symmetries. The general study of symmetries in mathematics is known as {\it group theory}. Due to its utility, the theory provides a foundation to many rich mathematical theories.

Let us reconsider symmetries from a more abstract viewpoint. One of the basic objects of mathematics is the function, which transforms elements of some set $A$ into elements of another set $B$. If the function is denoted $f$, which we often introduce using the abbreviated notation $f: A \to B$, then the $b$ associated to an $a$ is denoted $f(a)$. Given another map $g: B \to C$, we may consider the \emph{composition map} $g \circ f: A \to C$, which maps a point $a \in A$ to $g(f(a))$ -- that is, if $a$ is mapped to $b$ by $f$, and $g$ maps $b$ to $c$ then $g \circ f$ maps $a$ to $c$. A pleasant algebraic fact about the composition is that it satisfies the \emph{associative property}. Given a third map $h:C \to D$, we find that $h \circ (g \circ f) = (h \circ g) \circ f$, a relation taken for granted when we forget parenthesis and write $h \circ g \circ f$. The first idea leading to abstract algebra is that we can identify a {\it functional definition} of the identity map with an {\it algebraic definition} involving a series of algebraic relations with respect to composition. A key idea of group theory is that we can study the functional properties of symmetries by looking at the compositional properties of maps without losing essential information.

\begin{example}
    On each set $B$ we have an \emph{identity map} $\text{id}_B: B \to B$, such that $\text{id}(b) = b$ for each $b \in B$. For any $g: A \to B$ and $h: B \to C$, we find $\text{id}_B \circ g = g$, and $h \circ \text{id}_B = h$. If $f: B \to B$ is {\it any} map satisfying $f \circ g = g$ and $h \circ f$ for any $g$ and $h$, then $f$ is {\it equal} to the identity map, since $f = f \circ \text{id}_B = \text{id}_B$. Thus an `identity map' is just an idempotent element with respect to composition.
\end{example}

\begin{example}
    If a function $f: A \to B$ is bijective, then there is $f^{-1}: B \to A$, defined by mapping an element $b$ to the unique element $a$ with $f(a) = b$. We find that $f^{-1} \circ f = \text{id}_A$, and $f \circ f^{-1} = \text{id}_B$. If $g$ is any function such that $g \circ f = \text{id}_A$ and $f \circ g = \text{id}_B$, then
    %
    \[ g = g \circ \text{id}_B = g \circ f \circ f^{-1} = \text{id}_A \circ f^{-1} = f^{-1} \]
    %
    Thus the inverse of a map is precisely one which composes with the map to give the identity map.
\end{example}

Again, we see that functions can be identified by algebraic relations with respect to the composition operator. Abstract algebra is the mathematical field whose goal is to study mathematical objects via an understanding of the algebraic relations of operations on that set, with the hope that less obvious properties of the object will be unvealed via the underlying algebraic properties. In the case of the theory of functions, the operator studied is composition. In the theory of classical algebra, the operators studied are addition, multiplication, subtraction, and division. The key realization of abstract algebra is that it is often more simple to discuss arbitrary, `abstract operators' satisfying certain properties, for then we need not deal with the minutiae which occurs which studying the set theoretic aspects of functions. In these notes, we talk about a specific class of objects which generalizes the algebraic properties of a set of invertible functions from a set to itself. These objects are known as \emph{groups}.

Let us consider what properties the class of functions under composition should satisfy. Let $X$ be a set, and let $\circ: X \times X \to X$ be an abstract `composition function' on $X$. This means exactly that, given two objects $x,y \in X$, we may consider their composition $x \circ y \in X$. Now assume that $\circ$ satisfies the associative law $x \circ (y \circ z) = (x \circ y) \circ z$ for any three $x,y,z \in X$; This fact is no longer always true because our composition operation isn't necessarily a normal function composition operation. Elements of $X$ need not even be functions. An `identity' in $X$ can then be defined to be an element $e \in X$ such that $e \circ x = x \circ e = x$ for all $x \in X$. We may then define an `inverse' of an element $x \in X$ to be an element $y \in Y$ such that $x \circ y = y \circ x = e$. The element $y$ is rarely denoted by anything other than $x^{-1}$, to parallel the set theoretic notation. Thus if $\circ$ is associative and the underlying set has an identity, then the resulting pair $(X, \circ)$ imitates a subset of functions from a set to itself, which is closed under composition. We call the pair $(X, \circ)$ a \emph{monoid}. If every element of $X$ is invertible, then $(X, \circ)$ imitates a set of invertible functions from a set to itself, closed under inversion, and we call this pair a \emph{group}. Since symmetries can often be described as families of invertible functions, group theory describes the tools to understand these families. If the operation is obvious, we often abuse notation and just say that $X$ is a group. To be even more brief, the symbol for the operation is often ignored as well, so we write $xy$ for the composition $x \circ y$ of two elements.

\begin{example}
    Consider a topological space $X$. Then the set of all continuous functions from $X$ to itself forms a monoid, and the set of all homeomorphisms from $X$ to itself forms a group. This follows directly because if $f$ and $g$ are continuous, then $g \circ f$ is continuous, and the identity function is certainly continuous. If the space has a fixed metric, then the space of all isometries of the space forms a group as well.
\end{example}

\begin{example}
    The set of all linear maps from a vector space $V$ to itself forms a monoid. The set $\mathrm{GL}(V)$ of all invertible linear maps froms a group, known as the \emph{general linear group}. If $V$ has finite-dimension $n$, then we may essentially identify linear endomorphisms on $V$ with the set of all $n\times n$ matrices $M_n(k)$ with entries in the scalar field $k$ upon which $V$ is defined, which can be viewed as a set with the abstract composition operation
    %
    \begin{align*}
        \begin{pmatrix} a_{11} & \dots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{n1} & \dots & a_{nn} \end{pmatrix} & \begin{pmatrix} b_{11} & \dots & b_{1n} \\ \vdots & \ddots & \vdots \\ b_{n1} & \dots & b_{nn} \end{pmatrix}\\
        &= \begin{pmatrix} a_{11}b_{11} + \dots + a_{1n}b_{n1} & \dots & a_{11}b_{1n} + \dots + a_{1n}b_{nn} \\ \vdots & \ddots & \vdots \\ a_{n1}b_{11} + \dots + a_{nn}b_{n1} & \dots & a_{n1}b_{1n} + \dots + a_{nn}b_{nn} \end{pmatrix}
    \end{align*}
    %
    Then the matrix $I$ with ones on the diagonal operates as an identity, and $\mathrm{GL}(V)$ can be identified with the subfamily of matrices
    %
    \[ \mathrm{GL}_n(k) = \{ M \in M_n(k) : MN = NM = I\ \text{for some}\ N \in M_n(k) \}. \]
    %
    This family is also called the general linear group.
\end{example}

\begin{example}
    Certain vector fields $X: U \to \mathbf{R}^n$ on open subsets of Euclidean space induce `one parameter groups' $\phi: U \times \mathbf{R} \to U$ (where the image of $(x,t)$ is denoted $\phi_t(x)$, which we can view as a `parameterized' family of maps), which satisfy the differential equation
    %
    \[ \frac{d \phi_t(x)}{dt} = X_{\phi_t(x)} \]
    %
    and also satisfy $\phi_t \circ \phi_s = \phi_{t + s}$, and $\phi_0 = \text{id}_U$, so this set of functions really is a group. The study of differential equations is really just the study of the relationship between smooth vector fields and the one-parameter groups of diffeomorphisms they generate, especially in certain particular situations.
\end{example}

\begin{example}
    Combinatorics and algebra intertwine when we study finite groups. The classical finite group is the class of all bijective maps from a set containing $n$ elements to itself. This is the \emph{symmetric group} of order $n$, denoted $S_n$. The group contains $n!$ elements, because an arbitrary permutation $\pi: [n] \to [n]$ can be obtained by first choosing $\pi(1) \in [n]$ (for which we have $n$ choices), then choosing $\pi(2) \in [n] - \pi(1)$, and so on and so forth. More generally, if $X$ is a set, we can consider the group of bijections on $X$, denoted $\text{Sym}(X)$. Using cycle notation, we denote the permutation $\pi$ satisfying $\pi(a_1) = a_2$, $\pi(a_2) = a_3, \dots, \pi(a_m) = a_1$, and fixing all other elements by $(a_1\ a_2\ \dots\ a_m)$. We shall find that all permutations on a finite set have a unique cycle decomposition, when we discuss the symmetric group in more detail later.
\end{example}

\begin{example}
    The integers $\ZZ$ form a group under addition, with the inverse of an integer $n$ being $-n$. We can also consider the additive group $\ZZ_n$ of integers modulo $n$, which form a group modulo $n$. But most interestingly, we can combine the study of addition and multiplication by studying the multiplicative group $\ZZ_n^*$ of integers modulo $n$ which are \emph{relatively prime} to $n$. Thus group theory has many applications to number theory.
\end{example}

\begin{example}
    We can often form groups by abstractly defining relations between objects in a set. For instance, consider the set consisting of 8 symbols
    %
    \[ Q = \{ \pm 1, \pm i, \pm j, \pm k \}. \]
    %
    We can then try and find a composition operation $Q \times Q \to Q$ which makes $Q$ into a group. It is natural to expect that $(-1)i = -i$, $(-1)(-i) = 1$, and so on and so forth. Less trivially, we also want $i^2 = j^2 = k^2 = ijk = -1$. We obtain the multiplication table, with the element in the row labelled $x$ and column labelled $y$ giving the value of $xy$.
    %
    \begin{center}
    \begin{tabular}{| c | c c c}
        & $i$ & $j$ & $k$ \\ \hline $i$ & $-1$ & $k$ & $-j$ \\ $j$ & $-k$ & $-1$ & $i$ \\ $k$ & $j$ & $-i$ & $-1$
    \end{tabular}
    \end{center}
    %
    One can check that the induced operation is associative. The group $Q$ is known as the \emph{quaternion} group. It is a particular subset of the quaternions, which are expressions of the form $a + bi + cj + dk$ with $a,b,c,d \in \mathbf{R}$, which provide an algebraic model for the set of all rotations in three dimensional space. Thus, even though abstractly defined, $Q$ can still be interpreted in a meaningful way as a symmetry on a group.
\end{example}

\chapter{Basic Properties of Groups}

Now we shall start the general theory of groups, starting with the theory of `manipulating equations', of which every student of compulsory education should be very familiar. Consider a finite sequence of elements $x_1, \dots, x_n$ in a monoid $X$. Then the `pi' notation for multiplication is introduced, defined recursively by setting
%
\[ \prod_{j = i}^n x_j = \left( \prod_{j = i}^{n-1} x_j \right) x_n\ \ \ \ \ \ \prod_{j = i}^i x_j = x_i \]
%
It is a convention that if $k > i$, then $\prod_{j = k}^i x_j = e$. The similarity to $\Sigma$ notation used in arithmetical sums is intentional, and the two definitions correspond in the monoid $(\mathbf{Z}, +)$. Ultimately, the property of associativity means brackets in an equation are irrelevant. For instance, for any $a,b,c,d,e$, we have
%
\[ ((ab) c) (de) = a ((b (cd)) (e)). \]
%
Thus the expression $abcde$ is unambiguous. We prove this rigorously, and then dodge the use of brackets in the rest of these notes, except in emphasizing components of equations.

\begin{theorem}
Let there be given an associative operation on $S$, and a finite sequence $(x_1, \dots, x_n)$ of elements in $S$. Then, for any integer $1 \leq l < n$,
%
\[ \left( \prod_{k=1}^l x_k \right) \left( \prod_{k=l+1}^n x_k \right) = \prod_{k=1}^n x_k \]
\end{theorem}
\begin{proof}
    We prove by induction on $n$, the number of elements in the sequence $(x_1, \dots, x_n)$. When $n = 1$, the statement is obvious by definition. We now proceed inductively. If we are now given $n$ elements $(x_1, \dots, x_n)$, and an integer $1 \leq l < n$, then
    %
    \begin{align*}
        \left( \prod_{k=1}^l x_k \right) \left( \prod_{k=l+1}^n x_k \right) &= \left( \prod_{k=1}^l x_k \right) \left( \left( \prod_{k=l+1}^{n-1} x_k \right)\ x_n \right)\\
        &= \left( \prod_{k=1}^l x_k \prod_{k=l+1}^{n-1} x_k \right)\ x_m = \left( \prod_{k=1}^{n-1} x_k \right)\ x_m = \prod_{k=1}^n x_k
    \end{align*}
    %
    By induction, this statement holds for all values of $n$.
\end{proof}

The power of commutativity is that, given an associative and commmutative operation, we can permute any elements in an equation. Let us rigorously prove this.

\begin{theorem}
    For any finite sequence of elements $(x_1, \dots, x_n)$ from a set upon which an associative and commutative assignment is defined, and for any permutation $\pi \in S_n$,
    %
    \[ \prod_{k=1}^n x_k = \prod_{k=1}^n x_{\pi(k)} \]
\end{theorem}
\begin{proof}
    We again prove by induction on the number of elements in the sequence. When the number of elements is one, the statement is obvious; the only permutation of one element is the identity permutation, which changes nothing. Now suppose, by induction that the statement is true for an arbitrary permutation of $n-1$ elements. Let $(x_1, \dots, x_n)$ be a sequence of elements, and $\pi$ a permutation of the numbers in the range $1$ to $n$. Let $m$ be the number such that $\pi(n) = m$. The following calculation shows we can move $x_m$ to the end of the product.
    %
    \[ \prod_{k=1}^n x_k = \left( \prod_{k=1}^{m-1} x_k \right) \left( x_m \prod_{k=m+1}^n x_k \right) = \left( \left( \prod_{k=1}^{m-1} x_k \right) \left( \prod_{k=m+1}^n x_k \right) \right)\ x_m \]
    %
    The permutation which swaps the remaining $n-1$ elements really does only swap $n-1$ elements, hence by induction the equality is obtained, and we find that it is possible to reorder finite sequences of elements arbitrarily.
\end{proof}

So now we have seen proofs of facts intuitively obvious from a elementary school education. Of course, our main source of inspiration behind the concept of a group is a collection of invertible functions. A \emph{group of functions} $G$ on a set $X$ is a collection of bijections of $X$ which is closed under composition and inversion. Before we start our real work, we should establish that groups are not that much more general than sets of functions. Arthur Cayley is credited with noticing that the synthetic definition really is the same as the intutive one, so that our algebraic relations uniquely model the theory of bijective functions.

\begin{theorem}
    Any synthetic group is equivalent to a group of functions.
\end{theorem}
\begin{proof}
    Let $G$ be a synthetic group. For each $g \in G$, consider the function $g_*:G \to G$, defined by $g_*(h) = gh$. Then $g_*$ is a bijective function, for it has an inverse $(g^{-1})_*$. The transformation $g \mapsto g_*$ `preserves' the operation of the group, for $(gh)_* = g_* \circ h_*$ so $G_* = \{ g_* : g \in G \}$ really is a group of functions, which is essentially the same group as $G$, for the algebraic equations that occur in the one group are equivalent to the algebraic equations in the other group. We will later make these notions precise by saying $G$ and $G_*$ are {\it isomorphic} by this map.
\end{proof}

Thus we are back where we started. We have the abstract group theory at our tool belt, but when all is said and done, we really are just discussion groups of transformations. The formalism gives us abstract insight into these groups, but we aren't abstracting for an arbitrary reason, since every group can be considered as a concrete set of functions. The following properties are trivial for groups of functions, and thus also hold for general groups by Cayley's theorem.
%
\begin{itemize}
    \item Any group has a unique identity element.
    \item The inverse of any group element is uniquely determined.
    \item If $gh = e$ or $hg = e$, then $h = g^{-1}$.
\end{itemize}
%
Thus we have an algebraic system which describes precisely the semantics of families of invertible functions.

\begin{comment}
\begin{theorem}
    Let $G$ be a semigroup, with an element $e$ such that $eg = g$ for all $g \in G$, and suppose that every $g$ has a left inverse $h$ such that $hg = e$. Then $G$ is a group, and $e$ is the identity.
\end{theorem}
\begin{proof}
    Give $g$ and $h$ as in the theorem, we calculate $hgh = eh = h$, and if we find $k$ such that $kh = e$, we find $gh = egh = khgh = keh = kh = e$. Thus $h$ is also the right identity for $g$, hence a normal inverse. But this means that $ge = gee = geg^{-1}g = gg^{-1}g = g$, so $e$ is a right inverse as well.
\end{proof}
\end{comment}

\section{Subgroups and Cosets}

Often in math the `symmetries' to choose from are not completely obvious, and as we range our symmetries to preserve an increasingly strict set of properties, the number of symmetries we have reduces to a smaller and smaller family. Thus from our group of symmetries we obtain a \emph{subgroup}, a subset of a group whose elements also form a group. Even if we really do care about the entire group, the subgroups of the group enable us to understand what parts of the group are `self contained', which enables us to understand the entire group by the components it contains.

\begin{example}
    Define the special linear group $SL_n(k)$ to be the subset of matrices in the general linear group $GL_n(k)$ with determinant one. The determinant operation $\text{det}: GL_n(k) \to k^*$ satisfies
    %
    \[ \det(MN) = \det(M) \det(N)\ \ \ \ \ \det(M^{-1}) = \det(M)^{-1} \]
    %
    which enables us to easily show $SL_n(k)$ is closed under composition and inversion. We will later see that this is a special case of forming a subgroup from the {\it kernel of a homomorphism}.
\end{example}

\begin{example}
    Let $M$ be a set, and $N$ a subset. Then the set of bijective functions on $M$ that leave elements in $N$ fixed is a subgroup of $S_M$. In some sense, this set of functions is equivalent to $S_{M - N}$ as the elements that are in $N$ can be ignored in the definition of the function.
\end{example}

\begin{example}
    If $G$ is a group, then $G$ is trivially a subgroup of itself. Similarily, the subset $\gen{e} = \{ e \}$ is also a subgroup. These subgroups are known as the \emph{trivial subgroups} of $G$.
\end{example}

\begin{example}
    Consider the group $\ZZ$ of integers under addition, and let $G$ be a subgroup. If $G \neq \gen{0}$, then $G$ must containg a smallest positive integer $n$. The Euclidean algorithm then verifies that any element of $G$ must be a positive multiple of $n$, so that $G = \gen{n}$. To see this, we note that if $m$ is a positive integer in $G$, we can write $m = kn + r$, with $0 \leq r < n-1$. Since $r = m - kn \in G$ and is smaller than $n$, we conclude that $r$ cannot be positive, so $r = 0$, so that $m$ is a multiple of $n$.
\end{example}

For a fixed group $G$, the family of subgroups of $G$ form an interesting lattice structure. The next proposition shows that we can find a greatest lower bound to any set of subgroups of a group.

\begin{prop}
    If $\{ H_\alpha \}$ are subgroups of $G$, then $\bigcap H_\alpha$ is a subgroup of $G$.
\end{prop}
\begin{proof}
    Suppose $a,b \in \bigcap H_\alpha$. Then $a,b \in H_\alpha$ for each index $\alpha$, which means that $ab$ and $a^{-1}$ are in $H_\alpha$ since $H_\alpha$ is a subgroup. But this means that $ab$ and $a^{-1}$ are in $\bigcap H_\alpha$ since $\alpha$ was arbitrary.
\end{proof}

Conversely, let $G$ be a group, and $S$ a subset of elements, we can consider the set $\mathcal{M}$ of all subgroups of $G$ which contain $S$. Of course, $\mathcal{M}$ is non-empty, as $G$ is a subgroup which contains $S$. If we take $\bigcap \mathcal{M}$, then we obtain a group containing $S$, which is contained in every group which contains $S$. This `smallest' group is called the group \emph{generated by} $S$, denoted $\gen{S}$. Equivalently, the generated subgroup is the set of all elements of the form $x_1 x_2 \dots x_n$ where either $x_i$ or $x_i^{-1}$ is in $S$. This is because this set forms a subgroup of $G$, and also every subgroup that contains $S$ conversely must contain these elements. In this way, generators work for groups analogously to how bases work in vector spaces, which are formed by arbitrary sums of the generators.

\begin{example}
    Gaussian elimination shows that every invertible matrix is the product of elementary matrices, so $GL_n(k)$ is generated by the elementary matrices.
\end{example}

\begin{example}
    The integers form a group under addition, which is generated by $1$, because every positive integer $n > 0$ can be written as
    %
    \[ n = 1 + 1 + \dots + 1  \]
    %
    and is thus in the group generated by $1$, and thus $-n$ is also in the group generated by $1$ as well, hence all integers. A group generated by a single element is known as a \emph{cyclic group}.
\end{example}


The greatest lower bound of a set of subgroups of a group is the group generated by the union of the elements of a group. We have a maximal subgroup, which is the entire group, and a minimal subgroup, which is the trivial group $\{ e \}$. Thus the family of subgroups forms a \emph{bounded lattice} under the subgroup operation. We can ofte gain insight into the structure of finite groups by drawing a subgroup lattice; these lattices are essential in applications of group theory to Galois theory. Here are the lattices for $S_3$, and the group of quaternions.

\begin{center}
\begin{tikzpicture}[>=triangle 60]
    \coordinate (A) at (3,2.75);

    \coordinate (B) at (4,1.3);
    \coordinate (C) at (2,1.3);
    \coordinate (D) at (0,1.3);
    \coordinate (E) at (6,1.3);

    \coordinate (B') at (4,0.7);
    \coordinate (C') at (2,0.7);
    \coordinate (D') at (0,0.7);
    \coordinate (E') at (6,0.7);

    \coordinate (F) at (3,-0.7);

    \draw[thick] (A)--(B) node[midway,sloped,below] {};
    \draw[thick] (A)--(C) node[midway,sloped,below] {};
    \draw[thick] (A)--(D) node[midway,sloped,below] {};
    \draw[thick] (A)--(E) node[midway,sloped,below] {};

    \draw[thick] (F)--(B') node[midway,sloped,below] {};
    \draw[thick] (F)--(C') node[midway,sloped,below] {};
    \draw[thick] (F)--(D') node[midway,sloped,below] {};
    \draw[thick] (F)--(E') node[midway,sloped,below] {};

    \draw (3,3) node {$S_3$};

    \draw (4,1) node {$\gen{(2\ 3)}$};
    \draw (2,1) node {$\gen{(1\ 3)}$};
    \draw (0,1) node {$\gen{(1\ 2)}$};
    \draw (6,1) node {$\gen{(1\ 2\ 3)}$};

    \draw (3, -1) node {$\{ e \}$};
\end{tikzpicture}
\begin{tikzpicture}[scale=0.7]
    \coordinate (Z) at (10,2.75);

    \coordinate (J) at (10,1.5);
    \coordinate (K) at (12,1.5);
    \coordinate (I) at (8,1.5);

    \coordinate (J') at (10,0.6);
    \coordinate (K') at (12,0.6);
    \coordinate (I') at (8,0.6);

    \coordinate (Q) at (10,-0.7);
    \coordinate (Q') at (10,-2.1);
    \coordinate (Q'') at (10,-1.3);

    \draw[thick] (Z)--(J) node[midway,sloped,below] {};
    \draw[thick] (Z)--(K) node[midway,sloped,below] {};
    \draw[thick] (Z)--(I) node[midway,sloped,below] {};

    \draw[thick] (J')--(Q) node[midway,sloped,below] {};
    \draw[thick] (K')--(Q) node[midway,sloped,below] {};
    \draw[thick] (I')--(Q) node[midway,sloped,below] {};

    \draw[thick] (Q'')--(Q') node[midway,sloped,below] {};

    \draw (10,3.25) node {$Q$};

    \draw (10,1) node {$\gen{j}$};
    \draw (8,1) node {$\gen{i}$};
    \draw (12,1) node {$\gen{k}$};
    \draw (10,-1) node {$\gen{-1}$};

    \draw (10, -2.5) node {$\{ 1 \}$};
\end{tikzpicture}
\end{center}

We can gain a deeper understanding of the relations between elements of a group, because a subgroup neatly contains all possible algebraic structure between the elements trapped in the subgroup. A natural question is how much we can obtain about the relations obtained by composing elements outside of a subgroup with elements inside the subgroup. We cannot hope to understand this question by studying the subgroup as an isolated object, so we must see the subgroup as part of the overall group. One tool for understanding this containment is by studying cosets, which break apart the group by its relations with elements of a subgroup.

Let $H < G$. Define an equivalence relation $\sim$ on $G$ by $x \sim y$ if $x \in yH$. The collection of equivalence classes formed by the relation are denoted $G/H$, pronounced as `$G$ mod $H$'. Each element of $G/H$ is known as a \emph{left coset}, and every coset can be expressed as $gH = \{ gh : h \in H \}$, for some $g \in G$. Think of cosets are subgroups that are translated around by an element in a group, like subspaces in a vector space shifted by a vector.

\begin{remark}
    Right cosets can be defined equivalently by the equivalence relation $g \sim k$ if $g \in Hk$. Like left cosets, all right cosets can be written $Hg$ for some $g$. We denote the set of right cosets by $H \setminus G$. It doesn't really matter whether we talk about left or right cosets, because we have a natural map from one family to the other, mapping the coset $gH$ to the coset $Hg^{-1}$. We choose to use left cosets as a simple convention.
\end{remark}

The \emph{index} of a subgroup $H$ of a group $G$ is the quantity
%
\[ (G:H) = \#(G/H) = \#(H \setminus G). \]

\begin{example}
    For any non-zero integer $N$, the subgroup $N \cdot \ZZ$ of $\ZZ$ is a subgroup, and $[\ZZ: N \cdot \ZZ] = N$. The cosets of $\ZZ/N \ZZ$ are precisely
    %
    \[ \{ 0 + N \ZZ, \dots, (N-1) + N \ZZ \}. \]
\end{example}

\begin{example}
    The additive group $\QQ$ has the interesting property that it contains \emph{no nontrivial finite index subgroups}. Indeed, suppose that $G$ is a finite index subgroup, and set $[\QQ:G] = N$. Then, for any rational number $x \in \QQ$, $Nx \in H$. But clearly this implies that $G = \QQ$, so $N = 1$. The third isomorphism theorem (proved later) therefore shows that $\QQ/\ZZ$ has no finite index subgroups.
\end{example}

We note that each $A \in G/H$ has the same cardinality as $H$; indeed, if $A = gH$, then the map $h \mapsto gh$ gives a bijection between $H$ and $A$. Thus we conclude that if $G$ is a finite group, then
%
\[ \#(G) = \sum_{A \in G/H} \#(A) = \sum_{A \in G/H} \#(H) = (G:H) \cdot \#(H). \]
%
We remark that this theorem can be interpreted for infinite groups if we interpret $(G:H)$, $\#(H)$, and $\#(G)$ as a formula about cardinalities of groups. The fact that all cosets of a group have the same cardinality gives a very fruitful theorem, named after one of the pioneers of group theory, the french mathematician Joseph-Louis-Lagrange. It gives a useful characteristic of all subgroups of a finite group.

\begin{theorem}[Lagrange's Theorem]
    If $G$ is a finite group and $H < G$, then
    %
    \[ \#(H) \divides \#(G). \]
\end{theorem}

Lagrange did not give a complete proof, showing it only for subgroups of the symmetric group. The first complete theorem was published by Gauss in 1801.

\begin{corollary} Any group of prime order is cyclic. \end{corollary}
\begin{proof}
    Let $G$ be a group of prime order. Take a non-zero element $g \in G$, and consider $\gen{g}$. This is a subgroup, and thus the order of the group must divide $G$. But the only numbers that divide $G$ are 1 and the order of $G$, as the number is prime, and $\gen{g}$ definitely contains more than one element. Thus the order of $\gen{g}$ is the same as the order of $G$, so $G = \gen{g}$.
\end{proof}

\begin{corollary}If $L < H < G$, then $(G:L) = (G:H)(H:L)$.
\end{corollary}
\begin{proof}
    We have three equations, $\#(G) = (G:H) \#(H) = (G:L) \#(L)$, and $\#(H) = (H:L) \#(L)$. Putting these three equations together, we conclude that
    %
    \[ (G:H)(H:L) \#(L) = (G:L) \#(L). \]
    %
    Thus, dividing out be $\#(L)$, we conclude that $(G:H)(H:L) = (G:L)$.
\end{proof}

\begin{remark}
    If we are a little more careful, we can prove this formula for infinite groups if we interpret the product as the product of infinite cardinalities.
\end{remark}

We now have the power to prove another interesting number theoretic statement, known as Euler's theorem. Consider the totient function $\varphi$, which takes an integer $n$ and gives us the number of integers relatively prime to $n$, which are less than $n$. The theorem is simple with the power of the methods we now possess.

\begin{corollary}
    For any relatively prime $n,m$, $n^{\varphi(m)} \equiv 1 \pmod{m}$.
\end{corollary}
\begin{proof}
    For any integer $m$, let $\ZZ_m^*$ denote the set of integers $n \in \ZZ_m$ for which there exists an integer $a \in \ZZ_m$ such that $an \equiv 1 \pmod{m}$. It will suffice for us to prove the cardinality of $\ZZ_m^*$ is equal to $\varphi(m)$, since we can then apply Lagrange's theorem. We note that for any integers $n$ and $m$, the greatest common divisor of $n$ and $m$ is the smallest positive integer which can be written in the form $an + bm$, for $a,b \in \ZZ$. If $n$ and $m$ are relatively prime, then we can find $a$ and $b$ such that $an + bm = 1$, which implies $an \equiv 1 \pmod{m}$. Conversely, if $an \equiv 1 \pmod{m}$, then we can find an integer $b \in \ZZ$ such that $an + bm = 1$. But this means that the greatest common divisor of $n$ and $m$ is equal to $1$, hence $n$ and $m$ are relatively prime. For any $n \in \ZZ_m^*$, $\langle n \rangle$ is a subgroup containing $\text{ord}(n)$ elements. By Lagrange's theorem, $\text{ord}(n)$ divides $\varphi(m)$. But this means that $n^{\varphi(m)} = 1$.
\end{proof}

One corollary is Fermat's Little Theorem.

\begin{corollary}If $p$ is a prime, and $p \not \divides n$, then $n^{p-1} \equiv 1 \mod{p}$.
\end{corollary}

Lagrange's theorem is often very powerful, especially when analyzing finite subgroups. Here are some common applications of Lagrange's theorem, where we let $G$ be a group with subgroups $H_1$ and $H_2$:
%
\begin{itemize}
    \item If $H_1$ and $H_2$ are subgroups of $G$ and have relatively prime orders, then $H_1 \cap H_2 = \{ e \}$.

    \item If $[G:H_1]$ and $[G:H_2]$ are relatively prime, then
    %
    \[ [G:H_1 \cap H_2] = [G:H_1][G:H_2]. \]
    %
    To see this, we note $[G:H_1]$ and $[G:H_2]$ divide $[G:H_1 \cap H_2]$, and
    %
    \begin{align*}
        [G:H_1 \cap H_2] &= [G:H_1][H_1: H_1 \cap H_2]\\
        &= [G:H_1][H_1H_2: H_2] \leq [G:H_1][G:H_2].
    \end{align*}

    \item If $H_2 \lhd G$ and $\#(H_1)$ and $[G:H_2]$ are relatively prime, then $H_1 < H_2$. To see this, consider the homomorphism $\varphi: H_1 \to G/H_2$ given by setting $\varphi(x) = xH_2$. If $K$ is the kernel, then $[H_1:K]$ divides both $\#(H_1)$ and $\#(G/H_2) = [G:H_2]$, so $[H_1:K] = 1$, implying $K = H_1$, so $\varphi$ is trivial, so $H_1 \subset H_2$.

    \item We say a subgroup $H$ of a finite group $G$ is a \emph{Hall subgroup} if $\#(H)$ and $[G:H]$ are relatively prime. If $H$ is a Hall subgroup of $G$, and we consider a normal subgroup $N \lhd G$, then $N \cap H$ is a Hall subgroup of $N$. To see why, we note that $\#(H)$ and $[G:H]$ are relatively prime. Now $HN$ is a subgroup of $G$, and we can write $[G:H] = [G:HN][HN:H]$, and $\#(H) = [H:H \cap N] \#(H \cap N)$. By the second isomorphism theorem,
    %
    \[ [N:H \cap N] = [HN:H] \divides [G:H]. \]
    %
    Since $\#(H \cap N)$ divides $\#(H)$, we conclude $[N:H \cap N]$ and $\#(H \cap N)$ are relatively prime, completing the proof.

    If $\#(G) = p^k m$, where $p$ does not divide $m$, then we say a subgroup $H < G$ is a $p$ \emph{Sylow subgroup} if $\#(H) = p^k$. A subgroup of $G$ with a prime power order is Sylow precisely when it is a Hall subgroup. Thus our proof above shows that if $N \lhd G$ and $H$ is a Sylow subgroup of $G$, then $H \cap N$ is a Sylow subgroup of $N$.
\end{itemize}
%
Thus the cardinality of a group gives more structural information about the group than might be realized.

One might ask whether there is a converse to Lagrange's theorem. For an integer $n$ dividing the cardinality of a group $G$, is there a subgroup of order $G$? The easiest kind of theorem of this type is Cauchy's theorem, of which we give an elementary proof here.

\begin{theorem}
    If $p$ is a prime dividing $\#(G)$, then there is $x \in G$ of order $p$.
\end{theorem}
\begin{proof}
    Let
    %
    \[ S = \{ (x_1, \dots, x_p) \in G^p : x_1 \dots x_p = e \}. \]
    %
    Then $\#(S) = \#(G)^{p-1}$. Define an equivalence relation on $S$ by declaring $(x_1, \dots, x_p) \sim (x_i, \dots, x_p, x_1, \dots, x_{i-1})$ for any $i \in \{ 1, \dots, p \}$, i.e. elements are equivalent if one can obtain one sequence from the other by a cycle permutation. If $x_1 \dots x_p = e$, then
    %
    \[ x_p x_1 \dots x_{p-1} = x_p (x_1 \dots x_{p-1} x_p) x_p^{-1} = x_p (e) x_p^{-1} = e. \]
    %
    Since $p$ is prime, all cycle permutations of $(x_1, \dots, x_p)$ are distinct, except in the trivial case where $x_1 = \dots = x_p$. Thus if we let $k$ denote the number of size one equivalence classes, and $n$ denote the number of size $p$ equivalence classes, then we conclude
    %
    \[ \#(G)^{p-1} = \#(S) = k + pn. \]
    %
    Since $p$ divides $\#(G)$, $k$ is divisible by $p$. But $k \geq 1$, since $(e,\dots,e) \in S$. Thus we conclude $k \geq p$, and so, in particular, there exists $x \in G$ with $x^p = e$.
\end{proof}

Later on, we will prove a stronger statement, called Sylow's theorem, which shows that for each prime $p$ dividing the order of a finite group $G$, the group $G$ has a $p$ Sylow subgroup.

\section{Normal Subgroups}

Let $G$ be a group, and $H$ a subgroup. We emphasized previously that we can think of $G$ as a family of symmetries on some space. A natural operation associated with a symmetry is a `coordinate transformation'. If $e_1, \dots, e_n$ are the standard basis in $k^n$, then for each matrix $M \in GL_n(k)$, we can associate a new basis $f_1, \dots, f_n$ with $f_i = M e_i$ for each $i \in \{ 1, \dots, n \}$. If $T: \RR^k \to \RR^k$ is a linear transformation, we can associate it with two matrices $N_0$ and $N$, where for each $x \in \RR^n$,
%
\[ T(x_1e_1 + \dots + x_ne_n) = (N_0 x)_1 e_1 + \dots + (N_0 x)_n e_n, \]
%
and for each $y \in \RR^n$,
%
\[ T(y_1f_1 + \dots + y_nf_n) = (N y)_1 f_1 + \dots + (N y)_n f_n. \]
%
Thus $N_0$ represents the transformation $T$ in the standard coordinates of $\RR^n$ relative to the basis $\{ e_1, \dots, e_n \}$, and $N$ represents the transformation $T$ in the new coordinate system induced by the basis $\{ f_1, \dots, f_n \}$. It is not difficult to see that $N = M N_0 M^{-1}$, so the coordinate change is given by a conjugation operation.

Given this view of conjugation, we can see that some subgroups of a group are `compatible' with coordinate changes in the larger group, and some groups are not. For instance, the orthogonal group $O_n(\RR)$ is \emph{not} compatible with general coordinate changes in $GL_n(\RR)$; if we change a basis, a rotation need not be a basis anymore. More rigorously, there exists a matrix $M \in GL_n(\RR)$ and $N \in O_n(\RR)$ such that $MNM^{-1} \not \in O_n(\RR)$. On the other hand, for any matrix $N \in SL_n(\RR)$ and $M \in GL_n(\RR)$, $MNM^{-1} \in SL_n(\RR)$, since
%
\[ \det(MNM^{-1}) = \det(M) \det(N) \det(M)^{-1} = \det(M) \det(M)^{-1} = 1. \] 
%
Thus an element of $SL_n(\RR)$ `looks the same' under any coordinate change in $GL_n(\RR)$. The subgroups with this invariance property will be known as \emph{normal groups}.

\begin{theorem}
Let $H$ be a subgroup of a group $G$. The following statements are equivalent, and if any hold, we say $H$ is \emph{normal} in $G$ and write $H \lhd G$:
\begin{enumerate}
    \item $gHg^{-1} \subseteq H$ for all $g \in G$.
    \item $gHg^{-1} = H$ for all $g \in G$.
    \item $gH = Hg$ for all $g \in G$.
    \item $G/H = H \setminus G$.
\end{enumerate}
\end{theorem}
\begin{proof}
    Clearly (2) and (3) are equivalent, (2) implies (1) trivially, and (3) implies (4) trivially. To show (1) implies (2), we suppose $ghg^{-1} \subseteq H$ for all $g \in G$. Then $gH \subseteq Hg$. But also $g^{-1}Hg \subseteq H$, so that $Hg \subseteq gH$, which implies $Hg = gH$. From (4), we note that if $g_1, g_2 \in G$ and $g_1H = Hg_2$, then $g_1e \in g_1H = Hg_2$, so that $g_1 \in Hg_2$. Because cosets are equal or disjoint, this means that $Hg_2 = Hg_1$, and so $g_1H = Hg_1$.
\end{proof}

If $G$ is an abelian group, then every subgroup $H$ is normal, because
%
\[ gHg^{-1} = Hgg^{-1} = H. \]
%
Normality is only an interesting phenomenon in non-abelian groups.

\begin{example}
    We have already shown that $SL_n(k) \lhd GL_n(k)$.
\end{example}

\begin{example}
    Given a group $G$ and a set $S \subset G$, consider the \emph{normalizer subgroup}
    %
    \[ N_G(S) = \{ g \in G : gSg^{-1} = S \}. \]
    %
    If $S$ is a subgroup of $G$, then $S \lhd N_G(S)$, and moreover, $N_G(S)$ is the largest subgroup of $G$ in which $S$ is a normal subgroup. We can also define the \emph{centralizer subgroup}
    %
    \[ C_G(S) = \{ x \in G: xs = sx\ \text{for all $s \in S$} \}. \]
    %
    Then $C_G(S) \lhd N_G(S)$. If $S = G$, then we call $C_G(S)$ the \emph{center} of $G$, also denoted as $Z(G)$.
\end{example}

\begin{theorem}
    If $K < N_G(H)$, then $KH$ is a group, and $H \lhd KH$.
\end{theorem}
\begin{proof}
    We begin by noticing that $KH = HK$. Thus
    %
    \[ (KH)(KH) = (KH)(HK) = K((HH)K) = K(HK) = K(KH) = KH. \]
    %
    Thus $KH$ is closed under composition. Similarily,
    %
    \[ (KH)^{-1} = H^{-1}K^{-1} = HK = KH, \]
    %
    so $KH$ is closed under inversion. Thus $KH$ is a group. To see that $H$ is normal in $KH$, we note that for $k \in K$ and $h \in H$,
    %
    \[ khHh^{-1}k^{-1} = kHk^{-1} = H. \]
    %
    Thus $H \lhd KH$.
\end{proof}

Trivial subgroups of a group are always normal. Thus any group has normal subgroups. We say a group is \emph{simple} if it contains no non-trivial normal subgroups. Thus simple groups are the equivalent of prime numbers, they cannot be `broken up' into simpler groups. If $G$ is not a simple group, it contains a nontrivial normal subgroup $H$, and then the groups $G/H$ and $H$ can be viewed as a partition of the structure of $G$. These structures do not describe the structure of $G$ completely, but at least describe a large majority of the structure.

\begin{example}
    Let $G = 2 \ZZ_4$ and $H = \ZZ_2 \times \{ 0 \}$ be subgroups of $\ZZ_4$ and $\ZZ_2 \times \ZZ_2$ respectively. Then $G \cong H$, and $\ZZ_4/G \cong (\ZZ_2 \times \ZZ_2) / H$. Nonetheless, $\ZZ_4$ is not isomorhpic to $\ZZ_2 \times \ZZ_2$.
\end{example}

If we can characterize all simple groups, then intuitively we have made huge strides in characterizing the structure of all groups. The H\"{o}lder program of mathematics attempts to classify all finite simple groups. In 2008, over one hundred years after the program began, mathematicians succeeded in characterizing all groups. Each finite simple group can belong to one of 18 infinite families of groups, or is one of 26 `sporadic' groups, which do not seem to have a simple characterization. The proof of this result has taken over ten thousand journal articles, and modern work in this field has attempted to simplify parts of this proof so it is comprehensible to a single human.

\section{Homomorphisms and Quotients}

Another way to understand groups is to understand how they are interrelated to one another. In group theory, the interrelations between different groups are formalized as `homomorphisms'. If $G$ and $H$ are groups, a \emph{homomorphism} between $G$ and $H$ is a function $f: G \to H$ such that $f(g_1g_2) = f(g_1)f(g_2)$ for $g_1, g_2 \in G$. The homomorphism is an \emph{embedding} if it is injective, and a homomorphism from a group to itself is known as an \emph{endomorphism}. Intuitively, a homomorphism is map which preserves the group structure of $G$. For instance, if $\varphi: G \to H$ is a homomorphism, then one easily verifies that $\varphi(e) = e$, $\varphi(a^{-1}) = \varphi(a)^{-1}$, so the identity and inversion is preserved. Intuitively, a homomorphisms is a map which implants some of the information of $G$ into a subgroup of $H$, in such a way that certain elements may be identified. The \emph{kernel} of a homomorphism $\varphi$, denoted $\ker(\varphi)$, is the set of elements in the domain of the homomorphism which map to the identity. In some senses, it represents the information that is lost by the map $\varphi$.

\begin{lemma} If $\varphi: G \to H$, and $K$ is the kernel of $\varphi$, then $K \lhd G$. \end{lemma}
\begin{proof}
    Let $G$ and $H$ be groups, and $\varphi$ a homomorphism between $G$ and $H$. If $k \in K$, then $\varphi(k) = e$, and so for any $g \in G$,
    %
    \[ \varphi(gkg^{-1}) = \varphi(g) \varphi(k) \varphi(g^{-1}) = \varphi(g) \varphi(g)^{-1} = e. \]
    %
    Thus $gkg^{-1} \in K$.
\end{proof}

One verifies easily that a homomorphism is injective if and only if the kernel of the homomorphism is trivial. The image of a homomorphism is a subgroup of the range, but is not necessarily normal.

\begin{example}
    If $M$ and $N$ are matrices, the fact that $\det(MN) = \det(M) \det(N)$ implies that the map $\det: GL_n(k) \to k^\times$ is a homomorphism from the group of invertible matrices to the multiplicative group of non-zero elements in $k$.
\end{example}

\begin{example}
    If $g \in G$, the map $n \mapsto g^n$ is a homomorphism from the additive group $\
    mathbf{Z}$ to $G$. Similarily, we can consider the exponential map $e: \CC \to \CC^*$ from given by $e(x) = e^x$, which is a homomorphism from the additive group of complex numbers to the multiplicative group of nonzero complex numbers.
\end{example}

\begin{example}
    The absolute value map $\text{abs}: \CC^* \to \RR^*$ given by setting $\text{abs}(z) = |z|$, is a homomorphism, since $|zw| = |z| |w|$.
\end{example}

An \emph{isomorphism} is a bijective homomorphism. If there exists an isomorphism between two groups, $G$ and $H$, we denote this by writing $G \cong H$. It is easy to see, like in linear algebra, that the inverse of a group isomorphism $f: G \to H$ is a group isomorphism $f^{-1}: H \to G$. The existence of an isomorphism means that all algebraic information about the domain is preserved in the image, and conversely, all the information in the range is contained in the domain. An \emph{automorphism} is a bijective homomorphism from a group to itself. Thus an automorphism says that various objects in a group behave the same way. Note that the set of all automorphisms on a group $G$ is a set of invertible functions on a space preserving some structure, and thus forms a group, denoted $\text{Aut}(G)$.

\begin{example}
    The conjugation map $z \mapsto \overline{z}$ is an automorphism of both the multiplicative and additive group of complex numbers. Thus the number $i$, introduced to the real numbers to form the complex numbers, operates algebraically exactly the same as the number $-i$. Engineers sometimes work with $-i$, denoted $j$, to perform calculations; the only difference being that they work `clockwise', instead of `anticlockwise'.
\end{example}

\begin{example}
    For each $g_0,g_1 \in G$, we let $g_0^{g_1} = g_1^{-1} g_0 g_1$. The map $\varphi_{g_1}: G \to G$ given by setting $\varphi_{g_1}(g_0) = g_0^{g_1}$ is an automorhpism of $G$, known as an \emph{inner automorphism}. Moreover, the map $\varphi: G \to \text{Aut}(G)$ obtained by setting $\varphi(g) = \varphi_g$ is a homomorphism, since for $g_0,g_1,g_2 \in G$,
    %
    \[ g_0^{g_1g_2} = (g_1g_2)^{-1} g_0 (g_1g_2) = g_2^{-1}(g_1^{-1} g_0 g_1) g_2 = (g_0^{g_1})^{g_2}. \]
    %
    The kernel of $\varphi$ is equal to $Z(G)$.
\end{example}

The next theorem is essentially no different from the fact that two linear transformations which are equal when restricted to the basis elements of a vector space are equal in full.

\begin{theorem}
    Let $G$ be a group generated by a subset $S$. Given any other group $H$ and any function $f_0: S \to H$, there is at most one homomorphism $f: G \to H$ such that $f(s) = f_0(s)$ for each $s \in S$.
\end{theorem}
\begin{proof}
    Given two homomorphisms $f_1,f_2: G \to H$, the set of elements of $g \in G$ with $f_0(g) = f_1(g)$ is a subgroup of $G$ containing $S$, which is therefore equal to $G$.
\end{proof}

We can use the coset construction on a pair $H \lhd G$ to assign algebraically meaning information to $G/H$. Given two cosets $g_1H$ and $g_2H$, we find
%
\[ (g_1H)(g_2H) = g_1(Hg_2)H = g_1(g_2H)H = (g_1g_2)H. \]
%
Thus the operation on $G/H$ given by multiplication of cosets is a well defined composition operation. It is easy to see that the coset $H$ acts as an identity for this operation, and the inverse of a coset $gH$ is the coset $g^{-1}H$. It follows that $G/H$ has a natural group structure, known as the \emph{quotient group} of $G$ by $H$.

\begin{example}
    Consider the additive group of integers $\ZZ$. The set $n \ZZ$ of integer multiples of $n$ is a subgroup of $\ZZ$, trivially normal because $\ZZ$ is abelian. Thus we can consider the quotient group $\ZZ / (n \ZZ)$, which is precisely the additive group of integers modulo $n$.
\end{example}

For $H \lhd G$, the map $\pi: G \to G/H$ given by setting $\pi(g) = gH$ is a canonical surjective homomorphism between the two sets, with kernel $H$. In particular, we note this construction shows that any normal group is a kernel of some homomorphism. Thus we can view a normal group as a subgroup which `can be a kernel' for a homomorphism. The next theorem shows again that the kernel is the information `lost' by a homomorphism.

\begin{theorem}[The First Isomorphism Theorem] \index{First Isomorphism Theorem}
Let $\varphi: G \to H$ be a surjective homomorphism with kernel $K$. Then $G/K \cong H$.
\end{theorem}
\begin{proof}
    Let $K$ be the kernel of $\varphi$. If $A \in G/H$, and $g_0,g_1 \in A$, then $g_0 g_1^{-1} \in K$ so $\varphi(g_0) = \varphi(g_1)$. Thus we can define a map $\varphi_0: G/K \to H$ such that for each coset $gH \in G/H$, $\varphi_0(gH) = \varphi(g)$. The map $\varphi_0$ is a homomorphism with respect to the quotient group operations on $G/H$, because
    %
    \[ \varphi_0((g_1H)(g_2H)) = \varphi(g_1g_2) = \varphi(g_1) \varphi(g_2) = \varphi_0(g_1H) \varphi_0(g_2H). \]
    %
    If $\varphi_0(gH) = e$, then $\varphi_0(g) = e$, so $g \in K$. Thus the kernel of $\varphi_0$ is trivial. Thus $\varphi_0$ is bijective, so $\varphi_0$ is an isomorphism.
\end{proof}

\begin{wrapfigure}{1}{3.5cm}
\begin{tikzpicture}[description/.style={fill=white,inner sep=2pt}]
\coordinate (A) at (0.3,1.5);
\coordinate (B) at (2.7,1.5);
\coordinate (C) at (1.4,-0.1);
\coordinate (D) at (1.6,-0.1);
\coordinate (E) at (2.7,1.3);
\coordinate (F) at (0.3,1.4);

\draw (0,1.5) node {$G$};
\draw (3,1.5) node {$H$};
\draw (1.5,-0.5) node {$G/H$};

\draw[thick,->] (A)--(B) node[midway,above] {$\varphi$};
\draw[thick,->] (F)--(C) node[midway,below] {$\pi$};
\draw[thick,->] (D)--(E) node[midway,below] {$\overline{\varphi}$};

\end{tikzpicture}
\end{wrapfigure}

It is convenient here to introduce the concept of a \emph{commutative diagram}. A commutative diagram is a directed graph where vertices are sets and edges are functions between the sets it connects, with the following property. If there are two paths
%
\begin{align*}
    S \xrightarrow{f_1} A_1 \xrightarrow{f_2} \dots \xrightarrow{f_{n-1}} A_n \xrightarrow{f_n} E\\
    S \xrightarrow{g_1} B_1 \xrightarrow{g_2} \dots \xrightarrow{g_{m-1}} B_m \xrightarrow{g_m} E
\end{align*}
%
from $S$ to $E$, then $f_n \circ \dots \circ f_1 = g_m \circ \dots \circ f_1$. An example diagram represents the functions in the first isomorphism theorem. Another notation, more lateral is to consider sequences of groups
%
\[ G_1 \xrightarrow{f_1} G_2 \xrightarrow{f_2} \dots \xrightarrow{f_{n}}G_{n+1} \]
%
with arrows representing homomorphisms. This sequence is \emph{exact} whenever $\im(f_i) = \ker(f_{i+1})$ for any $i$ from 1 to $n-1$. To test your knowledge of this, note that the sequence $0 \to G \to H$ is exact precisely when the homomorphism between $G$ and $H$ is injective. Likewise, $G \to H \to 0$ is exact precisely when the map between $G$ and $H$ is surjective.

The first isomorphism is the catalyst for many other important isomorphism theorems, which enables us to construct canonical isomorphisms between objects.

\begin{theorem}[The Second Isomorphism Theorem] \index{Second Isomorphism Theorem} \index{Diamond Isomorphism Theorem}
    Let $G$ be a group, and consider subgroups $K,H < G$ such that $K \subset N_G(H)$. Then $(H \cap K) \lhd H$, and
    %
    \[ H/(H \cap K) \cong HK/K. \]
\end{theorem}
\begin{proof}
    We have already seen that $K \lhd HK$, so that the quotient group $HK/K$ makes sense. If we define $\varphi: H \to HK/K$ by setting $\varphi(h) = hK$, then $\varphi$ is a homomorphism, because for $h_1,h_2 \in H$,
    %
    \[ \varphi(h_1h_2) = (h_1h_2)K = (h_1K)(h_2K) = \varphi(h_1) \varphi(h_2). \]
    %
    The map $\varphi$ is also surjective, because any coset of $HK/K$ is of the form $hK$ for some $h \in H$. The kernel of $\varphi$ is equal to $H \cap K$, from which it follows that $H \cap K \lhd H$, and the first isomorphism theorem implies that $H/(H \cap K) \cong HK/K$.
\end{proof}

\begin{figure}
\begin{center}
\begin{tikzpicture}
    \coordinate (Z) at (1.9,-2.75);
    \coordinate (Z') at (2.1,-2.75);
    \coordinate (Z'') at (2,-3.25);

    \coordinate (J) at (2,-1.5);
    \coordinate (K) at (4,-1.5);
    \coordinate (I) at (0,-1.5);

    \coordinate (J') at (2,-0.6);
    \coordinate (K') at (4,-0.6);
    \coordinate (I') at (0,-0.6);

    \coordinate (Q) at (2.2,0.7);
    \coordinate (Q') at (2,2.1);
    \coordinate (Q'') at (2,1.3);
    \coordinate (Q''') at (1.8,0.7);

    \draw[thick] (K)--(Z') node[midway,sloped,below] {};
    \draw[thick] (I)--(Z) node[midway,sloped,below] {};

    \draw[thick] (Q)--(K') node[midway,sloped,below] {};
    \draw[thick] (Q''')--(I') node[midway,sloped,below] {};

    \draw[thick] (Q')--(Q'') node[midway,sloped,below] {};

    \draw (2,-3) node {$H \cap K$};

    \draw (0,-1) node {$H$};
    \draw (4,-1) node {$K$};
    \draw (2,1) node {$HK$};

    \draw (2, 2.5) node {$G$};
\end{tikzpicture}
\end{center}
\caption{\emph{The Diamond Isomorphism Theorem}}
\end{figure}

The second isomorphism theorem is known as the diamond isomorphism theorem because of the lattice of subgroups it forms. Let us consider an example application.

\begin{theorem}[The Third Isomorphism Theorem]
    Consider normal subgroups $H,N \lhd G$, with $N \lhd H$. Then $H/N \lhd G/N$, and
    %
    \[ (G/N)/(H/N) \cong G/H. \]
\end{theorem}
\begin{proof}
    Define $\varphi: G/N \to G/H$ by setting $\varphi(gN) = gH$. Then $\varphi$ is surjective, with kernel $H/N$. Thus $H/N$ is a normal subgroup of $G/N$, and the first isomorphism implies that
    %
    \[ (G/N)/(H/N) \cong G/H. \]
\end{proof}

All we showed in this theorem is that $0 \to H/N \to G/N \to G/H \to 0$ is an exact sequence of groups; the theorem then follows directly from the first isomorphism theorem.

\begin{theorem}[The Lattice/Fourth Isomorphism Theorem]
    Consider a normal subgroup $N \lhd G$. For each subgroup $H < G$, let $f_*(H) = HN/N$. Then $f_*(H)$ is a subgroup of $G/H$. Any subgroup of $G/N$ can be written uniquely as $H/N$ for some subgroup $H$ of $G$ with $N \subset H$, and we define $f^*(H/N) = H$. The functions $f_*$ and $f^*$ are order preserving, $f^*(f_*(H)) = HN$ for any group $H$, and $f_*(f^*(H/N)) = H/N$ for any subgroup of $G/N$. Thus we obtain an order preserving bijection between the family of subgroups of $G$ containing $N$, and the family of subgroups of $G/N$. The correspondence $(f_*,f^*)$ satisfies the following properties:
    %
    \begin{enumerate}
        \item If $N < K < H < G$, $(H:K) = (f_*(H):f_*(K))$.
        \item If $N < K,H < G$, then $f_*(K \cap H) = f_*(K) \cap f_*(H)$.
        \item If $N < K,H < G$, and $H \vee K$ denotes the smallest subgroup of $G$ containing $H$ and $K$, then $f_*(H \vee K) = f_*(H) \vee f_*(K)$.
        \item If $N < K < H < G$, then $K \lhd H$ if and only if $f_*(K) \lhd f_*(H)$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    Clearly $f_*$ is surjective, and that $f_* \circ f^*$ is the identity map. Since the image of $f^*$ contains all subgroups of $G$ containing $N$, this verifies the bijection. To obtain (1), we note that if $N < K < H < G$, a method like that used in the third isomorphism theorem shows that
    %
    \[ (f_*(H): f_*(K)) = (H/N: K/N) = (H:K). \]

    To prove (2), we calculate that
    %
    \[ f_*(K \cap H) = (K \cap H)/N \subset K/N \cap H/N = f_*(K) \cap f_*(H) \]
    %
    But if $f_*(K \cap H)$ was a proper subset of $K/N \cap H/N$, then there would be a group $L$ containing $K \cap H$ as a proper subgroup such that $L/N = K/N \cap H/N$. But if there is $k \in K$ and $h \in H$ such that $kN = hN$, then $h^{-1}k \in N \subset H$, so that $k \in H$, and similarily, $h^{-1}k \in N \subset K$, so $h \in K$. Thus $k,h \in H \cap K$, so that any element of $K/N \cap H/N$ is an element of $(K \cap H)/N$.

    To prove (3), we note that $f_*(H \vee K) = (H \vee K)/N$ contains $f_*(H) \vee f_*(K) = H/N \vee K/N$ as a subgroup, so $(H \vee K)/N$ contains $H/N$ and $K/N$. But if $L$ is any subgroup of $G$ containing $N$ such that $L/N$ contains $H/N$ and $K/N$, then $L$ contains $H$ and $K$, so $L$ contains $H \vee K$, which means that $H \vee K$ is a subgroup of $L$. Thus we conclude that $f_*(H) \vee f_*(K) = H/N \vee K/N$ contains $f_*(H \vee K) = (H \vee K)/N$.

    Finally, to prove (4), we note that if $N < K < H < G$, and if $K$ is normal in $H$, then the third isomorphism theorem shows that $K/N$ is a normal subgroup of $H/N$. Conversely, if $K/N$ is a normal subgroup of $H/N$, then for any $h \in H$,
    %
    \[ (hN)(K/N)(h^{-1}N) = K/N. \]
    %
    But $h^{-1}N = Nh^{-1}$, so this theorem says that
    %
    \[ h(N(K/N)N)h^{-1} = K/N. \]
    %
    In particular, for any $k_1 \in K$, there exists $k_2 \in K$ $h(kN)h^{-1} = k_2N \subset K$. But this clearly means that $hkh^{-1} \in K$, so that $K$ is normal in $H$.
\end{proof}

The list of properties above is not exhaustive. Almost all properties of subgroups are preserved by the mapping, so stop a while and think whether you can think of more.

\begin{comment}
\begin{theorem}
    If $H$ and $K$ are subgroups of a group $G$, then we define a $(H,K)$ coset to be a subset of $G$ of the form $HgK$, where $g \in G$. Then the set of $(H,K)$ cosets partitions the group, and if $G$ is the union of $Hg_1K, \dots, Hg_nK$,
    %
    \[ |G| = \sum (H:H \cap g_nKg_n^{-1}) \]
    %
    When $K$ is trivial, we obtain Lagrange's theorem.
\end{theorem}
\begin{proof}
    If $h_0g_0k_0 = h_1g_1k_1$, then $g_0 = h_0^{-1} h_1 g_1 k_1 k_0^{-1}$, and so
    %
    \[ Hg_0K = Hh_0^{-1} h_1 g_1 k_1 k_0^{-1} K = H g_1 K \]
    %
    so the $(H,K)$ cosets form a partition. If $Hg_1K, \dots, Hg_nK$ partition $G$, then
    %
    \[ |G| = \sum |Hg_nK| \]
    %
    The subgroup $H$ operates on the left on the set $G/K$, and $|Hg_nK|$ is the union of a particular orbit class of this action. One element of this orbit is $g_nK$, and $hg_nK = g_nK$ if and only if $h \in g_nK g_n^{-1} \cap H$, so the orbit stabilizer formula says
    %
    \[ |Hg_nK| = (H: H \cap g_nKg_n^{-1}) \]
    %
    This completes the proof.
\end{proof}
\end{comment}

\begin{theorem}
    Suppose $\varphi: G \to H$ is surjective, and $N_0$ is a normal subgroup of $H$. If we define $N = \varphi^{-1}(N_0)$, then $N \lhd G$, and $G/N \cong H/N_0$.
\end{theorem}
\begin{proof}
    The homomorphism $G \to H \to H/N_0$ is surjective and has kernel $N$, so we can apply the first isomorphism theorem.
\end{proof}

This theorem has important properties in the theory of solvable groups, a theory which we will study later on in the course.

\chapter{Examples}

\section{Cyclic Groups}

Recall that a group is cyclic if it is generated by a single element. A simple application of the first isomorphism theorem enables us to essentially determine the complete structure of these groups.

\begin{theorem}
    Every cyclic group is isomorphic to $\ZZ$ or $\ZZ_n$ for some integer $n$.
\end{theorem}
\begin{proof}
    Let $G$ be a cyclic group, generated by $g$. Define a surjective homomorphism $\varphi: \ZZ \to G$ by setting $\varphi(n) = g^n$. If $G$ has finite order $n$, the kernel of $\varphi$ is precisely integer multiples of $n$, and so the first isomorphism theorem says that $\ZZ_n$ is isomorphic to $G$. If $G$ has infinite order, then $\varphi$ itself is an isomorphism, so $\ZZ$ is isomorphic to $G$.
\end{proof}

The power of this theorem is that we need only look at $\ZZ$ and $\ZZ_n$ to prove general results about cyclic groups.

\begin{theorem}
    An infinite cyclic group has exactly two generators.
\end{theorem}
\begin{proof}
    Any infinite cyclic group is isomorphic to $\bint$. Distinct generators of these cyclic groups are mapped to distinct generators in $\bint$, hence if we prove that $\bint$ has only two generators, then every infinite cyclic group has this property. If $n$ is a generator for $\mathbf{Z}$, then there is an integer $m$ such that $mn = 1$. But this is clearly only possible if $n = \pm 1$.
\end{proof}

\begin{theorem}
    Let $G$ be a finite cyclic group of order $n$, generated by an element $g$. Then $g^m$ is a generator for $G$ if and only $(n,m) = 1$.
\end{theorem}

\begin{theorem}
    If $G$ is a cyclic group with two generators $x$ and $y$, then there exists a unique automorphism mapping $x$ onto $y$.
\end{theorem}

\begin{theorem}
    For every finite cyclic group $G$ of period $n$, and for any integer $d$ which divides $n$, there exists a unique subgroup of order $d$.
\end{theorem}

\begin{lemma} Let $g$ be an element of a group $G$, and suppose that the cardinality of $\gen{g}$ is a non-negative integer $n$. Then $g, g^2, \dots, g^n$ are all distinct elements of $G$.
\end{lemma}
\begin{proof}
    Suppose $g^i = g^j$, for $i \neq j$, and such that $0 \leq j < i < c$. Then $g^{i - j} = e$, for $i - j \neq 0$. Take any element $g^m$ in $\gen{g}$. Then, by the euclidean division algorithm,
    %
    \[ m = (i - j)q + r \]
    %
    for some integers $q$ and $r$, where $0 < r < i - j$. Then
    %
    \[ g^m = (g^{i - j})^q g^r = g^r \]
    %
    hence the size of $\gen{g}$, which we have denoted $c$, is less than or equal to $i - j$, for every element in the set is $g^r$ for some $r$ between 0 and $n-1$. But $i - j < c$, which leads us to our contradiction. Hence $g^i \neq g^j$ for numbers $i$ and $j$ in the range $0 < i < j < c$.
\end{proof}

\begin{corollary} For $0 < k < c$, $g^k \neq e$. \end{corollary}

\begin{corollary} If $\gen{g}$ is infinite, then $g^i \neq g^j$ if $i \neq j$. \end{corollary}
\begin{proof}
    If $g^i = g^j$ for some $i > j$, then $g^{i-j} = e$, showing the cyclic group is at most order $i - j$.
\end{proof}

\begin{corollary} $g^c = e$. \end{corollary}
\begin{proof} $g^c$ cannot be equal to any element between $g$ and $g^{c-1}$, so it must be the element of the group that is different from the other elements before it. Thus $g^c = e$, as no other element before $g^c$ is $e$, and this is the only such element. \end{proof}

\begin{lemma} $g^k = e$ if and only if $c \divides k$ \end{lemma}
\begin{proof} We leave this our argument to the reader. It is a simple application of euclidean division.
\end{proof}

Given an element $g$ in an arbitrary group $G$, we define the order of $g$ to be the cardinality of the group $\gen{g}$. Of course, if $\gen{g}$ is finite, this is exactly the least positive integer $a$ such that $g^a = e$. We also call this number the period of $a$. If this is infinite, we say $a$ has infinite period.

\begin{lemma} The order of an element $(ab)$ is the same as the order of an element $(ba)$. \end{lemma}
\begin{proof}
    Consider the group $\gen{ab}$. We know that $(ba)^{-1} = a^{-1}b^{-1}$. Suppose the order of $(ab)$ is finite, of order $k$. Then
    %
    \[ (ab)^k = e \]
    %
    which means
    %
    \[ b(ab)^k = b \]
    %
    and as $b(ab)^k = (ba)^k b$,
    %
    \[ (ba)^k b = b \]
    %
    We conclude $(ba)^k = e$. Thus the order of $(ba)$ is less than or equal to the order of $(ab)$. This process can be done backwards to determine that the order of $(ab)$ is less than or equal to the order of $(ba)$, so the two must be equal.
\end{proof}

Now for any cyclic group $\gen{g}$, and for any integer $a$, one can verify $\gen{g^a}$ is a subgroup of $\gen{g}$. What is surprising is that any subgroup is of this form.

\begin{theorem}
    $G$ is a subgroup of a cyclic group $\gen{g}$ if and only if $G$ is of the form $\gen{g^a}$ for some integer $a$. In short, the only subgroups of a cyclic group are cyclic.
\end{theorem}
\begin{proof}
    Let $G$ be a subgroup of $\gen{g}$. If $G = \{ e \}$, then $G = \gen{g^0}$. In any other case, $G$ has some non-zero element $g^a$. Thus $G$ contains an element with positive exponent, as if $a$ is negative, $-a$ is positive, and $g^{-a}$ must be an element of the group by the closure property of a subgroup. By the well-ordering principle, $G$ contains an element with smallest positive exponent $g^b$. Using euclidean division, every element $g^c \in G$ is of the form $g^{mb + n}$, where $0 < n < b$. Now $g^n \in G$, as $g^n = g^cg^{-mb}$, so we must conclude $n = 0$, as it cannot be a smaller positive exponent than $b$. Thus every exponent in $G$ is divisible by $b$, and every number divisible by $b$ is in $G$, so we conclude $G = \gen{g^b}$.
\end{proof}

Theorem (3.10) has some interesting repercussions in number theory. First, some notation is needed. For a group with two subsets $S$ and $M$, define
%
\[ SM = \{ sm : s \in S, m \in M \} \]
%
For a single element $a$, define $aM = \{ a \}M$, and $Ma$ equivalently.
\begin{itemize}
    \item For any numbers $a, b \in \bint^+$, $a\bint^+ + b\bint^+$ is a group. so it is equal to some cyclic group $c\bint^+$ for an integer $c$. It turns out $c$ is the greatest common denominator \index{Greatest Common Denominator} of $a$ and $b$, denoted $\gcd(a,b)$.
    \item Given $a,b \in \bint^+$, $a\bint^+ \cap b\bint^+$ is a subgroup of $\bint^+$, so it too is $c\bint^+$, and $c$ is the lowest common multiple \index{Lowest Common Multiple} of the two elements, denoted $\lcm(a,b)$.
\end{itemize}

\begin{theorem}
    Consider a group $G$, with two elements $g$ and $h$ such that $g$ is of order $n$ and $h$ is of order $m$. Then, if $g$ and $h$ commute (if $gh = hg$), and $m$ and $n$ are relatively prime, then the order of $(gh)$ is $mn$.
\end{theorem}
\begin{proof}
    Consider elements described above, and let the order of $(gh)$ be $p$. $(gh)^{mn} = g^mh^n = e$, hence $p \divides mn$. We know that
    %
    \[ (gh)^p = g^ph^p = e \]
    %
    hence, by multiplying both sides by $n$,
    %
    \[ g^{mp}h^{mp} = g^{mp} = e \]
    %
    so that $n$ divides $mp$. As $\gcd(m,n) = 1$, $n$ divides $p$.
\end{proof}

We have another interesting number theoretic theorem before we finish our talk of cyclic groups.

\begin{theorem}
    For any prime $p$, $(\bint/p\bint)^\times$ (consisting of all numbers that are invertible modulo $p$) is a cyclic group.
\end{theorem}
\begin{proof}
    We will use the fact that for any $r \geq 1$, the equation $x^r \equiv 1 \mod{p}$ has no more than $r$ solutions for $x$ in $(\bint/p\bint)^\times$ where $p$ is prime. This follows that fact that the group is a field, and thus roots of a polynomial decompose the polynomial into linear factors. Let $n$ be the maximal order of the cyclic subgroups $\gen{m}$, for $m \in (\bint/p\bint)^\times$, generated by an integer $g$. Consider the polynomial $X^n - 1$. For any $m \in (\bint/p\bint)^\times$, the order of $m$ divides $n$, since the order of $gm$ is the lowest common multiple of $m$ and $n$, and must be less than $n$, and is hence equal. Thus $X^n - 1$ has $p - 1$ different solutions, but this implies $n \geq p - 1$. Of course, $n \leq p - 1$, so equality is obtained, and thus the group is cyclic.
\end{proof}

A generator of this group is known as a \emph{primitive root}, and has many applications in number theory and cryptography. The problem with the above proof is that it gives us no method to find a generating element for the multiplicative group. This is an open problem that is incredibly important to cryptography, where multiplicative groups of the form above are used to construct encodings. Finding the primitive root for a really large prime is very difficult, which makes then very useful for cryptography.



\section{Permutation Groups}

Recall that $S_n$ is the group of all permutations of the finite set $\{ 1, \dots, n \}$. In this section we study it's properties in more detail. To begin with, it's simple to see that $S_n$ has at most $n!$ elements. More generally, given any set $X$, we can consider the set $S(X)$ of bijections from $X$ to itself. For any set $X$ and any bijection $\pi \in S(X)$, we let $\text{supp}(\pi)$, the \emph{support} of $\pi$, be the family of all $x \in X$ such that $\pi(x) \neq x$. If two permutations have disjoint support, then they commute with one another.

An \emph{$n$ cycle} $\pi \in S(X)$ is a permutation with finite support containing $n$ elements $x_1, \dots, x_n \in X$ such that $\pi(x_i) = x_{i+1}$, where addition is interpret modulo $n$. We write such a permutation as $(x_1 \cdots x_n)$. A cycle of length two is called a \emph{transposition}. It is intuitive that any element of $S_n$ can be written as a product of cycles with disjoint support, unique up to reordering of the cycles.

\begin{comment}

\begin{theorem}
    Every element of $S_n$ can be written as the product of cycles with disjoint support, unique up to reordering.
\end{theorem}

\end{comment}

\begin{comment}
\begin{proof}
    Let $\sigma$ be an arbitrary element of the symmetric group $S_n$, and consider the cyclic group generated by $\sigma$. Consider the set $\{ 1, 2, \dots, n \}$, with $\gen{\sigma}$ acting on the set by the mapping
    %
    \[ \pi k = \pi(k) \]
    %
    in the obvious manner. We obtain disjoint partitions of orbits from this action. We claim that $\pi$ when restricted to this orbit is a cycle, and thus $\pi$ consists of products of cycles from each orbit. Consider an orbit $(\gen{\pi} k)$ for some number $k$ between one and $n$. Every integer in $k$'s orbit can be written $\pi^m(k)$ for some integer $m$. For each integer $l$ in the range, associate it with the smallest positive integer $m$ such that $\pi^m(k) = l$. We obtain an ordering
    %
    \[ (\pi^0(k), \pi^1(k), \pi^2(k), \dots, \pi^n(k)) \]
    %
    such that $\pi^{n+1}(k) = k$. This generates a cycle, and we have shown what was needed.
\end{proof}
\end{comment}

A simple corollary of this is that $S_n$ is generated by transpositions. Indeed, we can write
%
\[ (a_1 \cdots a_n) = (a_1\; a_2) \dots (a_{n-1}\; a_n). \]
%
so any cycle is generated by transpositions, and any element of $S_n$ can be written as a product of cycles. This equation is not necessarily unique; there might be two different ways of writing an element of $S_n$ as a product of cycles. For instance,
%
\[ (1\; 2)(3\; 4)(2\; 3) = (1\; 2\; 4\; 3) = (1\; 2)(2\; 4)(4\; 3) \]
%
However, what is surprisingly true is that the \emph{parity} of the number of transpositions in this decomposition is independant of the decomposition.

\begin{theorem}
    There exists a unique homomorphism $\varepsilon: S_n \to \{ \pm 1 \}$ such that $\varepsilon(\tau) = -1$ for any transposition $\tau$. Thus $\varepsilon(\pi) = 1$ if $\pi$ is decomposable into an even number of transpositions, and $\varepsilon(\pi) = -1$ if $\pi$ is decomposable into an odd number of transpositions.
\end{theorem}
\begin{proof}
    Consider the homomorphism $\phi: S_n \to GL_n(\RR)$, where we associate $\pi \in S_n$ with the \emph{permutation matrix} $\phi(\pi) \in GL_n(\RR)$, which has the property that for each $x \in \RR^n$,
    %
    \[ \phi(\pi)(x)_i = x_{\pi(i)}.  \]
    %
    We define $\varepsilon(\pi) = \det(\phi(\pi))$. Since the determinant of a matrix negates when we swap two rows, it follows that $\varepsilon(\tau) = -1$ for any transposition, since $\phi(\tau)$ is obtained from the identity by swapping two rows for any transposition $\tau$. From this, it also follows that $\varepsilon(\pi) \in \{ \pm 1 \}$ for each permutation $\pi$.
\end{proof}

We say a permutation $\pi$ is \emph{even} if $\varepsilon(\pi) = 1$, and odd if $\varepsilon(\pi) = -1$. The theorem above shows that the set $A_n$ of even permutations is an index two normal subgroup of $S_n$, since it is the kernel of the map $\varepsilon$, thus having $n!/2$ elements. $A_n$ is called the \emph{alternating group} of order $n$.

\begin{comment}

\begin{theorem}
    There is a unique homomorphism from $S_n$ to $\{ \pm 1 \}$ such that the mapping from any transposition is $-1$.
\end{theorem}
\begin{proof}
    Consider a polynomial $P$ defined for any tuple of natural numbers by
    %
    \[ P(x_1, x_2, \dots, x_n) = \prod_{1 \leq i < j \leq n} (x_j - x_i) \]
    %
    Define the map $sgn$ from $S_n$ to $\{ \pm 1 \}$ by
    %
    \[ sgn(\pi) = \frac{P(\pi(1), \pi(2), \dots, \pi(n))}{P(1,2, \dots, n)} \]
    %
    For any factor $(x_i - x_j)$ in $P(1, 2, \dots, n)$, we either have the factor $(x_j - x_i)$ or the factor $(x_i - x_j)$ in $P(\pi(1), \pi(2), \dots, n)$ ($\pi$ just permutes the orders of the elements, hence the numerator and denominator only differ by sign, and the value of $sgn$ is always positive or negative one. We have that
    %
    \[ \frac{\pi(i) - \pi(j)}{i - j} = \frac{\pi(j) - \pi(i)}{j - i} \]
    %
    Therefore it does not matter whether $i < j$ as much as we do not add the same fraction twice. We conclude, for two permutations $\pi$ and $\sigma$, that
    %
    \begin{align*}
        sgn(\pi \circ \sigma) &= \prod_{1 \leq i < j \leq n} \frac{\pi(\sigma(i)) - \pi(\sigma(j))}{i - j}\\
        &= \prod_{1 \leq i < j \leq n} \frac{\pi(\sigma(i)) - \pi(\sigma(j))}{\sigma(i) - \sigma(j)} \prod_{1 \leq i < j \leq n} \frac{\sigma(i) - \sigma(j)}{i - j}\\
        &= \prod_{1 \leq \sigma(i) < \sigma(j) \leq n} \frac{\pi(\sigma(i)) - \pi(\sigma(j))}{\sigma(i) - \sigma(j)} sgn(\sigma)\\
        &= \prod_{1 \leq i < j \leq n} \frac{\pi(i) - \pi(j)}{i - j} sgn(\sigma)\\
        &= sgn(\pi) sgn(\sigma)
    \end{align*}
    %
    Here the third and fourth equality works because $\sigma$ is a permutation of the numbers from 1 to $n$. From this calculation, we conclude $sgn$ is a homomorphism; all that is left to prove is that for any transposition $(x_1, x_2)$, $sgm((x_1, x_2)) = -1$.
    %
    \begin{align*}
        sgn((x_1, x_2)) &= \Bigg( \prod_{\substack{1 \leq i < j \leq n\\(i,j) \neq (x_1, x_2)}} \frac{i - j}{i - j} \Bigg) \frac{x_2 - x_1}{x_1 - x_2}\\
        &= - \frac{x_2 - x_1}{x_2 - x_1}\\
        &= -1
    \end{align*}
    %
    Thus we have constructed the homomorphism that we wanted.
\end{proof}

\end{comment}

Before we move on to more sophisticated questions, it will be useful to find tractable families of generators for $S_n$ and $A_n$. We recall that for any set $X$ and any two permutations $\pi, \sigma \in S(X)$, and any $x \in X$, we let $\prescript{\pi}{}{\sigma} = \pi \circ \sigma \circ \pi^{-1}$. It then follows that
%
\[ \prescript{\pi}{}{\sigma}(\pi(x)) = \pi(\sigma(x)). \]
%
In particular, if $\sigma = (a_1 \cdots a_n)$, then $\prescript{\pi}{}{\sigma} = (\pi(a_1) \cdots \pi(a_n))$.

\begin{lemma}
    $S_n$ is generated by $\{ (i\; i+1): i \in \{ 1, \dots, n -1 \} \}$.
\end{lemma}
\begin{proof}
    Let $X$ be the set of transpositions above. All transpositions are of the form $(i\; i+k)$ where $i + k \leq n$. We shal show all such transpositions are in the group generated by $X$, from which it follows that $X$ generates $S_n$ since $S_n$ is generated by transpositions. We can do this by induction. Clearly $X$ contains $(i\; i+1)$. If $X$ generates $(i\; i+k)$, then
    %
    \[ \prescript{(i+k\; i+k+1)}{}{(i\; i+k)} = (i\; i+k+1), \]
    %
    so $X$ generates $(i\; i+k+1)$. But this gives the argument for all transpositions.
\end{proof}

\begin{lemma}
    $S_n$ is generated by $(1\; 2)$ and $(1 \cdots n)$.
\end{lemma}
\begin{proof}
    For each $i \in \{ 1, \dots, n-1 \}$,
    %
    \[ \prescript{(1 \cdots n)^{i-1}}{}{(1\; 2)} = (i\; i+1). \]
    %
    But these elements generate $S_n$.
\end{proof}

\begin{lemma}
    If $n$ is a prime number, then $S_n$ is generated by an $n$-cycle and any transposition.
\end{lemma}
\begin{proof}
    Let $\pi$ be an $n$-cycle and $\tau$ be a transposition. Write $\tau = (a_1 a_2)$. But then there exists some $i \in \{ 0, \dots, n-1 \}$ such that $\pi^i(a_1) = a_2$. Since $n$ is prime, $\pi^i$ is still an $n$-cycle of the form $(a_1 a_2 \dots a_n)$. But then the previous case verifies that $\pi^i$ and $\tau$ generate $S_n$.
\end{proof}

\begin{lemma}
    $A_n$ is generated by three cycles.
\end{lemma}
\begin{proof}
    It clearly suffices to show that the product of two transpositions can be written as the product of three cycles. If $a,b,c,d \in \{ 1, \dots, n \}$ are distinct, then
    %
    \[ (ab)(cd) = (abc)(bcd) \]
    %
    is the product of three cycles. This works for all products of transpositions but those of the form $(ab)(ac)$, for distinct $a,b,c \in \{ 1, \dots, n \}$. But
    %
    \[ (ab)(ac) = (acb) \]
    %
    is also the product of three cycles.
\end{proof}

\begin{corollary}
    If $n \geq 5$, then all $3$ cycles are conjugate in $A_n$.
\end{corollary}
\begin{proof}
    Consider two three cycles $\pi_1 = (a_1 a_2 a_3)$ and $\pi_2 = (b_1 b_2 b_3)$. Then consider any permutation $\eta \in S_n$ with $\eta(a_i) = b_i$ for $i \in \{ 1, 2, 3 \}$. If $\eta \in A_n$, we are done since $\prescript{\eta}{}{\pi_1} = \pi_2$. If $\eta$ is odd, then there exists $a_4, a_5 \in \{ 1, \dots, n \} - \{ a_1, a_2, a_3 \}$, and if $\eta' = (a_4 a_5) \pi$, then $\eta'$ is even and $\prescript{\eta'}{}{\pi_1} = \pi_2$.
\end{proof}

Now we are ready to prove the most strenuous proof of the chapter, the simplicity of the alternating group.

\begin{lemma} $A_n$ is simple if $n \geq 5$. \end{lemma}
\begin{proof}
    Let $G$ be a nontrivial normal subgroup of $A_n$. It suffices to show $G = A_n$, and to do this, it suffices to show a single three cycle. Let $\pi \in G$ be an element of $G$ which has the largest number of fixed points, excluding the identity. Then clearly all cycles of $\pi$ must have the same length, because if $\pi$ contains an $n$ and an $m$ cycle, for $n < m$, then $\pi^n \neq e$ and fixes more points than $\pi$, which gives a contradiction. Suppose $\pi$ consists of only 2-cycles. Then $\pi$ contains at least two $2$-cycles, which we may write as $(a_1 a_2)$ and $(a_3 a_4)$. If $a_5 \in \{ 1, \dots, n \} - \{ a_1, \dots, a_4 \}$, and we set $\eta = (a_3 a_4 a_5)$, then $\pi \prescript{\eta}{}{\pi} \in G$ is an element of $G$ which fixes all elements that $\pi$ does, except perhaps for $a_5$, but fixes $a_1$ and $a_2$, which gives a contradiction since $\pi \prescript{\eta}{}{\pi}(a_4) = a_5$, so $\pi \prescript{\eta}{}{\pi} \neq e$. Thus $\pi$ consists of only $n$-cycles, for some $n \geq 3$. If $\pi$ is not a $3$-cycle, then we may find distinct $a_1, a_2, a_3, a_4, a_5 \in \{ 1, \dots, n \}$ such that $\pi(a_1) = a_2$, $\pi(a_2) = a_3$, and $\pi(a_4) = a_5$. Let $\eta = (a_3 a_4 a_5)$. Then $\pi \prescript{\eta}{}{\pi} \in G$ fixes all points that $\pi$ does, but in addition, $\pi(a_1) = a_1$. Since $(\pi \prescript{\eta}{}{\pi})(a_2) = a_5$, $\pi \prescript{\eta}{}{\pi} \neq e$, which gives a contradiction showing that $G$ contains some 3-cycle.
\end{proof}

The fact that $A_n$ is simple for $n \geq 5$ has strong ramifications in Galois theory, where it implies there is no formula for finding the roots of quintic polynomials roots. In particular, it implies that $S_n$ is not solvable.

\begin{theorem}
    For $n \geq 5$, $S_n$ is not solvable.
\end{theorem}
\begin{proof}
    The group $A_n$ is a normal subgroup of $S_n$, and $S_n/A_n \cong \ZZ_2$ is abelian. Thus $S_n$ is solvable if and only if $A_n$ is solvable. Since $A_n$ is simple and non-abelian, it is not solvable.
\end{proof}

Let us end this section by describing the lattice structure of the subgroups of $A_4$, which has the useful property of being planar. $A_4$ has three subgroups of order $2$, corresponding to the three elements of $A_4$ of order two, i.e. $(1\; 2)(3\; 4)$, $(1\; 3)(2\; 4)$, and $(1\; 4)(2\; 3)$. There are four subgroups of $A_4$ of order 3, namely the groups generated by $(1\; 2\; 3)$, $(1\; 2\; 4)$, $(1\; 3\; 4)$ and $(2\; 3\; 4)$. Any group of order four must only contain elements of order two, and one can verify that the set $G$ consisting of the identity and all order two elements is indeed such a group. It is precisely the set of elements of $G$ that are either the identity, or fix no points. To prove these are all subgroups, it suffices to show that any subgroup $H$ of $A_4$ containing a three cycle and a product of 2-cycles is all of $A_4$. We can then clearly find distinct indices $a_1, a_2$, and $a_3$ such that $\pi = (a_1\; a_2\; a_3)$ and $\tau = (a_1\; a_2)(a_3\; a_4)$ are elements of $H$. But then $\prescript{\tau}{}{\pi} = (a_1\; a_4\; a_2) \in H$, $\pi \tau = (a_1\; a_3\; a_4) \in H$, and $\prescript{\pi \tau}{}{\pi} = (a_1 a_4 a_3) \in H$, which generate all three cycles, and hence all of $A_n$.

TODO: Draw lattice for $A_n$.

\begin{exercise}
    Suppose we have $n$ prisoner's in jail, sentenced to death. The executioner's offer the prisoners a way to escape their judgement. They place $n$ boxes in a room, each with a number from $1$ to $n$ in it, uniformly randomly, and a separate number from $1$ to $n$ inscribed on it (not related to the number inside the box in any way). They give each prisoner a unique number in the same manner of the boxes, and give each an opportunity. Each prisoner can open $n/2$ boxes, and if he finds inside a box a number sharing his or her own, then he succeeds his task. If every prisoner accomplishes this task, no-one will be executed, but if a single prisoner fails, everyone will be executed. The naive method of solving this problem accomplishes this with a probability of less than 1\%. Show, using the methods of permutations and cycles, that the prisoner's can design a strategy that leads to a greater than 30\% chance of success.
\end{exercise}
\begin{proof}
    The state of the boxes can be written as a bijection $\pi: \{ 1, \dots, n \} \to \{ 1, \dots, n \}$, such that box $i$ contains the number $\pi(i)$. We then consider the following strategy: the prisoner labelled $i_1$ picks the box labelled $i_1$. He then recieves a new number $i_2 = \pi(i_1)$. If $i_2 \neq i_1$, he then opens the box $i_2$, recieving a number $i_3 = \pi(i_2)$. He continues this way, only failing if he generates $n/2$ numbers $i_1, \dots, i_{n/2}$. In particular, we see that the prisoners only fail if the bijection $\pi$ contains a cycle with length greater than $n/2$. Clearly there can be at most one such cycle in any particular bijection. For each length $l > n/2$, there are exactly
    %
    \[ {n \choose l} (l-1)! (n-l)! = \frac{n!}{l} \]
    %
    different permutation with a cycle of length $l$. Thus the total number of permutations with a cycle of length exceeding $n/2$ is
    %
    \[ \sum_{l = n/2 + 1}^n \frac{n!}{l} = n! \cdot [\log(n) - \log(n/2) + O(1/n)] = n![\log(2) + O(1/n)] \]
    %
    Since there are $n!$ possible bijections of $S_n$, we conclude that that probability of the prisoners failing is $1 - \log(2) + O(1/n) \approx 0.31 + O(1/n)$.
\end{proof}







\section{Matrix Groups}







\section{Isometry Groups}




\section{Direct Products}

This section presents methods for constructing new groups from smaller ones. Conversely, one can break groups into simpler groups by reversing this technique. Let us begin with the simplest construction, the direct product.

Let $\{ G_\alpha : \alpha \in I \}$ be an indexed family of groups. The direct product of these groups, denoted by $\prod_{\alpha \in I} G_\alpha$, whose underlying set is the cartesian product of the groups, and with group operation
%
\[ \left( \prod_{\alpha \in I} g^1_\alpha \right) \left( \prod_{\alpha \in I} g^2_\alpha \right) = \prod_{\alpha \in I} (g^1_\alpha g^2_\alpha). \]
%
The direct product of a finite family of groups $\{ G_1, \dots, G_N \}$ is often denoted by $G_1 \times \dots \times G_N$. If a group $G$ is isomorphic to a direct product of subgroups $\{ G_\alpha \}$, then to understand $G$ it suffices to understand the individual groups $G_\alpha$. The following theorem gives criteria to ensure this occurs.

\begin{theorem}
    Suppose $G$ contains two normal subgroups $H$ and $K$. Then $HK$ is a subgroup of $G$, and
    %
    \[ HK/(H \cap K) \cong H/(H \cap K) \times K/(H \cap K). \]
    %
    In particular, if $H \cap K = \{ e \}$, then $HK$ is isomorphic to $H \cap K$.
    such that $H \cap K = \{ e \}$. 
\end{theorem}
\begin{proof}
    Assume first that $H \cap K = \{ e \}$. Then the assumptions guarantee that $hk = kh$ for all $h \in H$ and $k \in K$. Thus the map $\varphi: H \times K \to HK$ given by setting $\varphi(h,k) = hk$ is an isomorphism, completing the proof. For the general case, we note that $H/(H \cap K)$ and $K/(H \cap K)$ are normal subgroups of $HK/(H \cap K)$ with trivial intersection, so we can apply the previous case.
\end{proof}

\begin{example}
    If $n$ is odd, then $D_{2n}$ is isomorphic to the direct product $D_n \times \{ \pm 1 \}$. To see this, we let $r \in D_{2n}$ be a primitive rotation. Then $r^n \in Z(D_{2n})$, and so $H = \langle r^n \rangle$ is a normal subgroup of $D_{2n}$ isomorphic to $\{ \pm 1 \}$. To obtain a copy of $D_n$ in $D_{2n}$, we note that we can inscribe two regular $n$-vertex polygons in a regular $2n$-vertex polygon. The set of elements of $D_{2n}$ which preserve these two polygons is then a subgroup of $G$ of $D_{2n}$ isomorphic to $D_n$, which we can see to be normal since it has index two in $D_{2n}$. Since $n$ is odd, $r^n \not \in G$, so $G \cap H = \{ e \}$. But $\#(G) = 2n$, and $\#(H) = 2$, so $\#(G \times H) = 4n = \#(D_{2n})$. Thus we conclude that $D_{2n} = GH \cong G \times H \cong D_n \times \{ \pm 1 \}$.
\end{example}

For each group $G_\alpha$ is the direct product $G = \prod_{\alpha \in I} G_\alpha$, we have a surjective homomorphism $\pi_\alpha: G \to G_\alpha$. The kernel of this mapping is just the set of elements $g \in \prod_{\alpha \in I}$ with $g(\alpha) = e$. Thus we can quotient this kernel out to obtain a group isomorphic to $G_\alpha$. Given any family of homomorphisms $f_\alpha: H \to G_\alpha$ from a group $H$, there is a unique map $f: H \to \prod G_\alpha$ such that $\pi_\alpha \circ f = f_\alpha$ for each $\alpha$. This is a \emph{universal property} establishing the direct product, in the sense of category theory; up to isomorphism, $G_0 = \prod G_\alpha$ is the unique group for which there exists projections $\pi_\alpha: G_0 \to G_\alpha$ such that for each function $f_\alpha: H \to G_\alpha$, there exists a unique map $f: H \to G_0$ such that $f_\alpha = \pi_\alpha \circ f$ for each $\alpha$. This is because if some other group $H$ exists with this property with projections $\nu_\alpha: H \to \alpha$, then there exists a unique map $t: H \to \prod_\alpha G_\alpha$ such that $\pi_\alpha \circ t = \nu_\alpha$, as well as a map $s: \prod_\alpha G_\alpha \to H$ such that $\nu_\alpha \circ s = \pi_\alpha$. It does not take much work to show that $s$ is the inverse of $t$, because
%
\[ \pi_\alpha \circ (t \circ s) = \nu_\alpha \circ s = \pi_\alpha; \]
%
But the identity map $1: \prod_\alpha G_\alpha \to \prod_\alpha G_\alpha$ is the unique map such that $\pi_\alpha \circ i = \pi_\alpha$ for each $\alpha$; a similar argument works to show $s \circ t$ is the identity.

\section{Free Products}

The free product is another construction associated with a family of groups, and can also be described by a universal property `dual' to the previous property considered. Given a family of groups $\{ G_\alpha \}$, we desire to construct a group $\coprod_\alpha G_\alpha$, together with homomorphisms $i_\alpha: G_\alpha \to \coprod_\alpha G_\alpha$, such that for each family of homomorphisms $f_\alpha: G_\alpha \to H$, there exists a unique homomorphism $f: \coprod_\alpha G_\alpha \to H$ such that $f_\alpha = f \circ i_\alpha$ for each index $\alpha$. This describes the \emph{free product} of the groups $\{ G_\alpha \}$, up to isomorphism. Thus it suffices to show such a group exists. To achieve this, we start by constructing the \emph{free groups}.

Let $S$ be a set of symbols. We construct a group $F(S)$, known as the \emph{free group} generated by $S$. Let $\mathcal{S}$ denote the alphabet of $S$, consisting of all finite (possibly empty) words $w_1 \dots w_n$, where for each $i$, either $w_i = s$ or $w_i = s^{-1}$, for some $s \in S$. We consider the equivalence relationship generated by letting $w_1 ss^{-1} w_2 \sim w_1 w_2$ for each $s \in S$. We let $F(S)$ denote the set of equivalence classes of $\mathcal{S}$ under this relation. Clearly if $w_1 \sim w_2$ and $u_1 \sim u_2$, then $w_1u_1 \sim w_2u_2$, so composition is well defined on $F(S)$. This composition operation on $F(S)$ is associative, and has an identity (the empty string). Moreover, every element has an inverse; for instance, the inverse of $s_1 \dots s_n$ is $s_n^{-1} \dots s_1^{-1}$.

We note that if $G$ is a group, and $f: S \to G$ is a map, then there exists a unique homomorphism $f_*: F(S) \to G$ such that $f_*(s) = f(s)$ for each $s \in S$. We can use this as a universal property which uniquely specifies the free group of $G$ up to isomorphism. In the language of category theory, we might say that the free group construction is the \emph{left adjoint} to the forgetful functor from the category of groups to the category of sets.

A combinatorially complicated way of constructing groups is by means of \emph{generators and relations}. We consider a set $S$, and let $R$ be some family of elements of $F(S)$. If we let $N$ be the smallest normal subgroup of $F(S)$ generated by $R$, then we call the group $G = F(S)/N$ the \emph{group generated by $S$ with relations $R$}. If $S = \{ x_1, \dots, x_n \}$ and $R = \{ s_1, \dots, s_m \}$, we sometimes use the notation
%
\[ G = \langle x_1, \dots, x_n | s_1 = \dots = s_m = e \rangle. \]
%
If $G$ is any group generated by some set $S \subset G$, then the inclusion map $i: S \to G$ induces a surjective homomorphism $f: F(S) \to G$. Thus any group is generated by a set subject to certain relations.

\begin{example}
    Recall the Dihedral group $D_n$ of isometries of an $n$ sided regular polygon is generated by two elements $r$ and $s$, where $r$ is a primitive rotation, and $s$ is any reflection. For these elements, $r^n = s^2 = (rs)^2 = e$. We claim that
    %
    \[ D_n \cong \langle r,s | r^n = s^2 = (rs)^2 = e \rangle. \]
    %
    If $G$ denotes the generated group, then we certainly have a surjective homomorphism $f: G \to D_n$. If we can show that $G$ has at most $2n$ elements, then $f$ is an isomorphism, which would complete the proof. But the relations in $G$ imply that $rs = sr^{n-1}$, so any element of $G$ is of the form $s^i r^j$ for some $i \in \{ 0, 1 \}$ and $j \in \{ 0, \dots, n - 1 \}$, from which it clearly follows that $\#(G) \leq 2n$.
\end{example}

Now suppose $\{ G_\alpha \}$ is a family of groups; we assume the sets defining the groups $G_\alpha$ are disjoint from one another. Then we let $F$ be the free group generated by $\bigcup G_\alpha$, with inclusion maps $i_\alpha: G_\alpha \to F$. If $f_\alpha: G_\alpha \to H$ is a family of maps, the universal property of the free group implies there is a unique homomorphism $f: F \to H$ such that $f \circ i_\alpha = f_\alpha$. However, this does not imply $F$ is the coproduct, because the inclusion maps $i_\alpha$ are not homomorphisms. But we can fix this. Let $K$ be the smallest normal subgroup of $F$ containing all elements of the form $i_\alpha(g_1) i_\alpha(g_2) i_\alpha(g_1 g_2)^{-1}$ for each $\alpha$ and $g_1,g_2 \in G_\alpha$. Then the induced maps $j_\alpha: G_\alpha \to F/K$ are homomorphisms. Moreover, if $f_\alpha: G_\alpha \to H$ are \emph{homomorphisms}, then the induced homomorphism $f: F \to H$ factors through $K$, because the kernel of $f$ contains $i_\alpha(g_1) i_\alpha(g_2) i_\alpha(g_1 g_2)^{-1}$ for each $\alpha$ and each $g_1, g_2 \in G_\alpha$. Thus there is a unique homomorphism $g: F/K \to H$ such that $g \circ j_\alpha = f_\alpha$ for each $\alpha$. The group $F/K$ is therefore the coproduct we were looking for! Thus we have shown that the coproduct of an arbitrary family of groups exists.

\section{Semidirect Products, and Fibre Products}






\chapter{Group Actions and Symmetries}

Recall Cayley's theorem, which we proved in the introductory chapter, which says that every group $G$ is isomorphic to a subgroup of a symmetry group $S(A)$, for some set $A$. The goal of this chapter is to study what information about a group $G$ we can understand from homomorphisms $\phi: G \to S(X)$ for some set $X$, or, from the dual perspective, to try and understand what properties a homomorphism $\phi: G \to S(X)$ can have given knowledge of the structure of the group $G$.

A \emph{group action} of a group $G$ acting on a set $X$ is a map $F:G \times X \to X$, where we denote $F(g,x)$ by $gx$, such that $g_1(g_2x) = (g_1g_2)x$ for any $g_1,g_2 \in G$ and $x \in X$. If, for each $g \in G$, we let $\phi(g) \in S(X)$ denote the permutation such that $\phi(g)(x) = gx$, then we obtain a homomorphism $\phi: G \to S(X)$, which we call a \emph{permutation representation} of the group $G$. Conversely, given any permutation representation $\phi: G \to S(X)$, we obtain a group action of $G$ on $X$ by letting $gx = \phi(g)(x)$. Thus group actions are ways of studying homomorphisms of $G$. A set $X$ with an action from a set $G$ is called a \emph{$G$ set}. Here is some useful terminology to describe group actions:
%
\begin{itemize}
    \item A group action is \emph{faithful} if the induced permutation representation is injective. In other words, a group action of a group $G$ on a set $X$ is faithful if the only element $g \in G$ such that $gx = x$ for all $x \in X$ is the identity.

    \item Given a $G$ set $X$ and $x \in X$, we can consider the \emph{stabilizer}
    %
    \[ G_x = \{ g \in G: gx = x \}, \]
    %
    which forms a subgroup of $G$. 

    \item On any $G$ set $X$, we can consider an equivalence relation on $X$ by setting $x_1 \sim x_2$ if there is $g \in G$ such that $x_2 = gx_1$. The equivalence classes are known as \emph{orbits} of $X$. Each orbit can be written as $Gx$, for some $x \in X$, and we write the set of all such orbits as $X/G$. To understand the action of $G$, it then clearly suffices to analyze each orbit of $X$ individually.

    \item A group action is \emph{transitive} if there exists only a single orbit.

    \item An element $x$ in a $G$-set $X$ is a \emph{fixed point} if $gx = x$ for every $g \in G$. The set of all fixed points is denoted $X^G$.
\end{itemize}

\begin{example}
    The permutation group $S_n$ acts on $\{ 1, \dots, n \}$ by permuting integers in the obvious manner. The action is faithful and transitive, and for any $i$, the stabilizer subgroup $S_i$ is isomorphic to $S_{n-1}$.
\end{example}

\begin{example}
    The group $D_n$ acts faithfully and transitively on the set $V_n$ of vertices of the regular $n$ sided polygon. Then for each $p \in V_n$, the stabilizer subgroup $(D_n)_p$ consists of the identity and the unique reflection in $D_n$ about a line passing through $p$; thus $(D_n)_p$ is isomorphic to $\ZZ_2$.
\end{example}

\begin{example}
    Any set $G$ acts on itself by multiplication, i.e. for $g,x \in G$, $gx$ is just group multiplication. This is the representation we used to prove Cauchy's theorem. If $H$ is a subgroup of $G$, then $H$ operates on $G$ by multiplication on the right (as a right group action), and the orbits of this action are precisely the cosets $G/H$.
\end{example}

\begin{example}
    A deeper example of a group action of $G$ on itself is given by \emph{conjugation}, i.e. the group action given by the group representation
    %
    \[ \phi(g)(x) = \prescript{g}{}{x} = gxg^{-1}. \]
    %
    This action is never transitive; indeed, $\prescript{g}{}{e} = e$ for all $g \in G$, so the action always has a fixed point. The kernel of the conjugation representation is precisely the centre $Z(G)$ of $G$. For each $g \in G$, the \emph{stabilizer} of $g$ is precisely the \emph{centralizer subgroup}
    %
    \[ C_G(h) = \{ g \in G: gh = hg \}. \]
    %
    An interesting fact is that $\phi(g)$ is a \emph{homomorphism} of $G$ for each $g \in G$, any such homomorphism being known as an \emph{inner homomorphism}. There is a special name for such group actions. A homomorphism $\phi: G \to \text{Aut}(H)$ for two groups $G$ and $H$ is called a \emph{group representation} of $G$. An automorphism of $G$ is called \emph{inner} if it is induced by conjugation by some element of the group, and forms a subgroup $\text{Inn}(G)$.
\end{example}

\begin{example}
    A very similar group action is obtained by letting $X$ be the set of all subgroups of $G$. Then $G$ acts on $X$ by conjugation, i.e. letting $\phi: G \to S(X)$ by given by setting $\phi(g)(H) = \prescript{g}{}{H} = gHg^{-1}$. The stabilizer of a subgroup $H$ is the normalizer $N_G(H)$, and the fixed points of this action are precisely the normal subgroups of $G$. Note also that in this case $\phi(g)$ is not an arbitrary element of $X$; The collection of subgroups $X$ has the form of a lattice, and $\phi(g)$ acts as an order preserving bijection of this lattice.
\end{example}

\begin{example}
    If $M$ is a compact manifold, and $X$ is a vector field on $M$, then there is a unique family of diffeomorphisms $\{ \phi_t: t \in \RR \}$ of $M$ such that for each $x \in M$ and $s \in \RR$,
    %
    \[ \left. \frac{\partial \phi_t(x,s)}{\partial t} \right|_{t = 0} = X_s. \]
    %
    One verifies that $\phi_t \circ \phi_s = \phi_{t + s}$ for each $t,s \in \RR$, so the map $t \mapsto \phi_t$ is a permutation representation of $\RR$ on the set $M$.
\end{example}

\begin{example}
    Consider the group $SL_n(\mathbf{R})$ acting on the upper half of the complex plane, the set
    %
    \[ \mathbf{H} = \{ z \in \mathbf{C} : \im(z) > 0 \} \]
    %
    by the mobius transform
    %
    \[\begin{pmatrix} a & b \\ c & d \end{pmatrix} \cdot z = \frac{az + b}{cz + d}\]
    %
    This defines a transitive action. The isotropy subgroup of the imaginary unit $i$ is the special orthogonal group $SO(2)$, the set of matrices with orthonormal columns. A meromorphic function on $H$ invariant under $SO(2)$ is called a modular function, and is essential to the study of number theory, string theory, and the study of monstrous moonshine.
\end{example}

For any group action $G$ on a set $X$, it is easy to verify that $G_{gx} = \prescript{g}{}{G_x}$. Thus stabilizers of elements belonging to a common orbit are isomorphic by an inner automorphism. In particular, if $G$ acts faithfully and transitively on $X$, then for any $x_0 \in X$,
%
\[ \bigcap_{g \in G} \prescript{g}{}{G_{x_0}} = \bigcap_{x \in X} G_x = \{ e \}. \]
%
This gives a useful relation between the conjugate subgroups corresponding to stabilizers.

\begin{theorem}
    Suppose $G$ is an abelian group acting faithfully on a set $X$. Then if $g \neq e$, then $gx \neq x$ for all $x \in X$. If $G$ acts transitively on $X$, then $\#(G) = \#(X)$.
\end{theorem}
\begin{proof}
    For each $x \in X$ and $g \in G$, the fact that $G$ is Abelian implies $\prescript{g}{}{G_x} = G_x$. But then the previous equation tells us that
    %
    \[ G_x = \bigcap_{g \in G} \prescript{g}{}{G_x} = \{ e \}. \]
    %
    Thus we conclude that $gx \neq x$ for any $g \neq e$. If $G$ acts transitively on $X$, and we fix $x_0 \in X$, then the map $g \mapsto gx_0$ is a bijection from $G$ to $X$.
\end{proof}

Let $G$ be a group acting transitively on a set $X$. a \emph{block} of the action is a set $A \subset X$ such that for each $g \in G$, $g(A) = A$, or $A \cap G(A) = \emptyset$. The family of sets
%
\[ Y = \{ gA: g \in G \} \]
%
is then a partition of $X$. To understand the action of $G$ on $X$, it then clearly suffices to analyze the action of $G$ on $A$, and of the action of $G$ on $Y$. A \emph{primitive} group action is a transitive one for which only trivial blocks exist; the entire set $X$ and singletons.

\begin{theorem}
    A group action of a group $G$ on a set $X$ is primitive and transitive if and only if for each $x \in X$, the subgroup $G_x$ is a maximal proper subgroup of $G$.
\end{theorem}
\begin{proof}
    Suppose $G_x$ is maximal for each $x \in X$. If $A$ is a block in $G$ containing some $a \in A$, then we can look at the set
    %
    \[ G_A = \{ g \in G: g(A) = A \}. \]
    %
    Then $G_A$ is a group. If $A$ contains two distinct points $x_1$ and $x_2$, then by transitivity, there is $g \in G$ such that $gx_1 = x_2$. Because $A$ is a block, $g \in G_A$. But this means that $G_{x_1}$ is a proper subset of $G_A$, so $G_A = G$, which implies $A = X$ by transitivity. Thus all blocks are trivial.

    Conversely, suppose $G$ acts primitively and transitively on $X$. Fix $x \in X$, and suppose $H$ is a subgroup of $G$ containing $G_x$. We claim $A = Hx$ is a block. Indeed, if $g \in G$ and $g(A) \cap A$ is non-disjoint, we can find $h_1,h_2 \in H$ such that $gh_1 x = h_2 x$. Then $h_2^{-1}gh_1$ fixes $x$, so $h_2^{-1}gh_1 \in G_x \subset H$. But this means that $g \in H$, so that $g(A) = A$. Thus we conclude that either $A = X$ or $A$ is a singleton. If $A$ is a singleton, then $H = G_x$. If $A = X$, then $H = G$. Thus $G_x$ is a maximal proper subgroup of $G$ for each $x \in X$.
\end{proof}

We now give a theorem which establishes an intricate connection between $G$ and its $G$-sets, which proves especially useful in finite group theory.

\begin{theorem} [Orbit Stabilizer Lemma]
    Let $X$ be a $G$-set. Then, for every $x$ in $X$, there exists a $G$-isomorphism from $G/G_x$ to $Gx$. It follows that
    %
    \[ \#(Gx) = (G:G_x) \]
\end{theorem}
\begin{proof}
    Define a mapping by
    %
    \[ gG_x \mapsto gx \]
    %
    We leave the reader to verify this is a well defined function. The reasoning is similar to the verification of the function created in the first isomorphism theorem. This mapping is surjective by construction, and furthermore, the map is injective. If $gx = hx$, then $(h^{-1}g)x = x$, hence $(h^{-1}g) \in G_x$, so $gG_x = hG_x$. The mapping is also a $G$-isomorphism, hence we have constructed the required isomorphism.
\end{proof}

\begin{corollary}[The Orbit Decomposition Formula] \index{Orbit Decomposition Formula}
    Given a $G$-set $X$, with a finite number of orbits $(X_1, X_2, \dots, X_n)$. From each orbit, pick a representative $x_i$. Then we have
    %
    \[ \#(X) = \sum_{k=1}^n (G:G_{x_i}) \]
    %
    which we call the orbit decomposition formula. In particular, for every orbit which is a singleton $\{ x \}$, $G_x = G$, hence $(G:G_x) = 1$; thus, if we collect all these orbits, and remove them from the list we have, we obtain that
    %
    \[ \#(X) = \#(X^G) + \sum_{k=1}^{n} (G:G_{x_i}) \]
    %
    where $\{ x_1, \dots, x_n \}$ is the new set of orbit representatives with size greater than one.
\end{corollary}
\begin{proof}
    $X$ is the disjoint union of its orbits. Hence
    %
    \[ \#(X) = \sum_{k=1}^n \#(Gx_i) \]
    %
    But we have contructed an isomorphism from $Gx_i$ to $G/G_{x_i}$ above, hence
    %
    \[ \#(Gx_i) = \#(G/G_{x_i}) \]
    %
    and we obtain the final formula by Lagrange's theorem.
\end{proof}

This theorem will be very useful for our next topic of study, Sylow theory. Before we get into this theory, let us consider an example to show the power of the class equation. Consider a group of $G$ order 55 acting on a set $X$ of order 39. We claim there is at least one fixed point in the group action. The orbit decomposition formula entails that we have
%
\[ \#(X) = 39 = \#(X^G) + \sum_{k=1}^n (G:G_{x_i}) \]
%
Each $G_{x_i}$ forms a subgroup of $G$, hence by Lagrange's theorem, $\#(G_{x_i})$ divides $55$. Thus $\#(G_{x_i}) \in \{ 1, 5, 11, 55 \}$. If $\#(G_{x_i}) = k$, then $(G:G_{x_i}) = 55/k$, so if we let $m_j$ denote the number of orbits whose isotropy subgroups are order $j$. Then
%
\[ 39 = 55m_1 + 11m_5 + 5m_{11} + m_{55}. \]
%
Since $39$ cannot be written as a sum of positive multiples of $11$ and $5$, this means that in any solution to the equation above, $m_{55} \geq 1$. But this means there exists at least one isotropy subgroup with order $55$, which corresponds precisely to a fixed point. Thus the action of $G$ on $X$ has a  fixed point.

\section{Conjugacy Classes}

For any group $G$, $G$ acts on itself by conjugation; each $g$ acts on $G$ as the map $x \mapsto \prescript{g}{}{x}$. For each $g \in G$, the stabilizer of $g$ with respect to conjugation is precisely the \emph{centralizer} of $g$, i.e. $C_G(g)$. A simple application of the orbit stabilizer formula gives the \emph{class equation} for this group action. For each $x \in G$, we let $\text{Cl}(x)$ be the set of elements conjugate to $x$; this is precisely the orbit of $x$ with respect to the group action.

\begin{corollary}[The Class Equation] \index{Class Equation}
    For any group $G$, let $x_1,\dots,x_n$ be representatives of the conjugacy classes of elements of $G$. Then
    %
    \[ \#(G) = \#(Z(G)) + \sum_{i = 1}^n (G:C_G(x_i)). \]
\end{corollary}

\begin{example}
    Let us now apply conjugation to determine some structure of $S_n$. Clearly any two permutations of the same cycle type are conjugate in $S_n$. In particular, this implies that $Z(S_n) = \{ e \}$. In particular, if $\sigma$ is an $m$-cycle in $S_n$, then $\text{Cl}(\sigma)$ is the collection of all $m$ cycles, of which there are
    %
    \[ {{n} \choose {m}} \frac{m!}{m} = \frac{n!}{m (n-m)!}. \]
    %
    It thus follows that the centralizer $C(\sigma)$ has cardinality $m (n-m)!$. But clearly $\sigma$ commutes with all $m$-powers of itself, as well as any permutation fixing all the points in the cycle of $\sigma$. These two classes commute, and form a family of $m (n-m)!$ permutations. Thus these are the only permutations fixed by $\sigma$, so $C(\sigma) \cong \ZZ_m \times S_{n-m}$!
\end{example}

\begin{example}
    We can also use conjugation to understand the structure of $A_5$. Now the cycle types of $A_5$ are of the form $e$, $(1\; 2)(3\; 4)$, $(1\; 2\; 3)$, and $(1\; 2\; 3\; 4\; 5)$. Since $C_{A_5}(\sigma) = C_{S_5}(\sigma) \cap A_5$, and since we know the centralizers of 3-cycles and 5-cycles exactly, we easily calculate that $C_{A_5}(\sigma) = \langle \sigma \rangle$ for any cycle $\sigma$ in $A_5$. Thus the number of elements conjugate to a 3-cycle is $5!/2 \cdot 3 = 20$, which is precisely the number of 3-cycles in $A_5$. Thus all 3-cycles are conjugate. Similarily, the number of elements conjugate to a 5-cycle is $5!/2 \cdot 5 = 12$. Thus not all 5-cycles are conjugate in $A_5$, there are two families of 12 5-cycles which are conjugate. From this calculation, we know no product of disjoint two-cycles commutes with any 3 or 5 cycle. Thus it can only commute with products of 2 cycles and the identity. We calculate directly that $(1\; 2)(3\; 4)$ commutes with $(1\; 3)(2\; 4)$. Since $(1\; 2)(3\; 4)$ does not commute with any 3-cycle or any 5-cycle, the order of $C_{A_5}((1\; 2)(3\; 4))$ cannot be divisilbe by any odd number. But this means that $C_{A_5}((1\; 2)(3\; 4))$ has precisely four elements, i.e.
    %
    \[ C_{A_5}((1\; 2)(3\; 4)) = \{ e, (1\; 2)(3\; 4), (1\; 3)(2\; 4), (1\; 4)(2\; 3) \}. \]
    %
    This implies all products of 2-cycles are conjugate in $A_5$. But now we can prove very easily that $A_5$ is a simple group, since if $G$ is a normal subgroup of $A_5$, it is a union of conjugacy classes, which contain $1,15, 12, 12$, and $20$. But the constraint that the order of $G$ divides 60, and the fact that $G$ contains the identity conjugacy classes implies either $G = \{ e \}$ or $G = A_5$.
\end{example}

\section{Burnside's Lemma}

Burnside's lemma is a useful theorem for calculating objects with symmetry. Recall that for a group $G$ acting on a set $X$, and for each $g \in G$, then $X^g = \{ x \in X: gx = x \}$.

\begin{lemma}[Burnside's Lemma] \index{Burnside's Lemma}
    If $X$ is a finite $G$-set, then
    %
    \[ \#(X/G) \#(G) = \sum_{g \in G} \#(X^g) \]
\end{lemma}
\begin{proof}
    By a simple calculation,
    %
    \[ \sum_{g \in G} \#(X^g) = \#(\{(g,x): gx = x\}) = \sum_{x \in X} \#(G_x) \]
    %
    Combining this calculation with the orbit stabilizer lemma, we obtain that
    %
    \[ \sum_{x \in X} \#(G_x) = \sum_{x \in X} \#(G) (G:G_x)^{-1} = \#(G) \sum_{x \in X} (G:G_x)^{-1} \]
    %
    Now $(G:G_x) = \#(Gx)$, hence
    %
    \[ \#(G) \sum_{x \in X} (G:G_x)^{-1} = \#(G) \sum_{x \in X} \#(Gx)^{-1} \]
    %
    Now partition $X$ into its orbit $X/G$. For each $x$ and $y$ in a particular orbit, it is obvious that $\#(Gx) = \#(Gy)$. Hence, if we have a partition $(X_1, X_2, \dots, X_{\#(X/G)})$, and we pick representatives from each $x_i$ from each $X_i$, we have that
    %
    \[ \#(G) \sum_{x \in X} \#(Gx)^{-1} = \#(G) \sum_{k = 1}^{\#(X/G)} \#(X_k) \#(Gx_k)^{-1} \]
    %
    Now for each $\#(X_k)$, we have that $\#(Gx_i) = \#(X_k)$ by definition, so finally, we obtain that
    %
    \[ \#(G) \sum_{k = 1}^{\#(X/G)} \#(X_k)\#(Gx_k)^{-1} = \#(G) \sum_{k = 1}^{\#(X/G)} \#(Gx_i)/\#(Gx_i) = \#(G) \sum_{k = 1}^{\#(X/G)} 1 = \#(G) \#(X/G) \]
    %
    and by transitivity, our proof is complete.
\end{proof}

Burnside's lemma is often useful to count objects up to isomorphism.

\begin{example}
    Suppose we want to calculate the number of ways to color the vertices of a square with $n$ colors, up to rotations and reflections of the square. If we let $X$ be the set of all colorings where we do \emph{not} account for isomorphisms, then $X$ contains $n^4$ elements. If $G$ is the set of rotations of the square, then we wish to count $X/G$, which we will do by Burnside's lemma. There are five conjugacy classes; if $r$ and $s$ are primitive rotations and reflections, then the conjugacy classes are $\{ \{ e \}, \{ r^2 \}, \{ r, r^3 \}, \{ s, r^2s \}, \{ rs, r^3s \} \}$, and the number of points in $X^g$ depends only on the conjugacy class of $g$. Thus we calculate that
    %
    \begin{align*}
        \#(X/G) &= \frac{1}{8} \left( 1 \cdot n^4 + 1 \cdot n^2 + 2 \cdot n + 2 \cdot n^2 + 2 \cdot n^3 \right)\\
        &= \frac{n^4 + 2n^3 + 3n^2 + 2n}{8}.
    \end{align*}
\end{example}

\section{Sylow Theory}

In 1872, Norwegian mathematician Ludwig Sylow proved a collection of theorems, called the Sylow theorems\index{Sylow Theorems}, which give detailed information about subgroups of a certain size within a finite group. Let us begin by summarizing the results in a single proposition.

\begin{theorem}
    Let $G$ be a finite group of order $p^n q$, where $p$ does not divide $q$. Then
    %
    \begin{itemize}
        \item $p$-Sylow subgroups exist.
        \item Any $p$-group is contained in a $p$-Sylow subgroup, and all Sylow subgroups are conjugate.
        \item $n_p(G) \equiv 1 \pmod{p}$. and $n_p(G)$ divides $q$.
    \end{itemize}
\end{theorem}

\begin{theorem}
    For every finite abelian group, and every prime number which divides the order of the group, there is an element whose order is that prime number.
\end{theorem}
\begin{proof}
    Let $G$ be an abelian group, and $p$ a prime number such that $p \divides \#(G)$. We prove this statement by induction on $\#(G)$. When $\#(G) = 1$, the statement holds vacously. Now suppose this theorem holds for all group sizes less than the order of another group $G$. Take an element $g$ in $G$ that is not the identity. If the order of $g$ is $pm$, then $g^m$ is order $p$. Instead, assume that $g$'s order is not divisible by $p$. Since $G$ is abelian, $\gen{g}$ is normal, hence we can form the group $G/\gen{g}$. We know that $\#(G) = \#(G/\gen{g}) \#(\gen{g})$. We know that $\#(\gen{g})$ does not divide $p$, hence $p$ must divide $\#(G/\gen{g})$. As $g$ is not the identity, we know the factor group is smaller than $G$, hence by induction, there is some element $h$ in $G$ such that $h \gen{g}$ is order $p$. Let $n$ be the order of $h$. Then of course, since $h^n = e$, $p$ divides $n$. Using the same technique as before, we can obtain an element of order $p$ from powers of $h$.
\end{proof}

A theorem of Cauchy generalizes this idea to arbitrary groups.

\begin{theorem}[Cauchy's theorem] \index{Cauchy's Theorem}
    Given any group whose order divides a prime, there is an element whose order is that prime.
\end{theorem}
\begin{proof}
    We prove this theorem by induction again. We need no base case, as a group of any size less than 6 is abelian and thus we can apply the previous case. Now suppose the theorem holds for all groups of order less than a group $G$. Let $p$ be a prime, and suppose $p \divides \#(G)$. If $G$ contains a proper subgroup whose order is divisible by $p$, then we can apply induction rather easily to show that this theorem holds for $G$. The hard part is when $G$ contains no proper subgroup whose order is divisible by $p$. Consider $G$ acting on itself by conjugation. For every element $g$, the centralizer $C_G(g)$ is a subgroup of $G$. By Lagrange's theorem,
    %
    \[ \#(G) = \#(C_G(g))(G:C_G(g)) \]
    %
    The class equation also gives us that
    %
    \[ \#(G) = \#(Z(g)) + \sum_{k = 1}^{n-1} (G:C_G(x_i)) \]
    %
    If $g$ is not in $Z(g)$, $C_G(g)$ is a proper subgroup of $G$, so by our assumption $p \not \divides \#(C_G(g))$, and by the equation created by Lagrange's theorem, we obtain that $p \divides (G:C_G(g))$. But then by rearranging the class equation, we obtain that $p \divides \#(Z(g))$, hence $Z(g)$ cannot be a proper subgroup, and so $G = Z(g)$. Thus $G$ is abelian, and we can apply the previous theorem. By case to cases analysis we obtain the truth of the statement.
\end{proof}

A finite group $G$ is called a \emph{$p$-group} if it's order is equal to $p^n$ for some integer $n$. By Cauchy's theorem, we obtain an interesting corollary: a group is a $p$-group if and only if every element has order a power of a prime. The prime structure of a group gives powerful consequences about the structure of a group.

\begin{lemma}
    Let $G$ be a $p$-group. If $G$ acts on a finite set $X$, then the number of fixed point of the action is congruent to $\#(X)$ modulo $p$.
\end{lemma}
\begin{proof}
    The orbit stabilizer formula says that if $x_1,\dots,x_n$ are representatives of the nontrivial orbits, then
    %
    \[ \#(X) = \#(X^G) + \sum_{i = 1}^n (G:G_{x_i}). \]
    %
    Since $G_{x_i} \neq G$ for each $i$, $p$ divides each element of the sum, which gives the congruence statement.
\end{proof}

\begin{lemma}
    Let $G$ be a nontrivial $p$-group. Then $Z(G)$ is nontrivial.
\end{lemma}
\begin{proof}
    Let $G$ act on itself by conjugation. Then the last lemma implies that $\#(Z(G))$, which are the fixed points of conjugation, is divisible by $p$. But $\#(Z(G)) \geq 1$ so $\#(Z(G)) \geq p$.
\end{proof}

\begin{remark}
    It follows from this statement that all $p$-groups are nilpotent, and in particular, solvable.
\end{remark}

\begin{corollary}
    Let $p$ be a prime. Every group of order $p^2$ is abelian.
\end{corollary}
\begin{proof}
    Let $G$ be a group of order $p^2$. Then $Z(G)$ is nontrivial. But $G/Z(G)$ is cyclic, from which it follows that $G$ is abelian.
\end{proof}

Let $G$ be a group of order $p^mq$, where $p$ is a prime and $q$ and $p$ are relatively prime. Then a subgroup is called a \emph{p-Sylow subgroup} \index{Sylow Subgroup} if the order of the subgroup is a power of $p^m$ -- the maximum order of a $p$ subgroup in $G$. In the next few proofs, let $G$ be a group of cardinality $p^mq$ where $q$ does not divide $p$.

\begin{lemma}
    For every $k$ such that $1 \leq k \leq m$, $G$ has a subgroup of order $p^k$.
\end{lemma}
\begin{proof}
    We prove by induction on the size of $m$. Observe if $m = 0$, the theorem holds trivially; simply consider the trivial subgroup. Now suppose by induction that for all groups of smaller cardinality than $G$ the theorem holds. Consider the group action of conjugation of $G$ acting on itself, and let $x_1,\dots, x_n$ be representatives of the conjugacy classes. We know by the class equation that
    %
    \[ \#(G) = \#(Z(G)) + \sum_{i = 1}^n (G:C_G(x_i)) \]
    %
    We consider two cases to our proof. One where $p$ divides the order of $Z(G)$, and one where it does not. Suppose that $p$ does not divide the order of $Z(G)$. This implies that there is at least one $x_i$ such that $p \not \divides (G:C_G(x_i))$, and thus $p^m$ divides $C_G(x_i)$. Thus we can write $\#(C_G(x_i)) = p^mq'$, as the index takes no powers of $p$ away, and $q' < q$. Thus we can apply induction in this case. On the other hand, suppose $p$ divides the order of $Z(G)$. Pick $g \in Z(G)$ of order $p$. Then $H = \langle g \rangle$ is a normal subgroup of $G$ of order $p$, so we can apply induction on $G/H$.
\end{proof}

\begin{lemma}
    Let $H$ be a $p$-subgroup of $G$, and $P$ a $p$-Sylow subgroup. If $H \subset N_G(P)$, then $H \subset P$.
\end{lemma}
\begin{proof}
    Since $H \subset N_G(P)$, we know $HP$ is a subgroup of $G$ contained in $N_G(P)$, and $P$ is a normal subgroup of $HP$. By the second isomorphism theorem, $(HP:P) = (H: H \cap P)$. By Lagrange's theorem, $(H: H \cap P)$ is not divisible by $p$. But since $H$ is a $p$-group, $(H: H \cap P) = 1$, so $H \cap P = H$. Thus $H \subset P$.
\end{proof}

This theorem can be easily strengthened.

\begin{theorem}
    If $H$ is any $p$-subgroup of $G$, and $P$ a $p$-Sylow subgroup. Then $H$ is contained in some $p$-Sylow subgroup of $G$ that is conjugate to $P$.
\end{theorem}
\begin{proof}
    Let $X = G/P$, and let $H$ act on $X$ by left multiplication. The cardinality of $X$ is $p^n q / p^n = q$. Since $H$ is a $p$-group, we know that the number of fixed points of the action is congruent to $q$ modulo $p$, and since $p$ does not divide $q$, this means there is at least one fixed point to this action. Thus there exists $g \in G$ such that $hgP = gP$ for all $h \in H$. This means that $g^{-1}hg \in P$ for all $h \in H$. Thus $H \subset gPg^{-1}$. Since $gPg^{-1}$ is conjugate to $P$, it is also a $p$-Sylow subgroup, from which the claim follows.
\end{proof}

\begin{corollary}
    All $p$-Sylow subgroups are conjugate.
\end{corollary}

\begin{corollary}
    A $p$-Sylow subgroup is normal if and only if it is the unique $p$-Sylow subgroup.
\end{corollary}

For each group $G$ and prime $p$ dividing the order of $G$, we let $n_p(G)$ be the number of $p$-Sylow subgroups of $G$.

\begin{theorem}
    $n_p(G)$ divides $q$.
\end{theorem}
\begin{proof}
    Let $S$ be a $p$-Sylow subgroup of $G$ of order $p^k$, and let $X$ be the set of all $p$-sylow subgroups of $G$. Since all $p$-Sylow subgroups are conjugate to each other, the action of conjugation from $G$ on $X$ is transitive. Consider the normalizer $N_G(S)$. We obtain from the class equation that
    %
    \[ n_p(G) = \#(X) = (G:N_G(S)). \]
    %
    But since $p$ does not divide $(G:N_G(S))$ since $S \subset N_G(S)$, we conclude $(G:N_G(S))$ divides $q$. Thus $n_p(G)$ divides $q$.
\end{proof}

\begin{theorem}
    $n_p(G) \equiv 1 \pmod{p}$
\end{theorem}
\begin{proof}
    Let $S$ be a $p$-Sylow subgroup. Then $S$ acts on the set of all $p$-Sylow subgroups via conjugation. The group $S$ is the only group fixed by this action, for if $P$ is another $p$-Sylow subgroup and $sPs^{-1} = P$ for all $s \in S$, then $S \subset N_G(P)$, hence $S \subset P$, so that $S = P$. But since $S$ is a $p$-group, we obtain the theorem.
\end{proof}

This concludes the proof of the Sylow theorems. The remaining part of this section applies the results to classify groups of small order. For small groups, Sylow's theorem often forced a normal Sylow subgroup to appear. The largest prime power often gives the largest restriction on the number of Sylow subgroups. For larger groups one often also needs additional arguments to obtain the existence of a normal Sylow subgroup.

\begin{theorem}
    Let $p$ and $q$ be primes, with $p < q$ and such that $q-1$ is not divisible by $p$. Then any group of order $pq$ is abelian.
\end{theorem}
\begin{proof}
    Suppose $G$ is a group of order $pq$, where $p$ and $q$ are distinct primes with $p < q$. Sylow's theorem implies $n_p(G) \in \{ 1, q \}$ and $n_q(G) \in \{ 1, p \}$, where $n_p(G)$ is congruent to 1 modulo $p$ and $n_q(G)$ is congruent to 1 module $q$. The second statement implies $n_q(G) = 1$, and if $q - 1$ is not divisible by $p$, then the first statement forces $n_p(G) = 1$. Thus $G$ has two normal subgroups $H_1$ and $H_2$ of order $p$ and $q$ respectively. Now the action of conjugation of $H_2$ on $H_1$ induces a homomorphism from $H_2$ to $\text{Aut}(H_1) = \text{Aut}(\ZZ_p)$, which is isomorphic to $\ZZ_p^\times$, and has order $p-1$. But then the first isomorphism theorem combined with the prime division properties implies that this homomorphism is trivial. But this implies that all elements of $H_2$ commutes with elements in $H_1$. Since $G = H_1H_2$, this implies $H_2 \in Z(G)$, hence $G$ is abelian.
\end{proof}

\begin{remark}
    On the other hand, if $p$ divides $q-1$, then there is a unique nonabelian group of order $pq$. If $G$ is nonabelian and has order $pq$, then we have $n_q(G) = 1$ and $n_p(G) = q$. Let $H_1$ be a subgroup of order $p$, and let $H_2$ be the unique normal subgroup of $G$ of order $q$. Then $G = H_1H_2$, and the algebraic structure of $G$ is uniquely determined by the action of conjugation of $H_1$ on $H_2$, since $G$ has the structure of a semidirect product induced by this action. Conjugation induces a homomorphism from $H_1$ to $\text{Aut}(H_2) \cong \text{Aut}(\ZZ_q)$, which is cyclic of order $q-1$. Since $p$ divides $q-1$, there is a unique nontrivial homomorphism from $H_1$ to $\text{Aut}(H_2)$. Thus $G$ is uniquely determined.
\end{remark}

Let's up the difficulty to a higher power of $p$.

\begin{theorem}
    Let $G$ be a group with cardinality $p^2q$, where $p$ and $q$ are prime, $p < q$, and $p \not \divides (q-1)$. Then $G$ is abelian.
\end{theorem}
\begin{proof}
    The Sylow theorems imply that $G$ has a unique normal subgroup $H_1$ of order $p^2$ and a unique normal subgroup $H_2$ of order $q$. Then $G = H_1H_2$. Prime division imply the action of conjugation on $H_2$ is trivial. Thus $H_1$ and $H_2$ commute, so $G$ is isomorphic to $H_1 \oplus H_2$, and is Abelian since any group of order $p^2$ is Abelian.
\end{proof}

\begin{theorem}
    Let $G$ be a finite group, and $p$ the smallest prime of $G$. A subgroup of index $p$ is normal in $G$.
\end{theorem}
\begin{proof}
    Let $H$ be a subgroup of $G$ of index $p$. Consider $G/H$. $G$ acts on $G/H$ by operation on the left. This is a homomorphism from $G$ to $S_p$. Suppose $g$ is in the kernel of hormomorphism. Then $gg'H = g'H$ for every coset $g'H$. In particular, $gH = H$, hence $g$ is in $H$. Let the kernel of the homomorphism be $K$. Then $G/K$ is isomorphic to a subgroup of $S_p$, and hence its cardinality must divide $p!$. But this means that
    %
    \[ (G:K) = (G:H)(H:K) = p(H:K) \divides p! \]
    %
    hence $(H:K) \divides (p-1)!$. Now $p$ is the smallest factor in $\#(G)$, and $(H:K) \divides \#(G)$, hence the only possible conclusion is that $(H:K) = 1$, else $\#(G)$ has a smaller factor. This means exactly that $H = K$, and hence $H$ is normal in $G$ as it is the kernel of a homomorphism.
\end{proof}









\chapter{Solvability}

Solvability is the key to Galois' proof of the insolvability of the quintic. Furthermore, solvability is used in many more advanced settings throughout algebra. Thus it makes sense to introduce it in a group theory course before Galois theory to smoothen the transition between the theories.

Let $G$ be a group. A \emph{series} \index{Series} or \emph{tower} \index{Tower} is a finite sequence of groups beginning with $G$, and such that every sequential group is a subgroup of the previous.

To aid in remembering the definition, we write a sequence $(G_0, G_1, \dots, G_m)$ which forms a tower as
%
\[ G = G_0 \supset G_1 \supset G_2 \supset \dots \supset G_m \]
%
This chapter focuses on a very specific type of tower.

A tower is called a \emph{normal series}\index{Normal Series} if every group in the tower is normal in its predecessor, so for each $G_i$ that is not at the end, we may form the factor group $G_i/G_{i+1}$ with the next element in the sequence. A normal series is \emph{abelian}\index{Abelian Normal Series} if each such factor group is abelian, and \emph{cyclic} \index{Cyclic Normal Series} if every factor group is cyclic.

As with the notation for an ordinary tower, we write a normal series $(H_0, H_1, \dots, H_m)$ as
%
\[ H = H_0 \rhd H_1 \rhd \dots \rhd H_m \]
%
so you needn't remember the definition if you're reading someone elses work; the notation tells you all you need to know!

\begin{theorem}
    Consider a normal tower
    %
    \[ H = H_0 \rhd H_1 \rhd \dots \rhd H_m \]
    %
    and a homomorphism $\varphi$ from $G$ to $H$. Define a tower on $G$ by letting $G_i$ be $\varphi^{-1}(H_i)$. The tower then formed is a normal series. This tower is abelian/cyclic if and only if the other tower is abelian/cyclic.
\end{theorem}
\begin{proof}
    As $\varphi$ is any mapping from $G$ to $H$, we have that
    %
    \[ G_0 = \varphi^{-1}(H_0) = \varphi^{-1}(H) = G \]
    %
    Now we know $H_{i+1}$ is normal in $H_i$ for any index $i$ for which $H_i$ is defined. Restrict $\varphi$ to only elements of $G_i$. Then $\varphi$ is of course surjective onto $H_i$; we are then in the same position as Exercise (12), and we may conclude that $G_{i+1}$ is normal in $G_i$, and $G_i/G_{i+1} \cong H_i/H_{i+1}$, hence all algebraic properties needed transfer from the factor group of $H$ to the factor group of $G$.
\end{proof}

The property of having a normal tower is not special. For any group $G$, simply take the tower $G \supset \{e\}$, and that tower is trivially normal, but its factor groups do not really tell us anything about the group. The longer the tower, the more we separate the properties of the entire group as factor groups. It thus makes sense to take a tower that is maximalized in some way, to strain out as many properties as possible from the group.

A \emph{refinement}\index{Refinement (Series)} of a tower is a new tower obtained by inserting finitely more subgroups into the original tower.

We say two normal series $S$ and $T$ are \emph{equivalent} \index{Equivalency (Series)} if they have the same length and such that there is a permutation $\varphi$ such that, for any group $S_i$ in $S$ but the terminating subgroup, $S_i/S_{i+1} \cong T_{\varphi(i)}/T_{\varphi(i) + 1}$, so the factor groups obtained can really just be considered reorderings of one another.

The following lemma leads to an easy proof on the refinement of normal series. It's proof is perhaps the most technical in this report, but it at least has a nice picture corresponding with the lattice of subgroups to go along with it.%

%\begin{comment}
%\begin{center}
%\begin{tikzpicture}[description/.style={fill=white,inner sep=2pt}]
%\matrix (m) [matrix of math nodes, row sep=1.5em,
%column sep=0.3em, text height=1.5ex, text depth=0.25ex]
%{ U & & & & V \\
%&&&&\\
%U'(U \cap V)& & & &  V'(U \cap V) \\
%&&U \cap V&& \\ 
%U'(U \cap V')&&&& V'(U' \cap V)\\
%&&(U' \cap V)(U \cap V')&& \\
%U' &&&& V' \\
%&U' \cap V && U \cap V'& \\};
%
%\path[-] (m-1-1) edge (m-3-1)
%         (m-3-1) edge (m-4-3)
%                 edge (m-5-1)
%         (m-5-1) edge (m-6-3)
%                 edge (m-7-1)
%         (m-7-1) edge (m-8-2)
%         (m-8-2) edge (m-6-3)
%         (m-6-3) edge (m-8-4)
%                 edge (m-5-5)
%                 edge (m-4-3)
%         (m-7-5) edge (m-5-5)
%                 edge (m-8-4)
%         (m-1-5) edge (m-3-5)
%         (m-3-5) edge (m-4-3)
%                 edge (m-5-5);
%\end{tikzpicture}
%\end{center}

\begin{theorem}[The Butterfly Lemma (Zassenhaus' Lemma)] \index{Butterfly Lemma} \index{Zassenhaus' Lemma}
    Let $U$ and $V$ be subgroups of a group $G$, and let $U'$, $V'$ be such that $U' \lhd U$, $V' \lhd V$. Then
    %
    \[ U'(U \cap V') \lhd U'(U \cap V) \]
    %
    \[ V'(U \cap V) \lhd V'(U \cap V) \]
    %
    and the factor groups are isomorphic:
    %
    \[ \frac{U'(U \cap V)}{U'(U \cap V')} \cong \frac{(U \cap V)}{(U' \cap V)(U \cap V')} \cong \frac{V'(V \cap U)}{V'(V \cap U')} \]
\end{theorem}
\begin{proof}
    Our main strategy is to identify an isomorphism from the first formula to the second in the equation via the first isomorphism theorem. We will define a mapping from $U'(U \cap V)$ to $(U \cap V')/(U' \cap V)(U \cap V')$. Let the following mapping $u'x \mapsto x(U' \cap V)(U \cap V')$ be constructed. This mapping is well defined: If it is true that $ux = u'x'$, then $u'u^{-1} = xx'^{-1} \in U' \cap (U \cap V) = U' \cap V \subset (U' \cap V)(U \cap V')$, hence $x(U' \cap V)(U \cap V') = x'(U' \cap V)(U \cap V')$. Let us hope that the kernel of this mapping is $U'(U \cap V')$. We know that the kernel is precisely those elements representable as $u'x$, where $x \in (U' \cap V)(U \cap V')$, or that $u'x$ is an element of $U'(U' \cap V)(U \cap V') = U'(U \cap V')$, hence the kernel is $U'(U \cap V')$, and we have shown the isomorphism from first formula to second by the first isomorphism theorem, as the map is surjective. As the problem is symmetric, we obtain the isomorphism from third to second, and thus the entire chain of isomorphisms is created by transitivity of isomorphism.
\end{proof}

Do not worry if the statement above is unintuitive. It is only really a mechanic to be used in the next Theorem, and the author knows of no other use of it outside of this context.

\begin{theorem}[Shreier]
    Two normal series in a group $G$ ending with the trivial group have refinements that are equivalent.
\end{theorem}
\begin{proof}
    Consider two normal towers
    %
    \[ G = G_0 \rhd G_1 \rhd \dots \rhd G_n = \{ e \} \]
    %
    \[ G = G'_0 \rhd G'_1 \rhd \dots \rhd G'_m = \{ e \} \]
    %
    Define $G_{i,j} = G_{i+1}(G'_j \cap G_i)$ for $i$ between 0 and $n-1$ and $j$ between 0 and $m$. Then we have the tower
    %
    \begin{alignat*}{2}
    G = G_1(G) &= G_1(H_0 \cap G_0) &\\
               &= G_{0,0} \supset G_{0,1} \supset \dots \supset G_{0,m} \supset G_{1,0} \supset \dots &&\supset G_{n-1,m}\\
                &           &&= G_n(H_m \cap G_{n-1}) = \{ e \}
    \end{alignat*}
    %
    Similarily, if we define $G'_{i,j} = G'_{i+1}(G_j \cap G'_i)$, with a tower of $G'_j$ generated in a similar fashion. By the butterfly lemma, with $U = G_{i+1}$, $U' = G_i$, $V = G'_{j+1}$, and $V' = G'_j$, we obtain that
    %
    \[ G_{i,j}/G_{i,j+1} \cong H_{i,j}/H_{i,j+1} \]
    %
    We must also show the equivalency for $G_{i,m}$, $G_{i+1,0}$, $G'_{i,m}$, and $G'_{i+1,0}$. What are these groups?
    %
    \[ G_{i,m} = G_{i+1}(G'_m \cap G_i) = G_{i+1}\{e\} = G_{i+1} \]
    \[ G_{i+1,0} = G_{i+2}(G'_0 \cap G_{i+1}) = G_{i+2}G_{i+1} = G_{i+1} \]
    \[ G'_{i,m} = G'_{i+1} \]
    \[ G'_{i+1,0} = G'_{i+1} \]
    %
    and hence
    %
    \[ G_{i,m}/G_{i+1,0} \cong \{e\} \cong G'_{i,m}/G_{i+1} \]
    %
    We have verified the tower is normal and equivalent. They also refine the original towers as
    %
    \[ G_{k,0} = G_k(G'_0 \cap G_{k-1}) = G_k(G \cap G_{k-1}) = G_kG_{k-1} = G_k \]
    %
    and similarily for $G'_{k,1}$, so we may embed the original tower in the new one.
\end{proof}

The main corollary requires a new concept, which follows so simply we state it without proof.

A \emph{composition series}\index{Composition Series} is a normal series which cannot be refined.

\begin{corollary}[Jordan H\"{o}lder]
    All composition series of a set $G$ are equivalent.
\end{corollary}

All finite groups possess a composition series, as there are only finitely many subgroups of the group. We note this is not true of all groups. Consider the additive group $\bint$. Then every subgroup is of the form $a\bint$ for some $a$, and every subgroup is normal. Suppose we have a normal series
%
\[ \bint \rhd a_1\bint \rhd a_2\bint \rhd \dots \rhd a_n\bint \]
%
Then we can always refine it to
%
\[ \bint \rhd ma_1\bint \rhd a_1\bint \rhd a_2\bint \rhd \dots \rhd a_n\bint \]
%
for any integer $m$ greater than one. This shows that there are no composition series because, given any series, we can always refine it.

Composition series can be considered the maximality of a normal series. Simple groups are minimalizations of normality. It is intuitive to connect these concepts. This theorem characterizes this.

\begin{theorem}
    A normal series is a composition series if and only if all factor groups in the series are simple.
\end{theorem}
\begin{proof}
    Consider an arbitrary normal series
    %
    \[ G = G_0 \rhd G_1 \rhd \dots \rhd G_n = \{ e \} \]
    %
    Suppose $G_k/G_{k+1}$ is not simple, so the factor group posesses a normal subgroup $(G_k/G_{k+1})_S$. By the lattice isomorphism theorem, there is a subgroup $S$ such that $G_k \subset S \subset G_{k+1}$, and $S$ is normal in $G_{k+1}$. Since $G_{k+1}$ is normal in $G_k$, $G_{k+1}$ is also normal in $S$, hence we have a refined normal series. This proof by controposition shows that all factor groups are simple in a composition series. Of course, if a normal series is such that every factor group is simple, it must follow that the series cannot be refined, because the existence of a refinement shows exactly that there is a normal subgroup between the two, hence the tower is a composition series.
\end{proof}

We now proceed to specialize to a particular type of normal series. First, a lemma.

\begin{theorem}
    From any abelian tower of an abelian group we can construct a cyclic tower.
\end{theorem}
\begin{proof}
    Let us prove this for all abelian groups, by by induction on the order of the group. For a base case, we note any abelian tower on the trivial group $\{e\}$ is cyclic. Now, consider an abelian group $G$ of order $n$ where an abelian tower of any smaller group can be constructed into a cyclic tower. Suppose we have an abelian tower
    %
    \[ G = G_0 \rhd G_1 \rhd \dots \rhd G_m \]
    %
    Consider a non-zero group element $g$ in $G$, and the quotient group $G/\gen{g}$. We still have an abelian tower
    %
    \[ G = G_0/\gen{g} \rhd G_1/\gen{g} \rhd \dots \rhd G_m/\gen{g} \]
    %
    Because by the third isomorphism theorem, the quotient groups are isomorphic to the original abelian tower's quotient groups. By induction, we can construct refine this tower into a cyclic tower. We have the canonical homomorphism from $G$ to $G/\gen{g}$, hence the inverse image is a cyclic tower in $G$. Thus the statement holds for all finite abelian groups.
\end{proof}

\begin{corollary}
    An abelian tower on any group admits a cyclic refinement.
\end{corollary}
\begin{proof}
    Suppose for a group $G$ we have an abelian tower
    %
    \[ G = G_0 \rhd G_1 \rhd \dots \rhd G_m \]
    %
    The lattice isomorphism theorem establishes a bijection between subgroups of $G/X$ and subgroups of $G$ that contain $X$. Consider a pair $G_i$ and $G_{i+1}$ in the tower. We have an abelian tower $G_i/G_{i+1} \rhd \{e\}$, and $G_i/G_{i+1}$, so we have a cyclic refinement of this tower. By Theorem (7.1), we can bring this refinement back to $G$, and this will also be cyclic, beginning with $G_i$, and ending with $G_{i+1}$. Thus we can refine our original abelian tower with the cyclic tower constructed from each pair $G_i$ and $G_{i+1}$ to form a new abelian tower.
\end{proof}

A group is \emph{solvable} \index{Solvable} if it has an abelian tower whose last element is the trivial subgroup $\{e\}$.

Here we provide an explicit example before moving to the abstract. Consider the group $\mathrm{GL}_n(\mathbf{F})$. Let $N_n(\mathbf{F})$ be the set of elements that are zero both on and below the diagonal. For any $r$ between 1 and $n$, the set $U_r = I_n + (N_n(\mathbf{F}))^{r}$ is a subgroup of $GL_n(\mathbf{F})$ (the determinant of all the matrices is 1). For $U_k$, define a mapping from $U_k$ to the additive group $\mathbf{F}^{n-k}$ by taking the $k$'th upper diagonal. That is, if a matrix $M_n = [m_{i,j}]$. Then $M_n \mapsto (a_{1,k}, a_{2,k+1}, \dots, a_{n-k,n})$. This is an homomorphism because $U_k$ is a matrix of the form
%
\[ \begin{pmatrix} 1 & 0 & \dots & 0 & a_{1,k} & \dots & \dots & a_{1,n}\\
                   0 & 1 & \dots & 0 & 0 & a_{2,k+1} & \dots & a_{2,n}\\
                   0 & 0 & \ddots & 0 & 0 & 0 & \ddots & \vdots\\
                   0 & 0 & \dots & 1 & 0 & 0 & \dots & a_{n-r,n}\\
                   0 & 0 & \dots & \dots & \ddots & \dots & \dots & 0\\
                   \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
                   \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
                   0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \end{pmatrix} \]
%
and hence for any two matrices $M = [a_{i,j}]$ and $N = [b_{i,j}]$ in $U_k$, $MN = [c_{i,j}]$ fits the equations $c_{n,k + n - 1} = a_{n,k + n - 1} + b_{n,k + n - 1}$ (the identity matches up with the $r$'th column). The kernel of the homomorphism is $U_{k+1}$, hence $U_{k+1}$ is normal in $U_k$, and $U_k/U_{k+1} \cong F^{k-r}$ and the factor group is abelian. Thus the sequence $(U_k)$ is an abelian tower, and $U$ is solvable.

Here is a simpler example. Let $G$ be an abelian group. Then the series $G \rhd \{e\}$ is an abelian tower, because $G/\{e\} \cong G$, and is hence abelian. Thus $G$ is solvable.

\begin{theorem}
    A subgroup of a solvable group is solvable.
\end{theorem}
\begin{proof}
    Consider a solvable group $G$, and a subgroup $H$. Consider the tower that makes $G$ solvable.
    %
    \[ G = G_0 \supset G_1 \supset \dots \supset G_n = \{ e \} \]
    %
    From this tower, construct a new sequence $(H_k)$, where $H_k = G_k \cap H$. We know that, since $G_k$ is normal in $G_{k+1}$, so too are $H_k$ and $H_{k+1}$ The second isomorphism theorem tells us that
    %
    \[ (H \cap G_{i+1})/(H \cap G_i) = (H \cap G_{i+1})/(H \cap G_i \cap G) \cong (H \cap G_{i+1})G_i/G_i \subset G_{i+1}/G_i \]
    %
    and thus $H_i/H_{i+1}$ is abelian.
\end{proof}

\begin{theorem}
    Let $G$ be an arbitrary group, and $H$ an arbitrary normal subgroup. $G$ is solvable if and only if both $H$ and $G/H$ are.
\end{theorem}
\begin{proof}
    Let $G$ be a solvable group, with an abelian tower
    %
    \[ G = G_0 \supset G_1 \supset \dots \supset G_n = \{ e \} \]
    %
    Given this abelian tower, consider the canonical mapping $\pi$ from $G$ to $G/H$, and define a new sequence $(H_k)$ such that $H_k = \pi(G_k)$. We know $H_k$ is normal in $H_{k+1}$ by Exercise 13. Furthermore, we know that $H_k = G_k/H$, hence, by the third isomorphism theorem,
    %
    \[ H_k/H_{k+1} = (G_k/H)/(G_{k+1}/H) \cong G_k/G_{k+1} \]
    %
    Conversely, suppose that $H$ and $G/H$ is solvable. Then by Theorem (7.1) we can construct an abelian tower on $G$, which ends with $H = \pi^{-1}(e)$. Combine this with the abelian series on $H$, and we obtain that $G$ is solvable.
\end{proof}

Let $G$ be a group. A \emph{commutator}\index{Commutator} is an element of $G$ that can be written $ghg^{-1}h^{-1}$, for two elements $g$ and $h$ in $G$, which we also write as $[g,h]$. Define the \emph{commutator} or \emph{derived subgroup}\index{Commutator Subgroup}\index{Derived Subgroup} $D(G)$ of the group $G$ to be the group generated by the set of commutators in $G$.

\begin{lemma}
    For any $G$, $D(G)$ is normal in $G$.
\end{lemma}
\begin{proof}
    Let $g$ be an element of $G$, and $hkh^{-1}k^{-1}$ an element of $D(G)$,
    %
    \[ ghkh^{-1}k^{-1}g^{-1} = (ghg^{-1})(gkg^{-1})(gh^{-1}g^{-1})^{-1}(gkg^{-1})^{-1} \]
    %
    Hence it is an element of the commutator. We leave it to the reader to prove that, if $gkg^{-1}$ holds for every $k$ in a set $K$ which is a subset of a group $G$, from which $g$ reside, then $\gen{K}$ is normal in $G$.
\end{proof}

\begin{lemma}
    For any group $G$, $G/D(G)$ is commutative.
\end{lemma}
\begin{proof}
    For any $gh$, $g^{-1}h^{-1}gh$ is in $D(G)$, hence
    %
    \[ gD(G)hD(G) = ghD(G) = ghg^{-1}h^{-1}hgD(G) = hD(G)gD(G) \]
    %
    and we have calculated that the group is commutative.
\end{proof}

\begin{lemma}
    For any homomorphism from $G$ to $H$ such that $H$ is commutative, $D(G)$ is a subset of the kernel of $H$.
\end{lemma}
\begin{proof}
    Let $\varphi$ be the homomorphism above, and let $g$ and $h$ be arbitrary elements of $G$. By doing the following calculation,
    %
    \[ \varphi(ghg^{-1}h^{-1}) = \varphi(g)\varphi(h)\varphi(g^{-1})\varphi(h^{-1}) = \varphi(g)\varphi(g^{-1})\varphi(h)\varphi(h^{-1}) = e \]
    %
    Since these elements generate $D(G)$, every element in $D(G)$ is composed of elements like this, which all cancel out in $\varphi$, hence $D(G)$ is in the kernel of $\varphi$.
\end{proof}

\begin{corollary}
    If $G$ is a group with normal group $N$, and $G/N$ is abelian, then $D(G) \subset N$.
\end{corollary}

Commutator groups give us the key to unravelling the notion of solvability. We know $D(G)$ is normal in $G$, and we also know $D(D(G))$ is normal in $D(G)$, and so on and so forth, and each factor group created is abelian. Define $D^n(G)$ recursively by $D^n(G) = D(D^{n-1}(G))$. Via this, for each $n$ we get a normal series
%
\[ G \rhd D(G) \rhd D^2(G) \rhd \dots \rhd D^{n-1}(G) \rhd D^n(G) \]
%
If it eventually holds that $D^n(G) = \{e\}$ for some $n$, then we obtain an abelian series, and $G$ is solvable. What is amazing is this statement holds in reverse.

\begin{theorem}
    If a group $G$ is solvable, $D^n(G) = \{e\}$ for some $n$.
\end{theorem}
\begin{proof}
    Suppose $G$ is solvable, and hence has an abelian normal series
    %
    \[ G = G_0 \rhd G_1 \rhd \dots \rhd G_n = \{e\} \]
    %
    For each $r$, $D(G_r) \subset G_{r+1}$, as $G_r/G_{r+1}$ is abelian. We claim $D^r(G) \subset G_r$. How do we prove this? Well $D(G) \subset G_1$, hence $D^2(G) \subset D(G_1) \subset G_2$. Thus the claim follows by induction. It follows that $D^n(G) \subset \{e\}$, and the two groups are hence equal as the only subgroup of the trivial group is itself.
\end{proof}

The main use of this theorem is not to show other groups are solvable, but to show that some groups are not solvable. Solvability began solely to answer questions in Galois field theory, which considers permutations of polynomial equations. This is just a representation of the symmetric group. You should see the connection between the following theorem and the insolvability of the quintic equation, at least in the numbers used.

\begin{theorem}
    $S_n$ is not solvable when $n \geq 5$.
\end{theorem}
\begin{proof}
    Let $i$,$j$,$k$,$r$,$s$ be 5 distinct characters that are being permuted in $S_n$. Let $\sigma = (i\ j\ k)$, and let $\tau$ be $(k\ r\ s)$. Then
    %
    \[ [\sigma, \tau] = \sigma\tau\sigma^{-1}\tau^{-1} = (r\ k\ i) \]
    %
    As each $r$, $k$, and $i$ were arbitrary, we know all three cycles are in $D(S_n)$. As only three cycles were used in the commutators above, all three cycles are also in $D^2(S_n)$, and so on inductively, hence we will never have $D^m(S_n) = \{ \mathbf{1} \}$. Thus $S_n$ is not solvable.
\end{proof}

\begin{theorem}
    If $G$ is a $p$-group, $G$ is solvable.
\end{theorem}
\begin{proof}
    Let $G$ be of cardinality $p^m$. We proved in Lemma (6.6) that for any $k$ between 1 and $m-1$ there is a subgroup of order $p^k$. In particular, there is a subgroup of order $p^{m-1}$. Denote this group $G_1$. $G_1$ is normal in $G$, and $G/G_1$ is of order $p$, so the group must be cyclic as $p$ is prime. By induction, we must do this for $G_1$, $G_2$, etc. to construct a normal series where each factor group is cyclic.
\end{proof}

















\chapter{Direct Products and Abelian Groups}

Direct products are the key to classifying a certain class of abelian groups. The ideas of this classification you have probably learned before you even read this article; there is a distinct connection to the ideas of linear algebra. Here is the special class of abelian groups we will classify.

A group is finitely generated if it is generated from a finite set.

It will help to introduce some notation to deal with splitting up components of abelian groups. We note the formal definition in the infinite case is not used for now, but we include it for thoroughness.

Given a collection of abelian groups $(G_i)_{i \in I}$, we define the \emph{direct sum}\index{Direct Sum} $\bigoplus_{i \in I} G_i$ to be the subgroup of the direct product of those groups consisting of all elements where there are only finitely many elements that are non-identity elements. In the case of a finite product of elements, the direct sum is equivalent to the direct product.

You can probably see how abelian groups connect to vector spaces. In some sense, vector spaces are the canonical abelian if you consider their addition as the fundamental operation that defines them. The definitions below should be familiar to you from a study of vector spaces.

If an abelian group is generated by a set $S$, then that set is a \emph{basis}\index{Basis (Abelian Group)} if every element in the group is uniquely represented by a sum of elements in $S$. If a group has a basis, we say the group is \emph{free}\index{Free Group}.

For every set $S$, there is an abelian group whose basis is $S$. Let us construct this group. Consider the set of mappings from $S$ to $\bint$. In particular, consider the mappings that assign $1$ to some element $s$ in $S$, and $0$ to every other element. Then this set forms a basis to all of the function group, and we can consider $S$ to be the basis of this set. The group we have constructed is called the free abelian group generated by $S$, which is commonly denoted $F_{ab}(S)$. Every free group is isomorphic to the free abelian group generated by its basis.

\begin{theorem}
    Every abelian group is isomorphic to a factor group of a free abelian group.
\end{theorem}
\begin{proof}
    Consider an abelian group $G$. Take a generating set $S$ of $G$ (in the worst case, we may take $G$ as the generating set). Form the abelian group $F_{ab}(S)$. Define a homomorphism $\varphi$ from $F_{ab}(S)$ to $G$ by $\varphi(\sum_{k = 1}^n n_kg_k) = \sum_{k = 1}^n n_kg_k$. This homomorphism is surjective, hence $G$ is isomorphic to the factor group by the kernel of the homomorphism with $F_{ab}(S)$.
\end{proof}

In particular, if an abelian group is finitely generated, this group is isomorphic to a factor group of $\bint^n$ for some $n$. This means if we want to classify all finitely generated abelian groups, we first must must classify subgroups on $\bint^m$ for every $m$. We will now build up the mechanics of how we can classify this.

\begin{lemma}
    If a homomorphism $\varphi$ maps from an abelian group $G$ onto a free abelian group $H$, then $G$ is isomorphic to the direct sum of the kernel of $\varphi$ and $H$.
\end{lemma}
\begin{proof}
    Let ${h_i}_{i \in I}$ be a basis for $H$. For each $h_i$, consider some $g_i$ in $G$ such that $f(g_i) = h_i$. Take the group $C$ generated by the set of elements $g_i$. We claim $C$ is isomorphic to $H$. We know that $\varphi$ restricted to $C$ is still surjective, and if $\varphi(\sum_{i \in I} n_ig_i) = 0$, then $\sum_{i \in I} n_ih_i = 0$, hence all $n_i$ are zero, which means $\sum_{i \in I} n_ig_i = 0$. Hence $\varphi$ is injective when restricted to $C$, and we obtain an isomorphism. Let $K$ be the kernel of $\varphi$. We have shown $C \cap K = 0$. Now we must show $C + K = G$. Let $x$ be an arbitary element of $G$, and let $f(x) = \sum_{i \in I} n_ig_i$. Then $x - \sum_{i \in I} n_ih_i$. Thus $x - \sum_{i \in I}$ is in $K$, and $x \in K + C$. It follows that $G$ is isomorphic to the direct sum of $C$ and $K$.
\end{proof}

The next theorem allows us to characterize all subgroups of free groups, which connects to our objective of classifying subgroups of $\mathbf{Z}^n$.

\begin{theorem}
    Every subgroup of a free abelian group with a finite basis is free, with a basis of size less than or equal to the size of the entire group.
\end{theorem}
\begin{proof}
    We prove by induction on the size of the group. If $n = 1$, the group is cyclic, and thus every subgroup is cyclic, generated by a single element which forms the basis provided the group is infinite. Now suppose that for $m \leq n$ this theorem holds. Let $G$ be a free abelian group with basis $\{ g_1, g_2, \dots, g_n \}$, and consider a subgroup $H$. We have a homomorphism $\pi_1$ from $G$ to $\gen{g_1}$ defined by the mapping
    %
    \[ \pi_1(\sum_{k = 1}^n l_kg_k) = l_1g_1 \]
    %
    Consider the restriction of this homomorphism from $H$, and the resultant kernel $H'$. Then the range of this restricted homomorphism, and hence is of the form $\gen{ag_1}$ for some integer $a$. The kernel $H'$ is a subgroup contained in the group $\gen{g_2, \dots, g_n}$, and hence has a basis $h_1, h_2, \dots, h_q$, where $q \leq n-1$. If $a \neq 0$. By Lemma (8.3), there is a subgroup $C$ of $H$ isomorphic to $\gen{ag_1}$, and $H = H' \cdotp C$. Now $C$ is either zero or infinite cyclic, which proves that $H$ is free.
\end{proof}

\begin{corollary}
    Every pair of bases of a finitely generated free abelian group is of the same cardinality.
\end{corollary}
\begin{proof}
    Let $G$ a finitely generated free abelian group with two bases of size $T$ and $Q$ respectively. Using the basis corresponding to $T$, we conclude the group $G/pG$ is a sum of $T$ cyclic groups of order $p$, and is thus of cardinality $p^T$. Conversely, using the basis of $Q$, we conclude the basis is of order $p^Q$. But then $p^T = p^Q$, hence $T = Q$.
\end{proof}

The number of elements in the basis of a free abelian group is called the \emph{rank}\index{Rank (Free Abelian Group)} of the group. The problem with the proof above is it is not so easy to construct such a basis. For the next theorem, we will use the fact that any subgroup of a free group is finitely generated, but only to provide an algorithm to conclude our objective of classifying all subgroups of $\bint$.

\begin{theorem}
    Let $G$ be a finitely generated abelian group, generated by a set of $n$ elements. Then
    %
    \[ G \cong \bint/a_1\bint \oplus \bint/a_2\bint \oplus \dots \bint/a_r\bint \oplus \bint \oplus \dots \oplus \bint \]
    %
    where $r \leq n$ and such that $a_i \divides a_{i+1}$ for each $a_i$, and the number of $\bint$ groups in the direct product is $n - r$. This formulation is unique for any such subgroup.
\end{theorem}
\begin{proof}
    Consider the group $G$ defined above. We know that $G \cong \bint^n/K$ for some subgroup $K$ of $\bint^n$. Suppose we have an automorphism $\varphi$ on $\bint^n$. Then this induces a mapping from $K$ to another subgroup $K'$, and $\bint^n/K \cong \bint^n/K'$. Our strategy is thus to simplify $\bint/K$ via these automorphisms to determine that each such group $\bint/K$ is isomorphic to one of the sets above. What's good about this algorithm is that it gives us a method to find this isomorphism.

    Let $K$ be a subgroup of $\bint^n$. Then we know that $K$ is finitely generated by a set of elements $\{ k_1, k_2, \dots, k_l \}$. Each $k_i$ is an array of $n$ integers $(k_{i,1}, k_{i,2}, \dots, k_{i,n})$, as it is an element of $\bint^n$. This motivates that we construct the matrix
    %
    \[ \begin{pmatrix} k_{1,1} & k_{1,2} & \dots  & k_{1,n}\\
                       k_{2,1} & k_{2,2} & \dots  & k_{2,n}\\
                       \vdots  & \vdots  & \ddots & \vdots\\
                       k_{l,1} & k_{l,2} & \dots  & k_{l,n} \end{pmatrix} \]
    %
    Can we create row and column operations which correspond to isomorphisms of $\bint^n$. We wouldn't be constructing this matrix if not! these are the operations we require:
    %
    \begin{itemize}
        \item We may interchange two rows $i$ and $j$. This corresponds to swapping the order of two generators in the set, which does not change the subgroup $K$ we are operating on.
        \item Multiplying a row $i$ by negative one corresponds to swapping a generator $k_i$ with its inverse, $-k_i$. We note that this also does not change the subgroup $K$ we are operating on.
        \item Adding row $i$ to row $j$, where $i \neq j$, corresponds to replacing a generator $k_i$ with $k_i + k_j$. These generators are equivalent, so $K$ is the same.
        \item Interchanging Columns $i$ and $j$ corresponds to an automorphism of $\bint^n$ where we interchange two coordinates.
        \item Multiplying a column $i$ by negative one corresponds to an automorphism of $\bint^n$ where a specific coordinate is inverted in every element.
        \item Adding a column $i$ to a column $j$ corresponds to an automorphism of $\bint^n$. This is perhaps the only non-trivial automorphism to see. We map a vector $(x_1,\dots,x_i,\dots,x_j,\dots,x_n)$ to $(x_1,\dots,x_i, \dots, x_i + x_j, \dots, x_n)$. Then $(x_1 + y_1,\dots,x_i + y_i,\dots,x_j + y_j,\dots,x_n + y_n)$ is mapped to $(x_1 + y_1,\dots,x_i + y_i,\dots,x_i + x_j + y_i + y_j,\dots,x_n + y_n)$, which is precisely the addition of the individual mappings, hence the mapping is a homomorphism. Verification that this mapping is an automprhism is left to the reader.
    \end{itemize}
    %
    These actions are sufficient to reduce any matrix to the `Smith Normal Form', a matrix of the form
    %
    \[ \begin{pmatrix}
            \alpha_1 & \dots  & 0        & 0      & \dots  & 0\\
            \vdots   & \ddots & \vdots   & \vdots & \ddots & \vdots\\
            0        & \dots  & \alpha_n & 0      & \dots  & 0\\
            0        & \dots  & 0        & 0      & \dots  & 0\\
            \vdots   & \ddots & \vdots   & \vdots & \ddots & \vdots\\
            0        & \dots  & 0        & 0      & \dots  & 0
       \end{pmatrix} \]
    %
    where the only non-zero entries are on the diagonal, and each $\alpha_i$ divides $\alpha_{i+1}$. How is this useful to us? It means precisely that every subgroup $K$ can be by automorphisms transformed into $\alpha_1\bint \oplus \alpha_2\bint \oplus \dots \oplus \alpha_n\bint \oplus \{0\} \oplus \dots \oplus \{0\}$, and thus our original finitely generated abelian group is isomorphic to $\bint/\alpha_1\bint \oplus \bint/\alpha_2\bint \oplus \dots \oplus \bint/\alpha_n\bint \oplus \bint \oplus \dots \oplus \bint$. All that is left is show our method of reduction of an arbitrary integer matrix to Smith normal form. For now, we suppose it true, and we will establish the technique after this proof is complete.
\end{proof}

The technique to reducing an integer matrix to smith normal form turns out to be quite simple. Clearly, we need only provide a technique to reduce a matrix to the form
%
\[ \begin{pmatrix} \alpha & 0\\0 & M \end{pmatrix} \]
%
Where $M$ is a submatrix of one less column, and such that $\alpha$ divides every entry in $M$. By induction, the rest of the method is taken care of.

The first step of our algorithm is to check if the matrix you are reducing is the zero matrix; if this is true, we are done before we have even started. Otherwise, move the element in the matrix of smallest absolute value to the top left hand corner of the matrix, which we call the pivot. Secondly, repeatedly add or subtract the pivot row from each subsequent row such that the absolutel value of each pivot row and column entry is reduced. Do this for the pivot column from all other columns also.

Eventually, either all entries in the pivot row and column will be zero, or one will have absolute value smaller than the pivot entry. In this case, move this entry to the top left corner, and continue the process. We can only reduce the absolute value of an entry finitely many times before we are done, so eventually, the pivot row and column will be reduced to zero beside from the pivot entry.

Finally, check if the pivot entry divides every other entry in the matrix. If so, we can recurse to the submatrix. Otherwise, take the row that is not divisible by the pivot. Add this row to the first row, and return to adding and subtracting the rows and columns. This will reduce the size of the pivot, meaning we must eventually terminate.

It is best to learn an algorithm by computing out an example by hand. Here is an example. Consider a homomorphism from $\bint^3$ to a group $G$ with kernel $\gen{(6,3,3),(4,5,7),(3,2,2)}$. What group is $G$ isomorphic to. First, we form the matrix
%
\[ \begin{pmatrix} 6 & 3 & 3 \\ 4 & 5 & 7 \\ 3 & 2 & 2 \end{pmatrix} \]
%
We bring the smallest entry, the one with the value of two, up to the pivot entry,
%
\[ \begin{pmatrix} 2 & 3 & 2\\ 5 & 4 & 7\\ 3 & 6 & 3 \end{pmatrix} \]
%
then we reduce the row sizes
%
\[ \begin{pmatrix} 2 & 3 & 2\\ 1 & -2 & 3\\1 & 0 & -1 \end{pmatrix} \]
%
and the column sizes
%
\[ \begin{pmatrix} 2 & 1 & 0\\ 1 & -3 & 2\\ 1 & -1 & -2 \end{pmatrix} \]
%
We move the 1 on the first row to the pivot, and then reduce to get the matrix
%
\[ \begin{pmatrix} 1 & 0 & 0\\ 0 & -5 & 0\\ 0 & -3 & -4 \end{pmatrix} \]
%
Continuing by induction, you should end up with a matrix
%
\[ \begin{pmatrix} 1 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 6 \end{pmatrix} \]
%
Which means $K$ is isomorphic to $\bint \oplus \bint \oplus 6\bint$, and $\bint^3/K$ is isomorphic to $\bint/\bint \oplus \bint/\bint \oplus \bint/6\bint \cong \bint/6\bint$.

\clearpage

\begin{exercise}
    What is the order of $(\bigtimes_{i \in I} g_i)$ in relation to the order of each $g_i$ in the direct product.
\end{exercise}

\end{document}


















\begin{example}
    The pairs  $(\mathbf{N}, +)$ and $(\mathbf{Z}, +)$ are monoids. The identity is the number $0$. This monoid also has the additional relation that
    %
    \[ n + m = m + n \]
    %
    for all $n,m \in \mathbf{N}$. We call this the \emph{commutative} relation, and call any monoid with this property \emph{abelian}\footnote{Niels Henrik Abel (1802-1829) was one of the founding fathers of modern algebra, and the adjective `abelian' is named in honour.}.
\end{example}

In some sense, $\mathbf{N}$ is the fundamental `one dimensional' monoid, for if $(M, \circ)$ is any other monoid, and $x \in M$, then there is a unique map $f: \mathbf{N} \to M$ mapping $1$ to $x$, satisfying
%
\[ f(n + m) = f(n) \circ f(m) \]
%
This map is of course just the `power' map $f(n) = x^n = x \circ \dots \circ x$, which also satisfies $(x^n)^m = x^{nm}$. If $(G, \circ)$ is a group, and $g \in G$, then we can extend the power map to a unique map $f: \mathbf{Z} \to G$ mapping $1$ to $g$ and satisfying $f(n + m) = f(n) \circ f(m)$, and this is just the power map $n \mapsto g^n$, where $g^{-n}$ is just the inverse of $g^n$.

\begin{example}
    Let $n \sim m$ in $\mathbf{Z}$ if $n - m$ is an integer multiple of $k$. An equivalence class is denoted $n + k \mathbf{Z}$, for it really is the set
    %
    \[ \{ n + k m : m \in \mathbf{Z} \} \]
    %
    The quotient $\mathbf{Z} / {\sim}$ then forms a group, denoted $\mathbf{Z}_k$ and named the \emph{integers modulo $k$}, if we define
    %
    \[ (n + k \mathbf{Z}) + (m + k \mathbf{Z}) = (n + m) + k \mathbf{Z} \]
    %
    One just needs to carefully check this is well defined. If $g$ is an element of a group $G$ satisfying $g^k = e$, then one can define the `quotient power' $g^{n + k \mathbf{Z}}$, which is just equal to $g^n$. One can also see this operation as defined on $\{ 0, \dots, k-1 \}$, which is useful when labelling certain cyclic objects.
\end{example}

Later on, we will see that we can take equivalence classes of general groups to obtain groups, known as quotient groups. Of course, we will need to focus our attention only to equivalence classes which fit to the composition operator on the group.

\begin{example}
    The group of all bijective functions on the set $\{ 1, \dots, n \}$ is a group known as the \emph{symmetric group} on $n$ characters, denoted $S_n$. To describe elements of this group, we introduce cycle notation. A cycle is an element $\sigma$ of $S_n$ such that there is $n_1, \dots, n_m \in \mathbf{Z}_n$, such that $\sigma(n_i) = \sigma(n_{i+1})$ (where we think of $\{ 1, \dots, n \}$ as $\mathbf{Z}_n$), and such that $\sigma(x) = x$ for all other $x \in \mathbf{Z}_n$. We write the cycle as
    %
    \[ (n_1\ n_2\ \dots\ n_m) \]
    %
    Now let $\sigma \in S_n$ be an arbitrary element which is not the identity element. Fix $x \in \{ 1, \dots, n \}$ such that $\sigma(x) \neq x$, and consider the set
    %
    \[ \{ \sigma^k(x) : k \in \mathbf{N} \} \]
    %
    By the pidgeonhole principle, there is $i$ and $j$ such that $\sigma^i(x) = \sigma^j(x)$, and then it follows that $\sigma^{i-j}(x) = x$. Let $k$ be the least number such that $\sigma^k(x) = x$. Then
    %
    \[ \{ x, \sigma(x), \sigma^2(x), \dots, \sigma^{k-1}(x) \} \]
    %
    are distinct, and $\sigma$ cycles through these elements. Thus
    %
    \[ (x\ \sigma(x)\ \dots\ \sigma^{k-1})^{-1} \circ \sigma \]
    %
    is a permutation in $S_n$ which fixes $\{ x, \sigma(x), \dots, \sigma^{k-1}(x) \}$, and we may proceed by induction on the number of elements fixed by $\sigma$ to find cycles $\sigma_1, \dots, \sigma_m$ such that
    %
    \[ \sigma_1^{-1} \circ \dots \sigma_m^{-1} \circ \sigma = e \]
    %
    and then $\sigma = \sigma_1 \circ \sigma_2 \circ \dots \circ \sigma_m$ is the product of disjoint cycles.
\end{example}

\begin{example}
    Consider a regular polygon with $n > 2$ sides (for instance, a square, or a pentagon). The set of all isometries on the polygon is a group, known as the \emph{dihedral group} and denoted $D_n$. Let us determine its structure. The vertices of the polygon are at a unit distance from the centre of the polygon, and the centre of the polygon is the unique point at a unit distance from exactly $n$ distinct points. Thus if $f$ is an isometry of the polygon, it must fix the origin. Because of this, $f$ must also permute the vertices of the polygon, for these are the only points at a distance 1 from the origin. Thus we see that $D_n$ can be seen as a subset of $S_n$, once the vertices are numbered from $1$ to $n$ in clockwise order (which we see as elements of $\mathbf{Z}_n$). If the vertex $1$ is sent to vertex $k$, then the vertices $2$ and $n$ must be sent to vertex $k+1$ and $k-1$. A point in the plane is uniquely specified by its distance to three fixed points, so once $f$ is specified on $0$, and $1$ (and thus at $n$ as well), $f$ is uniquely determined on the whole polygon. Thus we see that the isometries of the polygon are generated by rotations and reflections. If we let $r$ be the rotation of the vertices defined by the cycle
    %
    \[ (1\ 2\ \dots\ n) \]
    %
    and let $i$ be the reflection across the horizontal axis, which maps the vertex $k$ to the vertex $n - k$, then
    %
    \[ r^k = \underbrace{r \circ r \circ \dots \circ r}_{k\ \text{times}} \]
    %
    maps $0$ to $k$, and $1$ to $k+1$. $r^k \circ i$ maps $0$ to $k$, and $1$ to $k-1$, so these are all possible isometries. These `generating isometries' satisfy the relations
    %
    \[ r^n = e\ \ \ \ \ i^2 = e\ \ \ \ \ ri = ir^{n-1} \]
    %
    and these relations uniquely define the dihedral group $D_n$, which consists of $2n$ isometries.
\end{example}

\begin{example}
    The set of integers, rational, real, and complex numbers under addition form the groups $\bint^+$, $\mathbf{Q}^+$, $\mathbf{R}^+$, and $\mathbf{C}^+$. The same sets with zero removed under the operation of multiplication form the groups $\bint^\times$, $\mathbf{Q}^\times$, $\mathbf{R}^\times$, and $\mathbf{C}^\times$.
\end{example}