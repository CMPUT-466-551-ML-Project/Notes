\input{../../style.tex}

\title{Partial Differential Equations}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}

\maketitle

\tableofcontents

\pagenumbering{arabic}

\part{Classical PDEs}

\chapter{Introduction}

Physicists were the first to consider partial differential equations. Physical problems provided the insight for many of the basic techniques to solving partial differential equations, and it removes something from the subject to forget the physical motivations for partial differential equations. In this chapter, we derive the four main partial differential equations which began the study of partial differential equations. Most involve the \emph{Laplacian} operator
%
\[ \Delta f = \sum_{i = 1}^d \frac{\partial^2 f}{\partial x_i^2} = \nabla \cdot (\nabla f) \]
%
which measures the local average deviation of a function about a point.

\section{The Transport Equation}

Suppose we wish to model mass travelling in the direction of a single vector $a \in \mathbf{R}^d$. If $u(x,t)$ is a mass distribution, and $\Omega$ is some region, then the change in volume in $\Omega$ over time is the amount of volume entering $\Omega$, and the amount leaving $\Omega$. This can just be measured by the directional derivative of $u$ in the direction of $a$. Thus
%
\[ \int_\Omega \frac{\partial u}{\partial t}\ dx = \frac{\partial}{\partial t} \int_\Omega u(x,t) dx = - \int_{\partial \Omega} (ua \cdot \nu)\ d\sigma(x) \]
%
Now applying the divergence theorem, we find
%
\[ \int_{\partial \Omega} (ua \cdot \nu)\ d\sigma(x) = \int_\Omega \nabla \cdot ua\  dx = \int_\Omega \nabla u \cdot a\ dx \]
%
Since $\Omega$ was an arbitrary subset of $\mathbf{R}^d$ with smooth boundary, we conclude that $u$ satisfies the partial differential equation
%
\[ \frac{\partial u}{\partial t} + \nabla u \cdot a = 0 \]
%
This is the {\bf transport equation}. If, in addition, there is a source function $f: \mathbf{R}^d \to \mathbf{R}$, which constantly either produces or removes mass from a location, then we find the integral equation is
%
\[ \int_\Omega \frac{\partial u}{\partial t}\ dx = \int_\Omega f - \nabla u \cdot a\ dx \]
%
and we obtain the non homogenous heat equation
%
\[ \frac{\partial u}{\partial t} + \nabla u \cdot a = f \]
%
which is only slightly harder to solve.

\section{The Heat Equation}

Consider a region $\Omega$ in space, where we fix a temperature on the boundary, and then allow temperature to fluctuate on the interior on its own volition. Let $u(t,x)$ represent a density for the temperature at $x \in \Omega$ at time $t$. Given a subregion $D \subset \Omega$, the value
%
\[ \frac{d}{dt} \int_D u(t,x)\ dx = \int_D \frac{\partial u}{\partial t}(t,x)\ dx \]
%
represents the rate of energy entering $D$. Newton's law of cooling says that the rate of energy leaving $D$ is proportional to the difference in temperatures between the boundary of the body and its immediate surroundings. To a first order, this is approximated by
%
\[ \int_{\partial D} (\nabla u \cdot \nu)(x)\; dS(x) \]
%
where $\nu$ is the outward pointing unit vector varying about the boundary of $D$. The divergence theorem implies that
%
\[ \int_{\partial D} (\nabla u \cdot \nu)(x)\ dS(x) = \int_D (\nabla \cdot \nabla u)(x)\ dx = \int_D \Delta u(x)\ dx \]
%
By conservation of energy, we therefore find that
%
\[ \int_D \frac{\partial u}{\partial t}(t,x)\ dx = k \int_D (\Delta u)(x)\ dx \]
%
where $k > 0$ is some constant of proportionality. Since this holds for any domain $D$ with smooth boundary, we conclude that heat must satisfy the partial differential equation
%
\[ \frac{\partial u}{\partial t} = k \Delta u \]
%
throughout the entire domain $\Omega$. This is the {\bf heat equation}. More generally, this equation describes the evolution of some density function $u$ which

We note that the equation can also be applied to understand the diffusion of anything over time in some region, provided the rate of diffusion is linearly related to the change of concentration.

\section{Wave Equation}

Consider a one dimensional string in two dimensions, or a two dimensional membrane in three dimensions, which is fixed on in boundary, but allowed to vibrate up or down on its interior. Let $\Omega$ denote the region upon which the object lies before it begins vibrating. Let $u(t,x)$ denote the displacement of a point $x \in \Omega$ at time $t$ from the plane. The average displacement over a region $D$ is therefore equal to
%
\[ \fint_D u(t,x)\ dx \]
%
For a single point mass, Hooke's law tells us that a point under tension will experience a force proportional to its distance from equilibrium. Here, our membrane has no equilibrium position, but if we fix $x$, and consider a small neighbourhood around $x$, the force at $x$ should be proportional to the difference between $u(t,x)$ and the average of $u(t,y)$ on a small neighbourhood $D$ around $x$. Using the fact that this difference is proportional to $|D|^2 (\Delta u)(x)$ for nice enough small neighbourhoods $D$, we can absorb constants and we find that the average force in a small region $D$ is equal to
%
\[ \fint_D k |D|^2 \Delta u(x) = k |D| \int_D k \Delta u(x) \]
%
Applying Newton's law, assuming that $u(t,x)$ has constant density $\rho$, we conclude that the average force in $D$ is also equal to
%
\[ \rho |D| \int_D \frac{\partial^2 u}{\partial t^2}(t,x) \ dx \]
%
And therefore
%
\[ \rho |D| \int_D \frac{\partial^2 u}{\partial t^2}(t,x) = k |D| \int_D (\Delta u)(t,x) \]
%
Dividing by $|D|$, and then taking the equation over all subregions $D$ of $\Omega$, we conclude that
%
\[ \rho \frac{\partial^2 u}{\partial t^2} = k \Delta u \]
%
This is the {\bf wave equation}.

\section{Laplace and Poisson's equations}

A special case to solutions of both the heat equation and the wave equation is Laplace's equation $\Delta u = 0$, which finds functions whose average difference is essentially equal to zero. Twice continously differentiable solutions to Laplace's equation are known as {\bf harmonic}. In terms of the heat equation, $u$ gives {\it steady state} temperature distributions which stay constant throughout time. In terms of the wave equation, $u$ also gives steady state wave distributions, because we assume the membrane upon which $u$ is defined is fixed at the boundary, so that if
%
\[ \frac{\partial^2 u}{\partial t^2} = 0 \]
%
then $u$ is in fact constant. More generally, we can try and solve the partial differential equation
%
\[ - \Delta u = f \]
%
where $f$ is some given function. This generalization of Laplace's equation is known as {\bf Poisson's equation}, and arises in many contexts, like in Electrostatics, where solving the Poisson equation amounts to finding a charge potential $u$ for a given charge distribution $f$.

\section{The Field of Partial Differential Equations}

In general, a partial differential equation is an equation involving a function and some of its partial derivatives. The utopian goal of the theory of partial differential equations is to {\it solve} a given partial differential equation, which means to find all functions which solve the equation, possibly assuming some additional restriction on the class of solutions, such as how the solution behaves at the boundary of the functions definition. In the best scenario, we can find explicit formulae for all of the solutions to the partial differential equations, but even in the case of ordinary differential equations, we know that this is not possible, so we instead deduce qualitative properties of the solutions.

For ordinary differential equations, there is a conclusive {\it existence and uniqueness} theory guaranteeing that a given differential equations is solvable, and uniquely solvable given some initial conditions. For a general partial differential equation, we have no such theory. Instead, we must argue for ourself whether a partial differential equation given to us is {\it well posed}, in the sense that it has a solution, the solution is unique given certain conditions, and whether the solution depends continuously based on the initial conditions. In some cases, we may have to enlarge the variety of functions we consider to solve the equation. As an example, the PDE
%
\[ \frac{\partial u}{\partial t} + \frac{\partial (f \circ u)}{\partial x} = 0 \]
%
governs the motion of shock waves in physics. In real life, the behavior of these waves is highly discontinuous, and as such we should not expect a solution to this equation to even be differentiable. It is true that this equation has no differentiable solutions, but if we enlarge the class of functions which can solve this equation to the class of {\it generalized} or {\it weak solutions}, then we do have a uniqueness theory. To summarize, the theory of partial differential equations focuses on three phenomena:
%
\begin{itemize}
    \item (Existence) Does a solution to a partial differential equation exist, and if so, is it possible to express the solution in a formula.

    \item (Uniqueness) Given initial conditions, is the solution to a given partial differential equation unique?

    \item (Regularity) Are the solutions to a given differential equation differentiable, and to what extent?
\end{itemize}
%
In these notes we discuss our ability to answer these types of questions.



\chapter{Solving Classical Partial Differential Equations}

\section{The Transport Equation}

The easiest PDE to analyze is the {\bf transport equation}
%
\[ \frac{\partial u}{\partial t} + a \cdot \nabla u = 0 \]
%
It is essentially an exercise in basic multivariate calculus to show the uniqueness and existence theory of this PDE. Intuitively, $u$ should just propogate in the direction of the vector $a$. If $u$ is any differentiable solution to this equation, and we define $z(s) = u(t+s, x + as)$, then we find that $z$ satisfies the differential equation $z'(s) = u_t(t+s,x + as) + a \cdot \nabla u(t+s, x - as) = 0$, so $z$ is constant, and we conclude $u$ is constant on each line parallel to the hyperplane through the origin generated by $(1,a)$. Conversely, any differentiable function constant on these hyperplanes is a solution. If we specify a set of values on any hyperplane not parallel to $v$, there is a unique differentiable solution $u$ extending these values to the entire plane. If the values are not smooth, then $u$ will not be smooth, but we shall find that we can still view $u$ as a {\it weak} solution to the equation, as we will find later. Thus even the most basic partial differentiable equations have non-differentiable solutions.

Now how do we solve the non-homogenous equation
%
\[ \frac{\partial u}{\partial t} + a \cdot \nabla u = f \]
%
By linearity of the physical situation, it makes sense that the initial mass of the equation should propogate independently of the source mass produced. Thus, given any particular solution $u$, and a fixed $x$, we consider the function $z(s) = u(t+s, x + as)$. Then
%
\[ z'(s) = \frac{\partial u}{\partial t} + a \cdot \nabla u = f(t + s, x - as) \]
%
It therefore follows that
%
\[ u(t + s, x - as) = u(t,x) + \int_0^s f(t+y,x-ay)\ dy \]
%
If we are given initial values $u(0,x) = g(x)$, then we have unique solution solving the nonhomogenous equation given by
%
\[ u(t,x) = g(x-ta) + \int_0^t f(x + (s-t)a,s)\ ds \]
%
So the initial values $g$ propogate throughout the equation at a velocity $a$, as well as build up occuring based on the function $f$. In this case, the regularity of $u$ depends on the smoothness of $g$ and $f$.

\section{The Laplacian and Fundamental Solutions}

Provided a partial differential equation is linear, then a linear combination of solutions to the partial differential equation is also a solution to the partial differential equation. A good strategy to finding {\it all} solutions to PDEs of this form is to find a set of explicit solutions to the PDE, and then to find arbitrary solutions by taking arbitrary linear combinations of explicit solutions. We begin by analyzing Laplace's equation $\Delta u = 0$, or more generally, Poisson's equation $\Delta u = f$. It will help to note that the Laplacian is invariant under translations.

\begin{theorem}
    $(\Delta u) \circ T = \Delta (u \circ T)$ for any rotation $T$.
\end{theorem}
\begin{proof}
    $\Delta u(x)$ is the trace of the Hessian operator
    %
    \[ (Hu)(x) = \left( \frac{\partial^2 u}{\partial x_ix_j} \right) = D(\nabla u)(x) \]
    %
    Now the chain rule implies $D(u \circ T)(x) = (Du)(Tx) \cdot T$, so
    %
    \begin{align*}
        \nabla (u \circ T)(x) &= [D(u \circ T)(x)]^T = [(Du)(Tx) \cdot T]^T\\
        &= T^T \cdot (Du)(Tx)^T = T^T \cdot (\nabla u)(Tx) = (T^{-1} \circ \nabla u \circ T)(x)
    \end{align*}
    %
    The equation $\nabla (u \circ T) = T^{-1} \circ \nabla u \circ T$ essentially says that the gradient respects a coordinate change obtained by a rotation matrix (Remark: This is one of the main reasons why we can define the gradient unambiguously on a Riemannian manifold). Now
    %
    \begin{align*}
        H(u \circ T)(x) &= D(\nabla (u \circ T))(x) = D(T^{-1} \circ \nabla u \circ T)(x)\\
        &= T^{-1} \cdot D(\nabla u)(Tx) \cdot T = T^{-1} \cdot (Hu)(Tx) \cdot T
    \end{align*}
    %
    The traces of two similar matrices are equal, so the traces of $H(u \circ T)(x)$ and $Hu(Tx)$ are equal, so $\Delta (u \circ T)(x) = (\Delta u)(Tx)$.
\end{proof}

We now illustrate how to solve Poisson's equation using the distributional method of fundamental solutions. Given the equation $\Delta u = f$, we find a tempered distribution $\Phi$ whose Laplacian $\Delta \Phi$ is the Dirac delta function $\delta$. Then for any Schwarz function $f$, the Schwarz function $\Phi * f$ satisfies $\Delta(\Phi * f) = (\Delta \Phi) * f = \delta * f = f$, so via convolution we can find a solution to virtually any smooth Poisson's equation we require. A function $\Phi$ for such an equation is known as a {\bf fundamental solution}.

To determine if a fundamental solution exists, we can determine it's structure by using the Fourier transform, since $\Phi$ is tempered. This gives
%
\[ - 4 \pi^2 |\xi|^2 \widehat{\Phi} = (\Delta \Phi)^\ft = \delta^\ft = 1 \]
%
so we can divide both hand sides of the equations to conclude
%
\[ \widehat{\Phi}(\xi) = \frac{-1}{4 \pi^2 |\xi|^2} \]
%
The right hand side is bounded away from the origin, and locally integrable for $d \geq 3$, so is actually a tempered distribution, and we find by taking the inverse Fourier transform that
%
\[ \Phi(x) = \frac{- \Gamma(d/2 - 1)}{4 \pi^{d/2} |x|^{d - 2}} \]
%
Notice that the fundamental solution is radially symmetric, reflecting the fact that the Laplacian involves dispersion from a point. Since we have the inequality
%
\[ |\Phi(\phi)| \lesssim \int \frac{\phi(x)}{|x|^{d-2}} \lesssim \| \phi \|_\infty + \| x^3 \phi \|_\infty \]
%
the distribution $\Phi$ extends continuously to the Banach space of all measurable functions for which $f, x^3 f \in L^\infty(\mathbf{R})$. In particular, we can define
%
\[ (\Phi * f)(x) = \Phi(T_x f^*) \]
%
where $f^*(y) = f(-y)$. We find
%
\[ \int \Phi(T_x f^*) \frac{\phi(x+h) - \phi(x)}{h}\; dx = \int \frac{\Phi(T_{x-h} f^*) - \Phi(T_x f^*)}{h} \phi(x)\; dx \]
%
TODO: Extend this to show that the convolution satisfies Poisson's equation when $f$ is in a more general space.

\begin{example}
    Suppose $\Lambda$ is a tempered distribution with $\Delta \Lambda = 0$. Then
    %
    \[ 0 = \widehat{\Delta \Lambda}(\xi) = - 4 \pi^2 |\xi|^2 \widehat{\Lambda}(\xi) \]
    %
    Thus $\widehat{\Lambda}$ has support at the origin. But if $\widehat{\Lambda} = \sum \lambda_\alpha D^\alpha$, then combined with the fact that
    %
    \[ (D^\alpha f^\vee)(0) = ((-2 \pi i x)^\alpha f)^\vee(0) = \int (- 2 \pi i x)^\alpha f(x)\; d\xi \]
    %
    so $(D^\alpha)^\vee = (- 2 \pi i \xi)^\alpha$, so
    %
    \[ \Lambda(x) = \sum \lambda_\alpha (D^\alpha)^\vee(x) = \sum \lambda_\alpha (- 2 \pi i x)^\alpha \]
    %
    Thus the only harmonic tempered distributions are the polynomials.
\end{example}

Since $\Delta u = 0$ has rotational symmetry, it makes sense to begin by finding solutions to Laplace's equation which are rotationally symmetric. These will form our explicit solutions we can put into superposition to construct other solutions. So suppose we have a rotationally symmetric function $u(x) = f(r)$, where $r = |x|$. In this case, we find
%
\[ D_iu(x) = f'(r) (x_i/r)\ \ \ \ \ D_{ii} u(x) = f''(r) (x_i/r)^2 + f'(r)(1/r - x_i^2/r^3) \]
%
Summing up, we find
%
\[ \Delta u(x) = f''(r) + f'(r) \frac{d - 1}{r} \]
%
If $\Delta u$ vanishes, this means
%
\[ \frac{f''(r)}{f'(r)} = \frac{1 - d}{r} \]
%
So, integrating both sides with respect to $r$, we find $f'(r)$ is proportional to $1/r^{d-1}$, and therefore there are constants $A$ and $B$ such that
%
\[ f(x) = \begin{cases} A|x|^{2-d} + B & : d > 2 \\ A \log r + B & : d = 2 \end{cases} \]
%
These solutions motivate the {\bf fundamental solution of the Laplacian}
%
\[ \Phi(x) = \begin{cases} \frac{1}{d(d-2) \alpha(d)} \frac{1}{|x|^{d-2}} & : d > 2 \\ \frac{-1}{2 \pi} \log |x| & : d = 2 \end{cases} \]
%
where $\alpha(d)$ is the volume of the unit ball in $d$ dimensions. Certainly these two equations solve Laplace's equation for $x \neq 0$, but we have a singularity at the origin. The integral
%
\[ \int_{\mathbf{R}^n} f(x) \Phi(x) \]
%
is well defined for any $f$ bounded in a ball around the origin, and with a suitable decay for the terms of the limit to exist (for instance, any decay on the order of $O(1/|x|^\delta)$ will do, for some $\delta$), because using the fact that $r^{d-1} \Phi(r) = O_\varepsilon(r^{1 + \varepsilon})$ for all $\varepsilon > 0$, we conclude that
%
\begin{align*}
    \left| \int_{\mathbf{R}^d} f(x) \Phi(x) \right| &= \left| \int_0^\infty r^{d-1} \Phi(r) \int_{S_1} f(ry)\; dy \right|\\
    &\lesssim_{d,\varepsilon} \left| \int_0^\infty r^{1 + \varepsilon} \| f \|_{L^\infty(S_r)}\; dr \right|\\
    &= \left| \int_0^N r^{1 + \varepsilon} \| f \|_{L^\infty(S_r)}\; dr \right| + \left| \int_N^\infty r^{1 + \varepsilon} \| f \|_{L^\infty(S_r)}\; dr \right|\\
    &\lesssim N^{2 + \varepsilon} \| f \|_{L^\infty(\mathbf{R}^d)} + \| x^3 f \|_{L^\infty(\mathbf{R}^d)}
\end{align*}
%
Thus $\Phi$ can be interpreted as a tempered distribution. In particular, we can calculate $\Delta \Phi$ in a distribution sense. We find that $\Delta \Phi = -\delta$, where $\delta$ is the Dirac delta function at the identity. As a consequence, if $f$ is any Schwartz function, then the function
%
\[ u(x) = (\Phi * f)(x) = \int \Phi(y) f(x - y)\; dy \]
%
is $C^\infty$ and satisfies $\Delta u = - f$.

\begin{theorem}
    If $\Phi$ is the fundamental solution, then $\Delta \Phi = - \delta$. More generally, if $f$ is a function with bounded continuous first and second derivatives, and with a Laplacian with enough decay at $\infty$ to define the integral below, then
    %
    \[ \int (- \Delta f)(x) \Phi(x) = f(0) \]
\end{theorem}
\begin{proof}
    We note that the first partial derivatives of $\Phi$ are $\lesssim 1/|x|^{d-1}$, and the second derivatives are $\lesssim 1/|x|^d$. If $f$ is a given Schwartz function, we consider
    %
    \[ \int_{\mathbf{R}^n} \Phi(x) \Delta f(x) = \int_{|x| \leq \varepsilon} \Phi(x) \Delta f(x) + \int_{|x| > \varepsilon} \Phi(x) \Delta f(x) = A_\varepsilon + B_\varepsilon \]
    %
    Now $|A_\varepsilon| \leq \| \Delta f \|_{L^\infty(B_\varepsilon)} \| \Phi \|_{L^1(B_0(\varepsilon))} = o(1)$ as $\varepsilon \to 0$. An integration by parts implies that
    %
    \[ B_\varepsilon = \int_{|x| > \varepsilon} \Phi(x) \Delta f(x) = - \int_{S_\varepsilon} \Phi [\nabla f \cdot \eta] - \int_{|x| > \varepsilon} \nabla \Phi \cdot \nabla f = C_\varepsilon + D_\varepsilon \]
    %
    Again, the boundedness of $\nabla f$ implies that $C_\varepsilon = o(1)$, and a further integration by parts implies that
    %
    \[ D_\varepsilon = - \int_{|x| > \varepsilon} \nabla \Phi \cdot \nabla f = \int_{S_\varepsilon} f (\nabla \Phi) \cdot \eta + \int_{|x| > \varepsilon} f \Delta \Phi = \int_{S_\varepsilon} f (\nabla \Phi \cdot \eta) \]
    %
    Now we use the actual features of $\Phi$ other than its decay. Indeed, we know
    %
    \[ (\nabla \Phi)(x) = [-1/d\alpha(d)] x/|x|^d \]
    %
    so
    %
    \[ (\nabla \Phi)(x) \cdot \eta = (\nabla \Phi)(x) \cdot x/|x| = \frac{-1}{d\alpha(d)|x|^{d-1}} \]
    %
    so
    %
    \[ \int_{S_\varepsilon} f\; \nabla \Phi \cdot \eta = \frac{-1}{d\alpha(d) \varepsilon^{d-1}} \int_{S_\varepsilon} f(x) = - \fint_{S_\varepsilon} f(x) = - f(0) + o(1) \]
    %
    This completes the calculation.
\end{proof}

Harmonic functions possess an incredibly useful integral formula which makes them as regular as we desire, known as the mean value formula.

\begin{theorem}
    If $f$ is harmonic in a region $D$ containing a closed ball $B$ centered at $x$, then
    %
    \[ f(x) = \fint_{\partial B} f(y)\; dy \]
\end{theorem}
\begin{proof}
    Set
    %
    \[ \phi(r) = \fint_{\partial B_r(x)} f(y)\; dy = \fint_{\partial B_1(x)} f(x + ry)\; dy \]
    %
    Then it is easy to see that
    %
    \[ \phi'(r) = \fint_{\partial B_1(x)} (\nabla f(x + ry) \cdot y)\; dy = \fint_{\partial B_r(x)} (\nabla f(y) \cdot (y-x))\; dy \]
    %
    But Green's formula implies
    %
    \[ \int_{\partial B_r(x)} (\nabla f(y) \cdot (y-x))\; dy = r \int_{\partial B_r(x)} (\nabla f(y) \cdot \eta) dy = r \int_{B_r(x)} \Delta f(y) = 0 \]
    %
    so $\phi' = 0$, and so $\phi$ is constant. Since $\phi(r) \to f(x)$ as $r \to 0$, we find $\phi(r) = f(x)$ for all $r$. This completes the proof.
\end{proof}

A simple corollary is that if $f$ is harmonic, then
%
\begin{align*}
    \fint_{B_r(x)} f(y)\; dy &= \frac{1}{r^d \alpha(d)} \int_0^r \int_{\partial B_s(x)} f(y)\; dy\; ds\\
    &= d\alpha(d) f(x) \int_0^r\; s^{d-1} ds = r^d d \alpha(d) f(x)
\end{align*}
%
This is a suitable generalization of the Cauchy integral formula to higher dimensions, which enables us to try and extend the results of complex analysis to a higher dimensional setting. The easiest argument to generalize, which passes through without any change, is the maximal principle.

\begin{theorem}
    If $u$ is harmonic in an open precompact set $U$, and continuous on $\overline{U}$, then the maximum of $u$ over $\overline{U}$ occurs on the boundary of $U$. If, in addition, $U$ is connected, and $u$ attains it's supremum in the interior of $U$, then $u$ is constant.
\end{theorem}
\begin{proof}
    Let $u(x_0)$ be a maximum for $U$, so $u(y) \leq u(x_0)$ for all $y$ in a ball $B$ around $x_0$ in $U$. Then the mean value formula just proved shows
    %
    \[ u(x_0) = \fint_{\partial B} u(y)\; dy \leq u(x_0) \]
    %
    and this can only be an equality if $u(y) = u(x_0)$ for all $y \in \partial B$. Thus $u$ is locally constant around every local maximum for $U$. In particular, $u$ is constant on the connected component of $U$ containing $x_0$, and in particular, any point on the boundary of this connected component is also a maximum.
\end{proof}

The initial claim is known as the weak maximum principle, and the second claim the strong maximum principle. Variants of these principles will hold for more general solutions to differential equations, which we will discuss in time. Note that if $u$ is harmonic on $U$, and is positive on $\partial U$, then $u$ is positive everywhere on $U$, because by negating $u$ we obtain a `minimum principle'.

The maximum principles are useful because they allow us to show a {\it uniqueness result} for Laplace's equation.

\begin{theorem}
    If $f$ and $g$ are continuous functions defined on $U$ and $\partial U$ respectively, then there is at most one twice differentiable function $u$ with $\Delta u = f$ on $U$, continuous on  and $u = g$ on $\partial U$.
\end{theorem}
\begin{proof}
    Let $u_1$ and $u_2$ be two functions satisfying the required conditions. Then $u_1 - u_2$ is harmonic on $U$, and vanishes on the boundary. But this implies by the maximum principle that $u_1 - u_2$ vanishes on $U$ as well, so $u_1 = u_2$.
\end{proof}

Next, we prove the {\it regularity} of solutions to Laplace's equation.

\begin{theorem}
    If $u$ is continuous in $U$, and satisfies the mean value formula, then $u$ is actually infinitely differentiable in $U$.
\end{theorem}
\begin{proof}
    Let $\eta_\varepsilon$ be a {\it mollifier} in $U$. 
\end{proof}









\chapter{Sobolev Spaces}

The theory of Sobolev spaces is most effective in enabling us to apply the methods of functional analysis to the study of partial differential equations. In order to apply these techniques, we need 



%\section{Appendix: The Laplacian}

%Using Taylor's theorem, we write
%
%\begin{align*}
%    f(x + y) &= f(x) + \langle (\nabla f)(x), y \rangle + \frac{1}{2} \sum_{i \leq j} y_iy_j f_{x_ix_j}(x) + \sum_{i \leq j \leq k} h_{ijk}(y) y_iy_jy_k
%\end{align*}
%
%where $h_{ijk}(y) \to 0$ as $y \to 0$. Note that since $y_i$ and $y_iy_j$ are odd functions of $i$ for $i \neq j$, the average value over a region symmetric in the $i$'th axis is equal to zero, and in particular for a ball of radius $r$, we find
%
%\[ \int_{B_r} y_i = \int_{B_r} y_iy_j = 0 \]
%
%If we let $\alpha(n)$ denote the volume of an $n$ dimensional unit ball, which can be calculated recursively by the formula
%
%\[ \alpha(n) = \alpha(n-1) \int_{-\pi/2}^{\pi/2} \cos^n(u)\ du \]
%
%with $\alpha(1) = 2$. We then find that
%
%\begin{align*}
%    \frac{1}{v(B^n_r)} \int_{B^n_r} [f(x+y) - f(x)] dy &= \sum \frac{f_{x_i}^2(x)}{2 v(B^n_r)} \int_{B^n_r} y_i^2 + \sum_{i \leq j \leq k} \frac{1}{v(B^n_r)} \int_{B^n_r} h_{ijk}(y) y_iy_jy_k\\
%    &= \sum \frac{f_{x_i^2}(x)}{v(B_r^n)} \int_0^r v\left(B^{n-1}_{\sqrt{1-y^2}}\right) y^2 dy + o(r^3)\\
%    &= (\Delta f)(x) \frac{\alpha(n-1)}{\alpha(n)} r^{-n} \int_0^r y^2(1-y^2)^{\frac{n-1}{2}} dy + o(r^3)\\
%    &= (\Delta f)(x) \frac{\alpha(n-1)}{\alpha(n)} r^2 \int_0^{\pi/2} [\cos^n(t) - \cos^{n+2}(t)] dt + o(r^3)\\
%    &= (\Delta f)(x) \frac{\alpha(n-1)}{\alpha(n)} r^2 (W_n/n) + o(r^3)
%\end{align*}
%
%So, up to a scalar factor depending on the dimension of the Laplacian, $(\Delta f)(x)$ measures the average difference in a small neighbourhood about the point.




\part{Nonlinear Partial Differential Equations}

\chapter{Ordinary Differential Equations}

We begin our discussion of nonlinear partial differential equations by building intuition for the phenomena that we will soon encounter with the general class of partial differential equations. In particular, many partial differential equations can be viewed as infinite dimensional variants of partial differential equations. An advantage that the theory of ordinary differential equations possesses is that we can work with \emph{classical solutions}, i.e. smooth solutions to equations. But the theory of regularity (blowup phenomena) still occurs in the theory of ordinary differential equations, and as we consider large systems of ordinary differential equations the theory possesses, asymptotically, many of the same phenomena as the more general situation of partial differential equations.

In order to solve nonlinear partial differential equations, a general theme is the study of \emph{feedback}. One must study how the behaviour of a solution to an equation at a particular time influences later behaviour of the equation. In the theory of nonlinear equations, one therefore often needs to try to control solutions in terms of themselves.

Let $X$ be a finite-dimensional vector space (real or complex) equipped with a norm $\| \cdot \|_X$. We call $X$ the state space. In the most general form, an ordinary differential equation on a time interval $I$ is a function $G: X^{k+1} \times I \to Y$, where $Y$ is another finite dimensional vector space (in particular, this is an \emph{order $k$ ordinary differential equation}. One then studies functions $u \in C^k(I,X)$ such that
%
\[ G(u,\partial_t u, \dots, \partial_t^k u,t) = 0. \]
%
Such solutions are known as \emph{classical solutions} to the partial differential equation.

The first simplification we can make is to assume our equation is \emph{autonomous}. This means that $G$ is not dependent on $t$. One can always convert a non-autonomous equation into an autonomous equation by incorporating $t$ into the state space $X$, i.e. letting $W = X \oplus k$, and defining $\tilde{G}: W^{k+1} \to Y \times k$ by setting
%
\[ \tilde{G}((v_0,t_0),\dots,(v_k,t_k)) = (G(v_1,\dots,v_{k+1},t_0),t_0 - 1). \]
%
For autonomous equations, we have \emph{time symmetry}, i.e. if $u \in C^k(I,X)$ is any classical solution to a partial differential equation then for any $s \in I$, the function $\text{Trans}_s u \in C^k(I + s,X)$ given by setting $[\text{Trans}_s u](t) = u(t - s)$ is also a solution. In particular, we can always trade this symmetry to assume that without loss of generality, for any solution $u \in C^k(I,X)$ we deal with, $0 \in I$, so that we can consider the \emph{initial value problem} at $0$.

Another simplification we make is that our ordinary differential equation can be rewritten in the form
%
\[ \partial_t^k u = F(u,\dots, \partial_t^{k-1} u). \]
%
where $F: X^k \to X$. One way to do this is to apply the implicit function theorem to the function $G$ we are considering, where the hypothesis are often available for any particular ordinary differential equation we study. Of course, this can't work at all if $\dim X \neq \dim Y$. If $\dim X > \dim Y$ then the system is \emph{overdetermined}, i.e. there are less degrees of freedom than constraints, and one likely must assume certain constraints on initial conditions in order to guarantee solutions. If $\dim X < \dim Y$ the system is \emph{underdetermined}, i.e. there are more degrees of freedom than constraints, and one likely has multiple solutions given any particular initial data. Given any initial conditions, one can either form a parametric family of different solutions or find a family of symmetries which, once reduced by adding additional solutions, make equations constraints. On the other hand, even if $\dim X = \dim Y$ the assumptions of the implicit function theorem do not always hold. In this case another option is to consider the equation
%
\[ 0 = \partial_t G(u,\dots,\partial_t^ku) = \sum_{i = 1}^{k+1} (\partial_i G)(u,\dots,\partial_t^{k-1} u) \cdot \partial_t^i u \]
%
and then re-expressing the equation as a differential equation \emph{of order $k+1$}. We refer to equations which cannot be reexpressed as an equation of the above form as \emph{degenerate}. Such equations are quite difficult to analyze in full generality and we do not deal with them here.

Now suppose $F \in C^\infty(X^{k+1},Y)$. If $u \in C^k(I,X)$ is a solution to the equation, then we actually have $u \in C^{k+1}(I,X)$, with
%
\[ \partial_t^{k+1} u = \sum (\partial_i F)(u,\dots,\partial_t^{k-1}u) \cdot \partial_t^i u. \]
%
Continuing this by considering successive derivatives in $t$ gives that $u \in C^\infty(I,X)$. In particular, we note that the derivatives $\partial_t^i u$ for $i \geq k$ depend \emph{only} on the regularity of the derivatives $\partial_t^i u$ for $i < k$. For analytic functions, this approach also easily gives a uniqueness result.

\begin{lemma}
    Suppose $u_1,u_2 \in C^\omega(I,X)$ and for $\alpha \in \{ 1, 2 \}$,
    %
    \[ \partial_t^k u_\alpha = F(u_\alpha, \dots, \partial_t^{k-1} u_\alpha). \]
    %
    Suppose furthermore, that there is $t_0 \in I^\circ$ such that for each $i < k$,
    %
    \[ (\partial_t^i u_1)(t_0) = (\partial_t^i u_2)(t_0). \]
    %
    Then $u_1 = u_2$.
\end{lemma}
\begin{proof}
    It follows that $\partial_t^i u_1 = \partial_t^i u_2$ for \emph{all} $i > 0$, which implies that $u_1 = u_2$ in a neighbourhood of $t_0$. But this means that
    %
    \[ S = \{ t \in I: u_1(t) = u_2(t), \dots, \partial_t^{k-1} u_1(t) = \partial_t^{k-1} u_2(t) \} \]
    %
    is open and closed in $I$. Since $I$ is connected, this implies $S = I$, and this implies that $u_1 = u_2$.
\end{proof}

Of course, we can only really expect all solutions of an ordinary differential equations to be analytic when the function $F$ is also analytic. In this case, we also have a local existence statement.

\begin{lemma}[Cauchy-Kowalevski]
    Let $k \geq 1$. Suppose $F: X^k \to X$ is analytic, let $t_0 \in \RR$, and consider any $u_0,\dots,u_{k-1} \in X$. Then there exists an open interval $I$ containing $t_0$, and a unique analytic function $u: I \to X$ such that
    %
    \[ \partial_t^k u = F(u,\dots,\partial_t^{k-1} u) \]
    %
    and $\partial_t^i u(t_0) = u_i$ for each $0 \leq i < k$.
\end{lemma}
\begin{proof}
    Without loss of generality, assume $t_0 = 0$, $u_0 = 0$, and $X = \RR^d$. We can also reduce our discussion to the case $k = 1$ by letting $W = (\RR^d)^k$ and considering the function $\tilde{F}: W \to W$ by setting
    %
    \[ \tilde{F}(v_1,\dots,v_k) = (v_2,\dots,v_{k-1},F(v_1,\dots,v_k)) \]
    %
    which is analytic if $F$ is analytic. Thus we need only analyze the equation
    %
    \[ \partial_t u = F(u). \]
    %
    Consider an open subset $W$ of $X$ containing the origin such that there exists a family of constants $\{ c_\alpha \}$ ranging over multi-indices $\alpha$ such that for each $w \in W$,
    %
    \[ F(w) = \sum_\alpha c_\alpha w^\alpha \]
    %
    If we did have an analytic solution $u$ with $u(0) = 0$, then we would find that there exists a polynomial $Q_r$ with non-negative integer coefficients such that
    %
    \[ \partial_t^r u(0) = \partial_t^{r-1} [F \circ u](0) = Q_r(F(0),\dots,\nabla^{r-1} F(0),b_1,\dots,b_{r-1}). \]
    %
    Applying recursion shows that there actually exists a polynomial $R_r$ with non-negative integer coefficients such that
    %
    \[ \partial_t^r u(0) = R_r(F(0),\dots,\nabla^{r-1} F(0)) \]
    %
    In particular, this means that a solution $u$ to the differential equation exists in the sense of formal power series, and it suffices to show the formal power series converges in a neighbourhood of the origin. To do this, we apply the \emph{method of majorants}. We deduce by monotonicity that if we can find an analytic function $G: X \to X$ such that if $|D^\alpha F_i(0)| \leq D^\alpha G_i(0)$ for each $\alpha$ and $i$ (known as the \emph{majorant} of $F$), then we conclude that for any solution $v$ of $G$,
    %
    \[ |\partial_t^r u(0)| \leq |R_r(F(0),\dots,\nabla^{r-1} F(0))| \leq R_r(G(0),\dots,\nabla^{r-1}G(0)) = |\partial_t^r v(0)|. \]
    %
    The idea is that if we can choose an explicit function $G$ and an analytic solution $v$, we can obtain the convergence of the series defining $u$. Since $F$ converges absolutely in a neighbourhood of the origin, we can find a small $\delta > 0$ and $M > 0$ such that for each multi-index $\alpha$ and $i \in \{ 1, \dots, d \}$,
    %
    \[ |D^\alpha F_i(0)| \leq M \cdot |\alpha|! \cdot \delta^{-|\alpha|}. \]
    %
    Thus if we set
    %
    \[ G(w) = \left( \frac{M}{1 - z_1/\delta - \dots - z_d/\delta}, \dots, \frac{CM}{1 - z_1/\delta - \dots - z_d/\delta} \right) \]
    %
    then $G$ majorizes $F$ and has a convergent power series for $|z_i| \lesssim \delta$. But now we note that a solution is given by setting $v = (w,\dots,w)$, where
    %
    \[ w'(t) = \frac{M}{1 - dw/\delta} \]
    %
    a separable equation with solution
    %
    \[ w(t) = (\delta/d)(1 - (1 - 2Mt/\delta)^{1/2}). \]
    %
    This equation is clearly analytic in a neighbourhood of the origin, which completes the proof.
\end{proof}

\begin{remark}
    The local existence statement is necessary. For instance, the initial value problem
    %
    \[ \partial_t u = u^2 \]
    %
    with $u(0) = 1$ has a unique analytic solution of the form
    %
    \[ \frac{1}{1 - t} \]
    %
    which is only defined on the interval $(-\infty,1)$. Thus we have `finite time blowup'.
\end{remark}

It is often interesting to have other existence statements for ordinary differential equations which do not rely so much on the analyticity of the function $F$. Thus we continue analyzing the theory of the first order equation
%
\[ \partial_t u = F(u) \]
%
on an interval $I$ about the origin, subject to the initial conditions $u(0) = u_0$ and under the very weak assumption that $F \in C(\Omega,X)$, where $\Omega$ is a subset of $X$. There are three useful perspectives in differential equations which enable one to see a solution to this partial differential equation:
%
\begin{itemize}
    \item (Classical Solution): $u(0) = u_0$, and for each $t \in I$, $\partial_t u(t) = F(u(t))$.
    \item (Strong Solution): For each $t \in I$,
    %
    \[ u(t) = u_0 + \int_0^t F(u(s))\; ds. \]
    %
    This equation is obtained by applying the Fundamental theorem of calculus to a classical solution.

    \item (Weak Solution): For any $\psi \in C_c^\infty(I)$,
    %
    \[ \int_I u(t) \psi(t)\; dt = u_0 \int_I \psi(t)\; dt + \int_I \left( \int_0^t F(u(s))\; ds \right) \psi(t) dt. \]
    %
    One need only take the equation defining a strong solution and expand to obtain the equation for a weak solution.
\end{itemize}
%
We note that the equation defining a classical solution can be interpreted for any $u \in C^1(I,\Omega)$. On the other hand, the equation defining a strong solution can be interpreted for any $u \in C(I,\Omega)$, and the equation defining a weak solution can be interpreted for any $u \in L^\infty(I,\Omega)$. Fortunately in the case of ordinary differential equations with $F \in C(\Omega,X)$, the class of all such solutions are equivalent, and only serve to provide various different perspectives to the theory.

\begin{lemma}
    Let $F \in C(\Omega,X)$, and $u \in L^\infty(I,\Omega)$ is a weak solution to the equation $\partial_t u = F(u)$ with initial condition $u(0) = u_0$. Then there exists a classical solution $v \in C^1(I,\Omega)$ such that $u(t) = v(t)$ for almost every $t \in I$.
\end{lemma}
\begin{proof}
    Then $F \circ u \in L^\infty(I,X)$, and so the function $f: I \to X$ obtained by setting
    %
    \[ f(t) = u_0 + \int_0^t F(u(s))\; ds \]
    %
    is a Lipschitz continuous function with $f'(t) = F(u(t))$ for almost every $t \in I$. Our assumptions imply that for any $\psi \in C_c^\infty(I)$,
    %
    \[ \int_I f(t) \psi(t)\; dt = \int_I u(t) \psi(t)\; dt. \]
    %
    Thus $f(t) = u(t)$ for almost every $t \in I$. Thus without loss of generality we may assume $u$ is Lipschitz continuous. But then $F \circ u \in C(I,X)$, and so, by the fundamental theorem of calculus, $f \in C^1(I,\Omega)$ with $f'(t) = F(u(t))$ for all $t$. But $u(t) = f(t)$ for almost every $t \in I$, so without loss of generality we may assume $u \in C^1(I,\Omega)$. But then $u'(t) = F(u(t))$ for almost every $t \in I$, and by continuity this implies this is true for \emph{all} $t \in I$.
\end{proof}

\begin{remark}
    We will often find it convenient to work distributionally, i.e. identifying two functions if they agree almost everywhere. Thus we might write the conclusions of this theorem that if $u \in L^\infty(I,\Omega)$ is a weak solution to $\partial_t u = F(u)$, then $u \in C^1(I,\Omega)$ is a classical solution to the equation.
\end{remark}

The classical perspective is useful for obtaining conservation laws, monotonicity formulae, and symmetries. The strong perspective is useful for studying the regularity of solutions. Finally, the weak perspective is useful for using compactness methods, since one is able to take weak limits of equations. Let us now use the strong solution perspective to obtain an existence theorem for ordinary differential equations.

\begin{theorem}[Picard]
    Fix $u_0 \in X$ and $\varepsilon > 0$, and let $\Omega = B_X(u_0,\varepsilon)$. Suppose $F: \Omega \to X$ is a Lipschitz function with
    %
    \[ \| F(x) - F(y) \|_X \leq M \| x - y \|_X \]
    %
    for all $x,y \in \Omega$. Let $A = |F(u_0)|$. Then for $0 < T < 1/(M + A/\varepsilon)$, if we set $I = [-T,T]$, then there exists a unique strong solution $u \in C(I,\Omega)$ to the equation $\partial_t u = F(u)$ with the initial conditions $u(0) = u_0$.
\end{theorem}
\begin{proof}
    Consider the operator $L: C(I,\Omega) \to C(I,X)$ defined by setting
    %
    \[ (Lu)(t) = \int_0^t F(u(s))\; ds. \]
    %
    Let $D = \{ u \in C(I,\Omega) : u(0) = u_0 \}$. Then for any $u,v \in D$, and $t \in I$,
    %
    \begin{align*}
        |(Lu)(t) - (Lv)(t)| &= \left| \int_0^t F(u(s)) - F(v(s)) \right|\\
        &\leq M \int_0^t |u(s) - v(s)|\; ds\\
        &\leq |t| M \| u - v \|_{L^\infty(I,X)}\\
        &\leq T M \| u - v \|_{L^\infty(I,X)}.
    \end{align*}
    %
    Thus
    %
    \[ \| Lu - Lv \|_{L^\infty(I,X)} \leq TM \| u - v \|_{L^\infty(I,X)}. \]
    %
    In particular, noting that $(Lu_0)(t) = t F(u_0)$ for any $t \in I$, this implies that for any $u \in D$,
    %
    \begin{align*}
        \| Lu \|_{L^\infty(I,X)} &= \| Lu - Lu_0 \|_{L^\infty(I,X)} + \| Lu_0 \|_{L^\infty(I,X)}\\
        &\leq TM \| u - u_0 \|_{L^\infty(I,X)} + T |F(u_0)|\\
        &\leq T[M \varepsilon + A] < \varepsilon.
    \end{align*}
    %
    Since $TM < 1$, this means $L$ restricts to a contraction map from $D$ to $D$. Thus the Banach fixed point theorem implies that there exists a unique $u \in C(I,\Omega)$ such that $Lu = u$. But this means precisely that $u$ is a unique strong solution to the equation in question.
\end{proof}

\begin{remark}
    The Picard theorem actually gives a practical way to solve a given PDE. Set $u_0 \in D$ by letting $u_0(t) = u_0$ for all $t \in I$, and then define $u_n = L^n u_0$ for all $n > 0$. If $u$ is the unique strong solution to the differential equation, then
    %
    \[ \| u - u_n \|_{L^\infty(I,X)} \leq (TM)^n \| u - u_0 \|_{L^\infty(I,X)} \leq 2 \varepsilon \cdot (TM)^n. \]
    %
    Thus $u_n$ converges geometrically to $u$, uniformly for $t \in I$.
\end{remark}

We also have a \emph{local regularity result}, which shows that two solutions with close initial conditions remain close to one another over small time changes.

\begin{theorem}
    Fix a nonempty subset $\Omega$ of $X$, let $\varepsilon > 0$, and suppose $F: N_\varepsilon(\Omega) \to X$ is a Lipschitz function with
    %
    \[ |F(x) - F(y)| \leq M |x - y| \]
    %
    for some $M > 0$ and $x,y \in N_\varepsilon(\Omega)$. Let $A = \| F \|_{L^\infty(N_\varepsilon(\Omega))}$, and suppose $0 < T < \min(\varepsilon/A,1/M)$. If $I = [-T,T]$, then we have a map $S: \Omega \to C(I,N_\varepsilon(\Omega))$ such that for each $u_0 \in \Omega$, $Su_0$ is a strong solution to the equation $\partial_t u = F(u)$ with $Su_0(0) = u_0$. Then $S$ is a Lipschitz continuous map, with
    %
    \[ \| Su_0 - Sv_0 \|_{L^\infty(I,N_\varepsilon(\Omega))} \leq \frac{\| u_0 - u_1 \|_X}{1 - TM}. \]
\end{theorem}
\begin{proof}
    Let $u = Su_0$ and $v = Sv_0$. Then
    %
    \begin{align*}
        \| u - v \|_{L^\infty(I,X)} &\leq \| u_0 - v_0 \|_X + \max_{t \in I} \int_0^t \| F(u(s)) - F(v(s)) \|_X\\
        &\leq \| u_0 - v_0 \|_X + MT \| u - v \|_{L^\infty(I,N_\varepsilon(\Omega))}.
    \end{align*}
    %
    Rearranging this equation gives that
    %
    \[ \| u - v \|_{L^\infty(I,X)} \leq \frac{\| u_0 - v_0 \|_X}{1 - MT}. \]
\end{proof}

Note that uniqueness \emph{automatically} follows from this regularity statement. If we assume the function $F$ has more regularity, then the differential equation also has more regularity. PROOF: TODO.

If $F$ is \emph{globally Lipschitz continuous}, then in the above theorem we can set $\varepsilon = \infty$ and thus find a strong solution $u: (-1/M,1/M) \to X$ to the differential equation with $u(0) = u_0$. But now iterating this theorem with different initial conditions and then patching solutions together using the time invariance of the autonomous differential equation, we can find a unique global strong solution $u: \RR \to X$ to the differential equation with $u(0) = u_0$. More generally, if $F$ is \emph{locally Lipschitz continuous}, then for any $u_0$ we can find a \emph{maximal interval} $(T_-,T_+)$ and $u: (T_-,T_+) \to B_X(0,\varepsilon)$ satisfying the differential equation. If $T_+ < \infty$, then
%
\[ \limsup_{t \to T_+} \| u(t) \|_X = \varepsilon. \]
%
Otherwise, the fact that $F$ is locally Lipschitz implies that $u_{T_+} = \lim_{t \to T_+} u(t)$ exists, and one can continue $u$ at $T_+$ since $F$ is Lipschitz in a neighbourhood of $u_{T_+}$. Similarily, if $T_+ < \infty$, then
%
\[ \limsup_{t \to T_+} \| u(t) \|_X = \varepsilon. \]
%
Thus solutions to suitably smooth differential equations can only fail to exist due to a `finite time blowup' (if the differential equation is not suitably smooth, one might also get oscillatory singularities).

To guarantee the global existence of solutions to differential equations, it thus becomes of importance to understand the asymptotic behaviour of ordinary differential equations. In the next section we introduce Gronwall's inequality, which helps one bound quantities which satisfy a `linear differential inequality'. Given nonlinear growth one must rely on continuity methods.

\section{Gronwall's Inequality}

In order to understand the solutions of high dimensional systems of differential equations, it is often useful to find scalar quantities $u(t)$ associated with the system which give the qualitative features of the entire system, such as the size of the solution, or the center of mass, or the energy. Often these quantities are constant over time, satisfy a differential equation themselves, or in the most general form, satisfy a \emph{differential inequality}. One then hopes to use these inequalities to bound the behaviour of the function $u(t)$ in the future. Gronwall's inequality is useful for obtaining bounds for quantities involving \emph{linear} differential inequalities, i.e. an equation of the form
%
\[ u'(t) \leq B(t) u(t). \]
%
We prefer to work in integral form.

\begin{theorem}
    Let $u: [0,T] \to [0,\infty)$ be continuous, such that for all $0 \leq t \leq T$,
    %
    \[ u(t) \leq A + \int_0^t B(s) u(s)\; ds. \]
    %
    where $A \geq 0$ and $B: I \to [0,\infty)$ is continuous. Then
    %
    \[ u(t) \leq A \exp \left( \int_0^t B(s)|; ds \right). \]
\end{theorem}
\begin{proof}
    Without loss of generality assume $A > 0$. Now
    %
    \[ \frac{d}{dt} \left( A + \int_0^t B(s) u(s)\; ds \right) = B(t)u(t) \leq B(t) \left( A + \int_0^t B(s) u(s) B(t)\; dt \right). \]
    %
    Thus
    %
    \[ \frac{d}{dt} \log \left( A + \int_0^t B(s)u(s)\; ds \right) \leq B(t). \]
    %
    But now integrating this inequality gives
    %
    \[ \log \left( A + \int_0^t B(s)u(s)\; ds \right) \leq \log(A) + \int_0^t B(s)\; ds. \]
    %
    It now suffices to take exponentials of both sides.
\end{proof}

If we can differentiate $u$, then we can also allow $B$ to be negative.

\begin{theorem}
    Suppose $u: [0,T] \to [0,\infty)$ is absolutely continuous and $\partial_t u(t) \leq B(t) u(t)$ for almost every $t \in [0,T]$, where $B: [0,T] \to \RR$ is continuous. Then
    %
    \[ u(t) \leq u(0) \exp \left( \int_0^t B(s)\; ds \right). \]
\end{theorem}
\begin{proof}
    Set
    %
    \[ v(t) = u(t) \exp \left( - \int_0^t B(s)\; ds \right). \]
    %
    Then $v$ is absolutely continuous, and for almost every $t \in [0,T]$,
    %
    \[ v'(t) = [u'(t) - u(t)B(t)] \exp \left( - \int_0^t B(s)\; ds \right) \leq 0. \]
    %
    Thus $v$ is a decreasing function, which completes the proof.
\end{proof}

\section{Continuity Methods}

Most nonlinear differentiable equations are not explicitly solvable. In place of solving these equations, one must instead rely on the qualitative behaviour of these equations, or quantitative asymptotics. Since one does not have an explicit formula for solutions $u(t)$, one must instead rely on integral formulae. For instance, that
%
\[ u(t) = u_0 + \int_0^t F(u(s))\; ds. \]
%
Thus if we have some control on $u$, we might use integration to \emph{improve our control}. This is the bootstrapping method of understanding partial differential equations.

\begin{theorem}[Bootstrapping]
    Let $I$ be an interval, and for each $t \in I$ consider a hypothesis $H(t)$ and a conclusion $C(t)$. Suppose
    %
    \begin{itemize}
        \item If $H(t)$ is true, then $C(t)$ is true.
        \item If $C(t)$ is true, there is $\varepsilon_t$ such that $H(s)$ is true for all $|s - t| \leq \varepsilon_t$.
        \item If $C(t_k)$ is true for all $t_k$ in a sequence of times $\{ t_k \}$, and $t_k \to t$, then $C(t)$ is true.
        \item $H(t_0)$ is true for some $t_0$.
    \end{itemize}
    %
    Then $C(t)$ is true for all $t \in I$.
\end{theorem}

The first principle is often the most difficult assumption to obtain; the latter three often follow easily. Let us consider a simple example.

\begin{theorem}
    Let $H$ be a finite dimensional Hilbert space, and let $V \in C^2(H)$ be such that $V(0) = 0$, $\nabla V(0) = 0$, and $\nabla^2 V(0)$ is strictly positive definite. Then there exists $\delta > 0$ such that for each $\| u_0 \|_H, \| u_0' \|_H \leq \delta$, there exists a unique global solution $u \in C^2(\RR,H)$ such that
    %
    \[ \partial_t^2 u = - \nabla V(u) \]
    %
    and with $u(0) = u_0$ and $\partial_t u(0) = u_0'$. Moreover, $u \in L^\infty(\RR,H)$.
\end{theorem}
\begin{proof}
    By Picard's theorem, there exists a maximal interval $(T_-, T_+)$ and a function $u \in C^2((T_-,T_+),H)$. solving the equation. Define
    %
    \[ E(t) = \frac{\| \partial_t u(t) \|_H^2}{2} + V(u(t)). \]
    %
    Then
    %
    \[ \partial_t E(t) = \langle \partial_t u(t), \partial_t^2 u(t) \rangle + \langle \partial_t u(t), \nabla V(u(t)) \rangle = 0. \]
    %
    Thus $E$ is a constant function on $(T_-,T_+)$, i.e for all $t \in (T_-,T_+)$,
    %
    \[ E(t) = \frac{\| u_0' \|_H^2}{2} + V(u_0). \]
    %
    In particular, for any $\varepsilon > 0$, we can pick $\delta > 0$ in the hypothesis such that $|E(t)| \leq c \varepsilon$ for $c > 0$ to be chosen later. Now let $H(t)$ be true if
    %
    \[ \sqrt{\| u(t) \|_H^2 + \| \partial_t u(t) \|_H^2} \leq 2 \varepsilon, \]
    %
    and let $C(t)$ be true if
    %
    \[ \sqrt{\| u(t) \|_H^2 + \| \partial_t u(t) \|_H^2} \leq \varepsilon. \]
    %
    Since $H(0)$ is true, to bootstrap it suffices to show that $H(t)$ implies $C(t)$ for all $t \in (T_-,T_+)$, from which case we will have shown finite time blowup cannot occur. But if $H(t)$ holds, performing a Taylor expansion for $V$ around the origin and noticing positive definitiveness shows that
    %
    \[ V(u(t)) \gtrsim \| u(t) \|_H^2 - O(\varepsilon^3) \]
    %
    But this means that
    %
    \[ \| \partial_t u(t) \|_H^2 + \| u(t) \|_H^2 \lesssim \frac{\| \partial_t u(t) \|_H^2}{2} + V(u(t)) + O(\varepsilon^3) \leq c \varepsilon + O(\varepsilon^3). \]
    %
    If $c$ is chosen to be appropriately small relative to $\varepsilon$, then $C(t)$ is satisfied.
\end{proof}



\begin{thebibliography}{9}

\bibitem{evans}
Lawrence C. Evans
\textit{Partial Differential Equations}

\end{thebibliography}

 \end{document}