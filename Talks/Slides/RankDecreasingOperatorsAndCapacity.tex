\documentclass[usenames,dvipsnames,12pt]{beamer}

\usepackage{tikz}
\usepackage{tkz-berge}
\usepackage{tkz-graph}
\usepackage{subcaption}
\usepackage{blkarray}
\usepackage{aligned-overset}
\usepackage{graphicx}
\usepackage{calc}

\usetikzlibrary{patterns,arrows,decorations.pathreplacing}

\usepackage{xcolor}
\definecolor{dblue}{RGB}{20,66,129}
\definecolor{rose}{RGB}{255,101,122}
\definecolor{crimsonred}{RGB}{132,22,23}
\definecolor{darkblue}{RGB}{72,61,139}

\definecolor{deepblue}{RGB}{36,123,160}
\definecolor{deepred}{RGB}{255,22,84}
\definecolor{deeporange}{RGB}{240,111,62}

\definecolor{olive}{rgb}{0.3, 0.4, .1}
\definecolor{fore}{RGB}{249,242,215}
\definecolor{back}{RGB}{51,51,51}
\definecolor{title}{RGB}{255,0,90}
\definecolor{dgreen}{rgb}{0.,0.6,0.}
\definecolor{gold}{rgb}{1.,0.84,0.}
\definecolor{JungleGreen}{cmyk}{0.99,0,0.52,0}
\definecolor{BlueGreen}{cmyk}{0.85,0,0.33,0}
\definecolor{RawSienna}{cmyk}{0,0.72,1,0.45}
\definecolor{Magenta}{cmyk}{0,1,0,0}

\DeclareMathOperator{\QQ}{\mathbf{Q}}
\DeclareMathOperator{\ZZ}{\mathbf{Z}}
\DeclareMathOperator{\RR}{\mathbf{R}}
\DeclareMathOperator{\HH}{\mathbf{H}}
\DeclareMathOperator{\CC}{\mathbf{C}}
\DeclareMathOperator{\AB}{\mathbf{A}}
\DeclareMathOperator{\PP}{\mathbf{P}}
\DeclareMathOperator{\MM}{\mathbf{M}}
\DeclareMathOperator{\VV}{\mathbf{V}}
\DeclareMathOperator{\TT}{\mathbf{T}}
\DeclareMathOperator{\LL}{\mathcal{L}}
\DeclareMathOperator{\EE}{\mathbf{E}}
\DeclareMathOperator{\NN}{\mathbf{N}}
\DeclareMathOperator{\DQ}{\mathcal{Q}}
\DeclareMathOperator{\IA}{\mathfrak{a}}
\DeclareMathOperator{\IB}{\mathfrak{b}}
\DeclareMathOperator{\IC}{\mathfrak{c}}
\DeclareMathOperator{\IP}{\mathfrak{p}}
\DeclareMathOperator{\IQ}{\mathfrak{q}}
\DeclareMathOperator{\IM}{\mathfrak{m}}
\DeclareMathOperator{\IN}{\mathfrak{n}}
\DeclareMathOperator{\IK}{\mathfrak{k}}
\DeclareMathOperator{\ord}{\text{ord}}
\DeclareMathOperator{\Ker}{\textsf{Ker}}
\DeclareMathOperator{\Coker}{\textsf{Coker}}
\DeclareMathOperator{\emphcoker}{\emph{coker}}
\DeclareMathOperator{\pp}{\partial}
\DeclareMathOperator{\tr}{\text{tr}}



\title{Algorithmic Aspects of the Brascamp Lieb Inequality}
\author{Jacob Denson (Based on Gurvits, 2004)}
\institute{University of Wisconsin Madison}

\begin{document}

\maketitle

\begin{frame}
    \frametitle{Lieb's Theorem}

    \vspace{-1em}
    \[ \int_{\RR^n} \prod_{i = 1}^m |f_i(B_i x)|^{p_i}\; dx \leq \text{BL}(B,p) \cdot \prod_{i=1}^m \| f_i \|_{L^1(\RR^{n_i})}^{p_i}.  \]

    \begin{itemize}
        \pause
        \item (Lieb, 1990) To calculate BL(B,p), plug in Gaussians.

        \pause
        \item If $f_i(x) = e^{- \pi (A_i x) \cdot x}$ for $A_i \succ 0$, then
        \pause
%        \vspace{-1em}
%        \small
        \[ \int_{\RR^n} \prod_{i = 1}^m |f_i(B_i x)|^{p_i}\; dx = (\sum p_i B_i^* A_i B_i)^{-1/2} \]
%        \normalsize
        \pause
%        \vspace{-0.8em}
%        \small
        \[ \prod_{i = 1}^m \| f_i \|_{L^1(\RR^{n_i})}^{p_i} = (\prod_{i = 1}^m \det(A_i)^{p_i})^{-1/2}. \]
%        \normalsize

        Thus
        %
        \[ (\sum p_i B_i^* A_i B_i)^{-1/2} \leq \text{BL}(B,p) \cdot (\prod_{i = 1}^m \det(A_i)^{p_i})^{-1/2} \]

    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Lieb's Theore}

    \begin{itemize}
        \item Rearranging gives
        %
        \[ \text{BL}(B,p) = \left( \sup_{A_1,\dots,A_m \succ 0} \frac{\prod_i \det(A_i)^{p_i}}{\det(\sum p_i B_i^* A_i B_i)} \right)^{1/2}. \]

        \pause
        \item Non convex objective, so tricky to optimize (NP hard).

        \pause
        \item Checking Finiteness is also non-obvious.

        \pause
        \item BCCT gives a family of conditions
        %
        \[ \dim(V) \leq \sum p_i \dim(B_i V). \]
        %
        Since $0 \leq \dim(V) \leq n$ and $0 \leq \dim(B_i V) \leq n_i$, only finitely many linear conditions on the 
        \pause
        \item But there can be exponentially many, so still tricky to compute
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Geometric Brascamp Lieb}

    \vspace{-1.5em}
    \[ \int_{\RR^n} \prod_{i = 1}^m |f_i(B_i x)|^{p_i}\; dx \leq \text{BL}(B,p) \cdot \prod_{i = 1}^m \| f_i \|_{L^1(\RR^{n_i})}^{p_i} \]

    \begin{itemize}
        \pause
        \item A Brascamp-Lieb inequality is \emph{geometric} if each $B_i$ is a projection ($B_i B_i^* = I$), and $\sum_i p_i B_i^* B_i = I$.

        \pause
        \item H\"{o}lder's inequality and Loomis-Whitney are special cases.

        \pause
        \item Plugging in $f_i(x) = e^{-\pi |x|^2}$ gives $\text{BL}(B,p) \geq 1$.

        \pause
        \item (Barthe, 1998), generalizing (Ball, 1989), showed that these functions are extremizers, i.e. $\text{BL}(B,p) = 1$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Operator Rescaling}

    \begin{itemize}
        \item What happens to $\text{BL}(B,p)$ if we `rescale / change coordinates'.

        \pause
        \item Fix invertible matrices $M$ and $M_1, \dots, M_m$, and consider the Brascamp-Lieb inequality with matrices $B_i' = M_i B_i M$,
        %
        \[ \int_{\RR^n} \prod_{i = 1}^m |f_i(M_i B_i M x)|^{p_i}\; dx \leq \text{BL}(B',p) \cdot \prod_{i = 1}^m \| f_i \|_{L^1(\RR^{n_i})}^{p_i}. \]
        %
        \pause
        \item Then
        %
        \begin{align*}
            \text{BL}(B',p) &= \sup_{A_1,\dots,A_m \succ 0} \frac{\prod_i \det(A_i)^{p_i}}{\det(\sum_i p_i M^* B_i^* M_i^* A_i M_i B_i M)}\\
            &= \sup_{A_1,\dots,A_m \succ 0} \frac{\prod_i \det((M_i^{-1})^* A_i M_i^{-1})^{p_i}}{\det(M^* (\sum_i p_i B_i^* A_i B_i) M)}\\
            &= \det(M)^{-2} \prod_i \det(M_i)^{-2p_i} \cdot \text{BL}(B,p). 
        \end{align*}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Operator Rescaling}

    \[ \text{BL}(B',p) = \det(M)^{-2} \prod\nolimits_i \det(M_i)^{-2p_i} \cdot \text{BL}(B,p) \]

    \begin{itemize}
        \pause
        \item If $(B',p)$ is geometric, $\text{BL}(B',p) = 1$, so
        %
        \[ \text{BL}(B,p) = \det(M)^2 \prod\nolimits_i \det(M_i)^{2p_i}. \]

        \pause
        \item (Bennett, Carbery, Christ, Tao, 2008) Geometric rescaling possible iff the Brascamp-Lieb equation has extremizers.

        \pause
        \item (Garg, Gurvits, Oliveira, Wigderson, 2018) If $\text{BL}(B,p) < \infty$, we can rescale to be {\emph within a $\varepsilon$} of a geometric Brascamp-Lieb equation.
        \begin{itemize}
            \pause
            \item We can do this algorithmically, i.e. a computer can compute a $\varepsilon$-approximate geometric rescaling in $\text{Poly}(\text{Bits}(B),\log(p),1/\varepsilon)$ computations. 

            \pause
            \item
            Conversely, we can determine if $\text{BL}(B,p) = \infty$ in $\text{Poly}(\text{Bits}(B),\log(p))$ computations.
        \end{itemize}
    \end{itemize}
\end{frame}

%\begin{frame}
%    \frametitle{The Avoidance Problem}

%        \begin{itemize}

%            \item Our method naturally considers an equivalent setup.

%            \item {\bf Fractal Avoidance Problem}: Given $Y \subset \mathbf{R}^{nd}$, find $X \subset \mathbf{R}^d$ such that $X^n \cap Y \subset \Delta$, where $\Delta = \{ x: x_i = x_j\ \text{for some $i,j$} \}$.

%            \item Equivalent by setting $Y = f^{-1}(0)$, or $f = \mathbf{I}_{Y^c}$.

%            \item Our method only uses the structure of the zero set, not $f$.
%        \end{itemize}
%\end{frame}

\begin{frame}
    \frametitle{Computing Permanents (An Analogous Problem)}

    \begin{itemize}
        \item Given an $n \times n$ matrix $A$ with non-negative entries, define
        %
        \[ \text{Perm}(A) = \sum\nolimits_{\sigma \in S_n} \prod\nolimits_i A_{i \sigma(i)}. \]
        
        \pause
        \item This is (maybe suprisingly) NP hard to compute.

        \begin{itemize}
            \pause
            \item If $R = \text{diag}(\lambda_1, \dots, \lambda_n)$ and $C = \text{diag}(\gamma_1,\dots,\gamma_n)$, then
            %
            \begin{align*}
                \text{Perm}(R A C) &= (\lambda_1 \dots \lambda_n)(\gamma_1 \dots \gamma_n) \text{Perm}(A)\\
                &= \det(R) \cdot \det(C) \cdot \text{Perm}(A).
            \end{align*}

            \pause
            \item (Egorychev, 1981), (Falikman, 1981) If $A$ is \emph{doubly stochastic}, $e^{-n} \leq \text{Perm}(A) \leq 1$.

            \pause
            \item If $RAC$ is doubly stochastic, then
            %
            \[ \text{Perm}(A) \leq \det(R)^{-1} \det(C)^{-1} \leq e^n \cdot \text{Perm}(A), \]
            %
            so $\text{Perm}(A) \approx \det(R)^{-1} \det(C)^{-1}$.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \begin{itemize}
        \item We continue this iteration, obtaining a sequence
        %
        \[ A \to A_1 \to A_2 \to \dots. \]

        \pause
        \item Claim: If $\text{Per}(A) > 0$, these matrices become arbitrarily close to being doubly stochastic as $i \to \infty$.

        \pause
        \item Let $\gamma_i = (\gamma_{i1}, \dots, \gamma_{in})$, be the row / column sums of $A_i$.

        \begin{itemize}
            \pause
            \item i.e. so that $A_1 = \text{diag}(1/\gamma_0) A$, $A_2 = A_1 \text{diag}(1/\gamma_1)$, etc.
        \end{itemize}

        \pause
        \item {\bf Two key facts}:
        \begin{itemize}
            \pause
            \item[(1)] $\text{Per}(A_i) \leq 1$ for all $i \geq 1$.
            \pause
            \item[(2)] Let $\Delta_i = \sum_j (\gamma_{ij} - 1)^2$.
            \begin{itemize}
                \pause
                \item If $\Delta_i \leq 1$, then $\text{Per}(A_{i+1}) \geq (1 + C \Delta_i) \cdot \text{Per}(A_i)$.

                \item If $\Delta_i \geq 1$, then $\text{Per}(A_{i+1}) \geq (1 + C) \text{Per}(A_i)$.
            \end{itemize}
        \end{itemize}

        \pause
        \item Thus $\text{Per}(A_i)$ is bounded, monotonic, converges to $P \leq 1$.

        \pause
        \item If $\text{Per}(A_i) \geq P - \varepsilon$ for $\varepsilon \ll 1$, then
        %
        \[ P \geq \text{Per}(A_{i+1}) \geq (1 + C \cdot \Delta_i) \cdot \text{Per}(A_i) \geq (1 + C \cdot \Delta_i)(P - \varepsilon). \]
        % C (\varepsilon / P) \geq C_2 \delta_i
        Thus $\Delta_i \leq (C_0/P) \varepsilon$. Taking $\varepsilon \to 0$ shows $\Delta \to 0$.
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Sinkhorn Iteration}

    \begin{itemize}
        \item {\bf Proof that $\text{Per}(A_i) \leq 1$:}
        \begin{itemize}
            \pause
            \item Simple inductive argument: the hypothesis is that if the rows of a matrix $B$ all sum up to less than one, then $\text{Per}(B) \leq 1$. Now expand along minors (ala the determinant).
        \end{itemize}

        \pause
        \item {\bf Proof that $\text{Per}(A_{i+1}) \geq (1 + C \Delta_i) \cdot \text{Per}(A_i)$:}
        \begin{itemize}
            \pause
            \item Write $\gamma_i = 1 + \delta_i$.

            \begin{itemize}
                \pause
                \item Then $\sum \delta_i^2 = \Delta_i$, and $\sum \delta_i = 0$.
            \end{itemize}

            \pause
            \item Since $1 + t \leq \exp(t - t^2/2 + t^3/3)$,

            \vspace{-2em}
            \begin{align*}
                \text{Per}(A_i) / \text{Per}(A_{i+1}) &= \gamma_1 \dots \gamma_n \\
                &= (1 + \delta_1) \dots (1 + \delta_n)\\
                &\leq \exp \left(\sum \delta_i - \sum \delta_i^2/2 + \sum \delta_i^3/3 \right)\\
                &\leq \exp(0 - \Delta/2 + \Delta^{3/2}/3)\\
                &= 1 - \Delta / 2 + O(\Delta^{3/2}).
            \end{align*}
        \end{itemize}
    \end{itemize}
\end{frame}


\begin{frame}

And now, back to our regularly scheduled programming

\[ \text{BL}(B,p) = \sup_{A_1,\dots,A_m \succ 0} \sqrt{ \frac{\prod_i \det(A_i)^{p_i}}{\det(\sum p_i \cdot B_i^* A_i B_i)}}. \]

\begin{itemize}
    \pause
    \item Goal: Rescale our inputs so that
    %
    \begin{itemize}
        \pause
        \item (Isotropy) $\sum p_i B_i^* B_i = I$.
        
        \pause
        \item (Projection) $B_i B_i^* = I$ for each $i$.
    \end{itemize}

\end{itemize}

\end{frame}


\begin{frame}
    \frametitle{Iteration of Operator Rescaling}

    \begin{itemize}
    \item Sinkhorn: Alternately apply the following two procedures:
    \begin{itemize}
        \pause
        \item (Isotropy Normalization)
        \begin{itemize}
            \pause
            \item Let $M = \sum_i p_i B_i^* B_i$.
            \pause
            \item Replace $B_i$ with $B_i' = B_i M^{-1/2}$.
            \pause
            \item Then $\sum p_i (B_i')^* B_i' = 1$, i.e. isotropy holds.
        \end{itemize}

        \pause
        \item (Projection Normalization)
        \begin{itemize}
            \pause
            \item Let $M_i = B_i B_i^*$.
            
            \pause
            \item Replace $B_i$ with $B_i' = M_i^{-1/2} B_i$.
            
            \pause
            \item Then $(B_i')^* B_i' = I$ for each $i$.
        \end{itemize}

        \pause
        \item We obtain a sequence $B \to B_1 \to B_2 \to \dots$.
    \end{itemize}
\end{itemize}

\end{frame}




\begin{frame}
    \frametitle{Iteration of Operator Rescaling}

    \begin{itemize}
        \item {\bf Three Key Facts Ensuring Convergence:}
        \begin{itemize}
            \pause
            \item[(1)] $\text{BL}(B_i,p) \geq 1$ for all $i \geq 1$.
            \pause
            \item[(2)]:

            \begin{itemize}
                \item If $1 + \varepsilon \leq \text{BL}(B_i,p) \leq 2$, $\text{BL}(B_{i+1},p) \leq (1 - C_1 \varepsilon^k) \text{BL}(B_i,p)$.

                \item If $\text{BL}(B_i,p) > 2$, then $\text{BL}(B_{i+1},p) \leq (1 - C_2) \text{BL}(B_i,p)$.
            \end{itemize}

            \pause
            \item[(3)] If isotropy or projection holds, and $\text{BL}(B_i,p) \leq 2$, then
            %
            \[ \| \sum p_i B_i^* B_i - I \| + \sum\nolimits_i \| B_i^* B_i - I \| \lesssim \log(\text{BL}(B,p)). \]
        \end{itemize}

        \pause
        \item Thus convergence to the family of geometric Brascamp-Lieb datum occurs as with Sinkhorn iteration provided that $\text{BL}(B,p) < \infty$.

        \pause
        \item Obtain (1), (2), and (3) by studying \emph{positive operators}.
    \end{itemize}

\end{frame}


\begin{frame}
    \frametitle{Another Viewpoint: Positive Operators}

    \begin{itemize}
        \item For simplicity, assume that all spaces have the same ambient dimension (all $B_i$ are square matrices). 

        \pause
        \item $\text{BL}(B,p) < \infty$ can only hold if $\sum p_i = 1$.

        \pause
        \item Also assume all $A_i$ are equal, and let us consider optimizing the quantity
        %
        \[ \inf_{A \succ 0} \frac{\det(\sum p_i B_i^* A B_i)}{\det(A)}  \]
        %
        analogous to
        %
        \[ \text{BL}(B,p) = \sup_{A_1,\dots,A_m \succ 0} \sqrt{ \frac{\prod_i \det(A_i)^{p_i}}{\det(\sum p_i \cdot B_i^* A_i B_i)}}. \]
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Positive Operators}

    \begin{itemize}
        \item A linear map $T: M_n \to M_m$ is \emph{completely positive} if there are $m \times n$ matrices $B_1,\dots,B_K$ such that
        %
        \[ T(A) = \sum B_i A B_i^*. \]
        %
        For simplicity, focus on the case $n = m$.

        \pause
        \item Important example: $T(A) = \sum p_i B_i^* A B_i$.

        \pause
        \item Given $T$, we have $T^*(A) = \sum B_i^* A B_i$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Capacity of Operators}

    \[ \inf_{A \succ 0} \frac{\det(\sum p_i B_i^* A B_i)}{\det(A)}  \]

    \begin{itemize}
        \pause
        \item The \emph{capacity} of a positive operator $T: M_n \to M_n$ is
        %
        \[ \text{Cap}(T) = \inf_{A \succ 0} \frac{\det(TA)}{\det(A)}. \]

        \pause
        \item Capacity introduced in (Gurvits, 2004) in $n = m$ case, in general introduced in (Garg et al, 2018).

        \pause
        \item For any Brascamp-Lieb data $(B,p)$, there exists a positive $T: M_n \to M_m$ such that $\text{Cap}(T) = 1/\text{BL}(B,p)^2$.

        \pause
        \item Positive operators are well studied in the quantum information theory literature, so reduction of BL to  this theory gives new insights.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Doubly Stochastic Positive Operators}

    \begin{itemize}
        \item (Isotropy) Let $T(A) = \sum p_i B_i^* A B_i$.
        \begin{itemize}
            \pause
            \item $\sum p_i B_i^* B_i = I$ holds iff $T(I) = I$.
        \end{itemize}

        \pause
        \item If $(B,p)$ is a Brascamp-Lieb datum with associated operator $T: M_n \to M_m$, then $(B,p)$ is geometric if and only if $T$ is \emph{doubly stochastic}. For $n = m$ this means $T(I) = I$ and $T^*(I) = I$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Operator Rescaling for Positive Operators}

    \begin{itemize}
        \item If $T$ is doubly stochastic, $\text{Cap}(T) = 1$.

        \pause
        \item We can rescale. If
        %
        \[ T_{M_1M_2}(A) = M_2^* T(M_1^* A M_1) M_2, \]
        %
        then $\text{Cap}(T_{M_1,M_2}) = \det(M_1)^2 \det(M_2)^2 \cdot \text{Cap}(T)$.

        \pause
        \item Sinkhorn iteration (alternately iterating $T \mapsto T_{I,T(I)^{-1/2}}$ and $T \mapsto T_{T^*(I)^{-1/2}, I}$) yields a method for rescaling any $T$ with $\text{Cap}(T) > 0$ to be arbitrarily close to a doubly stochastic operator, allowing us to approximate $\text{Cap}(T)$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Rank Decreasing Operators}

    \[ \int_{\RR^n} \prod_{i = 1}^m |f_i(B_i x)|^{p_i}\; dx \leq \text{BL}(B,p) \cdot \prod_{i=1}^m \| f_i \|_{L^1(\RR^{n_i})}^{p_i}.  \]

    \begin{itemize}
        \pause
        \item (Bennett et al, 2008) implies that $\text{BL}(B,p) < \infty$ if and only if $\sum p_i n_i = n$, and for any subspace $V \subset \RR^n$,
        %
        \[ \dim(V) \leq \sum p_i \dim(B_i V). \]

        \pause
        \item An operator $T: M_n \to M_n$ is \emph{rank non-decreasing} if for any $A \succeq 0$, $\text{Rank}(TA) \geq \text{Rank}(A)$.

        \pause
        \item (Gurvits, 2004) $T: M_n \to M_n$ is rank non-decreasing if and only if $\text{Cap}(T) > 0$.

        \pause
        \item Generalized in (Garg et al, 2018). For $T: M_n \to M_m$, $\text{Cap}(T) > 0$ if and only if $T$ is \emph{fractional rank non-decreasing}.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Proof Idea}

    (Gurvits, 2004) $T: M_n \to M_n$ is rank non-decreasing if and only if $\text{Cap}(T) > 0$.

    \begin{itemize}
        \pause
        \item Results from (Gurvits and Samorodnitsky, 2002) show the result is true if $T(X) = \sum X_{ii} A_i$, where $A_i \succeq 0$. The general case can be reduced to this case.

        \pause
        \item Given an orthonormal basis $U = \{ u_1, \dots, u_N \}$, we define the \emph{decoherence operator} $D_U(A) = \sum \langle Au_i, u_i \rangle \cdot u_i u_i^*$.

        \pause
        \item Define $T_U = D_U \circ T$.

        \pause
        \item Result follows from the following two facts:
        \begin{itemize}
            \pause
            \item[(1)] $T$ is rank non-decreasing if and only if $T_U$ is rank non-decreasing for all $U$.
            
            \pause
            \item[(2)] $\text{Cap}(T) = \inf_U \text{Cap}(T_U)$.
        \end{itemize}

        \pause
        \item To prove (1) and (2), use a simple trick: Given $A \succeq 0$, find $U$ diagonalizing $T(A)$. Then $T(A) = T_U(A)$.
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Thanks For Listening!}
\end{frame}

\end{document}