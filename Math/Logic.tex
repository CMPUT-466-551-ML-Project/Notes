\input{../style.tex}

\title{Metamathematics}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}
\maketitle
\tableofcontents

\part{Mathematical Logic}

\chapter{What's Logic All About?}

\pagenumbering{arabic}

Mathematics is the use of mathematical proof to determine truth from observation. Metamathematics is the use of mathematical proof to determine the truths of mathematics; mathematical procedures are scrutinized to determine their correctness and limitations. Though studied since the time of Aristotle, `hard' metamathematics arose in response to paradoxes in early 20th century mathematics, which counteract fundamental notions of definition and inference. Each individual step of the paradoxes is intuitive, yet the conclusion is ludicrous.

\begin{example}[Cantor]
    For any set $X$, Cantor proved that there is no injective function from $\mathcal{P}(X)$ to $X$. When we take $X$ to be the set of all things, then $\mathcal{P}(X) \subset X$, so injecting $\mathcal{P}(X)$ into $X$ is simple.
\end{example}

\begin{example}[Burali-Forti]
    Consider the set $\Omega$ of all ordinals. Then $\Omega$ is a well-ordered set, and therefore order isomorphic to some element of itself, and thus to a proper segment of itself. But no well-ordered set is order isomorphic to a proper subset of itself.
\end{example}

\begin{example}[Russell]
    Consider the following set.
    %
    \[ X = \{ x: x \not \in x \} \]
    %
    Russell's paradox rests on an innocent question: Is $X$ in itself? If $X$ was in $X$, then by definition, we would find that $X$ cannot be contained in itself. So we are lead to believe that $X \not \in X$. But then, by construction of $X$, we conclude $X$ is in $X$ after all! More colloquially, consider a town with a single barber, shaving everyone who does not shave themselves. Does the barber shave himself?
\end{example}

Set theory was the first major source of paradoxes, since it was the first mathematical language powerful enough to encapsulate subjects in a single mathematical object. Some considered these arguments the doom of set theory, a ``plague on mathematics'' to be cleansed from the field. But logical paradoxes were not limited to set theory. In their work on the Continuum hypothesis, Julius K\"{o}nig and Jules Richard found paradoxes attacking the notion of mathematical definability, the ability to specify some mathematical object by english phrases.

\begin{example}[Richard]
    There are countably many english expressions, yet uncountably many real numbers. We conclude that some real numbers cannot be described in english. Consider a particular enumeration containing all describable real numbers. From the enumeration, construct the expression
    %
    \begin{center}
        ``The number constructed from Cantor's diagonal argument on the enumeration considered.''
    \end{center}
    %
    This number is described in english, though not considered in the enumeration.
\end{example}

\begin{example}[K\"{o}nig]
    There are some real numbers which cannot be describable in english. If the real numbers could be well-ordered, we could consider the expression,
    %
    \begin{center}
        ``The least number not definable in english.''
    \end{center}
    %
    And this number is definable, a contradiction.
\end{example}

Analysis of definability was key to the discovery of G\"{o}del's incompleteness theorem, and Tarski's undefinability theorem. Related to definability are self-referential statements.

\begin{example}[L\"{o}b]
    Consider the Preposition $B$, defined to be true when $B \implies A$ is true. If $B$ is true, then $B \implies A$ is true, so $A$ is true. But then $B \implies A$ is true by construction, so $B$ is true, and we conclude $A$ is true. Since $A$ was arbitrary, so we conclude that every logical statement is true!
\end{example}

\begin{example}[Curry]
     Consider
    %
    \[ C = \{ x : (x \in x) \implies P(x) \} \]
    %
    Then $(C \in C)$ holds if and only if $(C \in C) \implies P(C)$ holds. We must have $C \in C$, for if $C \not \in C$, then $(C \in C) \implies P(C)$ holds vacantly. But this implies $(C \in C) \implies P(C)$, so $P(C)$ is true. We conclude $P(C)$ is true, irrespective of the content of the statement $P(C)$.
\end{example}

The classical diagnosis of the paradoxes is that knowledge of traditional logic must be sharpened; metamathematics provides the tools for such an endeavour. Which logical procedures summarize the construction of paradoxes? What standards do we use to define a mathematical statement? Our only hope is to apply the precise weapon of mathematical rigor. In the face of adversity, we do what mathematicians do best -- define and conquer.

\section{Formal Systems}

A real life situation can never be modelled completely accurately. A physicists's models are ideals, carved from reality in all senses but experimental parameters. No model describes a systems evolution exactly, and it is myopic to suggest a model's perfection. In spite of this, physics still does a bloody good job! In metamathematics, we attempt to form a mathematical model of mathematical principles. Some principles are pinned down for examination, others lost. We hope this model has enough vitality to provide key insights into real-life mathematics. Whether the method is successful it up to interpretation.

A source of confusion in physics is the stylistic treatment of assumptions as absolute facts. A physicist describes ``a planet moving according to the equation $\ddot{x} = -m/x^2$'' even if he is actually talking about the dynamical system whose evolution is described by the differential equation $\ddot{x} = -m/x^2$, which {\it models} the motion of a planet. Such expressions are unavoidable, since they are much more viceral, the formal equivalent dry to the bone. Keep this principle in mind as we begin to build models of logic.

The method of the mathematician proceeds as follows. First, she accepts some fundamental statements found by observation, known as axioms. Using accepted logical derivations, additional statements are derived. We shall construct formal models guided by this methodology. We shall focus first on modelling mathematical sentences, and then on modelling how other sentences are derived from each setence. By studying sets of these statements, we attempt to further our knowledge of the mathematical method. These models are known as formal systems.

The basic object of study in metamathematics is deceptively simple. We take a set $\Lambda$, known as an alphabet, and consider {\bf strings} over that alphabet, finite (possibly empty) sequences with elements in $\Lambda$. The set of all such strings over $\Lambda$ will be denoted $\Lambda^*$. The empty string is denoted $\varepsilon$. A string $(v_1, \dots, v_n)$ is often denoted $v_1 v_2 \dots v_n$; often the choice of alphabet will lead to no confusion. We shall identify an element in $\Lambda$ with the corresponding one letter string in $\Lambda^*$. The concatenation of two strings $s = s_1 \dots s_n$ and $w = w_1 \dots w_m$, denoted $sw$, is the string $s_1 \dots s_n w_1 \dots w_m$. A {\bf substring} of a string $s$ is a string $u$ such that $s = wuv$, for some strings $w$ and $v$. Substrings are consequtive subsequences of characters in $s$. If we view concatenation as an associative algebraic operation, then $\Lambda^*$ is the smallest {\it monoid} containing $\Lambda$, relative to the operation of concatenation. A {\bf language} is a subset of strings over an alphabet. We shall also want to perform {\bf substitution}. If $s \in \Lambda^*$, $x = (x_1, \dots, x_n)$ are distinct letters of $\Lambda$, and $w = (w_1, \dots, w_n) \in \Lambda^*$, then we shall let $s[w_1/x_1, \dots, w_n/x_n] = s[w/x]$ denote the string in $\Lambda^*$ obtained from swapping the $x_i$ with $w_i$.

In mathematical logic, we model certain mathematical statements as a language over an alphabet, and attempt to formalize the mathematical method via operations on these strings. This notion is encapsulated in what is called a {\bf formal system}. The most general definition consists of a language $L$, a set of axioms, which form a subset of $L$, and a set of inference rules, pairs $(\Gamma, s)$ where $\Gamma \subset L$ is the premises of the inference, and $s$ is the conclusion. The {\bf theorems} of a formal system are members of the smallest set such that
%
\begin{enumerate}
    \item every axiom is a theorem
    \item if $(\Gamma, s)$ is an inference rule, and all elements of $\Gamma$ are theorems, then $s$ is a theorem.
\end{enumerate}
%
If $F$ is a formal system, we shall let $\vdash_F s$ state that $s$ is a theorem of $F$.

The whole idea of metamathematics may seem circular. This is the reason for formality. By simulating mathematical understanding in a mathematical model, formal mathematics becomes a second order object. Proofs about this system rely on intuition, whereas proofs in the system have static, precise formulations. In the conversion mathematics becomes stale, abstract, and obtuse. Such is the price to pay for rigorous understanding.



\chapter{Prepositional Logic}

We shall begin with propositional logic, the simplest formal system to analyze truth. To understand propositional logic, we construct a mathematical model, known as a formal language, which represents the language in which mathematics is performed. The formal language is then analyzed by common mathematical notion. The standard formal language for logic is an analysis of strings, sequences of abstract symbols from a given alphabet. Strings represent mathematical statements; manipulating these strings models how a mathematician infers some mathematical statement from another. It is best to see the tool in action to understand its utility, so we proceed swiftly.

\section{Syntax}

Each abstract symbol in an alphabet represents a precise form in colloquial speech. We begin with prepositional logic, which analyzes basic notions of truth and falsity. Some statements are {\bf atomic}, because they cannot be divided into more base statements. ``Socrates is a man'' is an atomic statement, as is ``every woman is human''. ``Socrates is a man and every woman is a human'' is not atomic, for the statement consists of two separate statements, composed by the connective ``and''. In English, ``every woman is a human'' can be broken into statements such as ``Julie is a human'' and ``Laura is a human'', yet propositional logic still considers this statement as atomic; the model does not have the capability to precisely model understanding of these complex statements, which are the realm of predicate logic. We shall discuss predicate logic in the next chapter.

\begin{definition}
    Let $\Lambda$ be a set disjoint from $\{ (, ), \wedge, \vee, \neg, \Rightarrow, \Leftrightarrow \}$. The {\bf propositional language with atoms in $\Lambda$}, denoted $\text{SL}(\Lambda)$, is the smallest subset of
    %
    \[ (\Lambda \cup \{ (, ), \wedge, \vee, \neg, \Rightarrow, \Leftrightarrow \})^* \]
    %
    such that
    %
    \begin{enumerate}
        \item $\Lambda \subset \text{SL}(\Lambda)$.
        \item If $s, w \in \text{SL}(\Lambda)$, then $(\neg s), (s \wedge w), (s \vee w), (s \Rightarrow w), (s \Leftrightarrow w) \in \text{SL}(\Lambda)$.
    \end{enumerate}
    %
    An element of $\text{SL}(\Lambda)$ is called a {\bf formula} or {\bf statement}.
\end{definition}

Each {\bf connective} represents a certain linguistical form. Later on, the connection of symbols to meaning will become clear. For now, they are abstract symbols without intrinsic meaning.
%
\begin{center}
\begin{tabular}{| c | c | c |}
    \hline Connective & Name of Connective & Meaning of statement \\
    \hline $\neg s$ & Negation & ``$s$ is {\it not} true''\\
    $s \wedge w$ & Conjuction & ``$s$ {\it and} $w$ is true''\\
    $s \vee w$ & Disjunction & ``$s$ {\it or} $w$ is true''\\
    $s \Rightarrow w$ & Implication & ``{\it If} $s$ is true, {\it then} $w$ is true''\\
    $s \Leftrightarrow w$ & Bicondition & ``$s$ is true, {\it if, and only if,} $w$ is true''\\
    \hline
\end{tabular}
\end{center}

Take care to notice that $\text{SL}(\Lambda)$ is the {\it smallest} set constructed, in the same way that most `smallest objects' exist in mathematics, because the intersection of sets satisfying the set of statements defining the set also satisfy the statements. This property leads to the most useful proof method in logic.

\begin{theorem}[Structural Induction]
    Consider a proposition that can be applied to $\text{SL}(\Lambda)$. Suppose the proposition is true of all elements of $\Lambda$, and that if the proposition is true of $s$ and $w$, then the proposition is also true of $(\neg s), (s \wedge w), (s \vee w), (s \Rightarrow w)$, and $(s \Leftrightarrow w)$. Then the proposition is true for all of $\text{SL}(\Lambda)$.
\end{theorem}
\begin{proof}
    Let $P$ be some property to consider. Note the set
    %
    \[ K = \{ s \in (\Lambda \cup \{ (, ), \wedge, \vee, \neg, \Rightarrow, \Leftrightarrow \})^* : P(s)\ \text{is true} \} \]
    %
    satisfies axioms (1) and (2) which define $\text{SL}(\Lambda)$, so $\text{SL}(\Lambda) \subset K$.
\end{proof}

Consider the following application.

\begin{theorem}
    Any sentence in $\text{SL}(\Lambda)$ contains as many left as right brackets.
\end{theorem}
\begin{proof}
    Any atom in $\Lambda$ contains no left brackets, and no right brackets, and thus the same number of each. Let $s$ have $n$ pairs of brackets, and let $w$ have $m$. Then $(\neg s)$ contains $n + 1$ pairs of brackets, and $(s \circ w)$, where $\circ \in \{ \wedge, \vee, \neg, \Rightarrow, \Leftrightarrow \}$, contains $n + m + 1$ brackets. By structural induction, we have proved our claim.
\end{proof}

We need a more in depth theorem to correctly parse statements. If statements can be parsed in two different ways, they become ambiguous. For instance, what is the value of $2 + 7 - 5 - 4$? Is it, by collecting factors in pairs, $9 - 1 = 8$, or $9 - 9 = 0$. This is the reason for parenthesis. One splits mathematical logic into syntax, studying strings, and semantics, interpreting strings. We are studying syntax to begin with, so that we may begin semantics. Equations must be understood before we calculate with them.

\begin{theorem}
    If $wu = s \in \text{SL}(\Lambda)$, where $w \neq \varepsilon$, then $w$ has at least as many left brackets as right brackets, and $w = s$ if and only if $w$ has the same number of left and right brackets.
\end{theorem}
\begin{proof}
    If $s \in \Lambda$, then $s$ is only one letter long, so $s = w$, and has no brackets. Now let $s$ and $s'$ satisfy the theorem. We split our proof into two cases.
    %
    \begin{enumerate}
        \item $wu = (\neg s)$: March through all cases. Suppose $w$ has the same number of left brackets than right. $w$ cannot equal $($ or $(\neg$, nor $(\neg v$, where $v$ is a prefix of $s$; by induction, $v$ has at least as many left brackets as right brackets, and then $w$ has more left brackets than right brackets. Thus $w$ must equal $(\neg s)$.
        \item $wu = (s \circ s')$: Continue the string march. $w$ cannot equal $($, nor $(v$ by induction. Similarily, $w$ cannot equal $(s \circ$, nor $(s \circ v$, where $v$ is a substring of $s'$, so $w$ must equal $(s \circ s')$.
    \end{enumerate}
    %
    Careful analysis of each case also shows that $w$ must have at least as many left brackets as right brackets.
\end{proof}

\begin{corollary}
    Every string in $\text{SL}(\Lambda)$ can be written uniquely as an atom $\Lambda$, or $(\neg s)$ and $(s \circ w)$, where $s$ and $w$ are elements of $\text{SL}(\Lambda)$. The unique connective in the representative is known as the {\bf principal connective} of the statement.
\end{corollary}
\begin{proof}
    Such representations trivially exist. Suppose we have two representations. If one of the representations is an element of $\Lambda$, the other representation must have a string of length one, and is therefore equal. If we have two representations $(\neg s) = (\neg w)$, then by chopping off symbols, $s = w$. It is impossible to have two distinct representations $(s \circ w) = (\neg u)$, for no element of $\text{SL}(\Lambda)$ begins with $\neg$. Finally, suppose we have two representations $(s \circ w) = (u \circ v)$. Then either $u$ is a substring of $s$, or $s$ is a substring of $u$, and one is the prefix of the other. But both have balanced brackets, which implies $s = u$, and by chopping letters away, $w = v$. Thus representations are unique.
\end{proof}

The corollary above allows us to construct recursive definitions. Let $\Lambda = \{ A, B, C \}$, and $\Gamma = \{ X, Y, Z \}$. We would like to consider $\text{SL}(\Lambda)$ and $\text{SL}(\Gamma)$ the same, by considering a natural bijection. We extend the map
%
\[ X \mapsto A\ \ \ \ \ Y \mapsto B\ \ \ \ \ Z \mapsto C \]
%
by the definition
%
\[ f((s \circ w)) = (f(s) \circ f(w))\ \ \ \ \ f((\neg s)) = (\neg f(s)) \]
%
Such a map is well defined and injective, by the corollary.

Before we finish with syntax, it is interesting to discuss a less natural, but syntactically simpler method of forming sentences, called {\bf polish notation}, after its inventor Jan \L ukasiewicz. Rather than writing connectives in {\bf infix notation}, like $(u \wedge v)$ and $(u \Rightarrow v)$, we use {\bf prefix notation}, like $\wedge u v$ and $\Rightarrow u v$. We do not need brackets to parse statements anymore, but it is much more easy to read the statement $``((a \Rightarrow (b \vee c) \wedge d)''$, than $``\wedge \Rightarrow a \vee b c d''$.

\section{Semantics}

We can understand the discussion in the last section without any understanding of what symbols mean. Now we want to interpret the symbols, giving the symbols meaning. A basic semantic method is to define whether a statement is `true'. A {\bf truth assignment} on a set $\Lambda$ is a map $f: \Lambda \to \{ \top, \bot \}$.

\begin{example}
    An $n$-ary {\bf boolean function} is a truth assignment, where $\Lambda = \{ \top, \bot \}^n$. It is common to define such functions by truth tables. For $n$-ary boolean functions, we form a table with $n + 1$ columns, and $2^{n}$ rows. In each row, we fill in a particular truth assignment in $\{ \top, \bot \}^n$, and in the last column, the value of image of the truth assignment under $f$. One may combine multiple $n$-ary truth functions into the same table for brevity. There are $2^{2^{n}}$ $n$-ary truth functions.
    %
    \begin{center}
    \begin{tabular}{| c | c | c | c | c | c | c |}
        \hline $x$ & $y$ & $H_\wedge(x,y)$ & $H_\vee(x,y)$ & $H_\Rightarrow(x,y)$ & $H_\Leftrightarrow(x,y)$ & $H_\neg(x)$ \\
        \hline $\bot$ & $\bot$ & $\bot$ & $\bot$ & $\top$ & $\top$ & $\top$ \\
        $\bot$ & $\top$ & $\bot$ & $\top$ & $\top$ & $\bot$ & $\top$ \\
        $\top$ & $\bot$ & $\bot$ & $\top$ & $\bot$ & $\bot$ & $\bot$ \\
        $\top$ & $\top$ & $\top$ & $\top$ & $\top$ & $\top$ & $\bot$ \\
        \hline
    \end{tabular}
    \end{center}
    %
    This table defines the boolean functions $H_\circ$.
\end{example}

Homomorphisms between two rings $R$ and $S$, extend to homomorphisms between the polynomial rings $R[X]$ and $S[X]$. Similarily, we may extend truth assignments $f$ on $\Lambda$ to assignments $f_*$ on $\text{SL}(\Lambda)$. This is done recursively. Let $f$ be an arbitrary truth assignment. Define
%
\[ f_*(\neg s) = H_\neg(f_*(s))\ \ \ \ \ \ \ \ \ \ f_*(s \circ w) = H_\circ(f_*(s), f_*(w)) \]
%
then $f_*$ is defined on all of $\text{SL}(\Lambda)$.

Let $s \in \text{SL}(\Lambda)$ be a statement. $s$ is a {\bf tautology}, if, for any truth assignment $f$ on $\Lambda$, $f_*(s) = \top$, a {\bf contadiction} if $f_*(s) = \bot$, a {\bf cotingent statement} if $s$ is neither a contradiction nor a tautology, and {\bf satisfiable} if it is not a contradiction. We summarize the statement ``$s$ is a tautology'' by $\vDash s$. We say a statement $s$ {\bf semantically implies} $w$, written $s \vDash w$, if $\vDash s \Rightarrow w$, or correspondingly, if $f_*(w) = \top$ whenever $f_*(s) = \top$.

Suppose that we wish to verify whether $s \in \text{SL}(\Lambda)$ is a tautology. We say a variable $x \in \Lambda$ {\bf occurs} in $s$ if it is an element of the string. There can only be a finite number of variables $x_1, \dots, x_n$ which occur in $s$. Define a boolean function $g: \{ 0, 1 \}^n \to \{ 0, 1 \}$,
%
\[ g(y_1 \dots, y_n) = f^{(y_1, \dots, y_n)}_*(s) \]
%
where $f^{(y_1, \dots, y_n)}$ is the truth assignment formed by mapping $x_i$ to $y_i$. This is well defined, because if $h$ and $k$ are two truth assignments which agree on the $x_i$, then they agree at $s$. Then one verifies that $s$ is a tautology if and only if $g(y_1, \dots, y_n) = \top$ for all choices of the $y_i$. Thus one can determine whether a statement is a tautology by constructing a truth table. To prevent errors, it is best to construct a truth table for the subformulas in the principal connective of a formula before constructing the truth table of the formula. This may be done side by side, in the same table.

\begin{example}
    For instance, for any variable $A \in \Lambda$, $A \vee \neg A$ is a tautology, $A \wedge \neg A$ is a contradiction, and $\neg A$ is contingent, which is verified by the truth table
    %
    \begin{center}
    \begin{tabular}{| c | c | c | c | c | c | c |}
        \hline $A$ & $A \vee \neg A$ & $A \wedge \neg A$ & $\neg A$ \\
        \hline $\top$ & $\top$ & $\bot$ & $\bot$ \\
               $\bot$ & $\top$ & $\bot$ & $\top$ \\
        \hline
    \end{tabular}
    \end{center}
    %
    The first formula is the {\bf law of excluded middle}.
\end{example}

\begin{example}
    Let $A \vDash B$ be a tautology, and suppose that $A$ and $B$ have no variables in common. Then $A$ is a contradiction, or $B$ is a tautology, for if there is a truth assignment on the variables of $A$ which make the statement true, and a truth assignment on $B$ which cause the statement to be false, then we may combine the truth assignments to create a truth assignment in which $A \Rightarrow B$ is false.
\end{example}

\begin{theorem}[Semantic Modus Ponens]
    If $\vDash s$ and $s \vDash w$, then $\vDash w$.
\end{theorem}
\begin{proof}
    Let $f$ be a truth assignment. Then $f_*(s) = \top$ and
    %
    \[ f_*(s \Rightarrow w) = H_\Rightarrow(f_*(s),f_*(w)) = H_\Rightarrow(\top, f_*(w)) = \top \]
    %
    This holds only when $f_*(w) = \top$.
\end{proof}

\begin{theorem}
    If $\vDash w$, then $\vDash w[s/x]$
\end{theorem}
\begin{proof}
    Consider a truth assignment $f$. We shall define another truth assignment $\tilde{f}$ such that $f(v[s/x]) = \tilde{f}(v)$ for all $v$. Define $\tilde{f}(x_i) = f(s_i)$, and if $y \not \in x$, define $\tilde{f}(y) = f(y)$. Our base case, where $v$ is a variable, satisfies the claim by construction. Then, by induction, if $v = (u \circ w)$, then
    %
    \[ \tilde{f}_*(v) = H_\circ(\tilde{f}_*(u), \tilde{f}_*(w)) = H_\circ(f_*(u[s/x]), f_*(w[s/x])) = f_*(v[s/x]) \]
    %
    A similar proof answers the case where $u = (\neg v)$. Since $w$ is a tautology,
    %
    \[ f_*(w[s/x]) = \tilde{f}_*(w) = \top \]
    %
    So $w[s/x]$ is a tautology.
\end{proof}

\begin{corollary}
    If $v \vDash w$, then $v[s/x] \vDash w[s/x]$.
\end{corollary}

\begin{theorem}[Craig Interpolation]
    Let $A \vDash B$, and let the set of variables shared by $A$ and $B$ be $x_1, \dots, x_n$. Then there is a statement $C$, known as the {\bf interpolant}, containing only the variables $x_i$, such that $A \vDash C$ and $C \vDash B$.
\end{theorem}
\begin{proof}
    We proceed by induction on the number of variables in $A$ which do not occur in $B$. If every variable in $A$ occurs in $B$, let $C = A$. In the general case, fix some variable $x$ in $A$ but not in $B$, take $y$ in both $A$ and $B$, and define
    %
    \[ D = A[(y \wedge \neg y)/x] \vee A[(y \vee \neg y)/x] \]
    %
    If $f_*(A) = \top$, and $f_*(x) = \bot$, then $f_*(A[(y \wedge \neg y)/x]) = \top$. If $f_*(x) = \top$, then $f_*(A[(y \vee \neg y)/x]) = \top$, so $A \vDash D$. In addition, $D \vDash B$. Let $f_*(D) = \top$. Then $f_*(A[(y \wedge \neg y)/x]) = \top$, or $f_*(A[(y \vee \neg y)/x]) = \top$. In the first case, we modify the truth assignment $f$ so that $f_*(x) = \bot$ (without changing the values of $B$ or $D$). Then $f_*(A) = \top$, so $f_*(B) = \top$. The other case follows by letting $f_*(x) = \top$. By induction, there is an iterpolant $C$ for which $D \vDash C$ and $C \vDash B$, and then $A \vDash C$, since $(x \Rightarrow y) \wedge (y \Rightarrow z) \vDash x \Rightarrow z$.
\end{proof}

\begin{example}
    Consider $A = (y_1 \Rightarrow x) \wedge (x \Rightarrow y_2)$ and $B = (y_1 \wedge z) \Rightarrow (y_2 \wedge z)$. Then $A \vDash B$. We shall find an interpolant.
    %
    \begin{align*}
        &C = [(y_1 \Rightarrow (y_1 \vee \neg y_1)) \wedge ((y_1 \vee \neg y_1) \Rightarrow y_2)]\\
        &\ \ \ \ \ \ \vee [(y_1 \Rightarrow (y_1 \wedge \neg y_1)) \wedge ((y_1 \wedge \neg y_1) \Rightarrow y_2)]
    \end{align*}
    %
    This equation can be simplified to $\neg y_1 \vee y_2$.
\end{example}

\section{Truth Functional Completeness}

We hope that propositional logic can model all notions of truth, such that all truth functions can be formed from our original set. Here we argue why our logic can model all such notions. Let $\Lambda$ be a set of boolean functions. The {\bf clone} of $\Lambda$ is the smallest set containing $\Lambda$ and all projections $\pi^n_k : \{ 0, 1 \}^n \to \{ 0, 1 \}$, defined
%
\[ \pi^n_k(x_1, \dots, x_n) = x_k \]
%
and in addition, if $g: \{ 0, 1 \}^n \to \{ 0, 1 \}$ and $f_1, \dots, f_n : \{ 0, 1 \}^m \to \{ 0, 1 \}$ are in the clone, then so is $g(f_1, \dots, f_n): \{ 0, 1 \}^m \to \{ 0, 1 \}$. $\Lambda$ is {\bf truth functionally complete} if its clone is the set of all boolean functions.

\begin{example}
    $\{ H_\neg, H_\wedge, H_\vee \}$ is a truth functionally complete, since every formula can be put in {\bf conjunctive normal form}, for we may write the `true and false' functions
    %
    \[ \top(x) = \top = H_\vee(H_\neg(x), x)\ \ \ \ \ \bot(x) = \bot = H_\wedge(H_\neg(x), x) \]
    %
    and then we write
    %
    \[ f(x_1, \dots, x_n) = \bigvee_{\substack{(y_1, \dots, y_n) \in \{ 0, 1 \}^n\\f(y_1, \dots, y_n) = \top}}\ \  \bigwedge_{i = 1}^n H_\Leftrightarrow(x_i, y_i) \]
    %
    where we identify an element of $\{ \top, \bot \}$ with the constant boolean function, define $\bigcirc_{i = 1}^n f_i = H_\circ(f_n, \bigcirc_{i = 1}^{n-1})$, and take
    %
    \[ H_\Leftrightarrow(x,y) = H_\wedge(H_\Rightarrow(x,y), H_\Rightarrow(y,x)) = H_\wedge(H_\vee(H_\neg(x), y), H_\vee(H_\neg(y), x)) \]
    %
    This can be further reduced, by noticing that
    %
    \[ H_\wedge(x,y) = H_\neg(H_\vee(H_\neg(x), H_\neg(y))) \]
    %
    which is a truth functional form of Boole's inequality, implying $\{ \neg, \vee \}$ is truth functionally complete. We can also consider {\bf disjunctive normal form}, left to the reader to define.
\end{example}

\begin{example}
    The mathematician Henry M. Sheffer found a single truth function which is truth functionally complete. Consider the {\bf Sheffer stroke} $x|y$, also known as {\bf NAND}, defined by the truth table
    %
    \begin{center}
    \begin{tabular}{| c | c | c |}
        \hline $x$ & $y$ & $H_|(x,y)$\\
        \hline $\bot$ & $\bot$ & $\top$\\
        $\bot$ & $\top$ & $\top$\\
        $\top$ & $\bot$ & $\top$\\
        $\top$ & $\top$ & $\bot$\\
        \hline
    \end{tabular}
    \end{center}
    %
    Then $H_\neg(x) = H_|(x,x)$, and $H_\vee(x,y) = H_|(H_\neg(x), H_\neg(y))$, which implies, since this set is truth functionally complete, that the sheffer stroke is truth functionally complete.
\end{example}

The previous example is incredibly important to circuit design. Logical statements can be represented by boolean functions. Since all truth functions can be built from the sheffer stroke, we need only make an atomic circuit for the sheffer stroke, and then all other circuits are constructed by combining sheffer strokes. This is why computers consist of millions of NAND gates.




\section{Deduction}

When mathematicians want to derive whether a statement is true, they do not construct truth functions. Instead, they argue {\it why} the statement is true. Here we shall provide the mechanics for modelling a mathematical argument. We will show that truth tables and arguments are equivalent -- a statement is a tautology if and only if it can be proved.

The set of {\bf theorems} of $\text{SL}(\Lambda)$ are elements of the smallest set such that for any $s,w,u \in \text{SL}(\Lambda)$,
    %
    \begin{enumerate}
        \item Any axiom is a theorem, where the axioms are any statement of the form
        %
        \begin{align*}
            &(A1) & s \Rightarrow (w \Rightarrow s)\\
            &(A2) & (s \Rightarrow (w \Rightarrow u)) \Rightarrow ((s \Rightarrow w) \Rightarrow (s \Rightarrow u))\\
            &(A3) & (\neg s \Rightarrow \neg w) \Rightarrow ((\neg s \Rightarrow w) \Rightarrow s)
        \end{align*}
        %
        where $s,w$ are arbitrary statements.
        \item Modus Ponens holds in our system. If $s \Rightarrow w$ and $s$ are theorems, then $w$ is a theorem. When we apply modus ponens, we may write that the theorem was obtained by (MP).
    \end{enumerate}
    %
We shall write $\vdash s$ to state that $s$ is a theorem.

For statements, the `smallness' characterization gives us structural induction. For theorems, smallness gives us the notion of a `proof'. A statement $s$ is a theorem of $\text{SL}(\Lambda)$ if and only if there is a sequence of formulae $(s_1, \dots, s_n)$ such that $s_n = s$, and each $s_i$ is either an axiom, or is obtained from some $s_j$ and $s_k$ by modus ponens, where $j,k < i$. This sequence is known as a proof. Often, we list a proof from top to bottom, where we reference how we obtained each element of the sequence alongside the proof.

\begin{example}
    Let us construct a proof of $\vdash s \Rightarrow s$, for any $s \in \text{SL}(\Lambda)$.
    %
    \[ \fitchprf{\pline{s \Rightarrow s}}{
        \pline[1.]{(s \Rightarrow ((s \Rightarrow s) \Rightarrow s))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (A1)]\\
        \pline[2.]{(s \Rightarrow ((s \Rightarrow s) \Rightarrow s)) \Rightarrow ((s \Rightarrow (s \Rightarrow s)) \Rightarrow (s \Rightarrow s))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (A2)]\\
        \pline[3.]{((s \Rightarrow (s \Rightarrow s)) \Rightarrow (s \Rightarrow s))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1),(2),(MP)]\\
        \pline[4.]{(s \Rightarrow (s \Rightarrow s))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (A1)]\\
        \pline[5.]{s \Rightarrow s}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3),(4),(MP)]
    } \]
    %
    In future proofs, we shall be able to use $\vdash s \Rightarrow s$ implicitly, since we now know the statement can be proved in any of its forms. We will denote its application by $(I)$.
\end{example}

In mathematics, we work in systems where implicit assumptions are made. In group theory, we assume that operations are associative. In geometry, we assume there is a line between any two points. To perform mathematics, we add additional axioms to logic, and prove results from these axioms. If $\Gamma$ is a subset of $\text{SL}(\Lambda)$, then we may consider each member of $\Gamma$ to be an axiom. We write $\Gamma \vdash s$ if one can prove $s$ assuming all elements of $\Gamma$ are axioms. That is, we may write a sequence $(s_1, \dots, s_n)$, where $s_n = s$, and each $s_i$ is either an axiom, an element of $\Gamma$, or is obtained by modus ponens from previous elements of the sequence.

\begin{theorem}[Deduction Theorem]
    If $\Gamma \cup \{ s \} \vdash w$, then $\Gamma \vdash s \Rightarrow w$.
\end{theorem}
\begin{proof}
    We prove the theorem by induction of the size of the proof of $w$. Consider a particular proof $(s_1, \dots, s_n)$ of $w$ from $\Gamma \cup \{ s \}$. Suppose that $n = 1$. Then $s_1 = w$, and $w$ must either be an axiom, an element of $\Gamma$, or equal to $s$. In the first and second case, the proof is equally valid in $\Gamma$, and so $\Gamma \vdash s \Rightarrow w$ follows from the axiom $(w \Rightarrow (s \Rightarrow w))$. If $s = w$, Then we have shown that $\Rightarrow w \Rightarrow w$, so obviously $\Gamma \vdash s \Rightarrow w$.

    Now we consider the problem proved for $m < n$. $s_n = w$ is either an axiom, an element of $\Gamma$, equal to $s$, or proved by modus ponens from $s_i = (u \Rightarrow w)$ and $s_j = u$. We have already shown all but the last case. By induction, $\Gamma \vdash s \Rightarrow (u \Rightarrow w)$ and $\Gamma \vdash s \Rightarrow u$. But
    %
    \[ (s \Rightarrow (u \Rightarrow w)) \Rightarrow ((s \Rightarrow u) \Rightarrow (s \Rightarrow w)) \]
    %
    is an axiom, so $\Gamma \vdash s \Rightarrow w$.
\end{proof}

\begin{example}
    For any statements $s$ and $w$, $\{ s \Rightarrow w, w \Rightarrow u \} \vdash s \Rightarrow u$. This follows from a basic application of (A2). But this implies the two cut rules, that
    %
    \begin{gather*}
        \vdash (s \Rightarrow w) \Rightarrow ((w \Rightarrow u) \Rightarrow (s \Rightarrow u))\\
        \vdash (w \Rightarrow u) \Rightarrow ((s \Rightarrow w) \Rightarrow (s \Rightarrow u))
    \end{gather*}
    %
    which is much more tricky to prove.
\end{example}

\begin{example}
    Let us prove the double negation elimination axiom, $\vdash \neg \neg s \Rightarrow s$ by the deduction theorem.
    %
    \[
    \fitchprf{\pline{\neg \neg s \Rightarrow s}}{
        \subproof{\pline[1.]{\neg \neg s}} {
            \pline[2.]{((\neg s \Rightarrow \neg \neg s)) \Rightarrow ((\neg s \Rightarrow \neg s) \Rightarrow s)}[(A3)]\\
            \pline[3.]{(\neg \neg s) \Rightarrow (\neg s \Rightarrow \neg \neg s)}[(A1)]\\
            \pline[4.]{(\neg s \Rightarrow \neg \neg s)}[(1),(3),(MP)]\\
            \pline[5.]{(\neg s \Rightarrow \neg s) \Rightarrow s}[(2),(4),(MP)]\\
            \pline[6.]{\neg s \Rightarrow \neg s}[(I)]\\
            \pline[7.]{s}[(5),(6),(MP)]
        }
        \pline[8.]{\neg \neg s \Rightarrow s}[(1-7),(DT)]
    }
    \]
    %
    In future proofs, application of the statement will be denoted $(\neg \neg E)$. Now lets prove negation introduction, $\vdash s \Rightarrow \neg \neg s$.
    %
    \[
    \fitchprf{\pline{s \Rightarrow \neg \neg s}}{
        \pline[1.]{\neg \neg \neg s \Rightarrow \neg s}[\ \ \ \ \ \ \ $(\neg \neg E)$]\\
        \subproof{\pline[2.]{s}} {
            \pline[3.]{(\neg \neg \neg s \Rightarrow \neg s) \Rightarrow ((\neg \neg \neg s \Rightarrow s) \Rightarrow \neg \neg s)}[\ \ \ \ \ \ \ (A3)]\\
            \pline[4.]{(\neg \neg \neg s \Rightarrow s) \Rightarrow \neg \neg s}[\ \ \ \ \ \ \ (1), (3), (MP)]\\
            \pline[5.]{s \Rightarrow (\neg \neg \neg s \Rightarrow s)}[\ \ \ \ \ \ \ (A1)]\\
            \pline[6.]{\neg \neg \neg s \Rightarrow s}[\ \ \ \ \ \ \ (2),(5),(MP)]\\
            \pline[7.]{\neg \neg s}[\ \ \ \ \ \ \ (4),(6),(MP)]
        }
        \pline[8.]{s \Rightarrow \neg \neg s}[\ \ \ \ \ \ \ (2-7), (DT)]
    }
    \]
    %
    We shall denote this rule $(\neg \neg I)$.
\end{example}

\begin{example}
    Lets prove $\neg w \vdash w \Rightarrow u$, by proving $\neg w, w \vdash u$.
    %
    \[
    \fitchprf{\pline{\neg w \Rightarrow (w \Rightarrow u)}}{
        \subproof{\pline[1.]{\neg w}} {
            \subproof{\pline[2.]{w}} {
                \pline[3.]{(\neg u \Rightarrow \neg w) \Rightarrow ((\neg u \Rightarrow w) \Rightarrow u)}[(A3)]\\
                \pline[4.]{\neg w \Rightarrow (\neg u \Rightarrow \neg w)}[(A1)]\\
                \pline[5.]{\neg u \Rightarrow \neg w}[(1),(4), (MP)]\\
                \pline[6.]{(\neg u \Rightarrow w) \Rightarrow u}[(3),(5),(MP)]\\
                \pline[7.]{w \Rightarrow (\neg u \Rightarrow w)}[(A1)]\\
                \pline[8.]{\neg u \Rightarrow w}[(2),(7),(MP)]\\
                \pline[9.]{u}[(6),(8),(MP)]
            }
            \pline[10.]{w \Rightarrow u}[(2-9),(DT)]
        }
        \pline[11.]{\neg w \Rightarrow (w \Rightarrow u)}[(1-10), (DT)]
    }
    \]
    %
    This is a proof of {\bf the law of contradiction}, denoed $(LC)$.
\end{example}

\begin{example}
    Consider the following proof.
    %
    \[
    \fitchprf{\pline{(s \Rightarrow w) \Rightarrow (\neg w \Rightarrow \neg s)}}{
        \subproof{\pline[1.]{s \Rightarrow w}} {
            \subproof{\pline[2.]{\neg w}}{
                \pline[3.]{\neg w \Rightarrow (\neg \neg s \Rightarrow \neg w)}[(A1)]\\
                \pline[4.]{\neg \neg s \Rightarrow \neg w}[(2),(3),(MP)]\\
                \pline[5.]{(\neg \neg s \Rightarrow \neg w) \Rightarrow ((\neg \neg s \Rightarrow w) \Rightarrow \neg s)}[(A3)]\\
                \pline[6.]{(\neg \neg s \Rightarrow w) \Rightarrow \neg s}[(4),(5),(MP)]\\
                \subproof{\pline[7.]{\neg \neg s}}{
                    \pline[8.]{\neg \neg s \Rightarrow s}[$(\neg \neg E)$]\\
                    \pline[9.]{s}[(7),(8),(MP)]\\
                    \pline[10.]{w}[(1),(9),(MP)]
                }
                \pline[11.]{\neg \neg s \Rightarrow w}[(7-10), (DT)]\\
                \pline[12.]{\neg s}[(6),(11), (MP)]
            }
            \pline[13.]{\neg w \Rightarrow \neg s}[(2),(12),(MP)]
        }   
        \pline[11.]{(s \Rightarrow w) \Rightarrow (\neg w \Rightarrow \neg s)}[(1-10), (DT)]
    }
    \]
    %
    This is the {\bf law of contraposition} $(LCP)$.
\end{example}

\begin{example}
    Lets prove $\vdash (s \Rightarrow w) \Rightarrow ((\neg s \Rightarrow w) \Rightarrow w)$.
    %
    \[
    \fitchprf{\pline{(s \Rightarrow w) \Rightarrow ((\neg s \Rightarrow w) \Rightarrow w)}}{
        \subproof{\pline[1.]{s \Rightarrow w}} {
            \pline[2.]{(s \Rightarrow w) \Rightarrow (\neg w \Rightarrow \neg s)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (LCP)]\\
            \pline[3.]{\neg w \Rightarrow \neg s}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1),(2),(MP)]\\
            \subproof{\pline[4.]{\neg s \Rightarrow w}} {
                \pline[5.]{(\neg w \Rightarrow \neg \neg s)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (4),(LCP)]\\
                \pline[6.]{\neg \neg s \Rightarrow s}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $(\neg \neg E)$]\\
                \pline[7.]{(\neg w \Rightarrow \neg \neg s) \Rightarrow ((\neg \neg s \Rightarrow s) \Rightarrow (\neg w \Rightarrow s))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (\text{CUT})]\\
                \pline[8.]{(\neg \neg s \Rightarrow s) \Rightarrow (\neg w \Rightarrow s)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (5),(7),(MP)]\\
                \pline[9.]{\neg w \Rightarrow s}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6),(8),(MP)]\\
                \pline[10.]{(\neg w \Rightarrow \neg s) \Rightarrow ((\neg w \Rightarrow s) \Rightarrow w)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (A3)]\\
                \pline[11.]{(\neg w \Rightarrow s) \Rightarrow w}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $(3),(10),(MP)$]\\
                \pline[12.]{w}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (9), (11), (MP)]
            }
            \pline[13.]{(\neg s \Rightarrow w) \Rightarrow w}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $(2-9),(DT)$]
        }
        \pline[14.]{(s \Rightarrow w) \Rightarrow ((\neg s \Rightarrow w) \Rightarrow w)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $(1-10), (DT)$]
    }
    \]
    %
    This essentially proves that $(s \vee \neg s) \Rightarrow w$ implies $w$.
\end{example}

We shall verify that proofs never lead to contradiction.

\begin{theorem}
    If $\vdash s$, then $\vDash s$.
\end{theorem}
\begin{proof}
    This is a trivial proof by structural induction. Prove that all axioms are tautologies, and that the set of tautologies is closed under modus ponens.
\end{proof}

This theorem shows that there are some statements which are not provable in our system. In fact, if $s$ is provable, then $\neg s$ is not provable. We call an axiom system like this {\bf absolutely consistant}. We shall show that all tautologies are provable, which shows the system is {\bf complete}. A complete system is effectively one in which all theorems which were meant to be able to be proved, are able to be proved.

\begin{lemma}
    Let $x_1, \dots, x_n \in \Lambda$ be variables in $s \in \text{SL}(\Lambda)$. Let $f$ be a truth assignment, and define $s' = s$ if $f_*(s) = \top$, or $s' = \neg s$ if $f_*(s) = \bot$. Then
    %
    \[ x_1', \dots, x_n' \vdash s' \]
\end{lemma}
\begin{proof}
    We prove by structural induction. If $s = x_1$, then $x_1' = s'$, and $s \vdash s$ is a trivial theorem. If $s = \neg w$, we consider two cases. If $w' = w$, then $s' = \neg \neg w$, and we have already shown $w \vdash \neg \neg w$, hence $x_1', \dots, x_n' \vdash s'$. If $w' = \neg w$, then $s' = w'$, and the theorem is trivial. If $s = w \Rightarrow u$, then either $f_*(w) = \top$ and $f_*(u) = \top$, or $f_*(w) = \bot$. In the first case, we have $x_1', \dots, x_n' \vdash u$, from which $x_1', \dots, x_n' \vdash w \Rightarrow u$ follows. In the second case, $x_1', \dots, x_n' \vdash \neg w$, and we have $\neg w \vdash (w \Rightarrow u)$.
\end{proof}

\begin{corollary}[Completeness Theorem]
    If $\vDash s$, $\vdash s$.
\end{corollary}
\begin{proof}
    Let $x_1, \dots, x_n$ be the variables in $s$. By the last lemma, we have
    %
    \[ x_1, \dots, x_n \vdash s\ \ \ \ \ \ \ \ \ \ x_1, \dots, \neg \neg x_n \vdash s \]
    %
    By the deduction theorem
    %
    \[ x_1, \dots, x_{n-1} \vdash x_n \Rightarrow s\ \ \ \ \ \ \ \ \ \ x_1, \dots, x_{n-1} \vdash \neg x_n \Rightarrow s \]
    %
    But then, since $\vdash (x_n \Rightarrow s) \Rightarrow ((\neg x_n \Rightarrow s) \Rightarrow s)$,
    %
    \[ x_1, \dots, x_{n-1} \vdash s \]
    %
    By induction, we find $\vdash s$.
\end{proof}

Before we finish our discussion of semantics, we note that there are many other axioms systems which can be used to define a propositional calculus (in the sense that they prove all tautologies). Most interesting is the axiom system whose only connective is the sheffer stroke, and whose only axiom schema is
%
\[ (B|(C|D))|[\{E|(E|E)\}|[(F|C)|((B|F)|(B|F))]] \]
%
and whose rule of inference is to infer $D$ from $B|(C|D)$, and $B$. Of course, it is outlandish to attempt proofs in such a system, hence why we did not attempt this chapter using the system.

\section{Multivalued Logics and Independence}

The most basic assumption of boolean logic is that there are two truth values, $\bot$ and $\top$. Everything is either true or false. But what about the statement ``God exists''? Until the skys open up, it is impossible to determine whether this statement is true or false. If we model the statement with boolean values, we must decide whether the statement is true and false. It is the principle of three-valued logic, invented by Jan \L ukasiewicz, who decided that some statements have a third `truth value', that the statement is undecidable. After this introduction, there is nothing stopping us from considering a system with even more `truth values', which evaluate statements based on the degree of truth, representing false, possible, justifiable, likely, etc, or something more subtler.

In classical logic, proof systems are built to `preserve truth'. All axioms are tautologies, and anything deduced from tautologies are tautologies. How then, are we to evaluate proof systems for multi-valued logics? When creating proof systems for multi-valued logics, we hope the value of a statement which follows from other statements is less than or equal to the value of the original statement. We shouldn't be able to prove some `more true' than what we know. For instance, consider Sorites paradox
%
\begin{quote}
    (1) ``A grain of sand is not a heap of sand''.\\
    (2) ``Adding a grain of sand to something which is not a heap does not make it into a heap''\\
    (3) ``A grain cannot be turned into a heap by adding grains.''
\end{quote}
%
We fix this argument by considering it in a multi-valued logic system. For each $k$, consider the statements
%
\begin{quote}
    (1.$k$) ``k grains of sand do not make a heap''.\\
    (2.$k$) ``Adding a grain to $k$ grains of sand doesn't make $k+1$ grains a heap''\\
    (1.$k+1$) ``$k+1$ grains do not make a heap.''
\end{quote}
%
we are only allowed to conclude $(1.k+1)$ from $(1.k)$ and $(2.k)$ when $(1.k+1)$ is `less true' than $(1.k)$ and $(2.k)$. As $k \to \infty$, if we continue the argument, then we find the statement eventually becomes completely false, which is true, for eventually a certain number of grains do become a heap. This book is not intended to talk about non-standard logics in detail. Instead, we use the notion of multivalued logic to attack the notion of independance. Are the axioms of our Hilbert system minimal -- do some of the axioms apply some of the other axioms? The trick to finding whether an axiom is independent is to find a truth system in which the other axioms are suitable, but the removed axiom is not. It follows that any statements which follows from the axioms are not removed.

Let us start by discarding (A1) from our axiom system. We may only derive results from (A2), (A3), and (MP). Can we derive (A1) using these rules? We shall try and set a value system on this proof system which satisfies the decreasing property of proofs. We shall consider `truth assignments' based on the table
%
\begin{center}
    \begin{tabular}{| c | c | c | c | }
        \hline $x$ & $y$ & $H_\Rightarrow(x,y)$ & $H_\neg(x)$ \\
        \hline $0$ & $0$ & $0$ & $1$ \\
        $0$ & $1$ & $2$ & $1$ \\
        $0$ & $2$ & $2$ & $1$ \\
        $1$ & $0$ & $2$ & $1$ \\
        $1$ & $1$ & $2$ & $1$ \\
        $1$ & $2$ & $0$ & $1$ \\
        $2$ & $0$ & $0$ & $0$ \\
        $2$ & $1$ & $0$ & $0$ \\
        $2$ & $2$ & $0$ & $0$ \\
        \hline
    \end{tabular}
\end{center}
%
With this definition, we may consider truth values of formulae. Note that $x \Rightarrow (y \Rightarrow x)$ evaluates to 2 when $x$ is 1 and $y$ is 2. We shall show any provable formula from (A2) and (A3) always takes the value zero. One calculates that this is true of any instance of (A2) and (A3). Furthermore, if $s$ and $s \Rightarrow w$ is always zero, then $w$ is always zero. But this shows that $x \Rightarrow (y \Rightarrow x)$ can never be proven using (A2) and (A3).

To prove that (A2) is independent of (A1) and (A3), take the truth table
%
\begin{center}
    \begin{tabular}{| c | c | c | c | }
        \hline $x$ & $y$ & $H_\Rightarrow(x,y)$ & $H_\neg(x)$ \\
        \hline $0$ & $0$ & $0$ & $1$ \\
        $0$ & $1$ & $2$ & $1$ \\
        $0$ & $2$ & $1$ & $1$ \\
        $1$ & $0$ & $0$ & $0$ \\
        $1$ & $1$ & $2$ & $0$ \\
        $1$ & $2$ & $0$ & $0$ \\
        $2$ & $0$ & $0$ & $1$ \\
        $2$ & $1$ & $0$ & $1$ \\
        $2$ & $2$ & $0$ & $1$ \\
        \hline
    \end{tabular}
\end{center}
%
Then everything proved from( (A1) and (A3) takes the value zero always, yet (A2) takes the value $2$ when $s$ is 0, $w$ is 0, and $u$ is 1.

We prove the independence of (A3) using a different trick, related to the removal of (A3) in intuitionistic logic. If $s$ is a statement, consider $h(s)$, which is defined by
%
\[ h(x) = x\ \ \ \ \ h((x \Rightarrow y)) = (h(x) \Rightarrow h(y))\ \ \ \ \ h((\neg x)) = x \]
%
Then $h$ removes all negations signs from $s$. If $s$ is an instance of (A1) or (A2), then $s$ is a tautology, and $h(s)$ is a tautology. Furthermore, if $h(w \Rightarrow u)$, and $h(w)$ are tautologies, then $h(w \Rightarrow u) = h(w) \Rightarrow h(u)$ is a tautology. Yet
%
\[ h((\neg s \Rightarrow \neg w) \Rightarrow ((\neg s \Rightarrow w) \Rightarrow s)) = (s \Rightarrow w) \Rightarrow((s \Rightarrow w) \Rightarrow w) \]
%
which is not a tautology, and is thus an instance of (A3) which cannot be proved by (A1) and (A2).



\chapter{First Order Logic}

It is logical to conclude that ``Julie is a human'' and ``Laura is a human'' from the general statement that ``All women are human''? In Propositional logic, we are unable to model this deduction in any satisfactory manner. We shall attempt to design a formal system mimicking these derivations. This is predicate logic.

\section{Language}

The syntax of predicate logic is less homogenous, for our language must contain nouns, like ``Julie'' and ``Laura'', and separate words describing statements about these nouns. These are known as {\bf terms} and {\bf quantifiers} respectively.

Terms should be able to model certain definite nouns, such as ``Julie'' and ``Laura'', variables such as $X$ and $Y$, which can stand for many definite nouns, and relational nouns, such as ``The school $X$ went to'', a statement which describes a different object for each interpretation of $X$. Definite nouns are known as {\bf constants}, and relational nouns are known as {\bf functions}. Functions will be separated based on their {\bf arity}, the number of arguments they take. ``$X$'s favourite $Y$''is a `2-ary' function, ``$X$'s birthday'' is a `1-ary' function. We shall now define the terms formally. Let $\Lambda$ be a set of variables, $\Delta$ a set of constants, and for each $n$, a set $\Psi_n$ of $n$-ary functions.

\begin{definition}
    The set of {\bf terms} is the smallest set $T(\Lambda, \Delta, \{ \Psi_n \})$ such that $\Lambda, \Delta \subset T(\Lambda, \Delta, \{ \Psi_n \})$, and if $s_1, \dots, s_n \in T(\Lambda, \Delta, \{ \Psi_n \})$, and $f$ is a function in $\Psi_n$, then $f(s_1, \dots, s_n) \in T(\Lambda, \Delta, \{ \Psi_n \})$.
\end{definition}

It is fairly easy to construct truth functional statements from these term. In addition to the usual connectives of sentential logic, we also require {\bf predicates}, which are functions of nouns representing a statement about those nouns. For instance, ``X is a Human'' is a predicate. Predicates, like functions, are separated based on arity. For each $n$, let $\Pi_n$ be a set of $n$-ary predicates. Given $\Lambda, \Delta, \{ \Psi_n \}$, and $\{ \Pi_n \}$, we shall construct

\begin{definition}
    Fix a set of variables $\Lambda$, a set of constants $\Delta$, a family of functions $\{ \Psi_n \}$, and a family of predicates $\{ \Pi_n \}$. An {\bf atomic formula} is a string of the form $P(t_1, \dots, t_n)$, where $P \in \Pi_n$, and $t_1, \dots, t_n \in T(\Lambda, \Delta, \{ \Psi_n \})$. Then the first-order language $\text{FO}(\Lambda, \Delta, \{ \Psi_n \}, \{ \Pi_n \})$ is defined to be the smallest set containing all atomic formulae, which is also closed under the logical operations $\wedge, \vee, \Rightarrow, \neg, \Leftrightarrow$, as in sentential logic, and if $x \in \Lambda$ is a variable, and $s$ is a statement, then $(\forall x: s)$ and $(\exists x: s)$ are formulae in the language.
\end{definition}

Its a bit of faff to verify that one may interpret each statement of the language in a unique way. Given the readers experience, we leave them to fill in the syntactical details when needed. A modification of the brackets lemma used in the previous chapter will aid in this task.

In sentential logic, one may substitute arbitrary formulas for arbitrary variables, and the meaning of the statement will not change. In first order logic, things are more complicated. Consider the statement
%
\begin{quotation}
    ``there exists $X$, such that if $X$ is a man, then $X$ is mortal''
\end{quotation}
%
First off, we cannot replace the initial $X$, for when we replace it with a definite nount the statement becomes nonsense. We may replace the other $X$'s, but this changes the meaning of the statement
%
\[ \text{``there exists $X$, such that if Laura is a man, then Laura is mortal''} \]
%
The problem results because the instances of $X$ are faulty. Another case results when we substitute $X$ for $Y$ in the formula
%
\[ \text{``there is $Y$ such that if $X$ is a man, then $Y$ is a dog''} \]
%
The resulting substitution is
%
\[ \text{``there is $Y$ such that if $Y$ is a man, then $Y$ is dog''} \]
%
We shall perform these such of substitutions. Let $s$ be an arbitrary string in a first order language. An occurrence of a variable $x$ is {\bf bound} in $S$ if it occurs in a subformula $(\forall x: w)$ or $(\exists x: w)$. An occurrence is {\bf free} if it is not bound, and is {\bf free for a formula} $s$ if it does not occur in any subformula of the form $(\forall y: w)$ or $(\exists y: w)$, where $y$ is a free variable in $s$. A substitution $w[s_1/x_1, \dots, s_n/x_n]$ is only valid when each $x_i$ is free for $s_i$. This avoids the interpretation problems above. In the first example, $X$ is bound, so cannot be substituted. In the second $X$ is free, but is not free for $Y$.

\section{Interpretation}

Languages are defined in terms of the subject manner we wish to study, but may be interpreted in many different ways. For instance, the axioms which define the logic of group theory may be interpreted relative to whichever group we interpret the axioms as agreeing with. We would hope that a statement is true if and only if it is true in every interpretation of the axioms. To begin discussing this, we must precisely define what we mean by interpretation, as formulated by Alfred Tarski.

An {\bf interpretation} $M$ of a first order language $\text{FO}(\Lambda, \Delta, \{ \Phi_n \}, \{ \Pi_n \})$ is a {\bf universe of discourse} $M(U)$, and an interpretation of the relations; for each constant $c \in \Delta$, we have an associated element $c_M \in U$, for each function $f \in \Phi_n$, we have a function $f_M: U^n \to U$, and for each proposition $P \in \Pi_n$, we have an $n$-ary relation $P_M$ on elements of $U$.

In sentential logic, when we assign a truth value to a set of variables, we may extend the definition of truth to all formulas. When we assign a meaning to each variable in a first order language, we may define a meaning on all terms, and from these meanings, assign truth to statements in the corresponding language. Consider a particular interpretation of a first order language with variables $\Lambda$, and consider an assignment $f: \Lambda \to U$. Define $f_*: T(\Lambda, \Delta, \{ \Psi_n \}) \to U$, by the recursive formulation. Let $x$ be an arbitrary variable, $c$ an arbitrary constant, and $g$ an arbitrary formula,
%
\[ f_*(x) = f(x)\ \ \ \ \ f_*(c) = c_M\ \ \ \ \ f_*(g(t_1, \dots, t_n)) = g_M(f_*(t_1), \dots, f_*(t_n)) \]
%
Using this definition, we may define whether a formula is satisfied in a model of a first order theory. We shall now define what it means for an assignment to {\bf satisfy} a formula in an interpretation. For simplicity, write $f[a/x]$ for the assignment
%
\[ f[a/x](y) = \begin{cases} f(y) & y \neq x \\ a & y = x \end{cases} \]
%
\begin{enumerate}
    \item An assignment $f$ satisfies $P(t_1, \dots, t_n)$ if $P_M(f_*(t_1), \dots, f_*(t_n))$ holds.
    \item An assignment $f$ satisfies $(\forall x: s)$ if $s$ is satisfied by all $f[a/x]$, for all $a$ in the universe of discourse. A statement satisfies a formula $(\exists x: s)$ if there is some $a$ such that $f[a/x]$ satisfies $s$.
    \item An assignment $f$ satisfies $s \circ u$ or $\neg s$, where $\circ$ and $\neg$ are logical connectives, exactly how they were defined in sentential logic.
\end{enumerate}
%
If $M$ is an interpretation, then a formula $s$ is {\bf valid} for an interpretation, denoted $\vDash_M s$, if $s$ is true under every assignment under $M$. A statement is false if it is true under no interpretation, or alternatively, if the negation of the statement is true under the interpretation. An interpretation is a {\bf model} for a set of formulas $\Gamma$ if every formula in $\Gamma$ is true for the interpretation.

\begin{lemma} If $\vDash_M s$ and $\vDash_M s \Rightarrow w$, then $\vDash_M w$. \end{lemma}
\begin{proof}
    If $f$ satisfies $s$ and $s \Rightarrow w$, then $f$ satisfies $w$.
\end{proof}

\begin{lemma} If a formula $s$ contains free variables $x_1, \dots, x_n$, and two assignments $f$ and $g$ agree on the free variables, then $f$ satisfies $s$ if and only if $g$ satisfies $s$. \end{lemma}

\begin{lemma} $\vDash_M (\exists x: s)$ if and only if $\vDash_M \neg (\forall x: \neg s)$. \end{lemma}
\begin{proof}
    If an assignment $f$ satisfies $(\exists x: s)$, then $s$ is satisfied by some $f[a/x]$. But then $f$ does not satisfy $(\forall x: \neg s)$ for $f[a/x]$ does not satisfy $\neg s$. Conversely, if an assignment $f$ does not satisfy $(\exists x: s)$, then every $f[a/x]$ satisfies $\neg s$.
\end{proof}

\begin{lemma}
    $\vDash_M s$ if and only if $\vDash_M (\forall x: s)$.
\end{lemma}
\begin{proof}
    If $\vDash_M s$, then every assignment $f$ satisfies $s$, so certainly every $f[a/x]$ satisfies $s$, and thus $f$ satisfies $(\forall x: s)$, hence $\vDash_M (\forall x: s)$. Conversely, suppose $\vDash_M (\forall x: s)$. Then, every assignment satisfies $(\forall x: s)$, and thus in particular satisfies $s$.
\end{proof}

Let $s$ contain free variables $x_1, \dots, x_n$. The {\bf closure} of $s$ is the string $(\forall x_1 : (\forall x_2 : \dots (\forall x_n : s))\dots))$. We then verify by induction that a formula is satisfied by an interpretation if and only if its closure is.

\begin{theorem}
    Consider a form of sentential logic, whose variables are all atomic formulas, and formulas of the form $(\forall x: s)$, and $(\exists x: s)$. Then if a statement is a tautology, then it is satisfied under all interpretations.
\end{theorem}
\begin{proof}
    The connectives of predicate logic not summarized above are exactly the connectives of sentential logic. Thus regardless of how $f$ satisfies the formulas treated atomically in sentential logic, $f$ will satisfy the complete tautology.
\end{proof}

\begin{theorem}
    If $s$ contains no free variables, then either $\vDash_M s$ or $\vDash_M \neg s$.
\end{theorem}
\begin{proof}
    This follows from the fact that any two assignments that agree on the free variables of a formula agree on the satisfiability. Thus if there are no free variables, all assignments agree, and in particular all satisfy the formula or all do not satisfy the formula.
\end{proof}

\begin{lemma}
    If $t$ and $u$ are terms, $x$ is a variable, and $f$ is an assignment, then
    %
    \[ f[f_*(u)/x]_*(t) = f_*(t[u/x]) \]
\end{lemma}
\begin{proof}
    If $t$ is a variable unequal to $x$, or $t$ is a constant, then
    %
    \[ f[f_*(u)/x]_*(t) = f(t) = f_*(t[u/x]) \]
    %
    If $t = x$, then
    %
    \[ f[f_*(u)/x]_*(t) = f_*(u) = f_*(t[u/x]) \]
    %
    For a structural induction, let $t = g(t_1, \dots, t_n)$. Then
    %
    \begin{align*}
        f[f_*(u)/x]_*(t) &= g_M(f[u/x]_*(t_1), \dots, f[u/x]_*(t_n))\\
        &= g_M(f_*(t_1[u/x]), \dots, f_*(t_n[u/x])) = f_*(t[u/x])
    \end{align*}
    %
    Thus the theorem holds in general.
\end{proof}

\begin{lemma}
    $f$ satisfies $s[u/x]$ if and only if $f[f_*(u)/x]$ satisfies $s$.
\end{lemma}
\begin{proof}
    If $s$ is $P(t_1, \dots, t_n)$. Then $s[u/x] = P(t_1[u/x], \dots, t_n[u/x])$, and
    %
    \[ P_M(f_*(t_1[u/x]), \dots, f_*(t_n[u/x])) = P_M(f[f_*(u)/x]_*(t_1), \dots, f[f_*(u)/x]_*(t_n)) \]
    %
    Which shows that $f$ satisfies $s[u/x]$ if and only if $f[f_*(u)/x]$ satisfies $s$. If $s$ is formed by standard sentential connectives, the theorem is trivial. If $s = (\forall y: w)$, where $y \neq x$, then $s[u/x] = (\forall y: w[u/x])$, and by definition, $f$ satisfies $s[u/x]$ if and only if $f[a/y]$ satisfies $w[u/x]$ for all $a$, which by induction implies that $f[f_*(u)/x][a/y]$ satisfies $w$ for all $a$, so $f[f_*(u)/x]$ satisfies $s$. Similar results hold if $s$'s primitive connective is the existential quantifier.
\end{proof}

\begin{theorem}
    For any formula $s$ and term $t$ free for $x$, $\vDash_M (\forall x : s) \Rightarrow s[t/x]$
\end{theorem}
\begin{proof}
    Let $f$ be an assignment satisfying $(\forall x: s)$. Then $f[f_*(t)/x]$ satisfies $s$, so $f$ satisfies $s[t/x]$.
\end{proof}

\begin{theorem}
    If $s$ does not contain $x$ as a free variable, then
    %
    \[ \vDash_M (\forall x: s \Rightarrow w) \Rightarrow (s \Rightarrow (\forall x: w)) \]
\end{theorem}
\begin{proof}
    If $f$ satisfies $(\forall x: s \Rightarrow w)$ and $s$, then $f[a/x]$ satisfies $s \Rightarrow w$, and since $s$ does not contain $x$ as a free variable, $f[a/x]$ also satisfies $s$, so $f$ satisfies $(\forall x : w)$.
\end{proof}

Given a specific interpretation with universe of discourse $U$, we enhance the first order language we are discussing to include $U$ as constants. The interpretation naturally extends to this new language. Thus we can talk of whether our interpretation satisfies a formula
%
\[ \vDash_M s[u_1/x_1, \dots, u_n/x_n] \]
%
Where $u_1, \dots, u_n \in U$ are formulas containing elements of $U$. If the free variables of a formula $s$ are $x_1, \dots, x_n$, then the set of $u_1, \dots, u_n$ for which the equation above holds will be called the relation relative to $s$.

A statement is {\bf logically valid} if it is true under all possible interpretations. A statement is {\bf satisfiable} if it is true under at least one interpretation, and {\bf contradictory} if it is false under every interpretation. A set of statements is satisfiable if they are all true under a single interpretation. A statement $s$ is a {\bf logical consequence} of a set of statements $\Gamma$ if every interpretation which satisfies every statement of $\Gamma$ also satisfies $s$.






\section{First Order Formal Systems}

We have the syntax and semantics to begin discussing formal systems in the first-order languages. We would like to find axioms which only find theorems satisfied by every model of the system, a {\bf sound} axiom system. Even better, if we can find {\bf complete} axioms, which can proved anything satisfied by every model of the system. Axioms are essential, because it turns out that determining whether a formula is satisfied by all models of the system is an uncomputable problem. We shall use an axiom consisting of five schemata, the original (A1), (A2), and (A3) found in predicate logic, as well as two new first order schema (A4) and (A5). Let $s$ and $w$ be formula, and $t$ a term substitutable for a variable $x$. Then (A4) is
%
\[ (\forall x: s) \Rightarrow s[t/x] \]
%
provided that $x$ does not occur freely in $s$, we have (A5)
%
\[ (\forall x: s \Rightarrow w) \Rightarrow (s \Rightarrow (\forall x: w)) \]
%
Our rules of inference are modus ponens (MP), to infer $w$ from $s$ and $s \Rightarrow w$, as well as universal generalization (UG), inferring $(\forall x: s)$ from $s$. As in predicate logic, we shall let $\vdash s$ stand for $s$ is a theorem of the system.

From the theorems we proved about the semantics of first order logic, we already know that this system is sound, a simple use of structural induction. Now we show that the system is complete. Our system includes all the rules of propositional calculus, which justifies a very useful property. Let $s$ be a formula in first order logic, and replace it with a corresponding formula in sentential logic by replacing all the main occurences of the quantifiers $(\forall x: s)$ with variables separate from the rest of the variables occuring in $s$. Then if the formula obtained is a tautology, $s$ is provable in first order logic.

\begin{example}
    Find easy example of tautology usage to insert here!
\end{example}

The deduction theorem cannot be carried directly over to first order logic, since $s \vdash (\forall x: s)$ is always true, yet $\vdash s \Rightarrow (\forall x: s)$ cannot be a theorem of first order logic.

\begin{example}
    Consider the formula $P(x) \Rightarrow (\forall x: P(x))$. Take a model $M$ whose universe consists of two elements $a$ and $b$, and let $P_M$ only be satisfied by $a$. Then, under the assignment $f(x) = a$, $P(x)$ is satisfied, but $(\forall x: P(x))$ is not.
\end{example}






\part{Computability}

In 1931, Kurt G\"{o}del proved all sufficiently complicated axiomatic systems had unprovable theorems, but a fundamental question remained; how was one to decide whether a theorem could be proved? It took a decade for Alonzo Church and Alan Turing to deduce the impossibility of such a claim. Fifty years later, `theoretical computation' had become a common reality. We shall describe mathematical models of computation which have been developed to analyze the limitations of various computational methods.

Turing and Church's major breakthrough was precisely defining a `computational procedure'. It is often the case that precise definitions give rise to easy proofs of the most surprising consequence. Philosophically, one should be able to define a procedure without reference to a computer, for humans computed long before microchips. On the other hand, models should reflect physical reality, since one needs a physical mechanism in order to computer, whether electronic or mental. If your computational model is too strong or too weak, it will not accurately represent our limitations.

We will begin by analyzing the automaton, a model of computation without stored memory. We will expand the amount of expression of the automaton by considering context-free grammars. Finally, we add memory by considering a Turing machine. It is the Church Turing thesis that this is the ultimate model of computation -- any real world computation can be modelled as an action on a Turing machine. From this model, and with the hypothesis of Church and Turing, we can make precise, philisophically interesting statements about the nature of computation in the real world.

As in mathematical logic, the objects of study are strings of symbols over a certain alphabet. One studies the notion of computation syntactically. One of the main ideas of computability theory is that a mental decision can be modelled as a {\bf decision problem} -- find a computational model which will `accept' certain strings over an alphabet. Suppose our problem is to verify whether the addition of two numbers is correct. We are given $a$, $b$, and $c$, and we must decide whether $a + b = c$. Our symbol set is $\{ 0, 1, \dots, 9, : \}$, and we wish to model a computation which accepts all strings of the form $``a:b:c''$, where $a$, $b$, and $c$ are decimal strings for which $a + b = c$. Thus we must design machine to accept strings in a specified language, and determining whether a problem is solvable reduces to studying the structure of languages.

As a more dynamic discipline than mathematical logic, we need more operations on strings to obtain languages from other languages. We obviously need concatenation, but also {\bf reversal}, which will be denoted $s^R$. These operations are extended to languages by applying the operations on a component by component basis:
%
\[ S \circ W = \{ s \circ w : s \in S, w \in W \}\ \ \ \ \ S^R = \{ s^R : s \in S \} \]
%
A {\bf palindrome} is a string $s$ for which $s^R = s$. If $\Sigma$ is a set of strings, we shall let $\Sigma_\varepsilon = \Sigma \cup \{ \varepsilon \}$.



\chapter{Finite State Automata}

Our initial model of computability is a computer with severely limited memory. Surprisingly, we shall still be able to compute a great many things. The idea of this model rests on representing computation by a state diagram. Suppose we would like to describe an algorithm describing if a number is divisible by three. We shall represent a number by a string of dashes. For instance, $``-----''$ represents the number 5. We describe the algorithm in a flow chart below
%
\begin{center}
\begin{tikzpicture}[scale=0.2]
\tikzstyle{every node}+=[inner sep=0pt]
\draw [black] (39.8,-19.2) circle (3);
\draw [black] (39.8,-19.2) circle (2.4);
\draw [black] (46.4,-30) circle (3);
\draw [black] (34,-30) circle (3);
\draw [black] (39.8,-13.3) -- (39.8,-16.2);
\fill [black] (39.8,-16.2) -- (40.3,-15.4) -- (39.3,-15.4);
\draw [black] (41.36,-21.76) -- (44.84,-27.44);
\fill [black] (44.84,-27.44) -- (44.85,-26.5) -- (43.99,-27.02);
\draw (43.74,-23.32) node [right] {$-$};
\draw [black] (43.4,-30) -- (37,-30);
\fill [black] (37,-30) -- (37.8,-30.5) -- (37.8,-29.5);
\draw (40.2,-30.5) node [below] {$-$};
\draw [black] (35.42,-27.36) -- (38.38,-21.84);
\fill [black] (38.38,-21.84) -- (37.56,-22.31) -- (38.44,-22.78);
\draw (36.22,-23.43) node [left] {$-$};
\end{tikzpicture}
\end{center}
%
The algorithm proceeds as follows. Begin at the top node. For each dash, proceed clockwise around the triangle. If, at the end of our algorithm, we end up back at the top node, then the number is divisible by three. The basic idea of the finite automata is to describe computation via these flow charts -- we follow a string around a diagram, and if we end up at a `accept state', then we accept the string.

\begin{definition} 
    A {\bf deterministic finite state automaton} is a 5-tuple $(Q, \Sigma, \Delta, q_0, F)$, where $Q$ is a finite set of states, $\Sigma$ is a finite alphabet, $q_0 \in Q$ is a start state, $F \subset Q$ are the accept states, and $\Delta: Q \times \Sigma \to Q$ is the transition function
\end{definition}

A finite state machine `works' exactly how our original algorithm worked. Draw a directed graph whose nodes are states, and draw an edge between a state $q$ and each related state $\Delta(q, \sigma)$, where $\sigma \in \Sigma$. Take a string $s \in \Sigma^*$. We begin at the start state $q_0$. Sequentially, for each symbol in $s$, we follow the directed edge from our current state to the state on the edge related to the current symbol in $s$. If, we end at an accept state in $F$, then $s$ is an `accepted' string. Formally, we define this method by extending $\Delta$ to $Q \times \Sigma^*$. We define, for $\sigma \in \Sigma$,
%
\[ \Delta(q, \varepsilon) = q\ \ \ \ \ \Delta(q, st) = \Delta(\Delta(q,s), t) \]
%
A state machine $M$ {\bf accepts} a string $s$ in $\Sigma^*$ if $\Delta(s,q_0) \in F$. We call the set of all accepting strings the {\bf language} of $M$, and denote the set as $L(M)$. A subset of $\Sigma^*$ which is a language of a deterministic finite state automata is known as a {\bf regular language}.

\begin{example}
    Consider $\Sigma = \{ - \}$. Then the set of all `dashes divisible by three' is regular, as in the introductory diagram. Formally, take
    %
    \[ Q = \mathbf{Z}_3\ \ \ \ \ \Delta(x,-) = x+1\ \ \ \ \ q_0 = 0\ \ \ \ \ F = \{ 0 \} \]
    %
    then $(Q, \Sigma, \Delta, q_0, F)$ recognizes dashes divisible by three. The `graph' of the automata is exactly the graph we've already drawn.
\end{example}

The set of regular languages is closed under many useful operations. We shall find that all regular languages can be described by certain compositional operators.

\begin{theorem}
    If $A, B \subset \Sigma^*$ are regular languages, then $A \cup B$ is regular.
\end{theorem}
\begin{proof}
    let $M = (Q, \Sigma, \Delta, q_0, F)$ and $N = (R, \Sigma, \Gamma, r_0, G)$ be automata recognizing $A$ and $B$ respectively. We shall define a finite automata recognizing $A \cup B$. Define a function $(\Delta \times \Gamma) : (Q \times R) \times \Sigma \to (Q \times R)$, by letting
    %
    \[ (\Delta \times \Gamma)(q,r,\sigma) = (\Delta(q,\sigma), \Gamma(r,\sigma)) \]
    %
    Consider
    %
    \[ H = \{ (q,r) \in S : q \in F\ \text{or}\ r \in G \} \]
    %
    We contend that
    %
    \[ ((Q \times R), \Sigma, \Delta \times \Gamma, (q_0, r_0), H) \]
    %
    recognizes $A \cup B$. By induction, one verifies that
    %
    \[ (\Delta \times \Gamma)(q,r,s) = (\Delta(q,s), \Gamma(r,s)) \]
    %
    Thus $(\Delta \times \Gamma)(q_0, r_0, s) \in H$ if and only if $\Delta(q_0, s) \in F$ or $\Gamma(r_0, s) \in G$.
\end{proof}

\begin{theorem}
    If $A$ is a regular language, then $A^c$ is regular.
\end{theorem}
\begin{proof}
    If $M = (Q,\Sigma,\Delta,q_0,F)$ recognizes $A$. Then define a new machine $N = (Q,\Sigma,\Delta,q_0,F^c)$. It is easy to see that $L(N) = A^c$.
\end{proof}

\begin{corollary}
    If $A$ and $B$ are regular languages, then $A \cap B$ are regular.
\end{corollary}
\begin{proof} $A \cap B = (A^c \cup B^c)^c$. \end{proof}

\section{Non Deterministic Automata}

It shall be convenient to expand our definition of automaton, into a machine which is easier to work with in some situations. It is a version of a finite automata that can perform parallel-processing. This is the use of the non-deterministic automaton.

\begin{definition}
    A {\bf non-deterministic finite state automaton} is a 5-tuple $(Q, \Sigma, \Delta, q_0, F)$, where $Q$ is a finite set of states, $\Sigma$ is a finite alphabet, $q_0$ is the start state, $F \subset Q$ are the accept states, and $\Delta: Q \times \Sigma_\varepsilon \to \mathcal{P}(Q)$ is the non-deterministic transition function.
\end{definition}

In a non-deterministic finite state automata, we {\bf accept} a string $s$ if $s = s_1 \dots s_n$, where each $q_i \in \Sigma_\varepsilon$, and there are a sequence of states $t_0, \dots, t_{n+1}$, with $q_0 = t_0$ and $t_n \in F$, such that $t_{k+1} \in \Delta(t_k, s_k)$. The set of accepting strings of a machine $M$ form the language $L(M)$. There is a nicer criterion of acceptance than described above, which is easier to work with in proofs. First, assume there are no $\varepsilon$-transitions in a machine $M$; that is, $\Delta(q, \varepsilon) = \emptyset$ for all states $q$. We may then extend $\Delta: Q \times \Sigma \to \mathcal{P}(Q)$ to $\Delta: \mathcal{P}(Q) \times \Sigma^* \to \mathcal{P}(Q)$ recursively by
%
\[ \Delta(X, \varepsilon) = X \ \ \ \ \ \Delta(X, st) = \Delta(\Delta(X,s), t) \]
%
A string $s$ is accepted by $M$ if and only if an accept state is an element of $\Delta(q_0, s)$. If $s = s_1 \dots s_n$, and there are $t_0, \dots, t_n$ with $t_0 = q_0$, $t_n$ an accept state, and $t_{k+1} \in \Delta(t_k, s_k)$, then by induction one verifies that $t_n \in \Delta(q_0, s)$. Conversely, if $q \in \Delta(q_0, s)$, and if $s = s_1 \dots s_n$, then there is a state $q'$ with $q' \in \Delta(q_0, s_1 \dots s_{n-1})$, $q \in \Delta(q', s_n)$. But this implies that if there is an accept state in $\Delta(q_0, s)$, then $s$ is accepted by $M$. We can always perform this trick, for we may always remove $\varepsilon$ transitions.

\begin{lemma}
    Every non-deterministic automata is equivalent to a non-deterministic automata without $\varepsilon$ transitions.
\end{lemma}
\begin{proof}
    Let $M = (Q, \Sigma, \Delta, q_0, F)$ be a non-deterministic automata. Define a state $u$ to be $\varepsilon$ reachable from $t$ if there is a sequence of states $q_0, \dots, q_n$ with $q_0 = t$, $q_n = u$, and $q_{i + 1} \in \Delta(q_i, \varepsilon)$. Let $E(u)$ be the set of all states $\varepsilon$ rechable from $u$. Define
    %
    \[ N = (Q, \Sigma, \Delta', q_0, F) \]
    %
    Where
    %
    \[ \Delta'(q, s) = \begin{cases} \bigcup_{u \in E(q)} \Delta(u, s) & s \neq \varepsilon \\ \emptyset & s = \varepsilon \end{cases} \]
    %
    Then $L(N) = L(M)$. For any $\varepsilon$ transitions can be skipped in $N$.
\end{proof}

We draw a graph with nodes $Q$, and with directed edges $v$ to $w$ if $w \in \Delta(v, \Sigma_{\varepsilon})$. We begin at $q_0$. For a string $s$, we attempt to find a path from $q_0$ to an accept state, by following edges whose corresponding symbol is in $s$ (or whose symbol is $\varepsilon$, in which we get for free). The string is accepted if such a path is possible. Some call non-deterministic methods a lucky guess methods, since they always make a lucky guess of which deterministic path to take to accept a string.

It seems to be a much more complicated procedure to find if a string is accepted by a non-deterministic automata, but it turns out that every non-deterministic automata can be converted into a deterministic automata. The proof relies on the fact that we may exploit the operations of non-determinism, described using power sets of a set.

\begin{theorem}
    A non-deterministic finite state automata language is regular.
\end{theorem}
\begin{proof}
    Let $M = (Q, \Sigma, \Delta, q_0, F)$ be a non-deterministic automata. Assume $M$ has no $\varepsilon$ transitions, without loss of generality. Let
    %
    \[ N = (\mathcal{P}(Q), \Sigma, \Gamma, \{ q_0 \}, \{ S \in \mathcal{P}(Q) : S \cap F \neq \emptyset \} \]
    %
    where
    %
    \[ \Gamma(S, t) = \Delta(S, t) \]
    %
    We have already verified this deterministic machine recognizes $L(M)$, for $\Delta(\{ q_0 \}, s)$ contains an accept state if and only if $s$ is accepted.
\end{proof}

Non-determinsitic automata can now be employed to understand regular languages.

\begin{theorem}
    If $A$ and $B$ are regular languages, then $A \circ B$ is regular.
\end{theorem}
\begin{proof}
    Let $M = (Q, \Sigma, \Delta, q_0, F)$ and $N = (R, \Sigma, \Gamma, r_0, G)$ be deterministic automata accepting $A$ and $B$ respectively. Without loss of generality, assume $Q$ is disjoint from $R$. Consider the non-deterministic machine
    %
    \[ O = (Q \cup R, \Sigma, \Pi, q_0, G) \]
    %
    Define
    %
    \[ \Pi(s,t) = \begin{cases} \{ \Delta(s,t) \} & s \in Q\text{, and}\ t \neq \varepsilon\ \text{or}\ s \not \in F \\ \{ \Delta(s,t), r_0 \} & s \in F, t = \varepsilon\\ \{ \Gamma(s,t) \} & s \in R\\
    \emptyset & \text{otherwise} \end{cases} \]
    %
    We quickly verify that if $s \in L(M)$ and $w \in L(N)$, then $sw \in L(O)$. If $v \in L(O)$, there is some substring $k$ such that $r_0 \in \Pi(q_0, k)$ (for this is the only path from $q_0$ to $G$). If we choose $k$ to be the shortest such string, then $k$ must also be an accept string in $L(M)$, since any substring of $k$ cannot map to states in $R$, and thus $k$ must map via the $\varepsilon$ transition from an accept state of $M$. If $v = kr$, then $\Pi(r_0, r)$ is an accept state in $N$, so $r \in L(N)$. Thus $v \in L(M) \circ L(N)$.
\end{proof}

\begin{theorem}
    If $A$ is a regular language, $A^*$ is regular.
\end{theorem}
\begin{proof}
    Let $M = (Q, \Sigma, \Delta, q_0, F)$ be a deterministic language which accepts $A$. Form a non-deterministic automata $N = (Q \cup \{ i \}, \Sigma, \Gamma, i, F)$, where
    %
    \[ \Gamma(q,s) = \begin{cases} \{ \Delta(q,s) \} & s \neq \varepsilon \\ \{ q_0 \} & s = \varepsilon, q = i \\ \{ i \} & s = \varepsilon, q \in F \\ \emptyset & \text{otherwise} \end{cases} \]
    %
    Then $L(N) = A^*$, for if $s = a_1 \dots a_n$, with $a_i \in A$, then we may go from $i$ to $q_0$, then to an accept state via $a_1$, return to $i$ with a $\varepsilon$ transition, and continue, so $s$ is accepted. If we split a string accepted to $L(N)$ into when $\varepsilon$ transitions to $i$ are used, then we obtain strings in $A$.
\end{proof}

These operations are enough to describe all regular languages, as we shall now proceed to derive.

\section{Regular Expressions}

Automata are equivalent to a much more classical notion of computation -- regular expressions.

\begin{definition}
    A {\bf regular expression} over an alphabet $\Sigma$ is the smallest subset of $(\Sigma \cup \{ \emptyset, \cup, *, (, ) \})^*$ such that
    %
    \begin{enumerate}
        \item $\emptyset$ is a regular expression as are $s \in \Sigma^*$.
        \item If $s$ and $w$ are regular expressions, then so are $s^*, (s \cup w)$, and $sw$.
    \end{enumerate}
\end{definition}
%
Every regular expression describes a regular language. A string $s \in \Sigma^*$ is recognized by an regular expression $t$ if
%
\begin{enumerate}
    \item $t \in \Sigma^*$, and $s = t$.
    \item $t = (u \cup v)$, and $s$ is recognized by $u$ or by $v$.
    \item $t = uv$, and $s = wl$, where $w$ is recognized by $u$, and $l$ is recognized by $v$.
    \item $t = u^*$, and $s = \varepsilon$ or $s = s_1 \dots s_n$, where $s_i$ is recognized by $u$.
\end{enumerate}
%
The regular language corresponding to a regular expression $t$ is $L(t)$, the set of strings recognized by $t$. The set of all regular expressions on an alphabet $\Sigma$ will be denoted $R(\Sigma)$.

It is a simple consequence of our discourse that every language recognized by a regular expression {\it is actually} a regular language. In fact, we can show that every regular language is described by a regular expression. We shall describe an algorithm for converting a finite state automaton to a regular expression. We shall make a temporary generalization, by allowing non deterministic finite automata to have regular expressions in their transition functions.

\begin{definition}
    A {\bf generalized non-deterministic finite state automaton} is a 5-tuple $M = (Q,\Sigma, \Delta, q_0, f_0)$, where $\Delta: Q - \{ f_0 \} \times Q - \{ q_0 \} \to R(\Sigma)$ is the generalized transition function, and $q_0$ is the start state, $f_0$ is the end state.
\end{definition}

A generalized automaton accepts  $s \in \Sigma^*$ if we may write $s = s_1 \dots s_n$, and there are a sequence of states $t_0, \dots, t_{n+1}$ where $t_0 = q_0$, $t_n = f_0$, and $\Delta(q_i, q_{i+1})$ recognizes $s_i$.

\begin{theorem}
    Any generalized finite-state automaton describes the language of a regular expression.
\end{theorem}
\begin{proof}
    If the generalized automaton has two states, then there is only one transition from start state to begin state, and this transition describes a regular expression for the automaton. We will reduce every automaton to this form by induction. Suppose an automaton has $n$ states. Fix a state $q \in Q$, which is neither the beginning or accepting state. Define a new automaton
    %
    \[ N = (Q - \{ q \}, \Sigma, \Delta', q_0, F) \]
    %
    where $\Delta'(a,b) = (\Delta(a,b) \cup \Delta(a,q) \Delta(q,q)^* \Delta(q,b))$
    %
    Then $N$ is equivalent to $M$, and has one fewer state, and is thus equivalent to a regular expression.
\end{proof}

\begin{corollary}
    Every regular language is described by a regular expression.
\end{corollary}


\section{Limitations of Finite Automata}

We've discovered a menagerie of different problems we can solve with finite automata, but it has already been foreshadowed that better machines await. Here we attack languages which are not recognized by finite automata.

\begin{theorem}[Pumping Lemma]
    Let $L$ be a regular language. Then there is a number $p$, called the pumping length, such that any $s \in L$ which satisfies $|s| \geq p$, then we may write $s = wuv$, where $|u| > 0$, $|wu| \leq p$, and $wu^iv \in L$ for all $i \geq 0$.
\end{theorem}
\begin{proof}
    Let $L$ be a regular language, and $M$ a deterministic automata recognizing $L$ with $p$ states. Let $s$ be a string with $|s| \geq p$, with $s \in L(M)$. Write $s = s_1 \dots s_n$ and let $q_k = \Delta(q_0, s_1, \dots, s_k)$. Then we obtain $|s| + 1$ states $q_0, q_1, \dots, q_{|s|}$. By the pidgeonhole principle, since $q_i$ equals some $q_j$, for $i < j$. Let $w = s_1 \dots s_i$, $u = s_{i+1} \dots s_j$, $v = s_{j+1} \dots s_{|s|}$. Then $\Delta(\Delta(q_0, w), u) = \Delta(q_0, w)$, so
    %
    \[ \Delta(q_0, wu^iv) = \Delta(q_0, wuv) \]
    %
    So $wu^iv \in L$ for all $i$.
\end{proof}

\begin{example}
    $L = \{ 0^k10^k : k \geq 0 \}$ is not regular. If it was regular, it would have a pumping length $p$. Since $0^p10^p$ is in $L$, so we may write $0^p10^p = wuv$, where $|wu| \leq p$, and $wu^iv \in L$. Then $u = 0^k$, for $k > 0$, and $wv = 0^{p-k}10^p \in L$, which is clearly not in the language, contradicting regularity.
\end{example}

\begin{example}
    $L = \{ 1^{n^2} : n \in \mathbf{N} \}$ is not regular. Suppose we had a pumping length $p$. Then $1^{p^2} \in L$, so there is $0 < k \leq p$ with $1^{p^2 + k} \in L$. But
    %
    \[ (p + 1)^2 = p^2 + 2p + 1 > p^2 + k \]]
    %
    And there is no perefect square between $p^2$ and $(p+1)^2$, a contradiction.
\end{example}

\begin{example}
    The language $L = \{ 0^i1^j : i > j  \}$ is not regular. If we had a pumping lenght $p$, then $0^p1^{p-1} \in L$. But then there is $0 < k \leq p$ such that $0^{p + (i - 1)k}1^{p-1} \in L$ for all natural numbers $i$. In particular, for $i = 0$, we find $0^{p - k}1^{p-1} \in L$ But $p - k \leq p -1$, a contradiction.
\end{example}

\section{Modelling Problems}

Decision problems are sufficient to model a large variety of computational models, but for completeness, we should at least mention how we determine whether problems with multiple outputs can be. The counterpart of a finite-state automata is a finite-state transducer.

\begin{definition}
    A {\bf finite state transducer} is a 5 tuple $(Q, \Sigma, \Lambda, \Delta, \Gamma, q_0)$, where $Q$ is the set of states, $\Sigma$ is the input alphabet, $\Lambda$ is the output alphabet, $\Delta: Q \times \Sigma \to Q$ is the transition function, $\Gamma: Q \times \Sigma \to \Lambda$ is the transduction function, and $q_0$ is the start state.
\end{definition}

Each finite-state transducer $M$ gives rise to a function $f_M: \Sigma^* \to \Lambda^*$, defined by
%
\[ f_M(\varepsilon) = \varepsilon\ \ \ \ \ f_M(st) = f_M(s) \circ \Gamma(\Delta(q_0,s), t) \]
%
A regular function is one computable by a finite state transducer.

\begin{example}
    Consider an alphabet $\mathbf{F}_2$ and consider the function $f$ taking and returning strings over $\mathbf{F}_2$, inverting the strings on even positions, and leaving the strings on the odd positions. Then $f$ is a regular function, for it may be computed by the automata below.
\end{example}



\chapter{Context Free Languages}

Finite state machines are useful on a specific set of problems, but have limitations. We would like our notion of computability to decide on a much more complicated set of problems. Thus we must define new classes of computable languages.

\section{Context Free Grammars}

\begin{definition}
    A {\bf Context Free Grammar} is $(V,\Sigma,R,S)$, where $V$ is a set of variables, $\Sigma$ is a character set, disjoint from $V$, $R$ is a relation between $V$ and $(V \cup \Sigma)^*$, and $S \in V$ is the start variable. We call elements of $R$ derivation rules, and write $(a,s) \in R$ as $a \rightarrow s$.
\end{definition}

A string of the form $uvw$ is {\bf directly derivable} from $uAw$ if $A \rightarrow v$ is a derivation rule. The smallest transitive relation containing the `directly derivable' relation is the derivability relation. To reiterate, a string $u$ is {\bf derivable} from $w$ if there is a sequence of direct derivations
%
\[ w \rightarrow s_0 \rightarrow \dots \rightarrow s_n \rightarrow u \]
%
Such a sequence is known as a {\bf derivation}. The language of a grammar is the set of all strings in $\Sigma^*$ derivable from the start state $S$. As in finite automata, the language of a grammar $G$ will be denoted $L(G)$. A language is {\bf context-free} if it is the language of a context free grammar.

\begin{lemma}
    Let $G = (V,\Sigma,R,S)$ and $H = (V,\Sigma,R',S)$ be two different context free languages. If every $(v,s) \in R'$ is a derivation in $G$, then $L(H) \subset L(G)$.
\end{lemma}
\begin{proof}
    If we can show that every direct derivation in $H$ is a derivation in $G$, then all derivations in $H$ are derivations in $G$, since derivations form the smallest transitive relation containing the direct derivations in $H$, and the derivations in $G$ certainly satisfy this. If $w$ is directly derived from $s$ in $H$, then there is a rule $(B,u)$, $s = rBt$, and $w = rut$. There is a sequence $s_0 \dots s_n$ deriving $u$ from $B$ in $G$. But then $(rs_0t) \dots (rs_nt)$ is a derivation of $w$ from $s$, so $w$ is derivable from $s$ in $G$.
\end{proof}

A leftmost derivation is a derivation which always swaps out the leftmost variable. A string is {\bf ambiguous} if it has two different leftmost derivations. A grammar is ambiguous if its language contains an ambiguous string. Ambiguity is unfortunate when parsing a language, since it means we may be able to interpret elements of the language in two different ways.

\begin{example}
    First order logic can be defined as an unambiguous grammar. To develop the language, we took a bottom up approach, but a top up approach can also be taken. We must take a finite alphabet to develop the language, which we take to be
    %
    \[ \{ (, ), \forall, \exists, \neg, \wedge, \vee, \Rightarrow, x, f, P, 0, 1, \dots, 9 \} \]
    %
    we cannot take an `infinite number of variables', in the sense of an infinite number of symbols, for then formal language theory does not apply. We must instead assume our variables, predicates, and functions are themselves words in a finite alphabet. For instance, we will enumerate our variables
    %
    \[ \Lambda = \{ x, x_0, x_{00}, \dots, x_{0000000000}, \dots \} \]
    %
    The trick to forming functions and predicates is to put $n$ ones after an $f$ or a $P$ to denote that is is an $n$-ary function, so
    %
    \[ \mathcal{F}^n = \{ \left.f^{111 \dots 11}\right., \left.f^{111\dots11}\right._0, \dots, \left.f^{111 \dots 11}\right._{000000000}, \dots \} \]
    %
    \[ \mathcal{P}^n = \{ P^{1111\dots11}, \dots \} \]
    %
    Then we may form predicate logic as a context free grammar. The variables are easiest to form
    %
    \[ X \rightarrow {x}\ |\ {X_0} \]
    %
    Terms are tricky to define because we must form functions as well. The trick is to introduce new variables $Y$ and $Z$ which add enough terms to the function.
    %
    \begin{align*}
        T &\rightarrow {X}\ |\ {f^1U)}\\
        U &\rightarrow {V(T}\ |\ {^1U, T}\\
        V &\rightarrow \varepsilon\ |\ V_0
    \end{align*}
    %
    Finally, we form the formulas of the calculus. Again, the only trick part are the atomic formulae
    %
    \begin{align*}
        F &\rightarrow {(F \wedge F)}\ |\ {(F \vee F)}\ |\ {(\neg F)}\ |\ {(F \Rightarrow F)}\ |\ {(\forall x: F)}\ |\ {(\exists x: F)}\ |\ {P^1Z)}\\
        Y &\rightarrow {Z(T}\ |\ {^1Y, T}\\
        Z &\rightarrow \varepsilon\ |\ Z_0
    \end{align*}
    %
    We showed the formulas are derived unambiguously, but this took a lot of hard work. It is impossible to find a general procedure to decide whether a language is ambiguous, which is what makes verifying ambiguity so difficult.
\end{example}

It is useful to put grammars in a simple form for advanced theorems. A grammar $(V,\Sigma,R,S)$ is in {\bf Chomsky Normal Form} if the only relations in $R$ are of the form
%
\[ S \rightarrow \varepsilon\ \ \ \ \ A \rightarrow BC\ \ \ \ \ A \rightarrow a \]
%
where $A,B,C \in V$ and $B,C \neq S$, and $a \in \Sigma$.

\begin{theorem}
    Every context-free language can be recognized by a context-free grammar in Chomsky normal form.
\end{theorem}
\begin{proof}
    We shall reduce any context free grammar $G = (V,\Sigma,R,S)$ to a context free grammar in normal form in a systematic fashion, adding each restriction one at a time.

    \begin{enumerate}
        \item {\bf No derivation rules map to the start variable}:\\
        Create a new start variable mapping onto the old start variable.

        \item {\bf There are no $\varepsilon$-transition rules except from the start variable}:\\
        Define a variable $A \in V$ to be nullable if we may derive $\varepsilon$ from $A$. Let $W$ be the set of all nullable variables. Define a new language $G'(V,\Sigma,R',S)$ such that, if $A \rightarrow A_1 \dots A_n$ is a derivation rule in $G$, and $A_{i_1}, \dots, A_{i_m}$ are nullable, then we add $2^m$ new rules to $G'$ by removing some subset of the $A_{i_k}$. Then $L(G') = L(G) - \{ \varepsilon \}$, so that if $\varepsilon \in L(G)$, we need only add an $\varepsilon$ rule to $S$ to make the two languages equal.

        We will prove that if $A$ is a variable in $G'$, then $A$ can derive $w \in \Sigma^*$ in $G'$ if and only if it can derive it in $G$ and $w \neq \varepsilon$. One way is trivial, the other a proof by induction on the length of the derivation. Suppose we have a derivation in $G$
        %
        \[ A \rightarrow s_0 \rightarrow \dots \rightarrow s_n \rightarrow w \]
        %
        Let $s_0 = A_1 \dots A_n$. Then each $A_i$ derives $w_i$ in $G$, where $w = w_1 \dots w_n$. We can choose such a derivation to be shorter than the derivation of $G$. But this implies that $A_i$ derives $w_i$ in $G'$, provided $w_i \neq \varepsilon$. Let $w_{i_1} \dots w_{i_m} \neq \varepsilon$. Then $m \neq 0$, since $w \neq \varepsilon$. We have a corresponding production rule $A \rightarrow A_{i_1} \dots A_{i_n}$ in $G'$, since the other variables are nullable. Thus, by induction, $A$ can derive $w$.

        \item {\bf There are no derivation rules $A \rightarrow B$, where $B$ is a variable}:\\
        Call $B$ unitarily derivable from $A$ if there are a sequence of derivation rules
        %
        \[ A \rightarrow V_1 \dots V_n \rightarrow B \]
        %
        Define a new grammar $G' = (V,\Sigma,R',S)$. If $B$ is directly derivable from $A$, and $B$ has a derivation rule $B \rightarrow s$, then $G'$ has a production rule $A \rightarrow s$, provided that $s$ is not a variable. Then $G'$ has no rules of the form $A \rightarrow B$, and generates the same language. This is fairly clear, and left to the reader to prove.

        \item {\bf every rule is of the form $A \rightarrow AB$ or $A \rightarrow a$}:\\
        If we have a rule in $G$ of the form $A \rightarrow s$, where $s = s_1 \dots s_n$, and $s_{i_1}, \dots, s_{i_m} \in \Sigma$, then add new unique variables $A_{s_{i_k}}$ for each $s_i \in \Sigma$, and replace the rule with new rules of the form
        %
        \[ A \rightarrow s_1 \dots A_{s_{i_1}} \dots A_{s_{i_m}} \dots s_n \]
        %
        \[ A_{s_{i_m}} \rightarrow s_{i_m} \]
        %
        Thus we may assume every rule of the form $A \rightarrow A_1 \dots A_n$ (where we may assume $n \geq 2$) only maps to new variables. But then we may add new variables $V_2 \dots V_{n-1}$, and swap this rule with rules of the form
        %
        \[ A \rightarrow V_{n-1} A_n \]
        %
        \[ V_k \rightarrow V_{k-1} A_k \]
        %
        \[ V_2 \rightarrow A_1 A_2 \]
        %
    \end{enumerate}
    %
    Now every derivation rule is in the correct form, and we have reduced every grammar to Chomsky normal form.
\end{proof}

Chomsky normal form allows one to prove

\begin{theorem}
    If $L$ is a context free language, then there is $p > 0$, such that if $s \in L$, and $|s| \geq p$, then we may write $s = uvwxy$, where $|vwx| \leq p$, $v$ and $x$ are not both $\varepsilon$, and $uv^iwx^iy \in L$.
\end{theorem}
\begin{proof}
    Let $G = (V,\Sigma,S,R)$ be a grammar in Chompysky normal form accepting $L$. If $T$ is a parsing tree which yields $s$ which has a heigh of $l$, then $|s| \leq 2^{l-1}$. Set $p = 2^{|V|}$. If $w \in L$, and $|w| \geq p$, then the parsin tree of $w$ must be greater than or equal to $k+1$. Thus there is a variable which yields itself in the parsing tree (in less than or equal to $k$ steps). In this process, we yield less than
\end{proof}

\section{Pushdown Automata}

Regular languages have representations as the languages of regular expressions or as finite automata. Context-free languages also have dual representations, as `machines' or as abstraction operations. It is good to represent a language as a machine for it may hint as to what hardware capabilities a computer must have to be able to solve problems related to the languages. The key machine component for a context-free language is a stack. A pushdown automata is a finite state automata with the addition of a stack.

\begin{definition}
    A {\bf (non-deterministic) pushdown automata} is a tuple $(Q, \Sigma, \Lambda, \Delta, q_0, F)$, where $Q$ is a finite set of states, $\Sigma$ is an alphabet, $\Lambda$ is the stack alphabet, $\Delta: Q \times \Sigma_\varepsilon \times \Lambda_\varepsilon \to \mathcal{P}(Q \times \Lambda_\varepsilon)$ is the state transition function, $q_0 \in Q$ is the start state, and $F \subset Q$ are the accept states.
\end{definition}

It turns out that deterministic pushdown automata are less powerful than non-deterministic automata, so we do not discuss deterministic automata. Interestingly, languages of deterministic automata are connected to the notion of ambiguity in grammars.

Let us describe a pushdown automata intuitively. The automata has a stack of symbols from $\Lambda$, which it can push and pull from when deciding how to move through the machine. A stack is a string in $\Lambda^*$. A string $s$ is accepted by a push-down automata $M$ if there are a sequence of states $q_0, \dots, q_n$, and stacks $w_0, \dots, w_n \in \Lambda^*$ such that $q_n \in F$, $w_0 = \varepsilon$, and if we write $w_i = w \lambda$, with $\lambda \in \Lambda_\varepsilon$, then $w_{i+1} = w_i \lambda'$, with $(q_{i+1}, \lambda') \in \Delta(q_i,\lambda)$. Thus we pop and pull off the rightmost character in the string when moving between states.

\end{document}